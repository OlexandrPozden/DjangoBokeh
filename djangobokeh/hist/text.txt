I have been married to Marie, my wife, for 15 years now. We have two kids together; Aaron (10) and Priscilla (8). I have always pictured us as the perfect family, you know. Nice, spacious house in a good neighborhood, barely any arguments, well-behaved, healthy kids with good grades. Everything seemed spotless.



But lately I have been noticing things. Things that have made me question everything in my life.



But let us start at the beginning. I have always been a workaholic. For the last twenty years I have averaged 150 yearly commuter days. I spend more time in airports than I do with my own family. So it is only to be expected that things change when I suddenly find myself stuck in the house 24/7, right? That is exactly how I was trying to justify the weirdness; I�ve hardly spent a full weekend with them in years, it is gonna take time to get used to me hanging around here constantly.



I suffered a pretty serious neck-injury on the job a few months back, which kept me hospitalized for a good two weeks. I am mostly fine now, but because of the nature of the fracture I still have to wear a collar for stabilization, and there is at least a couple of months until I�m ready for work. So I spend my days just wandering around the house, not quite knowing what to do with myself.



My wife is a stay-at-home-mom. She is the love of my life. We met at a company retreat seventeen years ago and we hit it off immediately. Soon we fell in love, got married, spawned the kids, you know the deal. She left the company when Aaron was born. I was making enough for the both of us, so I was happy to see her happy.



But now things are different. I have no idea if she�s happy anymore. She always smiles, always laughs, but it feels so emotionless. Forced even. And she sneaks out when she thinks I�m napping. At first I thought she was having an affair or something, but I�m not so sure anymore.



My kids are just weird around me. Aaron won�t look me in the eyes, and Priscilla seems to avoid me at all cost. I shrugged it off the first few weeks; maybe they just needed a little more time. But time didn�t help. Time only made it worse. My wife keeps sending them to her parents� every weekend. They love it there, she says. She allows them to sleep over at their friends� place too often as well, even on school nights. I�ve tried to set some boundaries, but my wife just ignores them. She knows them best, she says. Can�t argue with that.



At night, when she thinks I�m sleeping, my wife sneaks out of bed and makes a phone call. Just one. She is away for maybe thirty minutes, before returning to bed. I have tried sneaking down after her, but I can never get close enough to listen in. I�ll get a few words and phrases, but nothing that makes any sense. She looks visibly upset, though, that much I have gathered. The first few times I confronted her about it, but she just said it was one of her friends needing some advice. I didn�t want to press matters too far, because of the way she looked at me. Cold and emotionless. I shudder at the thought of it.



I tried driving my kids to soccer practice and gymnastics twice a week, hoping to get some conversation started. They seemed really upset at the idea of me taking them anywhere, and my wife desperately tried to get me to reconsider, but I insisted. I stopped taking them after a week. The look in their eyes scared me. It was like the very presence of me made them so uncomfortable that it nearly induced panic attacks in them. I was at my wits end at this point.



Laying awake at night, my mind started drifting. I have always joked that I spend so much time away from them, that they could easily be replaced and I would hardly notice. And then I remembered the Aaron-incident. I sat up in bed, sweating. The Aaron-incident.



When Aaron was 2, a few months before Priscilla was born, I had been spending months at the time on a job. When I got home for a much needed long-weekend, my wife and son greeted me at the airport. Only it wasn�t my son. I didn�t recognize him at all. I stood there frozen for minutes, before my wife, looking quite flustered, snapped me out of it.



�Pick him up,� she said. �He just wants to hug his father.�



I picked him up and just stared at him. He didn�t seem familiar at all. At this point I was starting to feel unwell, like I had to throw up or something. I couldn�t understand why I didn�t recognize him. When we got home, I told everything to my wife, and she said it was because I spent too much time away. Kids that age grow and change like crazy, she explained. It took me weeks to accept it, but at some point I just realised I was acting like a lunatic, and got on with my life.



The thought of the Aaron-incident sparked something in me. I started thinking back to other strange, seemingly explainable, things that had happened. Like that I wasn�t present when my wife gave birth to any of the kids. That I was never around much the last trimester of her pregnancies. That I have never actually met any of her relatives, except for her parents, who�ve always been strange around me. I started feeling dizzy and nauseous just thinking about it.



Was the last seventeen years of my life a lie? It seemed so impossible, but at the same time I couldn�t shake the feeling that something was terribly wrong. The only thing I knew for certain was that I had to get to the bottom of this. Otherwise I was afraid I was going to lose my mind.



I started paying more attention to every little detail that went on in the house. Every conversation, every phone call, every movement; any little detail that could explain what was going on. There was always something that never quite made sense. Just a tiny, little thing that would catch my eye. How they always seem to talk in perfect order, like they were reading from a script. Like how they always seemed to know who was coming up the driveway just moments before they actually did. Like it all was some elaborate stage act. I was getting more paranoid by the minute, and I think they noticed something was wrong.



That�s when my wife sent me to therapy. She said I needed it, that I had been acting different ever since I got home. Like everything was my fault. I tried explaining to her that I was probably just a bit anxious because I wasn�t used to living there, but she wouldn�t hear it.



My first session went as I had suspected. The therapist was desperately trying to get me to question myself and my motives. I didn�t share anything with him. I simply couldn�t trust him. Maybe he was in on it? Maybe they were trying to label me insane? Lock me up in some godforsaken asylum? To what end? In any case, I couldn�t afford to spill my suspicions just yet. I needed some proof.



I told my wife everything went great, and that I understood I might be coming on too hard. I was going to take it easy, and not force them. I would leave them to it, and learn as I went along. All that jazz. She smiled one of her obviously fake smiles and gave me a cold hug. She was so pleased, she told me. I was going to get better in no time.



Sure lady.



I had come to realise I approached the matter from the wrong angle. I shouldn�t have given away my suspicions so easily. Instead I should have followed the one trail they could not hide; the money. If they were indeed actors, they had to get paid, right? And there had to be some evidence of some transaction somewhere? Even if they got paid in cash, I could perhaps follow them and catch them in the act. Yes, this was a plan.



I spent days without sleep going through bank records, receipts, the GPS of the car, without getting anywhere. Everything seemed just fine. Nothing out of the ordinary. I was tearing my hair out in despair, when fate suddenly intervened, and they slipped up. Just a tiny mistake, mind you. They could have easily gotten away with it if I wasn�t already in a state of complete awareness.



One morning my wife was getting the kids ready for school. Everything normal. Eating breakfast, packing their lunches, ushering them into the car. The old �we�re gonna be late for school�.



But it was just this one, tiny detail.



The school was closed that day.



My wife did not know this, but I had looked through all the papers I could get my hands on, one of them being the school calendar. And that day they were definitely closed.



I don�t know if you have experienced such a feeling; it is like a mix of total relief and utter devastation all at once. To prove to myself that I wasn�t insane, but at the same time realise my life was a lie. And the day wasn�t about to get any less absurd.



Not only did my wife not realise that the school was closed. She also forgot her purse. And in it I found the one thing I had been looking for the past few days; a paycheck.



Now the paycheck wasn�t made out in her name, or the name I knew her by at least. It was made out to one Lisa Garon. But it wasn�t that little detail that threw me off. It was something else. Something extremely disturbing. Something impossible. It was the name listed as the employer.



It was me.
After the initial shock had worn off, I sat down on the couch clutching the paycheck. Maybe I was a little bit unhinged, I pondered; after all there were still ways to explain all that had happened. The kids could have changed school. How would I know, right? The paycheck could be a payment for some company related business that my wife had found and just forgot to mail. All explanations within the realm of believability. I yawned tiredly. Maybe I should just sleep on it.



I was just about to close my eyes when the doorbell rang. I sighed deeply as I slouched towards the door. My energy levels were dangerously low, and my mind was wandering all over the place. I really couldn�t carry a conversation in this state, so I was hoping it was only some kids pranking me or something. I opened the door hesitantly, only to be greeted by the creepiest smile I had ever seen.



�Hi there,� the cheery old mailman insisted, �I have a package for you.�



I stood there blinking for what must have been ten seconds. I was just so fazed by that strange expression on his face. Even weirder still, he didn�t seem to flinch as I must�ve stood there staring like a total lunatic; he kept portraying that toothy grin like it was some sort of contest.



�Ummm,� I ventured eventually, �you sure?�

He offered me a square box, wrapped in brown paper. He pointed to the label on top of it.

�That your name?� he asked.

I read the label. Sure looked like my name.

�Yeah, but I haven�t...� I started, but the increasing intensity of his smile threw me off. I grabbed the package promptly and took a step back, signalling that the transaction was completed. He just stood there frozen, still smiling like a murderous devil doll.

�The lies have become the truth have become the lies, yes?� he said just as I was about to close the door.

�What was that?� I asked super rhetorically.

�Oh, you�ll come around,� he laughed heartily, �Do say hello to the missus for me, will you?�

�Sure thing buddy,� I whispered under my breath, slamming the door shut in his creepy face.



I immediately ran to the living room window, keenly watching the strangers every move. He just stood by the door for a solid minute, before turning on a dime and pacing down the street with some haste. I shook my head in frustration. I felt like I had walked into a Twilight Zone episode or something. Everything just seemed slightly too absurd to be real.



I placed the box gently on the living room table. It was light, and about the size of a...medium sized box, I guess. I eyed it suspiciously, contemplating all the possible bizarre things that could fit inside it. A severed head? Sure, but there�d be some blood or something, right? Not if it was wrapped in plastic, though. A bomb? That would be too mundane, surely. Some sort of elaborate puzzle-gizmo I�d spend days figuring out? That sounded more like it. I paced around the table restlessly. I knew I was bound to open it at some point, so there really wasn�t anything to wait around for. I gently unwrapped it and opened the lid with utmost care.



I studied the content with what I imagine was a puzzled expression on my face. There was just a picture. Nothing else. I flipped the box over and shook it a few times, but to no avail; that was it. Surely an envelope would�ve been enough? I looked at the picture. It was a fairly recent picture of my wife and the kids, where my wife was standing behind them with her hands placed on their shoulders. Obviously professional work. But what did it mean? Was it a threat? Or a gift?



I sat back down on the couch, examining the picture from all angles. There had to be something special about it. There just had to be some deeper meaning. Nothing else made any sense to me at that point. I wasn�t going insane, I just knew it. But wouldn�t that be exactly what an insane person would say? Eventually my mind just overloaded, and I must have fallen asleep.



I woke up in darkness. I must have slept through the day. I got up from the couch yawning, slouching over to the kitchen to get some water. That�s when it hit me. Why didn�t my wife or kids wake up me when they got home? I gargled the water thoughtfully. I walked over to the nearest window and peered through the curtains. No car in the driveway. I remembered the picture then; had it been a threat all along? Were they in some kind of danger?



Without thinking I bolted up the stairs and threw open the door to Aarons bedroom. Empty. Priscillas bedroom. Empty. Our bedroom. Empty. I felt my heart racing, and my mind kept wandering back to the eerie mailman. And his smile. That knowing, creepy grin. And what was that he said? The lies has become the truth has become the lies? Was that some sort of veiled threat? Did he abduct my family? Or know who did? Were they even in danger?



I sat down on the bed, unable to string together comprehensible thoughts. I felt a detachment from reality that I had never considered possible; like a feeling of utter confusion and hopelessness combined. Like I didn�t exist, or the world around me didn�t exist, or both. With nothing tangible to go on except for a mailman who more than likely wasn�t a mailman, and a picture by some anonymous photographer; I knew I was in no way mentally capable of solving this mess.



But wait. There was something that had slipped my mind.



Lisa Garon.



I hurried downstairs and uncovered the paycheck crumpled up into the cracks of the couch. I brought it over to the kitchen counter and studied it thoroughly. There was no phone number on it, but there was an address. I looked it up quickly on my phone. It was apparently registered to some sort of legal firm downtown, Vernon and Love. There was only one phone number listed on their info, so without delay I punched it in and hit dial.



I didn�t actually expect anyone to pick up. It was the middle of the night after all, and even if I by some miracle got a hold of anyone, I hadn�t the slightest clue what I would ask them. I was therefore quite caught off guard when a soft female voice suddenly answered.



�You have reached Vernon and Love, this is Gladys speaking, how may I be of assistance?�

My voice failed me. I tried putting together words that made any amount of sense, but failed miserably. I considered just hanging up, but instead I remained on the line, breathing heavily.

�Jesse?� the woman whispered hoarsely. �Is that you?�

I felt a cold chill run up my spine, but I couldn�t quite put my finger on why. My name wasn�t Jesse, yet it felt so familiar. Like it somehow was a part of me. Or had been a part of me.

�No,� I finally said, �I am not Jesse. I am looking for Lisa. Lisa Garon.�

Total silence. I couldn�t even hear if the woman on the other end was still breathing.

�Hello? Are you still there?�

�I am sorry,� she said, her voice now shaking, �There is currently no one here by that name.�

The peculiar phrasing made me curious. Currently?

�OK, but did you at some point�� I started.

�I am sorry, I just can�t help you. Goodbye.�



She hung up.



I tried calling again. And again. And again. But no answer.



I slammed my head on the kitchen counter repeatedly and let out what can only be described as hysterical screams. I was losing it. I threw my phone into the wall, bits and pieces flying about like expensive confetti. I sighed deeply. There was nothing else. Nothing I could think of. I thought about driving down to Vernon and Love, knock in their door or something, but to what end? I didn�t even know if they had anything to do with anything.



I started gathering the pieces of what had once been a phone, slowly letting everything that had happened sink in. I was searching around the living room for the missing battery when I noticed it.



There up on a shelf.



A framed picture exactly like the one I found in the box.



Except for one rather important detail.



I was in it.



Part 3

Final Part
I think my wife and kids are actors (part three)
Series
Part 1

Part 2

I dropped the remains of my phone on the floor and grabbed the frame. Fumbling to get the picture out, I resorted to smashing the frame into the shelf instead, glass now sprinkling around me. I quickly recovered the picture I got from the box and laid them both out on the kitchen counter. They were identical. Well, apart from me missing from one they were. Marie and the kids had the exact same facial expressions on both, and the position of her hands were also the same. Either I had been photoshopped into the one, or photoshopped out of the other. To be honest I didn�t know which I preferred.



I had no recollection of the picture ever being taken, but then again my memory was rather flimsy these days. I sat down, trying desperately to shake something loose. Any memory at all. But there was nothing. Literally nothing. How on earth did it take me so long to notice?



The brain is a curious thing, you see. It will hide away all the bad shit if it means it�s protecting you somehow. Like if there�s some trauma it just don�t want you to revisit, or some dark secret it doesn�t want you to know. The funny thing is, you won�t always realise you�ve forgotten. You�ll just gather the pieces of what you have, and fill in the blanks with whatever makes sense to you personally. And you will be none the wiser.



Until your patchwork story starts falling apart.



The more I thought about what was missing, the more I realised I could be misremembering just about anything. I had no real grasp on reality. What had happened to me? What kind of accident had caused this? According to my doctors no one seemed to know. They just found me unconscious on the ground, my neck broken. They assumed I had fallen off the scaffolding of the buildsite I was overseeing, but since there were no witnesses, and I couldn�t remember a damn thing, it remained a mystery.



And who exactly were my employers?



This question was haunting me. I�d never considered it before now, but I really couldn�t seem to remember what my company�s name was. Or who my boss was. Did I have a boss? All I recalled was that it had something to do with construction. I�d spend a lot of time on construction sites and in airports. Overseeing? That�s the word the doctors used.



It was nearing dawn, and the darkness was slowly retreating as I headed to the front door. I was contemplating going to the police, but I had no idea what to tell them. My wife, the actor, and my kids, the actors, had gone missing? Then I had to explain that maybe they weren�t actors, and that I recently suffered a head/neck-injury that could mean I was just imagining it all. I�m no police officer, but I would promptly show myself the door if it was my call.



No, I had just one tangible clue; Vernon and Love. They definitely seemed dodgy on the phone, and I couldn�t shake that strange feeling that came over me when that woman Gladys called me Jesse. I had no car, so I had no choice but to walk downtown. It would take me about an hour, but at least I�d get the chance to clear my head.



The walk was largely uneventful, but I did realise that I had never walked downtown before. Isn�t that strange? I bought the house eleven years ago, when Marie was pregnant with Aaron (or was she?), but I couldn�t for the life of me recall ever walking anywhere. What a wasted life, I depressingly pondered, as I finally stood before an anonymous neglected brick building where the offices of Vernon and Love were supposed to be located.



The front door was barely on its hinges, and an eerie scraping noise echoed through empty hallways when I pushed it open. There were no signs of life anywhere, let alone any semblance of a thriving law practice. I wandered the bleak hallways of the first floor for ten minutes, peering through every open door, finding nothing but long abandoned, decaying offices. I was about to climb the rather off-putting stairs when I heard the distinct sound of the elevator in motion.



I had ignored the elevator because of the state this place was in. It didn�t even occur to me that it might still be operational. Not quite knowing what to expect, I hid inside the office opposite to the elevator, nervously hugging the wall. What was my plan, exactly? Jump out and scare them? I had no idea, but my mind kept insisting we stay put, so my body confusedly obliged.



I could hear a loud *ding* as the elevator doors opened with a screeching noise, and then the rustling sound of feet pacing down the hall. A woman, I guessed, and stuck my head out the door to confirm. But whoever it was had turned the corner and was nowhere to be seen. I quickly darted into the elevator just before the doors closed.



Staring at the panel, I had no idea which floor to choose, so I just pressed one blindly. I had the strangest feeling of deja-vu, like I had been in this elevator doing the exact same thing, but before I was allowed to explore the sensation, the elevator stopped, and the doors dinged open. I got out carefully and crept down another empty hallway, littered with decaying offices. But something felt different. I had a strong sense I�d been here before.



In the white room.



The image of the room blinked in and out of my mind, and my body was now doing its own thing. Muscle-memory I guess they call it. It was like I was just a passenger along for the ride, as my feet carried me down endless hallways, turning left and right seemingly at random. After about five minutes of this, I spotted the door at the end of the hallway. It stood out like a gaping wound in its surroundings; the unwholesome red presence in stark contrast to the crumbling off-white walls. I dreaded what awaited me on the other side, but I couldn�t help it; I had virtually no control at this point. With a swift movement my hand reached the handle, and I yanked the door open.



The white room.



It was blindingly white, like the walls were reflecting light back on themselves. I suspect it was just a very white coat of paint, but it felt so unreal there and then as I was standing in the doorway taking it all in. In the middle of the room three rather ordinary office chairs were placed, only one of which was occupied.



�You!� I halfway shouted at the creepily smiling man facing me.



I didn�t know how I was supposed to feel. Angry? Confused? Furious? All I knew was that it didn�t really surprise me. So whatever emotion was desperately trying to overcome me, did so to no avail. Instead I just kept staring at him and his unnecessarily unpleasant grin, and sat down in the chair opposite.



�You came around,� he said, �I knew you would eventually.�

�I have no idea what you�re on about,� I said through gritted teeth, �But you better have some answers.�



The old man threw his head back and laughed. It wasn�t a jolly old laugh; more like the kind you�d expect coming from the hidden cells of an insane asylum. The sound startled me, I�ll admit it, and I spent a few seconds gathering my composure. Eventually the echoing madness resided, and we sat again in silence.



�You�re still not fully here, then,� the creepy old man whispered, �Such a shame.�



What did he mean, not here? I examined the man with what was starting to feel like murderous intent. He was old, that much was clear. Completely bald, short, thin. But there was something really off-putting about his general presence that I couldn�t quite put my finger on. He stood up from the chair, his fine, blue suit now noticeably several sizes too large. His long, red tie flapped wildly about his neck for a few moments as he turned to face the wall.



�We need you here,� he said, presumably to me and not the wall, �You have something we desire.�

�We?� I asked, �Who are we?�



The man turned to face me again, his ominous grin widening as his gaze wandered to the ceiling.



�Why, the Company, of course, � he said, �We are all so grateful for all the hard work you�ve done for us. All the gruelling hours, days, weeks, months and years. But it would have all been for nothing if you don�t come back from your last mission.�



I buried my head in my hands. It didn�t make any sense. This didn�t sound like something a construction overseer would have to deal with at all. What in God�s name had I gotten myself involved in? And why couldn�t I remember any of it? It had to be the accident, that much seemed obvious, but was it even an accident then? Or something far more sinister?



I snapped back to reality by the sound of the door opening.



�Ah,� the old man said, �You are here. Maybe she can help you come back to us.�



I turned to the door, expecting to see Lisa Garon or whatever her real name was. I was actually looking forward to speaking with her. Maybe she could tell me why the name Jesse felt so eerily familiar.



�Marie,� I gasped, �What are you doing here?�
I think my wife and kids are actors (finale)
Series
Part 1

Part 2

Part 3

�Jesse?� the woman asked, her face strangely familiar, but definitely not my Marie. Or Lisa. Or whatever her name was.



I stared at her confusedly. Why did I instinctively call her Marie, then? It was like a part of me knew her, and a part of me didn�t recognize her at all. It was all very disorienting, and I resorted to burying my head in my hands yet again.



�I think he is coming back,� the old man said, �He is starting to remember.�



My mind filled up with images, each one uncannily intimate to me, but none of which I could immediately remember. Some were horrible, filled with screams, blood, and death, while others were boring and mundane. I felt them all clawing away at my manifested self, as if trying to break down a mental wall. A throbbing pain spread all the way from my forehead to the back of my neck, and I was unable to form any comprehensible thoughts.



�A part of you knows her as Gladys,� the old man whispered into my ear, �Another as Marie.�



I opened my eyes. Some of it was coming back to me. Not all, and not in the correct order, but I definitely remembered her. She was my Marie after all. I remembered her face, her scent, her body, movement, laughter, ambitions and inner thoughts. It was like I suddenly learned all about her in an instant. The intensity of this feeling overwhelmed me, leaving me a sobbing, broken mess.



�Yes!� the old man triumphed, �You can see now, can�t you?�



Gladys, Marie, cradled me in her arms, hushing me gently as a tsunami of emotions and memories washed over me; the barriers of my mind now cracking as reality seeped in through the ever collapsing patchwork of my delusions. I was both of them, and none of them. But still I couldn�t become them. I feared I was too far gone, now trapped between the two, never to become whole again.



�Lisa,� I whispered hoarsely, �That�s her name, isn�t it?�

The old man bent down to face the trembling, nervous wreck I currently inhabited.

�Yes,� he said, �That is the name of your fake wife.�

�And Aaron, Priscilla�� I continued.

�All fake,� the old man quickly responded, �Actors, counterfeits, forgeries, shams. A facade.�



I can�t tell you if this admission just served to devastate me further, or if it brought me some semblance of peace, because I was pretty out of it at this point. I was honestly just trying my best to not collapse as my mind was overflowing with information.



�But how?� I murmured, my trembling voice barely audible. �How could I have��

�Now there�s the million dollar question!� the old man laughed. �How, indeed!�



He was pacing around me now. Small, dwarfish steps; the creepy grin still ever present on his wrinkled face.



�You know, they called you the King,� he said darkly, �The Killer King. You were the best at what you do. Still are, I suppose. It will come back to you.�



I sat up slowly, Marie still cradling me in her arms, my eyes following the old man as he kept pacing around.



�Uncountable bodies to your name,� he continued, �All virtually traceless. A true craftsman.�



He stopped suddenly, just staring into the white wall, gently tapping the heel of his shoe on the floor.



�Then, what?� he started, �You grew a conscience? Unlikely as it may seem, it remains the only possible explanation, no?�



He turned on a dime, now facing Marie and me. His smile had by now vanished, and I didn�t particularly like the new expression any better.



�Explanation to what?� I asked.



This question seemed to amuse him, and he threw his head back and laughed maniacally. The madness of the laughter kept causing me some general discomfort, I still don�t mind admitting. After a while he regained his composure, and just stood there, without blinking, staring directly into my eyes.

�To why you tried to kill yourself,� he said after some time, �To why you hung yourself.�



The words uttered pierced through my ears and into my brain like a needle. It was like they manifested physically, and ripped through the last remnants of sanity I had left, leaving my self naked and vulnerable; a single strike now enough to completely shut me down. I knew in my heart it was true. I did it. I hung myself. But I failed. I failed at dying.



�Project Memphis was the last one,� the man continued mercilessly, by now knowing I was brought to my knees; that he had me in the palm of his hand, �The culmination of decades of meticulous planning, scheming, killing��



His voice trailed off as he sat down on the chair again.



�All we needed was that last puzzle-piece,� he sighed deeply, �But you hid it, didn�t you?�



He wiped some sweat from his forehead, his breathing now noticeably strained. He didn�t want to appear flustered, but I could clearly see that he was. I could tell by the rapid movement of his jugular that his heart was racing. How did I know this? How could I see this?



�Then you retreated into this ridiculous persona,� he scoffed, �The family man, the homeowner, the husband, the father, the suburban nobody. What a complete waste, no? Our actors, impostors, suffered a sudden change of heart when they had to take care of some deranged lunatic 24/7, and who could blame them? We paid them through the roof to stay, but you just wouldn�t come back. So we had to, how do I put it, accelerate matters. And here we are.�



I could see five different ways I could kill him within seconds. Many more to make him suffer. Then we, Marie and me, could run away, maybe even find a life worth living, somewhere far away from this madness. Pretend it was all a movie. Something that happened to other people. It would be so easy.



But I didn�t anticipate the loyalty my new found old love had for the Company. I suddenly felt the cold steel resting on my neck, and saw my own reflection in the razorsharp blade.



�Marie?� I said startled. �Why��



Her demeanor had changed. There was no love in her eyes anymore, no feelings. Just stone-cold determination. And maybe just a little touch of psychopathy.



�Just tell him, Jesse,� she said, her voice now as cold as her eyes, �Tell him, and let�s get back to the way things were.�



The old man laughed again. It seemed to be one of his many unwholesome character traits; laughing in awkward moments. I raised my hands above my head, indicating that I was no threat.



�You two always made a great couple,� he grinned, �Never a witness left alive.�



He stood up again, his heart rate now slowly stabilizing. He sighed deeply as he put a hand on my shoulder, squeezing weakly.



�Just tell me where the girl is,� he said, �And this can all be over.�



I still had no idea what he was talking about. I didn�t really have any tangible memories of Jesse. Just fragments; bits and pieces. Fleeting notions of a past life. A person trapped somewhere deep in my psyche. Even if I wanted to, I couldn�t deliver whatever it was they wanted.



�What does she mean to you?� I asked, �Why is she so important?�

The old man stared at me, his eyes widening, like the very question was somehow sacrilege.

�She is everything, � he whispered inauspiciously, �Grace is the key to everything.�

�How exactly?� I demanded, my eyes now scanning for every little detail I could use to escape. �How is she the key to everything?�

�No one understands,� he said, his voice now trembling, �No one ever will.�



His eyes kept wandering to the ceiling, and his heart rate was rising again. It was apparent that whoever this Grace was, the very idea of her was enough to unsettle the old man.



�What�s your name?� I asked bluntly, eyes locked on the ominous halfling.

�What?� he responded bewildered, his gaze now reverting back to face me.

�I just want to know the name of the person I�m about to kill,� I said.



Oh, if I had any artistic talent whatsoever, I would still spend a lifetime trying to capture the look on his face. It was a thing of pure beauty; that creepy grin instantly altering into an expression of blood-curdling fear. My right elbow connected with Marie�s chin with some force; her once pretty face now in need of several stitches and a decent dentist. With a continuous move I grabbed the hideous red tie flapping about the old man�s neck, pulling his creepy face right into the floor with all my might. There was an elegant fountainous explosion of the crimson variety as my left hand, now wielding Marie�s razorsharp blade, cut into his jugular vein.



I watched with some amount of joy as the pool of blood expanded, almost covering the entirety of the white floor. Aesthetically perfect, Jesse explained. Then I put my boot to the back of the old man�s head repeatedly, observing with some interest how the brain looks when mashed into a pulp.



�Poor Marie,� I said, �We could have made it. We really could.�



I let Jesse snap her neck, since I was unsure about the general motoric of the move. It was a fairly pleasurable audio-experience, but I could sense that the lack of blood troubled old Jesse. She deserved a quick death, I thought; we did in fact once love her.



I stumbled to the door, the blood-frenzy still passing significant amounts of dopamine to my brain. The way I saw it, I had two possible paths to follow, and I had to choose. We had to choose.



Either we go through with Jesse�s original plan, and off ourselves, keeping the identity and location of Grace a secret forever. I figured the quickest way was to just jump off the roof if we were to follow this route.



Or we could disappear. Live life, maybe. We�d always have to look over our shoulders, but at least we�d have some semblance of a normal existence, if only for a brief moment.



These are my final words. I hope they make it to you, and that you can somehow find a suitable media to publish them. People deserve to know what can happen, to anyone, at any given moment. Don�t trust anyone or anything, not even yourself. Doubt is self-perseverance.



And just to be clear;



I have left the building.
Machine Learning in Python� : Essential Techniques for Predictive Analysis
Published by
John Wiley & Sons, Inc.
10475 Crosspoint Boulevard
Indianapolis, IN 46256
www.wiley.com
Copyright � 2015 by John Wiley & Sons, Inc., Indianapolis, Indiana
Published simultaneously in Canada
ISBN: 978-1-118-96174-2
ISBN: 978-1-118-96176-6 (ebk)
ISBN: 978-1-118-96175-9 (ebk)
Manufactured in the United States of America
10 9 8 7 6 5 4 3 2 1
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means,
electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or
108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization
through payment of the appropriate per-copy fee to the Copyright Clearance Center, 222 Rosewood Drive,
Danvers, MA 01923, (978) 750-8400, fax (978) 646-8600. Requests to the Publisher for permission should be addressed
to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201)
748-6008, or online at http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or warranties with
respect to the accuracy or completeness of the contents of this work and specifically disclaim all warranties, including
without limitation warranties of fitness for a particular purpose. No warranty may be created or extended by sales or
promotional materials. The advice and strategies contained herein may not be suitable for every situation. This work
is sold with the understanding that the publisher is not engaged in rendering legal, accounting, or other professional
services. If professional assistance is required, the services of a competent professional person should be sought.
Neither the publisher nor the author shall be liable for damages arising herefrom. The fact that an organization or
Web site is referred to in this work as a citation and/or a potential source of further information does not mean that
the author or the publisher endorses the information the organization or website may provide or recommendations
it may make. Further, readers should be aware that Internet websites listed in this work may have changed or disappeared
between when this work was written and when it is read.
For general information on our other products and services please contact our Customer Care Department within the
United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included with
standard print versions of this book may not be included in e-books or in print-on-demand. If this book refers to media
such as a CD or DVD that is not included in the version you purchased, you may download this material at http://
booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.
Library of Congress Control Number: 2015930541
Trademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, Inc. and/or
its affiliates, in the United States and other countries, and may not be used without written permission. Python is a
registered trademark of Python Software Foundation. All other trademarks are the property of their respective owners.
John Wiley & Sons, Inc. is not associated with any product or vendor mentioned in this book.
Sign in
Get started
FinTechExplained
ALL
DATA SCIENCE
FINANCE
TECHNOLOGY
Responses (19)

What are your thoughts?
Cancel
Respond
Zion Oyemade
Zion Oyemade
over 1 year ago

5
1
Very great writeup Farhad Malik
I just have to politely disagree with the Heading and Intro sentence. Yes the article is wonderful as a reference point for both beginners and Advanced users but it wont make you an advanced user on its own.
Midhun Thaduru
Midhun Thaduru
over 1 year ago

c
Thanks for sharing such a great article and explaining important topics for anyone get started in Python.
I would like to suggest to add print statements so that people could understand what exactly happened.
6

Iratxe ec
Iratxe ec
over 1 year ago

Thanks for the great article! I think it really helps you to get an overall understanding of Python, and then you can investigate more on the topics you find more interesting.
6

Robert Hirsch
Robert Hirsch
over 1 year ago

Thank you! But what happened to the super great lessons you had posted? I got to the second class about mapping and then they were taken down! They were excellent!
6

Abhijit Ray
Abhijit Ray
over 1 year ago

This is a very nice reference article for budding Python developers.
5

Robert Hirsch
Robert Hirsch
over 1 year ago

1
Going through this post as a python beginner (but C expert). Use my comments or don�t, up to you of course.
I think a little text and some output between steps on slicing and reversing strings would be helpful
Yif
Yif
over 1 year ago

How about one on JavaScript? ;)
5

ypersyntelykos
ypersyntelykos
over 1 year ago

Great work !!
5

morvan bliasby
morvan bliasby
over 1 year ago

5
Great article. I have too, with, of course, less robustness, but important, too, an article about Python. You can view it <a href=�https://morvlab.wordpress.com/2018/10/24/escolhendo-uma-linguagem-de-programacao-por-que-o-python/�><strong>here</strong></a> (in Portuguese).
Khadeer Akbar
Khadeer Akbar
over 1 year ago

Very nice details. Helpful.
I request you please post exercises and solutions
5

You have 2 free stories left this month. Sign up and get an extra one for free.
Everything About Python � Beginner To Advanced
Everything You Need To Know In One Article
Farhad Malik
Farhad Malik
Follow
Mar 10, 2019 � 30 min read



This article aims to outline all of the key points of the Python programming language. My target is to keep the information short, relevant, and focus on the most important topics which are absolutely required to be understood.
After reading this blog, you will be able to use any Python library or implement your own Python packages.
You are not expected to have any prior programming knowledge and it will be very quick to grasp all of the required concepts.
I will also highlight top discussion questions that people usually query regarding Python programming language.
Lets build the knowledge gradually and by the end of the article, you will have a thorough understanding of Python.
Image for post
This article contains 25 key topics. Let�s Start.
1. Introducing Python
What Is Python?
Interpreted high-level object-oriented dynamically-typed scripting language.
As a result, run time errors are usually encountered.
Why Python?
Python is the most popular language due to the fact that it�s easier to code and understand it.
Python is an object-oriented programming language and can be used to write functional code too.
It is a suitable language that bridges the gaps between business and developers.
Subsequently, it takes less time to bring a Python program to market compared to other languages such as C#/Java.
Additionally, there are a large number of python machine learning and analytical packages.
A large number of communities and books are available to support Python developers.
Nearly all types of applications, ranging from forecasting analytical to UI, can be implemented in Python.
There is no need to declare variable types. Thus it is quicker to implement a Python application.
Why Not Python?
Python is slower than C++, C#, Java. This is due to the lack of Just In Time optimisers in Python.
Python syntactical white-space constraint makes it slightly difficult to implement for new coders.
Python does not offer advanced statistical features as R does.
Python is not suitable for low-level systems and hardware interaction.
How Does Python Work?
This image illustrates how python runs on our machines:
Image for post
The key here is the Interpreter that is responsible for translating high-level Python language to low-level machine language.
The way Python works is as follows:
A Python virtual machine is created where the packages (libraries) are installed. Think of a virtual machine as a container.
The python code is then written in .py files
CPython compiles the Python code to bytecode. This bytecode is for the Python virtual machine.
Now, this virtual machine is machine-dependent but the Python code isn�t.
4. When you want to execute the bytecode then the code will be interpreted at runtime. The code will then be translated from the bytecode into the machine code. The bytecode is not dependent on the machine on which you are running the code. This makes Python machine-independent.
Python byte code is Python virtual machine-dependent and this makes Python machine-independent.
The point to note is that we can write Python code in one OS, copy it to another OS and simply run it.
2. Variables � Object Types And Scope
Variables store information that can be used and/or changed in your program. This information can be an integer, text, collection, etc.
Variables are used to hold user inputs, local states of your program, etc.
Variables have a name so that they can be referenced in the code.
The fundamental concept to understand is that everything is an object in Python.
Python supports numbers, strings, sets, lists, tuples, and dictionaries. These are the standard data types. I will explain each of them in detail.
Declare And Assign Value To Variable
Assignment sets a value to a variable.
To assign variable a value, use the equals sign (=)
myFirstVariable = 1
mySecondVariable = 2
myFirstVariable = "Hello You"
Assigning a value is known as binding in Python. In the example above, we have assigned the value of 1 to myFirstVariable.
Note how I assigned an integer value of 1 and then a string value of �Hello You� to the same myFirstVariable variable. This is possible due to the fact that the data types are dynamically typed in python.
This is why Python is known as a dynamically typed programming language.
If you want to assign the same value to more than one variables then you can use the chained assignment:
myFirstVariable = mySecondVariable = 1
Numeric
Integers, decimals, floats are supported.
value = 1 #integer
value = 1.2 #float with a floating point
Longs are also supported. They have a suffix of L e.g. 9999999999999L
Strings
Textual information. Strings are sequence of letters.
A string is an array of characters
A string value is enclosed in quotation marks: single, double or triple quotes.
name = 'farhad'
name = "farhad"
name = """farhad"""
Strings are immutable. Once they are created, they cannot be changed e.g.
a = 'me'
Updating it will fail:
a[1]='y'
It will throw a Type Error
When string variables are assigned a new value then internally, Python creates a new object to store the value.
Therefore a reference/pointer to an object is created. This pointer is then assigned to the variable and as a result, the variable can be used.
We can also assign one variable to another variable. All it does is that a new pointer is created which points to the same object:
a = 1 #new object is created and 1 is stored there, new pointer is created, the pointer connects a to 1
b = a #new object is not created, new pointer is created that connects b to 1
Variables can have local or global scope.
Local Scope
Variables declared within a function, as an example, can only exist within the block.
Once the block exists, the variables also become inaccessible.
def some_funcion():
  TestMode = False
print(TestMode) <- Breaks as the variable doesn't exist outside
In Python, if-else and for/while loop block doesn�t create any local scope.
for i in range(1, 11):
    test_scope = "variable inside for loop"
print(test_scope)
Output:
variable inside for loop
With if-else block
is_python_awesome = True
if is_python_awesome:
    test_scope = "Python is awesome"
print(test_scope)
Output:
Python is awesome
Global Scope
Variables that can be accessed from any function have a global scope. They exist in the __main__ frame.
You can declare a global variable outside of functions. It�s important to note that to assign a global variable a new value, you will have to use the �global� keyword:
TestMode = True
def some_function():
  global TestMode
  TestMode = False
some_function()
print(TestMode) <--Returns False
Removing the line �global TestMode� will only set the variable to False within the some_function() function.
Note: Although I will write more on the concept of modules later, but if you want to share a global variable across a number of modules then you can create a shared module file e.g. configuration.py and locate your variable there. Finally, import the shared module in your consumer modules.
Finding Variable Type
If you want to find the type of a variable, you can implement:
type('farhad')
--> Returns <type 'str'>
Comma In Integer Variables
Commas are treated as a sequence of variables e.g.
9,8,7 are three numeric variables
3. Operations
Allows us to perform computation on variables
Numeric Operations
Python supports basic *, /, +, -
Python also supports floor division
1//3  #returns 0
1/3 #returns 0.333 
Note: the return type of division is always float as shown below:
a = 10/5
print(type(a)) #prints float
Additionally, python supports exponentiation via ** operator:
2**3 = 2 * 2 * 2 = 8
Python supports Modulus (remainder) operator too:
7%2 = 1
There is also a divmod in-built method. It returns the divider and modulus:
print(divmod(10,3)) #it will print 3 and 1 as 3*3 = 9 +1 = 10
String Operations
Concat Strings:
'A' + 'B' = 'AB'
Remember string is an immutable data type, therefore, concatenating strings creates a new string object.
Repeat String:
�A�*3 will repeat A three times:  AAA
Slicing:
y = 'Abc'
y[:2] = ab
y[1:] = bc
y[:-2] = a
y[-2:] = bc
Reversing:
x = 'abc'
x = x[::-1]
Negative Index:
If you want to start from the last character then use negative index.
y = 'abc'
print(y[:-1]) # will return ab
It is also used to remove any new line carriages/spaces.
Each element in an array gets two indexes:
From left to right, the index starts at 0 and increments by 1
From right to left, the index starts at -1 and decrements by 1
Therefore, if we do y[0] and y[-len(y)] then both will return the same value: �a�
y = 'abc'
print(y[0])
print(y[-len(y)])
Finding Index
name = 'farhad'
index = name.find('r')
#returns 2
name = 'farhad'
index = name.find('a', 2) # finds index of second a
#returns 4
For Regex, use:
split(): splits a string into a list via regex
sub(): replaces matched string via regex
subn(): replaces matched string via regex and returns number of replacements
Casting
str(x): To string
int(x): To integer
float(x): To floats
Set Operations
A set is an unordered data collection without any duplicates. We can define a set variable as:
set = {9,1,-1,5,2,8,3, 8}
print(set)
This will print: {1, 2, 3, 5, 8, 9, -1}
Note duplicates are removed.
Set has a item in set, len(set) and for item in set operations. However it does not support indexing, slicing like lists.
Some of the most important set operations are:
set.add(item) � adds item to the set
set.remove(item) � removes item from the set and raises error if it is not present
set.discard(item) � removes item from the set if it is present
set.pop() � returns any item from the set, raises KeyError if the set is empty
set.clear() clears the set
Intersect Sets
To get what�s common in two sets
a = {1,2,3}
b = {3,4,5}
c = a.intersection(b)
Difference In Sets
To retrieve the difference between two sets:
a = {1,2,3}
b = {3,4,5}
c = a.difference(b)
Union Of Collections
To get a distinct combined set of two sets
a = {1,2,3}
b = {3,4,5}
c = a.union(b)
Ternary Operator
Used to write conditional statements in a single line.
Syntax:
[If True] if [Expression] Else [If False]
For example:
Received = True if x == 'Yes' else False
4. Comments
Single Line Comments
#this is a single line comment
Multiple Line Comments
One can use:
```this is a multi
line
comment```
5. Expressions
Expressions can perform boolean operations such as:
Equality: ==
Not Equal: !=
Greater: >
Less: <
Greater Or Equal >=
Less Or Equal <=
6. Pickling
Converting an object into a string and dumping the string into a binary file is known as pickling. The reverse is known as unpickling.
7. Functions
Functions are sequence of statements that you can execute in your code. If you see repetition in your code then create a reusable function and use it in your program.
Functions can also reference other functions.
Functions eliminate repetition in your code. They make it easier to debug and find issues.
Finally, functions enable code to be understandable and easier to manage.
In short, functions allow us to split a large application into smaller chunks.
Define New Function
def my_new_function():
  print('this is my new function')
Calling Function
my_new_function()
Finding Length Of String
Call the len(x) function
len('hello')
prints 5
Arguments
Arguments can be added to a function to make the functions generic.
This exercise is known as generalization.
You can pass in variables to a method:
def my_new_function(my_value):
  print('this is my new function with ' + my_value)
Optional arguments:
We can pass in optional arguments by providing a default value to an argument:
def my_new_function(my_value='hello'):
  print(my_value)
#Calling
my_new_function() => prints hello
my_new_function('test') => prints test
*arguments:
If your function can take in any number of arguments then add a * in front of the parameter name:
def myfunc(*arguments):
  return a
myfunc(a)
myfunc(a,b)
myfunc(a,b,c)
**arguments:
It allows you to pass a varying number of keyword arguments to a function.
You can also pass in dictionary values as keyword arguments.
def test(*args, **kargs):
    print(args)
    print(kargs)
    print(args[0])
print(kargs.get('a'))

alpha = 'alpha'
beta = 'beta'
test(alpha, beta, a=1, b=2)
This will print:
(3, 1)
(�alpha�, �beta�)
{�a�: 1, �b�: 2}
alpha
1
Return
Functions can return values such as:
def my_function(input):
  return input + 2
If a function is required to return multiple values then it�s suitable to return a tuple (comma separated values). I will explain tuples later on:
resultA, resultB = get_result()
get_result() can return ('a', 1) which is a tuple
Lambda
Single expression anonymous function.
It is an inline function.
my_lambda = lambda x,y,z : x - 100 + y - z
my_lambda(100, 100, 100) # returns 0
Syntax:
variable = lambda arguments: expression
Lambda functions can be passed as arguments to other functions.
Object Identity
I will attempt to explain the important subject of Object Identity now.
Whenever we create an object in Python such as a variable, function, etc, the underlying Python interpreter creates a number that uniquely identifies that object. Some of the objects are created up-front.
When an object is not referenced anymore in the code then it is removed and its identity number can be used by other variables.
dir() and help()
dir() -displays defined symbols
help() � displays documentation
Let�s understand it in detail:
Consider the code below:
var_one = 123
def func_one(var_one):
    var_one = 234
    var_three = 'abc'
var_two = 456
print(dir())
var_one and var_two are the two variables that are defined in the code above. Along with the variables, a function named func_one is also defined. An important note to keep in mind is that everything is an object in Python, including a function.
Within the function, we have assigned the value of 234 to var_one and created a new variable named var_three and assigned it a value of �abc�.
Now, let�s understand the code with the help of dir() and id()
The above code and its variables and functions will be loaded in the Global frame. The global frame will hold all of the objects that the other frames require. As an example, there are many built-in methods loaded in Python that are available to all of the frames. These are the function frames.
Running the above code will print:
[�__annotations__�, �__builtins__�, �__cached__�, �__doc__�, �__file__�, �__loader__�, �__name__�, �__package__�, �__spec__�, �func_one�, �var_one�, �var_two�]
The variables that are prefixed with __ are known as the special variables.
Notice that the var_three is not available yet. Let�s execute func_one(var_one) and then assess the dir():
var_one = 123
def func_one(var_one):
    var_one = 234
    var_three = 'abc'

var_two = 456
func_one(var_one)
print(dir())
We will again see the same list:
['__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'func_one', 'var_one', 'var_two']
This means that the variables within the func_one are only within the func_one. When func_one is executed then a Frame is created. Python is top-down therefore it always executes the lines from top to the bottom.
The function frame can reference the variables in the global frame but any other function frame cannot reference the same variables that are created within itself. As an instance, if I create a new function func_two that tries to print var_three then it will fail:
var_one = 123
def func_one(var_one):
    var_one = 234
    var_three = 'abc'

var_two = 456


def func_two():
    print(var_three)

func_one(var_one)
func_two()
print(dir())
We get an error that NameError: name �var_three� is not defined
What if we create a new variable inside func_two() and then print the dir()?
var_one = 123
def func_one(var_one):
    var_one = 234
    var_three = 'abc'

var_two = 456


def func_two():
    var_four = 123
    print(dir())
func_two()
This will print var_four as it is local to func_two.
How does Assignment work In Python?
This is by far one of the most important concepts to understand in Python. Python has an id() function.
When an object (function, variable, etc.) is created, CPython allocates it an address in memory. The id() function returns the �identity� of an object. It is essentially a unique integer.
As an instance, let�s create four variables and assign them values:
variable1 = 1
variable2 = "abc"
variable3 = (1,2)
variable4 = ['a',1]
#Print their Ids
print('Variable1: ', id(variable1))
print('Variable2: ', id(variable2))
print('Variable3: ', id(variable3))
print('Variable4: ', id(variable4))
The ids will be printed as follows:
Variable1: 1747938368
Variable2: 152386423976
Variable3: 152382712136
Variable4: 152382633160
Each variable has been assigned a new integer value.
The first assumption is that whenever we use the assignment �=� then Python creates a new memory address to store the variable. Is it 100% true, not really!
I am going to create two variables and assign them to the existing variables.
variable5 = variable1
variable6 = variable4

print('Variable1: ', id(variable1))
print('Variable4: ', id(variable4))
print('Variable5: ', id(variable5))
print('Variable6: ', id(variable6))
Python printed:
Variable1: 1747938368
Variable4: 819035469000
Variable5: 1747938368
Variable6: 819035469000
Notice that Python did not create new memory addresses for the two variables. This time, it pointed both of the variables to the same memory location.
Let�s set a new value to the variable1. Remember 2 is an integer and integer is an immutable data type.
print('Variable1: ', id(variable1))
variable1 = 2
print('Variable1: ', id(variable1))
This will print:
Variable1: 1747938368
Variable1: 1747938400
It means whenever we use the = and assign a new value to a variable that is not a variable then internally a new memory address is created to store the variable. Let�s see if it holds!
What happens when the value is a mutable data type?
variable6 is a list. Let�s append an item to it and print its memory address:
print('Variable6: ', id(variable6))
variable6.append('new')
print('Variable6: ', id(variable6))
Note that the memory address remained the same for the variable as it is a mutable data type and we simply updated its elements.
Variable6: 678181106888
Variable6: 678181106888
Let�s create a function and pass a variable to it. If we set the value of the variable inside the function, what will it do internally? let�s assess
def update_variable(variable_to_update):
    print(id(variable_to_update))
update_variable(variable6)
print('Variable6: ', id(variable6))
We get:
678181106888
Variable6: 678181106888
Notice that the id of variable_to_update points to the id of the variable 6.
This means that if we update the variable_to_update in a function and if variable_to_update is a mutable data type then we�ll update variable6.
variable6 = ['new']
print('Variable6: ', variable6)

def update_variable(variable_to_update):
    variable_to_update.append('inside')
update_variable(variable6)
print('Variable6: ', variable6)
This printed:
Variable6: [�new�]
Variable6: [�new�, �inside�]
It shows us that the same object is updated within the function as it was expected by both having the same ID.
If we assign a new value to a variable, regardless of if it�s immutable and mutable data type then the change will be lost once we come out of the function:
print('Variable6: ', variable6)

def update_variable(variable_to_update):
    print(id(variable_to_update))
    variable_to_update = ['inside']
update_variable(variable6)
print('Variable6: ', variable6)
Variable6: [�new�]
344115201992
Variable6: [�new�]
Now an interesting scenario: Python doesn�t always create a new memory address for all new variables.
Finally, what if we assign two different variables a string value such as �a�. Will it create two memory addresses?
variable_nine = "a"
variable_ten = "a"
print('Variable9: ', id(variable_nine))
print('Variable10: ', id(variable_ten))
Notice, both the variables have the same memory location:
Variable9: 792473698064
Variable10: 792473698064
What if we create two different variables and assign them a long string value:
variable_nine = "a"*21
variable_ten = "a"*21
print('Variable9: ', id(variable_nine))
print('Variable10: ', id(variable_ten))
This time Python created two memory locations for the two variables:
Variable9: 541949933872
Variable10: 541949933944
This is because Python creates an internal cache of values when it starts up. This is done to provide faster results. It creates a handful of memory addresses for small integers such as between -5 to 256 and smaller string values. This is the reason why both of the variables in our example had the same ID.
Image for postImage for post
Photo by Noah Silliman on Unsplash
== vs is
Sometimes we want to check whether two objects are equal.
If we use == then it will check whether the two arguments contain the same data
If we use is then Python will check whether the two objects refer to the same object. The id() needs to be the same for both of the objects
var1 = "a"*30
var2 = "a"*30
print('var1: ', id(var1)) #318966315648
print('var2: ', id(var2)) #168966317364

print('== :', var1 == var2) #returns True
print('is :', var1 is var2) #returns False
8. Modules
What is a module?
Python is shipped with over 200 standard modules.
A module is a component that groups similar functionality of your python solution.
Any python code file can be packaged as a module and then it can be imported.
Modules encourage componentised design in your solution.
They provide the concept of namespaces to help you share data and services.
Modules encourage code reusability and reduce variable name clashes.
PYTHONPATH
This environment variable indicates where the Python interpreter needs to navigate to locate the modules. PYTHONHOME is an alternative module search path.
How To Import Modules?
If you have a file: MyFirstPythonFile.py and it contains multiple functions, variables and objects then you can import the functionality into another class by simply doing:
import MyFirstPythonFile
Internally, python runtime will compile the module�s file to bytes and then it will run the module code.
If you want to import everything in a module, you can do:
import my_module
If your module contains a function or object named my_object then you will have to do:
print(my_module.my_object)
Note: If you do not want the interpreter to execute the module when it is loaded then you can check whether the __name__ == �__main__�
2. From
If you only want to access an object or parts of a module from a module then you can implement:
from my_module import my_object
This will enable you to access your object without referencing your module:
print(my_object)
We can also do from * to import all objects
from my_module import *
Note: Modules are only imported on the first import.
If you want to use a Python module in C:
Use PyImport_ImportModule(module_name)
Namespace � two modules with same object name:
Use import over from if we want to use the same name defined in two different modules.
9. Packages
Package is a directory of modules.
If your Python solution offers a large set of functionalities that are grouped into module files then you can create a package out of your modules to better distribute and manage your modules.
Packages enable us to organise our modules better which helps us in resolving issues and finding modules easier.
Third-party packages can be imported into your code such as pandas/sci-kit learn and tensor flow to name a few.
A package can contain a large number of modules.
If our solution offers similar functionality then we can group the modules into a package:
from packageroot.packagefolder.mod import my_object
In the example above, packageroot is the root folder. packagefolder is the subfolder under packageroot. my_module is a module python file in the packagefolder folder.
Additionally, the name of the folder can serve as the namespace e.g.
from data_service.database_data_service.microsoft_sql.mod
Note: Ensure each directory within your package import contains a file __init__.py.
Feel free to leave the files blank. As __init__.py files are imported before the modules are imported, you can add custom logic such as start service status checks or to open database connections, etc.
PIP
PIP is a Python package manager.
Use PIP to download packages:
pip install package_name
10. Conditions
To write if then else:
if a == b:
  print 'a is b'
elif a < b:
  print 'a is less than b'
elif a > b:
  print 'a  is greater than b'
else:
  print 'a is different'
Note how colons and indentations are used to express the conditional logic.
Checking Types
if not isinstance(input, int):
  print 'Expected int'
  return None
You can also add conditional logic in the else part. This is known as nested condition.
#let's write conditions within else
else:
 if a == 2:
    print 'within if of else'
 else:
     print 'within else of else'
11. Loops
While
Provide a condition and run the loop until the condition is met:
while (input < 0):
 do_something(input)
 input = input-1
For
Loop for a number of times
for  i in range(0,10)
Loop over items or characters of a string
for letter in 'hello'
  print letter
One-Liner For
Syntax:
[Variable] AggregateFunction([Value] for [item] in [collection])
Yielding
Let�s assume your list contains a trillion records and you are required to count the number of even numbers from the list. It will not be optimum to load the entire list in the memory. You can instead yield each item from the list.
range(start, stop, step):
Generates numerical values that start at start, stop at stop with the provided steps. As an instance, to generate odd numbers from 1 to 9, do:
rint(list(range(1,10,2)))
Combine For with If
Let�s do a simple exercise to find if a character is in two words
name = 'onename'
anothername = 'onenameonename'
for character in name:
  if character in anothername
     print character
Break
If you want to end the loop
for i in range(0,10):
 if (i==5):
   break

while True:
  x = get_value()
  if (x==1):
     break
Let�s write Fibonacci for loop:
def fib(input):
 if (input <=1):
   return(str(input))
 else:
   first = 0
   second = 1
   count = 0
   for count in range(input):
     result = first + second
     print(first)
     first = second
     second = result
     count = count+1
#print statement will output the correct fib value
fib(7)
12. Recursion
A function calling itself is known as recursion.
Let�s implement a factorial recursive function:
Rules:
0! = 1 #Factorial of 0 is 1
n! = n(n-1)! #Factorial of n is n * factorial of n-1
Steps:
Create a function called factorial with input n
If n = 0 return 1 else do n x factorial of n-1
def factorial(n):
  if n==0:
    return 1
  else:
    return n * factorial(n-1)
Another Example: Let�s write Fibonacci recursive function:
Rules:
First two digits are 0 and 1
Rest add last two digits
0, 1, 1, 2, 3, 5, 8�
Steps:
Create a function called fibonacci that takes in an input n
Create two variables first and second and assign them with values 0 and 1
if n =0 return 0, if n = 1 return 1 else return (n-1) + (n-2)
def fibonacci(n):
 if (n<=1):
   return n
 else:
   return fibonacci(n-1)+fibonacci(n-2)
print(fibonacci(6))
It is important to have an exit check otherwise the function will end up in an infinite loop.
13. Frames And Call Stack
Python code is loaded into frames that are located into a Stack.
Functions are loaded in a frame along with the parameters and variables.
Subsequently, frames are loaded into a stack in the right order of execution.
Stack outlines the execution of functions. Variables that are declared outside of functions are stored in __main__
Stacks executes the last frame first.
You can use traceback to find the list of functions if an error is encountered.
14. Collections
Lists
Lists are data structures that can hold a sequence of values of any data types. They are mutable (update-able).
Lists are indexed by integers.
To create a list, use square brackets:
my_list = ['A', 'B']
To add/update/delete an item, use index:
my_list.append('C') #adds at the end
my_list[1] = 'D' #update
my_list.pop(1) # removes
or
del my_list[1:2] # removes
my_list.extend(another_list) # adds second list at end
Addition, repetition and slices can be applied to lists (just like strings).
List also supports sorting:
my_list.sort() #this is inplace sort
Note on pop():
Pop from the end is O(1)
Pop from start (.pop(0)) is O(N) : all the elements in list[1:] are physically moved one position to the left.
If you need to delete index position 0 frequently, use collections.deque.
Tuples
Tuples are like lists in the sense that they can store a sequence of objects. The objects, again, can be of any type.
Tuples are faster than lists.
These collections are indexed by integers.
Tuples are immutable (non-update-able)
my_tuple = tuple()
or
my_tuple = 'f','m'
or
my_tuple = ('f', 'm')
Note: If a tuple contains a list of items then we can modify the list. Also if you assign a value to an object and you store the object in a list and then change the object then the object within the list will get updated.
Dictionaries
Dictionary is one of the most important data structure in the programming world. It stores key/value pair objects.
It has many benefits e.g. optimised data retrieval functionality.
my_dictionary = dict()
my_dictionary['my_key'] = 1
my_dictionary['another_key'] = 2
You can also create a dictionary as:
my_dictionary = {'my_key':1, 'another_key':2}
Print dictionary contents
for key in dictionary:
  print key, dictionary[key]
Values of a dictionary can be of any type including strings, numerical, boolean, lists or even dictionaries.
dictionary.items() # returns items
#checking if a key exists in a dictionary
if ('some key' in dictionary):
  #do something
Few of the important functions:
get(key, default): returns value for key else returns default
pop(key, default): returns value for key and deletes the item with key else returns default
popitem(): removes random item from the dictionary
dictionary1.update(dictionary2): merges two dictionaries
Note: If you want to perform vectorised/matrix operations on a list then use NumPy Python package
15. Compilation And Linking
These features can be utilised to use a file that is written in another language e.g. C or C++ etc, as long as it is supported by the compiler.
Once the code is written into a file then the file can be placed in the Modules directory of the distribution.
It is important to add a line in the Setup.local file to ensure that the newly created file can be loaded. Run the file using spam file.o
Changing the file requires running rebuildMakefile
Compilation:
Allows compilation of new extensions without any error
Linking:
Once the extensions are compiled, they can be linked.
16. Iterators
Iterators
Allow traversing through a collection
All iterators contain __iter__() and __next__() functions
Simply execute iter(x) on lists, dictionaries, strings or sets.
Therefore we can execute next(iter) where iter = iter(list) as an instance.
Iterators are useful if we have a large number of items in a collection and we do not intend to load all of the files in memory at once. Iterators are memory efficient and can represent an infinite stream.
There are common iterators which enable developers to implement functional programming language paradigm:
Filter
Filter out values based on a condition
Map
Applies a computation on each value of a collection. It maps one value to another value e.g. Convert Text To Integers
Reduce
Reduces a collection of values to one single value (or a smaller collection) e.g. the sum of a collection. It can be iterative in nature.
Zip
Takes multiple collections and returns a new collection.
The new collection contains items where each item contains one element from each input collection.
It allows us to transverse multiple collections at the same time
name = 'farhad'
suffix = [1,2,3,4,5,6]
zip(name, suffix)
--> returns (f,1),(a,2),(r,3),(h,4),(a,5),(d,6)
enumerate
enumerate can loop over an iterable and returns an automatic index for each of the elements:
for index, element in enumerate(iterator):
   pass
Iterators require us to implement two key methods:
__iter__() and __next__()
Additionally, StopIteration needs to be raised when there are no values left to be returned.
We can make a function return an iterator by using a Generator. All we have to do is to use the yield keyword instead of return. The key is to yield one item at a time.
17. Object-Oriented Design � Classes
Python allows us to create our custom types. These user-defined types are known as classes. The classes can have custom properties/attributes and functions.
The object-oriented design allows programmers to define their business model as objects with their required properties and functions.
A property can reference another object too.
Python classes can reference other classes.
Python supports encapsulation � instance functions and variables.
Python supports inheritance.
class MyClass:
  def MyClassFunction(self):  #self = reference to the object
    return 5
#Create new instance of MyClass and then call the function
m = MyClass()
returned_value = m.MyClassFunction()
An instance of a class is known as an object. The objects are mutable and their properties can be updated once the objects are created.
Note: If a tuple (immutable collection) contains a list (mutable collection) of items then we can modify the list. Also if you assign a value to an object and you store the object in a list and then change the object then the object within the list will get updated.
__init__
__init__ function is present in all classes. It is executed when we are required to instantiate an object of a class. __init__ function can take any properties which we want to set:
class MyClass:
   def __init__(self, first_property):
       self.first_property = first_property
   def MyClassFunction(self):
      return self.first_property

#Create an instance
m = MyClass(123)
r = m.MyClassFunction()
r will be 123
Note: self parameter will contain the reference of the object, also referred to as �this� in other programming languages such as C#
__str__
Returns stringified version of an object when we call �print�:
m = MyClass(123)
print m #Calls __str__
Therefore, __str__ is executed when we execute print.
__cmp__
Use the __cmp__ instance function if we want to provide custom logic to compare two objects of the same instance.
It returns 1 (greater), -1 (lower) and 0 (equal) to indicate the equality of two objects.
Think of __cmp__ like Equals() method in other programming language.
Overloading
Methods of an object can be overloaded by providing more arguments as an instance.
We can also overload an operator such as + by implementing our own implementation for __add__
Shallow Vs Deep Copy Of Objects
Equivalent objects � Contains the same values
Identical objects � Reference the same object � same address in memory
If you want to copy an entire object then you can use the copy module
import copy
m = MyClass(123)
mm = copy.copy(m)
This will result in shallow copy as the reference pointers of the properties will be copied.
Therefore, if one of the properties of the object is an object reference then it will simply point to the same reference address as the original object.
As a result, updating the property in the source object will result in updating the property in the target object.
Therefore shallow copy copies reference pointers.
Fortunately, we can utilise deep-copy:
import copy
m = MyClass(123)
mm = copy.deepcopy(m)
If MyClass contains a property that references MyOtherClass object then the contents of the property will be copied in the newly created object via deepcopy.
Therefore, deep copy makes a new reference of the object.
18. Object-Oriented Design � Inheritance
Python supports the inheritance of objects. As a result, an object can inherit functions and properties of its parent.
An inherited class can contain different logic in its functions.
If you have a class: ParentClass and two subclasses: SubClass1, SubClass2 then you can use Python to create the classes as:
class ParentClass:
 def my_function(self):
   print 'I am here'

class SubClass1(ParentClass):
class SubClass2(ParentClass):
Consequently, both subclasses will contain the function my_function().
Inheritance can encourage code reusability and maintenance.
Note: Python supports multiple inheritances unlike C#
Use Python�s abc module to ensure all subclasses contain the required features of the Abstract Base Class.
Multi-Inheritance:
class A(B,C):  #A implments B and C
If you want to call parent class function then you can do:
super(A, self).function_name()
There are a number of internal methods that we should be familiar with.
__str__(self): This function returns a user-friendly value to represent the object to the users.
__repr__(self): This function returns a developer-friendly value to represent the object to the users. If __str__() is missing then __repr()__ is called.
__eq__(self, other): This function can help us compute how to find if two objects are equal.
class Human:
    def __init__(self, name, friends=[]):
        self.name = name
        self.friends = friends

    def __str__(self):
        return f'Name is: {self.name}'

    def __repr__(self):
        return 'Human({self.name}, {self.friends})'

    def __eq__(self, other):
        return self.name == other.name


human = Human('Farhad', ['Friend1', 'Friend2'])
print(human)
This will print: Name is: Farhad
If we remove __str__ function then it will call __repr__ instead:
This will print: Human(Farhad, [�Friend1�, �Friend2�])
If we create two human instances with the same name and different friends then it will mark both of them as equal due to the __eq__ function:
human1 = Human('Farhad', ['Friend1', 'Friend2'])
human2 = Human('Farhad', ['ABC', 'DEF'])
print(human1 == human2) #prints True
19. Garbage Collection � Memory Management
All of the objects in Python are stored in heap space. This space is accessible to the Python interpreter. It is private.
Python has an in-built garbage collection mechanism.
It means Python can allocate and de-allocate the memory for your program automatically such as in C++ or C#.
Its responsibility is to clear the space in memory of those objects that are not referenced/used in the program.
As multiple objects can share memory references, python employs two mechanisms:
Reference counting: Count the number of items an object is referenced, deallocate an object if its count is 0. The reference is incremented when the variable is assigned an object or when it is passed as an argument to any method.
The second mechanism takes care of circular references, also known as cyclic references, by only de-allocating the objects where the allocation � deallocation number is greater than the threshold.
New objects are created in Generation 0 in python. They can be inspected by:
import gc
collected_objects = gc.collect()
Manual garbage collection can be performed on a timely or event-based mechanism.
When a Python program exists then Python attempts to remove global objects and ones that have a circular reference.
20. I/O
From Keyboard
Use the raw_input() function
user_says = raw_input()
print(user_says)
Files
Use a with/as a statement to open and read a file. It is equivalent to the using statement in C#.
with statement can take care of the closing of connections and other cleanup activities.
Open files
with open(file path, 'r') as my_file:
  for line in my_file
#File is closed due to with/as
Note: readline() can also be executed to read a line of a file.
To open two files
with open(file path) as my_file, open(another path) as second_file:
  for (line number, (line1, line2)) in enumerate(zip(my_file, second_file):
Writing files
with open(file path, 'w') as my_file:
  my_file.write('test')
Note: Use os and shutil modules for files.
Note: There are a large number of modes available, such as:
rw � read-write mode
r � read mode
rb � read in binary format mode e.g. pickle file
r+ � read for both reading and writing mode
a � append mode.
w+ � for reading and writing, overwrites if the file exists
SQL
Open a connection
import MySQLdb
database = MySQLdb.connect(�host�=�server�, �database-user�=�my username�, �password�=�my password�, �database-name�=�my database�)
cursor = database.cursor()
Execute a SQL statement
cursor.fetch("Select * From MyTable")
database.close()
Web Services
To query a rest service
import requests
url = 'http://myblog.com'
response = requests.get(url).text
To Serialise and Deserialise JSON
Deserialise:
import json
my_json = {"A":"1","B":"2"}
json_object = json.loads(my_json)
value_of_B = json_object["B"]
Serialise:
import json
a = "1"
json_a = json.dumps(a)
21. Error Handling
Raise Exceptions
If you want to raise exceptions then use the raise keyword:
try:
  raise TypError
except:
  print('exception')
Catching Exceptions
To catch exceptions, you can do:
try:
   do_something()
except:
   print('exception')
If you want to catch specific exceptions then you can do:
try:
   do_something()
except TypeError:
   print('exception')
If you want to use try/catch/finally then you can do:
try:
   do_something()
except TypeError:
   print('exception')
finally:
   close_connections()
finally part of the code is triggered regardless, you can use finally to close the connections to database/files, etc.
Try/Except/Else
try:
  do something
except IfTryRaisedException1:
  do something else
except (IfTryRaisedException2, IfTryRaisedException3)
  if exception 2 or 3 is raised then do something
else:
  no exceptions were raised
else is executed when no exceptions have been raised.
We can also assign exception to a variable by doing:
try:
  do something
except Exception1 as my_exception:
  do something about my_exception
If you want to define user-defined constraints then use assert:
assert <bool>, 'error to throw'
Note: Python supports inheritance in exceptions
You can create your own exception class by:
class MyException(Exception): pass
22. Multi-Threading And GIL
GIL is Global Interpreter Lock.
It ensures that the threads can execute at any one time and allows CPU cycles to pick the required thread to be executed.
GIL is passed onto the threads that are currently being executed.
Python supports multi-threading.
Note: GIL adds overheads to the execution. Therefore, be sure that you want to run multiple threads.
23. Decorators
Decorators can add functionality to code. They are essentially functions that call other objects/functions.
Decorators are callable functions � therefore they return the object to be called later when the decorated function is invoked.
Think of decorates that enable aspect-oriented programming
We can wrap a class/function and then a specific code will be executed any time the function is called.
We can implement generic logic to log, check for security checks, etc and then attribute the method with the @property tag.
The most important concept to understand is that in Python, functions are objects. This means a function can return another function and one function can take another function as an argument. We can also define a function within another function. Functions can also be assigned to a variable.
Understanding decorators:
Essentially a decorator is a function. It accepts another function as a parameter. It also returns a function. Let�s understand the steps:
The decorator is a function that accepts a function as an input and returns an inner function:
def my_decorator(input):
  def my_inner_decorator(*args, **kwargs):
     print('Hi FinTechExplained')
     input(*args, **kwargs)
     print('Bye FinTechExplained')
  
  return my_inner_decorator
2. The input function is the one that the decorator is going to decorate.
3. The internal function is the function that performs the appropriate actions to the input function and then the decorator returns the inner function.
To use the decorator, use the @ sign:
@my_decorator
def print_num(number):
  print('inside function:',number)
print_num(1)
The function printed:
Hi FinTechExplained
inside function: 1
Bye FinTechExplained
We can also pass other parameters to the decorator if required.
24. Unit Testing In Python
There are a number of unit testing and mocking libraries available in Python.
An example is to use unittest:
1.Assume your function simply decrements an input by 1
def my_function(input):
  return input - 1
2. You can unit test it by:
import unittest
class TestClass(unittest.TestCase):
 def my_test(self):
    self.assertEqual(my_function(1), 0)) #checking 1 becomes 0
We can also use doctest to test code written in docstrings.
25. Top Python Discussion Questions
Why Should I Use Python?
Simple to code and learn
Object-oriented programming language
Great Analytics and ML packages
Faster to develop and bring my solution to market
Offers in-built memory management facilities
Huge community support and applications available
No need to compile as it�s an interpreted language
Dynamically typed � no need to declare variables
How To Make Python Run Fast?
Python is a high-level language and is not suitable to access system-level programs or hardware.
Additionally, it is not suitable for cross-platform applications.
The fact that Python is a dynamically typed interpreted language makes it slower to be optimised and run when compared to the low-level languages.
Implement C language-based extensions.
Create multi-processes by using Spark or Hadoop
Utilise Cython, Numba, and PyPy to speed up your Python code or write it in C and expose it in Python like NumPy
Which IDEs Do People Use?
Spyder, PyCharm. Additionally various notebooks are used e.g. Jupyter
What Are The Top Python Frameworks And Packages?
There are a large number of must-use packages:
PyUnit (unit testing), PyDoc (documentation), SciPy (algebra and numerical), Pandas (data management), Sci-Kit learn (ML and data science), Tensorflow (AI), Numpy (array and numerical), BeautifulSoap (web pages scrapping), Flask (microframework), Pyramid (enterprise applications), Django (UI MVVM), urllib (web pages scraping), Tkinter (GUI), mock (mocking library), PyChecker(bug detector), Pylint (module code analysis)
How To Host Python Packages?
For Unix: Make script file mode Executable and the first line begin with:
#(#!/my account/local/bin/python)
2. You can use command-line tool and execute it
3. Use PyPRI or PyPI server
Can Python and R be combined?
A large number of rich statistical libraries have been written in R
One can execute R code within Python by using Rpy2 python package or by using a beaker notebook or IR kernel within Jupyter.
Is there a way to catch errors before running Python?
We can use PyChecker and PyLink to catch errors before running the code.
Image for postImage for post
Photo by Curtis MacNewton on Unsplash
Summary
This article outlined the most important 25 concepts of Python in a short, relevant and focused manner. I genuinely hope it has helped someone get a better understanding of Python.
I believe I have concentrated on the must-know topics that are absolutely required to be understood. This knowledge is sufficient to write your own python packages in the future or using existing Python packages.
Image for postImage for post
Thank you for reading.
Rest, just practice as much as possible and you can implement your own library in Python because this article contains all the knowledge you need.
Here are 5 interesting Python exercises to test your knowledge :
5 Python Exercises
Best Way To Practice And Strengthen Your Python Skills
medium.com
I highly recommend this article as it shows how you can use your Python knowledge to build a robust application:
How To Build Future Proof Applications?
Programming Python Using Design Patterns
medium.com
If there are any questions/comments then do please let me know.
FinTechExplained
This blog aims to bridge the gap between technologists�
Follow
4.1K

19
Programming
Python
Fintech
Data Science
Technology
4.1K claps

19 responses



Farhad Malik
WRITTEN BY

Farhad Malik
Follow
My personal blog, aiming to explain complex mathematical, financial and technological concepts in simple terms. Contact: FarhadMalik84@googlemail.com
FinTechExplained
FinTechExplained
Follow
This blog aims to bridge the gap between technologists, mathematicians and financial experts and helps them understand how fundamental concepts work within each field. Articles
More From Medium
Chain of Responsibility to the Rescue
Aditya Athalye in Better Programming

Java Generics as Assortment Box
Gene Zeiniss in The Startup

Running Apache Kafka Mirror Maker on Kubernetes
aruja

How to Send Messages with the Broadcast Channel API
John Au-Yeung in JavaScript In Plain English

Implementing a K-Means Clustering Algorithm From Scratch
Zack Murray in The Startup

Protocol Chaos
Jan Machacek in disney-streaming

Ultimate Golang String Formatting Cheat Sheet
Arindam Roy in The Startup

The Toils of Software Development
Bob Roebling in Better Programming

Discover Medium
Welcome to a place where words matter. On Medium, smart voices and original ideas take center stage - with no ads in sight. Watch
Make Medium yours
Follow all the topics you care about, and we�ll deliver the best stories for you to your homepage and inbox. Explore
Become a member
Get unlimited access to the best stories on Medium � and support writers while you�re at it. Just $5/month. Upgrade
About
Help
Legal
Full Stack Python logo Full Stack Python
All topics | Blog | Supporter's Edition | @fullstackpython | Facebook | What's new?
Build, Deploy and Operate Python Applications
You're knee deep in learning Python programming. The syntax is starting to make sense. The first few ahh-ha! moments hit you as you learn to use conditional statements, for loops and classes while coding with the open source libraries that make Python such an amazing programming ecosystem.

Now you want to take your initial Python knowledge and make something real, like a web application to show off to friends or sell as a service to customers. That's where Full Stack Python comes in. You have come to the right place to learn everything you need to create, deploy and operate Python-powered applications.

Full Stack Python is an open source book that explains technical concepts in plain language. Read everything online for free or purchase the Supporter's Edition for nicely-formatted ebook (PDF, EPUB, MOBI) versions. This guide branches out on topic because your learning requirements depend on what you're working on. Choose a topic from the links below or view the full table of contents to see even more subjects you can learn.

What do you need to learn first?
Sponsored By
Sentry logo
Software errors are inevitable. Chaos is not. Try Sentry for free.

AssemblyAI logo
The most accurate speech-to-text API. Built for Python developers.

1. Introduction
1.1 Learning Programming
The Python Language
Why Use Python?
Python 2 or 3?
Enterprise Python
1.2 Python Community
Companies Using Python
Best Python Resources
Must-watch Python Videos
Podcasts
2. Development Environments
2.1 Text Editors and IDEs
Vim
Emacs
Sublime Text
PyCharm
Jupyter Notebook
2.2 Shells
Bourne-again shell (Bash)
Zsh
PowerShell
2.3 Terminal multiplexers
tmux
Screen
2.4 Environment configuration
Application dependencies
virtual environments (virtualenvs)
Localhost tunnels
2.5 Source Control
Git
Mercurial
3. Data
3.1 Relational databases
PostgreSQL
MySQL
SQLite
3.2 Object-relational mappers
SQLAlchemy
Peewee
Django ORM
Pony ORM
3.3 NoSQL
Redis
MongoDB
Apache Cassandra
Neo4j
3.4 Data analysis
pandas
SciPy & Numpy
3.5 Data visualization
Bokeh
d3.js
Matplotlib
3.6 Markup Languages
Markdown
reStructuredText
4. Web Development
4.1 Web Frameworks
Django
Flask
Bottle
Pyramid
TurboGears
Falcon
Morepath
Sanic
Other web frameworks
4.2 Template Engines
Jinja2
Mako
Django Templates
4.3 Web design
HTML
CSS
Responsive Design
Minification
4.4 CSS Frameworks
Bootstrap
Foundation
4.5 JavaScript
React
Vue.js
Angular
4.6 Task queues
Celery
Redis Queue (RQ)
Dramatiq
4.7 Static site generators
Pelican
Lektor
MkDocs
4.8 Testing
Unit testing
Integration testing
Debugging
Code Metrics
4.9 Networking
HTTPS
WebSockets
WebRTC
4.10 Web APIs
Microservices
Webhooks
Bots
4.11 API creation
API Frameworks
Django REST Framework
4.12 API integration
Twilio
Stripe
Slack
Okta
4.13 Web application security
SQL injection
Cross-Site Request Forgery
5. Web App Deployment
5.1 Hosting
Servers
Static content
Content Delivery Networks (CDNs)
5.2 Virtual Private Servers (VPSs)
Linode
DigitalOcean
Lightsail
5.3 Platform-as-a-Service
Heroku
PythonAnywhere
AWS CodeStar
5.4 Operating systems
Ubuntu Linux
macOS
FreeBSD
Windows
5.5 Web servers
Apache HTTP Server
Nginx
Caddy
5.6 WSGI servers
Green Unicorn
uWSGI
mod_wsgi
5.7 Continuous integration
Jenkins
GoCD
5.8 Configuration management
Ansible
Salt
5.9 Containers
Docker
Kubernetes
5.10 Serverless Architectures
AWS Lambda
Azure Functions
Google Cloud Functions
6. DevOps
6.1 Monitoring
Datadog
Prometheus
Rollbar
Sentry
6.2 Web App Performance
Logging
Caching
Web Analytics
7. Meta
Change log
About the author
What "full stack" means
Page Statuses
Future directions
Matt Makai 2012-2020

Categories 
Reading Lists 
Search 


Long Page Scrolling Designs That Work
by JASON AMUNWA
In the early days of the internet, scrolling was a faux-pas � a dirty interaction that was to be minimized. Design gurus, fearing that novice web users were missing the content �below the fold�, would exhort designers to cram as much as possible above this dreaded invisible line.

Today, the ubiquity of mobile touchscreen devices, coupled with the steady increase in the resolution of consumer screens both in the mobile form as well as traditional laptop and desktop settings,  has revived scrolling as an effective and intuitive way to navigate through web content. With its rise, there has been a corresponding growth in the acceptable length of content-heavy pages, as designers experiment with using single pages to tell compelling stories with their content. The following is a collection of examples we�ve encountered that are poster children for the long scroll trend.

1. Navigation via narrative
Concise, and to the point has become a popular style of content on the web today. Simple contact forms, brief but clear �about� sections, and scannable �features� callouts � to name but a few � are leading the way in consolidating content into a single page. The pressure on the traditional horizontal navigation bar is easing, as it no longer must accommodate vast numbers of internal site pages. Instead, nav bars are being leveraged to lead users deeper into the story of the site�s main page. Keeping the story contained to a single page helps preserve the seamlessness of the experience, and helps guard against key content possibly being missed, due to the linear nature of how long pages present content.

Single: http://singlecard.co.uk


Single utilizes a slider to deliver a simple story to its users. All of the nav elements on the site serve the purpose of leading the user through their story � even the top right navigation  tab takes you to the very last slide that contains the main call to action.

Pur-Suit: http://www.pursuityourself.com


Pur-Suit is another site that opts for a clean and easy-to-navigate layout that tells its story. The site does have a small navigation bar, but it is confined to the very top of the page and its purpose is focused on helping navigate users down the page.

Whitmans New York: http://www.whitmansnyc.com


Whitmans provides all of the necessary sections a user would need to navigate through a restaurant site in a simple scrolling experience. The navigation sticks to the top right and remains a tool for jumping through different sections of the page, allowing users to quickly skip to the information they need.

2. Promoting engagement with metaphors & interactivity
Incorporating meaningful metaphors and rich interaction throughout the linear path of a long scroll page is a great way to keep users engaged with the story. Humans tend not to do well with uncertainty, so simple metaphors woven into the structure of your long pages help foreshadow what the user will encounter along the way. Sprinkling interactivity throughout these pages is similar to the good practice of stopping to stretch one�s legs on a long roadtrip � offering these mental breaks to your users keeps them focused, and helps them return to the journey refreshed and ready to continue.

Online Portfolio of Jan Ploch: http://www.janploch.de


Jan Ploch uses the simple but fun metaphor of drinking a bottle of soda to compel users to reach the end of his page � thus concluding a satisfying drinking experience (*burp* � ahhh). This subtle visual interaction keeps users scrolling down the page to reach the end goal of seeing his final call to action.

La Moulade: http://lamoulade.com


La Moulade is a design firm that utilizes a wide range of scrolling from various angles � not just horizontal and vertical � in their web pages. The surprise of angular motion in a medium that is almost terminally married to up-down/left-right movement captures your attention almost immediately, but what keeps you moving forward is the gradual reveal of additional content. The text and images act as building blocks for an easily comprehended layout, as the viewer keeps scrolling through, creating a satisfying experience all the way through.

Dangers of Fracking: http://www.dangersoffracking.com


The Dangers of Fracking site is an excellent example of creating engagement and interaction between the user and the content using long-page scrolling. The page tells the fascinating story of a single drop of water as it goes through the fracking process � making sure to provide informative snippets of content at each step along the way. The innovative transformation of the content encourages users to keep scrolling and discover what other surprises are in store.

Wunderkit: http://get.wunderkit.com


Wunderkit�s site is filled with all sorts of rich interactions that are cued to start when a user scrolls to certain sections down the page. The sum of all the small visual delights nestled within this page keeps users alert, and on the lookout to learn more about the product.

3. Improved comprehension
As anyone who�s infuriated a sibling by shouting random numbers while they�re memorizing a phone number can tell you, the human brain can only hold between 7 and 9 pieces of new information in short term memory at one time. Traditional sites with many separate pages break up content, therefore placing a greater burden on the user�s short term memory to piece together the holistic message from disparate fragments. Collecting content within a single experience allows users to follow an unbroken thread, thereby aiding in comprehension and recollection down the road.

Rdio: http://www.rdio.com


Rdio�s long scroll pages break up their content into digestible chunks of information that all relate to the main theme of each page. For example, the Apps page explores the various ways of using Rdio on different platforms � taking care to differentiate each section with good use of color blocks, while maintaining consistent style and flow from one section to another.

Think Green Meeting: http://thinkgreenmeeting.com


Think Green Meeting allows readers to jump to the information they need by clicking on the consistent navigation menu to the left. While the layout varies significantly from section to section, the panning motion between them connects the pieces together, reinforcing the main message.

Simple: https://www.simple.com


Simple is a site for a new money management service�that�s not a bank. The concept requires some explaining. Their challenge is to demonstrate their key value proposition quickly and convincingly, and to motivate visitors to sign up. The one-page scrolling design is a dramatic departure from the majority of online financial services websites, which typically contain oceans of content � most of it text. Simple presents their key points briefly, linking to pertinent examples. This layered approach aids in comprehension of their new concept without overwhelming the visitor with details too early in the exploration.

Long page scrolling designs are great for delivering end-to-end stories to your audience, and with the proper execution can propel your users toward the main goal � whether that�s a form submission, a product purchase, or even just reaching understanding of an idea you want to share.

Have you been impressed by a long page scrolling site recently? Share it with us in the comments below!

Editor�s note: Looking for the winners of the Arrrows font giveaway? We�ve extended the deadline by one more week so check back on next week�s post to see the winners!


read more by

JASON AMUNWA
Jason is a software entrepreneur, product manager, XCOM addict & curious problem-solver. Formerly the Director of Products & Marketing at Digital Telepathy, Jason now helps web and mobile app developers grow their products at GrowthLook.

   
Home
Work
Services
About
Careers
Blog
� 2000-2020 Telepathy (formerly Digital Telepathy). All rights reserved. Privacy Policy
Skip to content
Platform
QA & User Acceptance Testing
- Features
Customer Experience Feedback
- Features
Industries
Web agencies
Software companies
e-Commerce
Pricing
Customers
Resources
Amazing Blog!
e-Books
Videos
Documentation
Contact
Login
Try it free
Long-Scrolling Websites: Tips and Best Practices

by Rebecca Vogels
Web Development

Facebook
Twitter
LinkedIn
WhatsApp
Flipboard
Long-scrolling websites, i.e. sites that let you scroll down for an extended amount of time, have been around for years.

The reason? Users love them!

That isn�t about to change, which is why we wanted to give you some tips and best practices in this article. Because long-scrolling or infinite-scrolling alone doesn�t mean the user is going to spend a long time on your website.

Have fun reading and scrolling ??


What Was Changed Through Long-Scrolling or Infinite-Scrolling Websites
Back in the day, Websites followed the same rules as newspapers: The most important content was supposed to be on top. There was even an expression for that: above the fold.

It still applies to Facebook ads and blog post teasers. The teaser determines if the user will click on the blog post or Facebook ad or not.

But this is exactly where the difference lies: When clicking something, the user makes a conscious decision. When scrolling, the user just keeps doing what he or she is doing already. The conscious decision would be to stop scrolling.

The argument for above the fold content therefore only partially applies to websites. The front page may contain the main claim and maybe a teaser, but it might not contain all of the relevant information.

You can see the difference at first sight:

old-website-apple-1024x640

Apple Website 1997.

 bildschirmfoto-2016-08-31-um-12-23-07

Apple Website 2016.

Contrary to the original idea, endless scrolling is not causing higher bounce rates. Quite the contrary, it keeps the user entertained. At least, if it�s done right. The longer a user stays on your website, the more likely he or she is to react to your call to action.

When Will Users Keep Scrolling?
Users keep on scrolling as long as the content is interesting to them. The more the content becomes irrelevant to the user, the more likely they are to abort a session.



Another reason for the popularity of long-scrolling websites is the growing usage of mobile devices. Scrolling is especially convenient on the small screen compared to using buttons for navigating a website.

Long-Scrolling & Story-Telling
The challenge for designers and marketers is telling a story with their website.

There are multiple design possibilities and effects for this. Examples are animations or parallax scrolling (more on this shortly).



Alternating between different types of content, text, pics, videos and gifs is another thing that makes long-scrolling more attractive.

The Feeling of Control
Long-scrolling gives users a feeling of control, which is extremely important for user satisfaction.

Users can navigate long-scrolling sites at their own pace: they can stay on relevant content or keep scrolling if the content is not interesting for them.

Parallax scrolling and animations can partly be controlled by the user, giving him or her a feeling of control.

Scrolling also keeps the user focused and on topic instead of instead of having him click around all the time. Clicking a link always creates a delay. If the page takes too long to load, the danger of the user leaving is relatively high. This can�t happen with long-scrolling websites.

Structuring Content Appropriately
One challenge with long-scrolling websites is structuring the content in a way that makes sense. This is also important from a SEO perspective.

Theoretically you only need one page, although this is not reasonable in most cases. It makes more sense to have a horizontal navigation bar with the most important categories on your website. On the Usersnap homepage these are:

Tour / Our Customers / Pricing / Integrations / Blog

Each of these pages has different parts, but we chose not to use long dropdown-menus.

Our navigation bar takes the visitor to different sections, from which he or she can navigate further within the site.

A big challenge with long-scrolling sites is determining the optimal length for the user.

Too much content will overwhelm the user.

Too much content can overwhelm users and result in a bad user experience. It�s hard to give general tips about this issue, as it largely depends on the type of industry or niche the site is in. Long-scrolling or infinite-scrolling sites with products are more widely accepted on e-commerce sites.

Long-Scrolling and Interaction
Animations and hovering have been around before long-scrolling, yet the effects are still popular. The reason is simple: long-scrolling creates entertainment and interaction for the user.

Interactive elements keep users busy and also create a little mental break. A good example is the website of Pirate Code: http://piratecode.ru/en/

http://piratecode.ru/en/pirate code

This site shows animated waves and lets the user scroll horizontally.

Parallax & Long-Scrolling
The parallax effect has been around for years. In the beginning, it was especially popular in retro video games, before being quickly adapted by the web design community.

The parallax effect incorporates different backgrounds that move at varying speeds. The result is a pseudo 3D effect. The term �parallaxis� stems from the Greek and means �change�.

A fantastic example of a combination of storytelling, parallax and long-scrolling is the website The Boat.

It takes Nam Le�s graphic novel about escaping Vietnam and transfers it using technology of modern web design.

You can watch the video below, but I suggest you visit the site so you can scroll it yourself.



The parallax effect is still a little limited on mobile devices, but that should change shortly due to improvements to Ajax. Parallax is an effect that entertains users and is therefore becoming increasingly important.

Wrapping it up.
Long-scrolling websites have established themselves as an integral part of the internet. Users love them and companies have the possibility to present different kinds of content and offers to their users.

Still they involve challenges, as only content that is consistently relevant and presented through stellar web design will keep the user scrolling down.

Here is an overview of the pros and cons of long-scrolling websites:

PRO:

Interactive story telling
Low bounce rates
Higher immersion
CON:

Challenging for SEO
Hard to measure
Content is difficult to set up
Complicated cross-browser testing
 

We are excited to hear about your favourite long-scrolling website!

We at Usersnap hope that you like this post and would love it if you follow us on Twitter. We tweet about web development, web design and UAT on a regular basis.

Usersnap is a bug-tracking and screenshot tool for every web-project. Get immediate feedback from the visitors of your websites. Try it now for free!

Did you enjoy this post?
 ??  ??  ??  ??  ??

You might be also interested in
Best web development podcasts
15 INSPIRATIONAL PODCASTS FOR EVERYONE WORKING IN WEB DEVELOPMENT
Read >
best web design blogs
24 AMAZING WEB DESIGN BLOGS YOU SHOULD FOLLOW THIS MONTH!
Read >

3 ADVANCED CONFIGURATION TRICKS FOR USERSNAP
Read >
Join our 45,000+ newsletter subscribers
LinkedIn, Microsoft, Codeship, Pivotal and Benefit Cosmetics leaders are reading our blog!

Your name
Type your name
Email*
Type your email
Subscribe
Post navigation
< Previous Article
Next Article >
Search for:
Search �
Categories
Quality Assurance
User Acceptance Tests
Bug Tracking
Website Feedback
Customer Feedback and Experience
Web Development
Usersnap Updates
Most read
GitLab vs GitHub: Key differences & similarities
User Acceptance Testing � How To Do It Right!
16 extremely useful Chrome extensions for developers
5 Types Of User Acceptance Testing
Designing a language switch: Examples and best practices
Follow Us
Facebook
Twitter
LinkedIn
Instagram
YouTube
Usersnap
COMPANY
WE�RE HIRING ??
About us
Team
Customers� stories
Press status
Live demo
Contact us
� 2020 Usersnap
PRODUCT
Usersnap QA (Classic)
Usersnap CX
Pricing
Use Cases
GDPR & Data Protection
Security
Privacy Policy
INTEGRATIONS
Jira
Asana
Zendesk
Slack
Trello
30+ Integrations
Terms of Service
RESOURCES
Free E-Books
What is UAT?
Customer Feedback
Chrome Extensions
How Usersnap Works
Alternatives
Facebook Twitter LinkedIn

Learning Python
https://d33wubrfki0l68.cloudfront.net/c53b5025effe2f48ace10fee7a4ffc3c2d9abb89/e9350/_images/32800783863_11a00db52c_k_d.jpg
Beginner
The Python Tutorial
This is the official tutorial. It covers all the basics, and offers a tour of the language and the standard library. Recommended for those who need a quick-start guide to the language.

The Python Tutorial
Real Python
Real Python is a repository of free and in-depth Python tutorials created by a diverse team of professional Python developers. At Real Python you can learn all things Python from the ground up. Everything from the absolute basics of Python, to web development and web scraping, to data visualization, and beyond.

Real Python
Python Basics
pythonbasics.org is an introductory tutorial for beginners. The tutorial includes exercises. It covers the basics and there are also in-depth lessons like object oriented programming and regular expressions.

Python basics
Python for Beginners
thepythonguru.com is a tutorial focused on beginner programmers. It covers many Python concepts in depth. It also teaches you some advanced constructs of Python like lambda expressions and regular expressions. And last it finishes off with the tutorial �How to access MySQL db using Python�

Python for Beginners
Learn Python Interactive Tutorial
Learnpython.org is an easy non-intimidating way to get introduced to Python. The website takes the same approach used on the popular Try Ruby website. It has an interactive Python interpreter built into the site that allows you to go through the lessons without having to install Python locally.

Learn Python
Python for You and Me
If you want a more traditional book, Python For You and Me is an excellent resource for learning all aspects of the language.

Python for You and Me
Learn Python Step by Step
Techbeamers.com provides step-by-step tutorials to teach Python. Each tutorial is supplemented with logically added coding snippets and equips with a follow-up quiz on the subject learned. There is a section for Python interview questions to help job seekers. You can also read essential Python tips and learn best coding practices for writing quality code. Here, you�ll get the right platform to learn Python quickly.

Learn Python Basic to Advanced

Online Python Tutor
Online Python Tutor gives you a visual step-by-step representation of how your program runs. Python Tutor helps people overcome a fundamental barrier to learning programming by understanding what happens as the computer executes each line of a program�s source code.

Online Python Tutor
Invent Your Own Computer Games with Python
This beginner�s book is for those with no programming experience at all. Each chapter has the source code to a small game, using these example programs to demonstrate programming concepts to give the reader an idea of what programs �look like�.

Invent Your Own Computer Games with Python
Hacking Secret Ciphers with Python
This book teaches Python programming and basic cryptography for absolute beginners. The chapters provide the source code for various ciphers, as well as programs that can break them.

Hacking Secret Ciphers with Python
Learn Python the Hard Way
This is an excellent beginner programmer�s guide to Python. It covers �hello world� from the console to the web.

Learn Python the Hard Way
Crash into Python
Also known as Python for Programmers with 3 Hours, this guide gives experienced developers from other languages a crash course on Python.

Crash into Python
Dive Into Python 3
Dive Into Python 3 is a good book for those ready to jump in to Python 3. It�s a good read if you are moving from Python 2 to 3 or if you already have some experience programming in another language.

Dive Into Python 3
Think Python: How to Think Like a Computer Scientist
Think Python attempts to give an introduction to basic concepts in computer science through the use of the Python language. The focus was to create a book with plenty of exercises, minimal jargon, and a section in each chapter devoted to the subject of debugging.

While exploring the various features available in the Python language the author weaves in various design patterns and best practices.

The book also includes several case studies which have the reader explore the topics discussed in the book in greater detail by applying those topics to real-world examples. Case studies include assignments in GUI programming and Markov Analysis.

Think Python
Python Koans
Python Koans is a port of Edgecase�s Ruby Koans. It uses a test-driven approach to provide an interactive tutorial teaching basic Python concepts. By fixing assertion statements that fail in a test script, this provides sequential steps to learning Python.

For those used to languages and figuring out puzzles on their own, this can be a fun, attractive option. For those new to Python and programming, having an additional resource or reference will be helpful.

Python Koans
More information about test driven development can be found at these resources:

Test Driven Development
A Byte of Python
A free introductory book that teaches Python at the beginner level, it assumes no previous programming experience.

A Byte of Python for Python 2.x A Byte of Python for Python 3.x
Computer Science Path on Codecademy
A Codecademy course for the absolute Python beginner. This free and interactive course provides and teaches the basics (and beyond) of Python programming while testing the user�s knowledge in between progress. This course also features a built-in interpreter for receiving instant feedback on your learning.

Computer Science Path on Codecademy
Code the blocks
Code the blocks provides free and interactive Python tutorials for beginners. It combines Python programming with a 3D environment where you �place blocks� and construct structures. The tutorials teach you how to use Python to create progressively more elaborate 3D structures, making the process of learning Python fun and engaging.

Code the blocks
Intermediate
Python Tricks: The Book
Discover Python�s best practices with simple examples and start writing even more beautiful + Pythonic code. Python Tricks: The Book shows you exactly how.

You�ll master intermediate and advanced-level features in Python with practical examples and a clear narrative.

Python Tricks: The Book
Effective Python
This book contains 59 specific ways to improve writing Pythonic code. At 227 pages, it is a very brief overview of some of the most common adaptations programmers need to make to become efficient intermediate level Python programmers.

Effective Python
Advanced
Pro Python
This book is for intermediate to advanced Python programmers who are looking to understand how and why Python works the way it does and how they can take their code to the next level.

Pro Python
Expert Python Programming
Expert Python Programming deals with best practices in programming Python and is focused on the more advanced crowd.

It starts with topics like decorators (with caching, proxy, and context manager case studies), method resolution order, using super() and meta-programming, and general PEP 8 best practices.

It has a detailed, multi-chapter case study on writing and releasing a package and eventually an application, including a chapter on using zc.buildout. Later chapters detail best practices such as writing documentation, test-driven development, version control, optimization, and profiling.

Expert Python Programming
A Guide to Python�s Magic Methods
This is a collection of blog posts by Rafe Kettler which explain �magic methods� in Python. Magic methods are surrounded by double underscores (i.e. __init__) and can make classes and objects behave in different and magical ways.

A Guide to Python�s Magic Methods
Note
Rafekettler.com is currently down; you can go to their GitHub version directly. Here you can find a PDF version: A Guide to Python�s Magic Methods (repo on GitHub)

For Engineers and Scientists
A Primer on Scientific Programming with Python
A Primer on Scientific Programming with Python, written by Hans Petter Langtangen, mainly covers Python�s usage in the scientific field. In the book, examples are chosen from mathematics and the natural sciences.

A Primer on Scientific Programming with Python
Numerical Methods in Engineering with Python
Numerical Methods in Engineering with Python, written by Jaan Kiusalaas, puts the emphasis on numerical methods and how to implement them in Python.

Numerical Methods in Engineering with Python
Miscellaneous Topics
Problem Solving with Algorithms and Data Structures
Problem Solving with Algorithms and Data Structures covers a range of data structures and algorithms. All concepts are illustrated with Python code along with interactive samples that can be run directly in the browser.

Problem Solving with Algorithms and Data Structures
Programming Collective Intelligence
Programming Collective Intelligence introduces a wide array of basic machine learning and data mining methods. The exposition is not very mathematically formal, but rather focuses on explaining the underlying intuition and shows how to implement the algorithms in Python.

Programming Collective Intelligence
Transforming Code into Beautiful, Idiomatic Python
Transforming Code into Beautiful, Idiomatic Python is a video by Raymond Hettinger. Learn to take better advantage of Python�s best features and improve existing code through a series of code transformations: �When you see this, do that instead.�

Transforming Code into Beautiful, Idiomatic Python
Fullstack Python
Fullstack Python offers a complete top-to-bottom resource for web development using Python.

From setting up the web server, to designing the front-end, choosing a database, optimizing/scaling, etc.

As the name suggests, it covers everything you need to build and run a complete web app from scratch.

Fullstack Python
PythonistaCafe
PythonistaCafe is an invite-only, online community of Python and software development enthusiasts helping each other succeed and grow. Think of it as a club of mutual improvement for Pythonistas where a broad range of programming questions, career advice, and other topics are discussed every day.

PythonistaCafe
References
Python in a Nutshell
Python in a Nutshell, written by Alex Martelli, covers most cross-platform Python usage, from its syntax to built-in libraries to advanced topics such as writing C extensions.

Python in a Nutshell
The Python Language Reference
This is Python�s reference manual. It covers the syntax and the core semantics of the language.

The Python Language Reference
Python Essential Reference
Python Essential Reference, written by David Beazley, is the definitive reference guide to Python. It concisely explains both the core language and the most essential parts of the standard library. It covers Python 3 and 2.6 versions.

Python Essential Reference
Python Pocket Reference
Python Pocket Reference, written by Mark Lutz, is an easy to use reference to the core language, with descriptions of commonly used modules and toolkits. It covers Python 3 and 2.6 versions.

Python Pocket Reference
Python Cookbook
Python Cookbook, written by David Beazley and Brian K. Jones, is packed with practical recipes. This book covers the core Python language as well as tasks common to a wide variety of application domains.

Python Cookbook
Writing Idiomatic Python
Writing Idiomatic Python, written by Jeff Knupp, contains the most common and important Python idioms in a format that maximizes identification and understanding. Each idiom is presented as a recommendation of a way to write some commonly used piece of code, followed by an explanation of why the idiom is important. It also contains two code samples for each idiom: the �Harmful� way to write it and the �Idiomatic� way.

For Python 2.7.3+

For Python 3.3+

The Hitchhiker's Guide to Python



Search the doc
This opinionated guide exists to provide both novice and expert Python developers a best practice handbook to the installation, configuration, and usage of Python on a daily basis.


O'Reilly Book
This guide is now available in tangible book form!

Python Guide Book Cover

All proceeds are being directly donated to the DjangoGirls organization.

Translations
English
French
Chinese
Japanese
Korean
Filipino
Brazilian Portuguese
Table Of Contents
Learning Python
Beginner
The Python Tutorial
Real Python
Python Basics
Python for Beginners
Learn Python Interactive Tutorial
Python for You and Me
Learn Python Step by Step
Online Python Tutor
Invent Your Own Computer Games with Python
Hacking Secret Ciphers with Python
Learn Python the Hard Way
Crash into Python
Dive Into Python 3
Think Python: How to Think Like a Computer Scientist
Python Koans
A Byte of Python
Computer Science Path on Codecademy
Code the blocks
Intermediate
Python Tricks: The Book
Effective Python
Advanced
Pro Python
Expert Python Programming
A Guide to Python�s Magic Methods
For Engineers and Scientists
A Primer on Scientific Programming with Python
Numerical Methods in Engineering with Python
Miscellaneous Topics
Problem Solving with Algorithms and Data Structures
Programming Collective Intelligence
Transforming Code into Beautiful, Idiomatic Python
Fullstack Python
PythonistaCafe
References
Python in a Nutshell
Skip to main content
O'Reilly home
SIGN IN
TRY NOW
TEAMS
INDIVIDUALS
FEATURES
Certifications
Interactive learning
Live online sessions
WHAT�S NEW
O�REILLY FOR MARKETERS
See everything available through O�Reilly online learning and start a free trial. Explore now.
Python in a Nutshell
Python in a Nutshell
by
Released March 2003
Publisher(s): O'Reilly Media, Inc.
ISBN: 9780596001889
Explore a preview version of Python in a Nutshell right now.

O�Reilly members get unlimited access to live online training experiences, plus books, videos, and digital content from 200+ publishers.

START YOUR FREE TRIAL BUY ON AMAZON All of O�Reilly�s books are available for purchase in print on Amazon.com.
Book description
Ask any Python aficionado and you'll hear that Python programmers have it all: an elegant language that offers object-oriented programming support, a readable, maintainable syntax, integration with C components, and an enormous collection of precoded standard library and extension modules. Moreover, Python is easy to learn but powerful enough to take on the most ambitious programming challenges. But what Python programmers have lacked is one concise and clear reference resource, with the appropriate measure of guidance in how best to use Python's great power. Now Python in a Nutshell fills this need. In the tradition of O'Reilly's "In a Nutshell" series, this book offers Python programmers one place to look when they need help remembering or deciphering the syntax of this open source language and its many modules. This comprehensive reference guide makes it easy to look up all the most frequently needed information--not just about the Python language itself, but also the most frequently used parts of the standard library and the most important third-party extensions. Python in a Nutshell focuses on Python 2.2 (and all its point releases), currently the most stable and widespread Python release. This book includes:

A fast-paced tutorial on the syntax of the Python language itself

An explanation of object-oriented programming in Python, covering both the classic and new-style object models

Coverage of other core topics, including exceptions, modules, strings, and regular expressions

A quick reference for Python's built-in types and functions, as well as the key modules in the Python standard library, including sys, os, time, thread, math, and socket, among many others

Reference material on important third-party extensions, such as Numeric and Tkinter

Information about extending Python and embedding it into other applications

Python in a Nutshell provides a solid, no-nonsense quick reference to information that programmers rely on the most. This latest addition to the best-selling "In a Nutshell" series will immediately earn its place in any Python programmer's library.

Publisher resources
Download Example Code

Table of contentsProduct information
Table of contents
Python in a Nutshell
Preface
How This Book Is Organized
Conventions Used in This Book
Show and hide more
You might also like
BOOK

Automate the Boring Stuff with Python
by Al Sweigart

Automate the Boring Stuff with Python teaches simple programming skills to automate everyday computer tasks.

BOOK

Python Crash Course, 2nd Edition
by Eric Matthes

This is the second edition of the best selling Python book in the world. Python Crash �

BOOK

Python for Programmers, First Edition
by Paul J. Deitel, Paul Deitel, Harvey Deitel

The professional programmer's Deitel� guide to Python� with introductory artificial intelligence case studies Written for programmers �

BOOK

Effective Python: 90 Specific Ways to Write Better Python, 2nd Edition
by Brett Slatkin

Updated and Expanded for Python 3 It�s easy to start developing programs with Python, which is �

ABOUT O�REILLY
Teach/write/train
Careers
Community partners
Affiliate program
Diversity
SUPPORT
Contact us
Newsletters
Privacy policy
   
DOWNLOAD THE O�REILLY APP
Apple app store Google play store
Take O�Reilly online learning with you and learn anywhere, anytime on your phone and tablet.

Get unlimited access to books, videos, and live training.
Sync all your devices and never lose your place.
Learn even when there�s no signal with offline access.
DO NOT SELL MY PERSONAL INFORMATION
Exercise your consumer rights by contacting us at donotsell@oreilly.com.

O'Reilly home
� 2020, O�Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.

We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.

Terms of service � Privacy policy � Editorial independence
Introduction to Networking
Charles Severance
Credits
Illustrations: Mauro Toselli
Editorial Support: Sue Blumenberg
Cover Design: Aimee Andrion
The SketchNote illustrations were drawn on an iPad using Paper
by www.fiftythree.com using a dedicated stylus pencil. The illustrations
were converted from PNG to SVG and EPS vector formats
using www.vectormagic.com. The technical figures for the book
were drawn with OmniGraffle.
Printing History
2015-May-25 Original Printing - CreateSpace
Copyright Details
This book is Copyright Charles R. Severance.
The paper/print version of this book is licensed under a Creative
Commons Attribution-NonCommercial 4.0 International License.
Permission is specifically granted to make copies as necessary
of all or part of the materials in this book as part of a course or
course packet.
http://creativecommons.org/licenses/by-nc/4.0
Electronic copies of this book in EPUB, PDF, and HTML are licensed
under a Creative Commons Attribution 4.0 International License.
http://creativecommons.org/licenses/by/4.0/
If you are interested in translating this book into a language other
than English, please contact me. I am willing to give commercial
print distribution rights for a complete and responsible translation.
Preface
The goal of this book is to provide a basic understanding of the
technical design and architecture of the Internet. The book is
aimed at all audiences � even those with absolutely no prior technical
experience or math skills. The Internet is an amazingly beautiful
design and should be understood by all who use it.
While this book is not about the Network+ or CCNA certifications,
I hope it serves as a way to give students interested in these
certifications a good starting point.
I want to thank Pamela Fox of Khan Academy for coming up with
the idea of an introductory network technology course using open
materials.
I initially developed this material as a single week�s lecture in the
SI502 - Networked Computing course that I taught at the University
of Michigan School of Information starting in 2008. I refined
and expanded the material to be three weeks of the Internet History,
Technology, and Security (IHTS) course that I have taught to
over 100,000 students on Coursera starting in 2012. This book
adds further detail to create a standalone text that can be read
for enjoyment or used to support an introductory course that focuses
on the Internet�s architecture.
This book has been particularly fun in that it is a collaboration with
my friends Mauro Toselli (@xlontrax) and Sue Blumenberg. I met
Mauro and Sue in 2012 when they became volunteer Community
Teaching Assistants (CTAs) for my IHTS course on Coursera. Over
the past three years we have become friends and colleagues. It
is a great example of how open education can bring people together.
There is supporting material for this book at
http://www.net-intro.com/
If you like the book, let us know. Send us a tweet with your
thoughts. You can also send a tweet if you find an error in the
book.
Charles R. Severance (@drchuck)
www.dr-chuck.com
Ann Arbor, MI USA
May 20, 2015
iv
Contents
1 Introduction 1
1.1 Communicating at a Distance . . . . . . . . . . . . . . . . 1
1.2 Computers Communicate Differently . . . . . . . . . . . 4
1.3 Early Wide Area Store-and-Forward Networks . . . . . . 5
1.4 Packets and Routers . . . . . . . . . . . . . . . . . . . . . . 6
1.5 Addressing and Packets . . . . . . . . . . . . . . . . . . . . 7
1.6 Putting It All Together . . . . . . . . . . . . . . . . . . . . . 8
1.7 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.8 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2 Network Architecture 13
2.1 The Link Layer . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2 The Internetwork Layer (IP) . . . . . . . . . . . . . . . . . 16
2.3 The Transport Layer (TCP) . . . . . . . . . . . . . . . . . . 18
2.4 The Application Layer . . . . . . . . . . . . . . . . . . . . . 20
2.5 Stacking the Layers . . . . . . . . . . . . . . . . . . . . . . 21
2.6 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.7 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3 Link Layer 25
3.1 Sharing the Air . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2 Courtesy and Coordination . . . . . . . . . . . . . . . . . . 28
3.3 Coordination in Other Link Layers . . . . . . . . . . . . . 29
3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
v
vi CONTENTS
3.5 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.6 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4 Internetworking Layer (IP) 35
4.1 Internet Protocol (IP) Addresses . . . . . . . . . . . . . . . 37
4.2 How Routers Determine the Routes . . . . . . . . . . . . 39
4.3 When Things Get Worse and Better . . . . . . . . . . . . 39
4.4 Determining Your Route . . . . . . . . . . . . . . . . . . . . 41
4.5 Getting an IP Address . . . . . . . . . . . . . . . . . . . . . 45
4.6 A Different Kind of Address Reuse . . . . . . . . . . . . . 47
4.7 Global IP Address Allocation . . . . . . . . . . . . . . . . . 48
4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.9 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.10Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
5 The Domain Name System 57
5.1 Allocating Domain Names . . . . . . . . . . . . . . . . . . 58
5.2 Reading Domain Names . . . . . . . . . . . . . . . . . . . . 59
5.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.4 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.5 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
6 Transport Layer 63
6.1 Packet Headers . . . . . . . . . . . . . . . . . . . . . . . . . 64
6.2 Packet Reassembly and Retransmission . . . . . . . . . 65
6.3 The Transport Layer In Operation . . . . . . . . . . . . . . 67
6.4 Application Clients and Servers . . . . . . . . . . . . . . . 68
6.5 Server Applications and Ports . . . . . . . . . . . . . . . . 68
6.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.7 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.8 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
CONTENTS vii
7 Application Layer 73
7.1 Client and Server Applications . . . . . . . . . . . . . . . 73
7.2 Application Layer Protocols . . . . . . . . . . . . . . . . . . 75
7.3 Exploring the HTTP Protocol . . . . . . . . . . . . . . . . . 76
7.4 The IMAP Protocol for Retrieving Mail . . . . . . . . . . . 80
7.5 Flow Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
7.6 Writing Networked Applications . . . . . . . . . . . . . . . 83
7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
7.8 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
7.9 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
8 Secure Transport Layer 89
8.1 Encrypting and Decrypting Data . . . . . . . . . . . . . . 90
8.2 Two Kinds of Secrets . . . . . . . . . . . . . . . . . . . . . . 91
8.3 Secure Sockets Layer (SSL) . . . . . . . . . . . . . . . . . 92
8.4 Encrypting Web Browser Traffic . . . . . . . . . . . . . . . 93
8.5 Certificates and Certificate Authorities . . . . . . . . . . 94
8.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
8.7 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
8.8 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
9 The OSI Model 101
9.1 Physical (Layer 1) . . . . . . . . . . . . . . . . . . . . . . . . 102
9.2 Data Link (Layer 2) . . . . . . . . . . . . . . . . . . . . . . . 102
9.3 Network (Layer 3) . . . . . . . . . . . . . . . . . . . . . . . . 102
9.4 Transport (Layer 4) . . . . . . . . . . . . . . . . . . . . . . . 103
9.5 Session (Layer 5) . . . . . . . . . . . . . . . . . . . . . . . . 103
9.6 Presentation (Layer 6) . . . . . . . . . . . . . . . . . . . . . 103
9.7 Application (Layer 7) . . . . . . . . . . . . . . . . . . . . . . 103
9.8 Comparing the OSI and TCP/IP Models . . . . . . . . . . 104
9.9 Link Layer (TCP/IP) . . . . . . . . . . . . . . . . . . . . . . . 104
9.10Internetwork Layer (TCP/IP) . . . . . . . . . . . . . . . . . 105
viii CONTENTS
9.11Transport Layer (TCP/IP) . . . . . . . . . . . . . . . . . . . . 105
9.12Application Layer (TCP/IP) . . . . . . . . . . . . . . . . . . 105
9.13Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
9.14Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
9.15Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
10Wrap Up 109
Chapter 1
Introduction
Using the Internet seems pretty easy. We go to a web address
and up comes a page. Or we go to our favorite social site and
see pictures of our friends, families, and pets. But it takes a lot
of complex software and hardware to make the Internet seem so
simple. The design of the technologies that make today�s Internet
work started in the 1960s, and there were over 20 years of
research into how to build internetworking technologies before
the first �Internet� was built in the late 1980s by academics in a
project called NSFNet. Since then, the research and development
into improving network technologies has continued as networks
have become far larger and faster and globally distributed with
billions of computers.
In order to better understand how today�s Internet works, we will
take a look at how humans and computers have communicated
using technology over the years.
1.1 Communicating at a Distance
Imagine a group of five people in a room sitting in a circle. As long
as they are courteous and don�t have more than one conversation
at the same time, it�s quite natural for any person to talk to any
other person in the room. They just need to be able to hear each
other and coordinate how to use the shared space in the room.
But what if we put these people in different rooms so they can
no longer see or hear each other? How could pairs of people
communicate with each other then? One way might be to run a
wire between each pair of people with a microphone on one end
and a speaker on the other end. Now everyone could still hear all
1
2 CHAPTER 1. INTRODUCTION
the conversations. They would still need to be courteous to make
sure that there was only one conversation going on at the same
time.
Each person would need four speakers (one for each of the other
people) and enough pieces of wire to connect all the microphones
and speakers. This is a problem with five people and it gets far
worse when there are hundreds or thousands of people.
Using wires, microphones, and speakers is how early telephone
systems from the 1900s allowed people to make phone calls. Because
they could not have separate wires between every pair of
telephones, these systems did not allow all pairs of people to be
connected at the same time. Each person had a single connection
to a human �operator�. The operator would connect two wires together
to allow a pair of people to talk, and then disconnect them
when the conversation was finished.
Figure 1.1: Connecting Using Telephone Operators
The first local telephone systems worked well when a customer�s
home or business was close to the operator�s building and a wire
could be strung directly from the operator�s building to the person�s
home.
1.1. COMMUNICATING AT A DISTANCE 3
But what if thousands people who are hundreds of kilometers
apart need to be able to communicate? We can�t run 100-
kilometer wires from each home to a single central office. What
the telephone companies did instead was to have many central
offices and run a few wires between the central offices, then
share connections between central offices. For long distances, a
connection might run through a number of central offices. Before
the advent of fiber optic, long-distance telephone calls were
carried between cities on poles with lots of separate wires. The
number of wires on the poles represented the number of possible
simultaneous long-distance phone calls that could use those
wires.
Figure 1.2: Long-Distance Telephone Poles
Since the cost of the wires went up as the length of the wire
increased, these longer connections between offices were quite
expensive to install and maintain, and they were scarce. So in the
early days of telephones, local calls were generally quite inexpensive.
But long-distance calls were more expensive and they were
charged by the minute. This made sense because each minute
you talked on a long-distance call, your use of the long-distance
wires meant no one else could use them. The telephone companies
wanted you to keep your calls short so their long-distance
lines would be available for other customers.
When telephone companies started using fiber optic, more advanced
techniques were used to carry many simultaneous longdistance
conversations on a single fiber. When you look at an old
4 CHAPTER 1. INTRODUCTION
photo and see lots of wires on a single pole, it generally means
they were telephone wires and not used to carry electricity.
1.2 Computers Communicate Differently
When humans talk on the phone, they make a call, talk for a
while, and then hang up. Statistically, most of the time, humans
are not talking on the phone. At least they weren�t before everyone
had smartphones. But computers, including the applications
on your smartphone, communicate differently than humans
do. Sometimes computers send short messages to check if another
computer is available. Computers sometimes send mediumsized
information like a single picture or a long email message.
And sometimes computers send a lot of information like a whole
movie or a piece of software to install that might take minutes or
even hours to download. So messages between computers can
be short, medium, or long.
In the earliest days of connecting computers to one another, pairs
of computers were connected with wires. The simplest way to
send data from one computer to another was to line up the outgoing
messages in a queue and send the messages one after
another as fast as the computers and the wires could carry the
data. Each message would wait for its turn until the messages
ahead of it were sent, and then it would get its chance to be sent
across the connection.
When the computers were in the same building, the building
owner could run wires to connect them. If the computers were
in the same town, the owners of the computers generally had
to lease wires from the telephone companies to connect their
computers. They often would have the phone company connect
the wires together in their central office so that it was not
necessary for one computer to �dial� the other computer to
send data. These leased lines were convenient for computer
communications because they were �always on�, but they were
also quite expensive because they were used 24 hours a day.
When the computers were even farther away, in different cities,
the leased lines were extended using the longer wires connecting
the central offices. Since there were so few wires between
central offices, these long-distance leased lines were quite expensive
and their cost increased dramatically as the length of the
leased line increased. But if you had enough money, you could
lease direct connections between your computers so they could
1.3. EARLY WIDE AREA STORE-AND-FORWARD NETWORKS 5
exchange data. This worked pretty well as long as you were only
using one brand of computers, because each computer company
had their own way of using telephone wires to connect their computers
together and send data.
1.3 Early Wide Area Store-and-Forward
Networks
In the 1970s and 1980s, people working at universities around
the world wanted to send each other data and messages using
these computer-to-computer connections. Since the cost for each
connection was so high and increased with distance, computers
generally only had connections to other nearby computers. But
if the computer that you were connected to was connected to
another computer and that computer in turn was connected to
another computer, and so on, you could send a message a long
distance as long as each of the computers along the route of the
message agreed to store and forward your message.
Figure 1.3: Store-and-Forward Networks
Over time, with relatively few connections you could send data
long distances across a patchwork of network connections as long
as you were patient. Along the way, after your message reached
one computer, it would have to wait until its turn came to be
sent to the next computer along the route. A message would
arrive at an intermediate computer, be stored for a while (perhaps
hours, depending on traffic), and then be forwarded one more
connection (or �hop�).
6 CHAPTER 1. INTRODUCTION
Sending entire messages one at a time this way, a message might
take minutes, hours, or even days to arrive at its ultimate destination,
depending on the traffic at each of the hops. But even if
it took a few hours for an email message to find its way from one
part of the country to another, this was still much quicker and
easier than sending a letter or postcard.
1.4 Packets and Routers
The most important innovation that allowed messages to move
more quickly across a multi-hop network was to break each message
into small fragments and send each fragment individually.
In networking terms, these pieces of messages are called �packets�.
The idea of breaking a message into packets was pioneered
in the 1960s, but it was not widely used until the 1980s because it
required more computing power and more sophisticated networking
software.
When messages are broken into packets and each packet is sent
separately, if a short message was sent after a large message
had begun, the short message did not have to wait until the entire
long message was finished. The first packet of the short message
only had to wait for the current packet of the large message to be
finished. The system alternated sending packets from the long
and short messages until after a while the short message was
completely sent and the long message resumed making full use
of the network connection.
Breaking the message into packets also greatly reduced the
amount of storage needed in the intermediate computers because
instead of needing to store an entire message for as long
as a few hours, the intermediate computer only needed to store
a few packets for a few seconds while the packets waited for
their turns on the outbound link.
As networks moved away from the store-and-forward approach,
they started to include special-purpose computers that specialized
in moving packets. These were initially called �Interface Message
Processors� or �IMPs� because they acted as the interface
between general-purpose computers and the rest of the network.
Later these computers dedicated to communications were called
�routers� because their purpose was to route the packets they
received towards their ultimate destination.
By building routers that specialized in moving packets across multiple
hops, it became simpler to connect computers from multiple
1.5. ADDRESSING AND PACKETS 7
Figure 1.4: Sending Packets
vendors to the same network. To connect any computer to the
network, now all you needed to do was connect it to one router
and then the rest of the communication details were handled by
the other routers.
When multiple computers at one location were connected together
in a �Local Area Network� (or LAN) using physical wiring,
you would connect a router to the local area network. By sending
data through the router, all the computers on the local area
network could send data across the �Wide Area Network� (or
WAN).
1.5 Addressing and Packets
In the early store-and-forward networks it was important to know
the source and destination computers for every message. Each
computer was given a unique name or number that was called
the �address� of the computer. To send a message to another
computer, you needed to add the source and destination address
to the message before sending the message along its way. By
having a source and destination address in each message, the
computers that stored and forwarded the message would be able
to pick the best path for the message if more than one path was
8 CHAPTER 1. INTRODUCTION
available.
When a long message was split into much smaller packets and
each packet was sent individually, the source and destination addresses
had to be added to each packet, so that routers could
choose the best path to forward each packet of the message. In
addition to the source and destination addresses, it was also necessary
to add data to each packet indicating the �offset� or position
of the packet in the overall message so that the receiving
computer could put the packets back together in the right order
to reconstruct the original message.
1.6 Putting It All Together
So when we combine all this together we can understand the basic
operation of today�s Internet. We have specialized computers
called �routers� that know how to route packets along a path from
a source to a destination. Each packet will pass through multiple
routers during its journey from the source computer to the destination
computer.
Even though the packets may be part of a larger message, the
routers forward each packet separately based on its source and
destination addresses. Different packets from the same message
may take different routes from the source to the destination. And
sometimes packets even arrive out of order; a later packet might
arrive before an earlier packet, perhaps because of a data �traffic
jam�. Each packet contains an �offset� from the beginning of the
message so that the destination computer can reassemble the
packets in the correct order to reconstruct the original message.
By creating a network using multiple short hops, the overall cost
of communicating across a large geographical area could be
spread across a large number of connecting groups and individuals.
Normally, packets would find the shortest path between
the source and destination, but if a link on that path was an
overloaded or broken, the routers could cooperate and reroute
traffic to take slightly longer paths that would get packets from a
source to a destination as quickly as possible.
The core of the Internet is a set of cooperating routers that move
packets from many sources to many destinations at the same
time. Each computer or local area network is connected to a
router that forwards the traffic from its location to the various destinations
on the Internet. A router might handle data from a single
1.7. GLOSSARY 9
Figure 1.5: Connecting Around the World
computer like a smartphone, from several computers in the same
building, or from thousands of computers connected to a university
campus network. The term �Internet� comes from the idea of
�internetworking�, which captures the idea of connecting many
networks together. Our computers connect to local networks and
the Internet connects the local networks together so all of our
computers can talk to each other.
1.7 Glossary
address: A number that is assigned to a computer so that messages
can be routed to the computer.
hop: A single physical network connection. A packet on the Internet
will typically make several �hops� to get from its source
computer to its destination.
LAN: Local Area Network. A network covering an area that is
limited by the ability for an organization to run wires or the power
of a radio transmitter.
leased line: An �always up� connection that an organization
leased from a telephone company or other utility to send data
across longer distances.
10 CHAPTER 1. INTRODUCTION
operator (telephone): A person who works for a telephone company
and helps people make telephone calls.
packet: A limited-size fragment of a large message. Large messages
or files are split into many packets and sent across the
Internet. The typical maximum packet size is between 1000 and
3000 characters.
router: A specialized computer that is designed to receive incoming
packets on many links and quickly forward the packets on the
best outbound link to speed the packet to its destination.
store-and-forward network: A network where data is sent
from one computer to another with the message being stored
for relatively long periods of time in an intermediate computer
waiting for an outbound network connection to become available.
WAN: Wide Area Network. A network that covers longer distances,
up to sending data completely around the world. A WAN
is generally constructed using communication links owned and
managed by a number of different organizations.
1.8 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. What did early telephone operators do?
a) Maintained cell phone towers
b) Connected pairs of wires to allow people to talk
c) Installed copper wire between cities
d) Sorted packets as they went to the correct destination
2. What is a leased line?
a) A boundary between leased and owned telephone equipment
b) A connection between a keyboard and monitor
c) A wire that ran from one phone company office to another
d) An �always on� telephone connection
3. How long might a message be stored in an intermediate computer
for a store-and-forward network?
1.8. QUESTIONS 11
a) less than a second
b) no more than four seconds
c) less than a minute
d) possibly as long as several hours
4. What is a packet?
a) A technique for wrapping items for shipping
b) A small box used for storage
c) A portion of a larger message that is sent across a network
d) The amount of data that could be stored on an early punched
card
5. Which of these is most like a router?
a) A mail sorting facility
b) A refrigerator
c) A high-speed train
d) An undersea telecommunications cable
6. What was the name given to early network routers?
a) Interfaith Message Processors
b) Internet Motion Perceptrons
c) Instant Message Programs
d) Interface Message Processors
7. In addition to breaking large messages into smaller segments
to be sent, what else was needed to properly route
each message segment?
a) A source and destination address on each message segment
b) An ID and password for each message segment
c) A small battery to maintain the storage for each message
segment
d) A small tracking unit like a GPS to find lost messages
8. Why is it virtually free to send messages around the world
using the Internet?
a) Because governments pay for all the connections
b) Because advertising pays for all the connections
c) Because so many people share all the resources
d) Because it is illegal to charge for long-distance connections
12 CHAPTER 1. INTRODUCTION
Chapter 2
Network Architecture
To engineer and build a system as complex as the Internet, engineers
try to break a single challenging problem into a set of
smaller problems that can be solved independently and then put
back together to solve the original large problem. The engineers
who built the first internets broke the overall problem into four
basic subproblems that could be worked on independently by different
groups.
Transport
Application
Internetwork
Link
Figure 2.1: The Four-Layer TCP/IP Model
They gave these four areas of engineering the following names:
(1) Link, (2) Internetwork, (3) Transport, and (4) Application. We
visualize these different areas as layers stacked on top of each
other, with the Link layer on the bottom and the Application layer
on the top. The Link layer deals with the wired or wireless connection
from your computer to the local area network and the
Application layer is what we as end users interact with. A web
13
14 CHAPTER 2. NETWORK ARCHITECTURE
browser is one example of an application in this Internet architecture.
We informally refer to this model as the �TCP/IP model� in reference
to the Transport Control Protocol (TCP) used to implement
the Transport layer and Internet Protocol (IP) used to implement
the Internetwork layer.
We will take a quick look at each of the layers, starting from the
�bottom� of the stack.
2.1 The Link Layer
The Link layer is responsible for connecting your computer to its
local network and moving the data across a single hop. The
most common Link layer technology today is wireless networking.
When you are using a wireless device, the device is only
sending data a limited distance. A smartphone communicates
with a tower that is a few kilometers away. If you are using your
smartphone on a train, it needs to switch to a new tower every
few minutes when the train is moving. A laptop that is connected
to a WiFi network is usually communicating with a base station
within 200 meters. A desktop computer that is connected using
a wired connection is usually using a cable that is 100 meters
long or shorter. Link layer technologies are often shared amongst
multiple computers at the same location.
The Link layer needs to solve two basic problems when dealing
with these shared local area networks. The first problem is how
to encode and send data across the link. If the link is wireless,
engineers must agree on which radio frequencies are to be used
to transmit data and how the digital data is to be encoded in the
radio signal. For wired connections, they must agree on what
voltage to use on the wire and how fast to send the bits across
the wire. For Link layer technologies that use fiber optics, they
must agree on the frequencies of light to be used and how fast to
send the data.
In addition to agreeing on how to send data using a shared
medium such as a wireless network, they also need to agree on
how to cooperate with other computers that might want to send
data at the same time. If all the computers on the network tried
to transmit whenever they had data to send, their messages
would collide. The result would be chaos, and receiving stations
would only receive noise. So we need to find a fair way to allow
each station to wait its turn to use the shared network.
2.1. THE LINK LAYER 15
The idea of breaking a large message into packets and then sending
each packet separately makes this sharing easier. If only one
computer wants to send data, it will send its packets one right
after another and move its data across the network as quickly as
it can. But if three computers want to send data at the same time,
each computer will send one packet and then wait while the other
two computers send packets. After each of the other computers
sends a packet, the first computer will send its next packet. This
way the computers are sharing access to the network in a fair
way.
But how does a computer know if other computers want to send
data at the same time? Engineers designed an ingenious method
to solve this problem called �Carrier Sense Multiple Access with
Collision Detection�, or CSMA/CD. It is a long name for a simple
and elegant concept. When your computer wants to send data,
it first listens to see if another computer is already sending data
on the network (Carrier Sense). If no other computer is sending
data, your computer starts sending its data. As your computer is
sending data it also listens to see if it can receive its own data. If
your computer receives its own data, it knows that the channel
is still clear and continues transmitting. But if two computers
started sending at about the same time, the data collides, and
your computer does not receive its own data. When a collision is
detected, both computers stop transmitting, wait a bit, and retry
the transmission. The two computers that collided wait different
lengths of time to retry their transmissions to reduce the chances
of a second collision.
When your computer finishes sending a packet of data, it pauses
to give other computers that have been waiting a chance to send
data. If another computer senses that your computer has stopped
sending data (Carrier Sense) and starts sending its own packet,
your computer will detect the other computer�s use of the network
and wait until that computer�s packet is complete before
attempting to send its next packet.
This simple mechanism works well when only one computer
wants to send data. It also works well when many computers
want to send data at the same time. When only one computer is
sending data, that computer can make good use of the shared
network by sending packets one after another, and when many
computers want to use the shared network at the same time,
each computer gets a fair share of the link.
Some link layers, like a cellular connection for a smartphone, a
WiFi connection, or a satellite or cable modem, are shared con16
CHAPTER 2. NETWORK ARCHITECTURE
Figure 2.2: Carrier Sense/Collision Detection
nections and need techniques like CSMA/CD to insure fair access
to the many different computers connected to the network. Other
link layers like fiber optic cables and leased lines are generally
not shared and are used for connections between routers. These
non-shared connections are still part of the Link layer.
The engineers working on Link layer technologies focus solving
the issues so computers can transmit data across a single link
that ranges in distance from a few meters to as long as hundreds
of kilometers. But to move data greater distances, we need to
send our packets through multiple routers connected by multiple
link layers. Each time our packet passes through another link
layer from one router to another we call it a �hop�. To send data
halfway around the world, it will pass through about 20 routers,
or make 20 �hops�.
2.2 The Internetwork Layer (IP)
Once your packet destined for the Internet makes it across the
first link, it will be in a router. Your packet has a source address
2.2. THE INTERNETWORK LAYER (IP) 17
and destination address and the router needs to look at the destination
address to figure out how to best move your packet towards
its destination. With each router handling packets destined
for any of many billions of destination computers, it�s not possible
for every router to know the exact location and best route to every
possible destination computer. So the router makes its best
guess as to how to get your packet closer to its destination.
Each of the other routers along the way also does its best to get
your packet closer to the destination computer. As your packet
gets closer to its final destination, the routers have a better idea
of exactly where your packet needs to go. When the packet
reaches the last link in its journey, the link layer knows exactly
where to send your packet.
We use a similar approach to route ourselves when going on holiday.
A holiday trip also has many hops. Perhaps the first hop is
driving your car or taking a cab or bus to a train station. Then
you take a local train from your small town to a larger city. In the
larger city you take a long-distance train to a large city in another
country. Then you take another local train to the small village
where you will stay for your holiday. When you get off the train,
you take a bus, and when you get off the bus, you walk to your
hotel.
If you were on the train between the two large cities and you
asked the conductor the exact location of your hotel in the small
village, the conductor would not know. The conductor only knows
how to get you closer to your destination, and while you are on
the long-distance train that is all that matters. When you get on
the bus at your destination village, you can ask the bus driver
which stop is closest to your hotel. And when you get off the bus
at the right bus stop, you can probably ask a person on the street
where to find the hotel and get an exact answer.
The further you are from your destination, the less you need to
know the exact details of how to get there. When you are far
away, all you need to know is how to get �closer� to your destination.
Routers on the Internet work the same way. Only the routers
that are closest to the destination computer know the exact path
to that computer. All of the routers in the middle of the journey
work to get your message closer to its destination.
But just like when you are traveling, unexpected problems or delays
can come up that require a change in plans as your packets
are sent across the network.
Routers exchange special messages to inform each other about
18 CHAPTER 2. NETWORK ARCHITECTURE
Figure 2.3: A Multi-Step Trip
any kind of traffic delay or network outage so that packets can
be switched from a route that is no longer working to a different
route. The routers that make up the core of the Internet are smart
and adapt quickly to both small and large outages or failures of
network connections. Sometimes a connection slows down because
it is overloaded. Other times a connection is physically broken
when a construction crew mistakenly digs up a buried wire
and cuts it. Sometimes there is a natural disaster like a hurricane
or typhoon that shuts down the routers and links in a large geographical
area. The routers quickly detect these outages and
reroute around them if possible.
But sometimes things go wrong and packets are lost. Dealing with
lost packets is the reason for the next layer in our architecture.
2.3 The Transport Layer (TCP)
The Internetwork layer is both simple and complex. It looks at
a packet�s destination address and finds a path across multiple
network hops to deliver the packet to the destination computer.
2.3. THE TRANSPORT LAYER (TCP) 19
But sometimes these packets get lost or badly delayed. Other
times the packets arrive at their destination out of order because
a later packet found a quicker path through the network than an
earlier packet. Each packet contains the source computer�s address,
the destination computer�s address, and an offset of where
this packet �fits� relative to the beginning of the message. Knowing
the offset of each packet from the beginning of the message
and the length of the packet, the destination computer can reconstruct
the original message even if the packets were received out
of order.
As the destination computer reconstructs the message and delivers
it to the receiving application, it periodically sends an acknowledgement
back to the source computer indicating how much of
the message it has received and reconstructed. But if the destination
computer finds that parts of the reconstructed message
are missing, this probably means that these packets were lost
or badly delayed. After waiting a bit, the destination computer
sends a request to the source computer to resend the data that
seems to be missing.
The sending computer must store a copy of the parts of the original
message that have been sent until the destination computer
acknowledges successful receipt of the packets. Once the source
computer receives the acknowledgment of successful receipt of
a portion of the message, it can discard the data that has been
acknowledged and send some more data.
The amount of data that the source computer sends before waiting
for an acknowledgement is called the �window size�. If the
window size is too small, the data transmission is slowed because
the source computer is always waiting for acknowledgments. If
the source computer sends too much data before waiting for an
acknowledgment, it can unintentionally cause traffic problems by
overloading routers or long-distance communication lines. This
problem is solved by keeping the window size small at the beginning
and timing how long it takes to receive the first acknowledgements.
If the acknowledgments come back quickly, the source
computer slowly increases the window size and if the acknowledgements
come back slowly, the source computer keeps the
window size small so as not to overload the network. Just like at
the Link layer, a little courtesy on the Internet goes a long way
toward ensuring good use of the shared network infrastructure.
This strategy means that when the network has high-speed connections
and is lightly loaded the data will be sent quickly, and
if the network is heavily loaded or has slow connections the data
20 CHAPTER 2. NETWORK ARCHITECTURE
will be slowed down to match the limitations of the network connections
between the source and destination computers.
2.4 The Application Layer
The Link, Internetwork, and Transport layers work together to
quickly and reliably move data between two computers across
a shared network of networks. With this capability to move data
reliably, the next question is what networked applications will be
built to make use of these network connections.
When the first widely used Internet came into being in the mid-
1980s, the first networked applications allowed users to log in to
remote computers, transfer files between computers, send mail
between computers, and even do real-time text chats between
computers.
In the early 1990s, as the Internet came to more people and computers�
abilities to handle images improved, the World Wide Web
application was developed by scientists at the CERN high-energy
physics facility. The web was focused on reading and editing networked
hypertext documents with images. Today the web is the
most common network application in use around the world. But
all the other older Internet applications are still in wide use.
Each application is generally broken into two halves. One half of
the application is called the �server�. It runs on the destination
computer and waits for incoming networking connections. The
other half of the application is called the �client� and runs on the
source computer. When you are browsing the web using software
like Firefox, Chrome, or Internet Explorer, you are running a �web
client� application which is making connections to web servers
and displaying the pages and documents stored on those web
servers. The Uniform Resource Locators (URLs) that your web
browser shows in its address bar are the web servers that your
client is contacting to retrieve documents for you to view.
When we develop the server half and the client half of a networked
application, we must also define an �application protocol�
that describes how the two halves of the application will exchange
messages over the network. The protocols used for each application
are quite different and specialized to meet the needs of the
particular application. Later we will explore some of these Application
layer protocols.
2.5. STACKING THE LAYERS 21
2.5 Stacking the Layers
We usually show the four different layers (Link, Internetwork,
Transport, and Application) stacked on top of each other with
the Application layer at the top and the Link layer at the bottom.
The reason we show them this way is because each layer makes
use of the layers above and below it to achieve networked
communications.
All four layers run in your computer where you run the client application
(like a browser), and all four layers also run in the destination
computer where the application server is running. You
as the end user interact with the applications that make up the
top layer of the stack, and the bottom layer represents the WiFi,
cellular, or wired connection between your computer and the rest
of the Internet.
The routers that forward your packets from one to another to
move your packets towards their destination have no understanding
of either the Transport or Application layers. Routers operate
at the Internetwork and Link layers. The source and destination
addresses at the Internetwork layer are all that is needed for
routers to move your packets across the series of links (hops) to
get them to the destination. The Transport and Application layers
only come into play after the Internetwork layer delivers your
packets to the destination computer.
If you wanted to write your own networked application, you would
likely only talk to the Transport layer and be completely unconcerned
about the Internetwork and Link layers. They are essential
to the function of the Transport layer, but as you write your
program, you do not need to be aware of any of the lower-layer
details. The layered network model makes it simpler to write networked
applications because so many of the complex details of
moving data from one computer to another can be ignored.
Up next, we will talk about these four layers in more detail.
2.6 Glossary
client: In a networked application, the client application is the
one that requests services or initiates connections.
fiber optic: A data transmission technology that encodes data
using light and sends the light down a very long strand of thin
22 CHAPTER 2. NETWORK ARCHITECTURE
glass or plastic. Fiber optic connections are fast and can cover
very long distances.
offset: The relative position of a packet within an overall message
or stream of data.
server: In a networked application, the server application is the
one that responds to requests for services or waits for incoming
connections.
window size: The amount of data that the sending computer is
allowed to send before waiting for an acknowledgement.
2.7 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. Why do engineers use a �model� to organize their approach
to solving a large and complex problem?
a) Because it allows them to build something small and test it
in a wind tunnel
b) Because talking about a model delays the actual start of the
hard work
c) Because they can break a problem down into a set of smaller
problems that can be solved independently
d) Because it helps in developing marketing materials
2. Which is the top layer of the network model used by TCP/IP
networks?
a) Application
b) Transport
c) Internetwork
d) Link
3. Which of the layers concerns itself with getting a packet of
data across a single physical connection?
a) Application
b) Transport
c) Internetwork
d) Link
2.7. QUESTIONS 23
4. What does CSMA/CD stand for?
a) Carrier Sense Multiple Access with Collision Detection
b) Collision Sense Media Access with Continuous Direction
c) Correlated Space Media Allocation with Constant Division
d) Constant State Multiple Address Channel Divison
5. What is the goal of the Internetwork layer?
a) Insure that no data is lost while enroute
b) Get a packet of data moved across multiple networks from
its source to its destination
c) Make sure that only logged-in users can use the Internet
d) Insure than WiFi is fairly shared across multiple computers
6. In addition to the data, source, and destination addresses,
what else is needed to make sure that a message can be
reassembled when it reaches its destination?
a) An offset of where the packet belongs relative to the beginning
of the message
b) A location to send the data to if the destination computer is
down
c) A compressed and uncompressed version of the data in the
packet
d) The GPS coordinates of the destination computer
7. What is �window size�?
a) The sum of the length and width of a packet
b) The maximum size of a single packet
c) The maximum number of packets that can make up a message
d) The maximum amount of data a computer can send before
receiving an acknowledgement
8. In a typical networked client/server application, where does
the client application run?
a) On your laptop, desktop, or mobile computer
b) On a wireless access point
c) On the closest router
24 CHAPTER 2. NETWORK ARCHITECTURE
d) In an undersea fiber optic cable
9. What does URL stand for?
a) Universal Routing Linkage
b) Uniform Retransmission Logic
c) Uniform Resource Locator
d) Unified Recovery List
Chapter 3
Link Layer
The lowest layer of our Internet Architecture is the Link layer. We
call it the �lowest layer� because it is closest to the physical network
media. Often the Link layer transmits data using a wire, a
fiber optic cable, or a radio signal. A key element of the Link layer
is that usually data can only be transmitted part of the way from
the source computer to the destination computer. Wired Ethernet,
WiFi, and the cellular phone network are examples of link
layers that can transmit data about a kilometer. Fiber optic cables,
particularly those under the oceans, can transmit data up to
thousands of kilometers. Satellite links can also send data over
long distances.
Transport
Application
Internetwork
Link
Figure 3.1: The Link Layer
Regardless of the distance we can send the data, it is still traveling
over a single link, and to reach the ultimate destination com-
25
26 CHAPTER 3. LINK LAYER
puter requires forwarding packets across multiple links. In this
section we will look at how one of the most common link layers
functions in some detail. WiFi is a great way to look at many
issues that must be solved at the link layer.1
3.1 Sharing the Air
When your laptop or phone is using WiFi to connect to the Internet,
it is sending and receiving data with a small, low-powered radio.
The radio in your computer can only send data about 300 meters,
so your computer sends your packets to the router in your home,
which forwards the packets using a link to the rest of the Internet.
Sometimes we call the first router that handles your computer�s
packets the �base station� or �gateway�.
All computers that are close enough to the base station with their
radios turned on receive all of the packets the base station transmits,
regardless of which computer the packet is supposed to be
sent to. They also �hear� all the packets sent by every other
nearby computer. So your computer needs a way to know which
packets to treat as its own and which packets are being sent to
other computers and can be safely ignored.
An interesting side effect of the fact that all the computers within
range can hear all packets is that a rogue computer could also be
listening to and capturing your packets, perhaps getting ahold of
important data like bank account numbers or passwords to online
services. We will come back to the issue of protecting your data
from prying eyes and ears in a later section.
Every WiFi radio in every device that is ever built is given a unique
serial number at the time it is manufactured. This means that
each of the computers using WiFi has its own serial number, and
the radio in the gateway also has a serial number. You can usually
go into a settings screen on your device and look up the serial
number for the WiFi radio in your device. It is generally shown in
the following form:
0f:2a:b3:1f:b3:1a
This is just a representation of a 48-bit serial number for your
WiFi radio. It is also called the �Media Access Control� or �MAC�
1We simplify some of the technical detail in these descriptions to make them
easier to understand.
3.1. SHARING THE AIR 27
address. A MAC address is like a �from� or �to� address on a
postcard. Every packet (radio postcard) sent across the WiFi has
a source and destination address, so all of the computers know
which messages are theirs.
When you turn on your computer and connect to a WiFi network,
your computer needs to figure out which of the MAC addresses
on the WiFi can be used to send packets to the router. When you
move from one physical location to another, your computer will
be talking to different gateways and each of those gateways will
have a different serial number. So when you first connect to a
new WiFi, your computer must discover the MAC address for the
gateway of that particular WiFi.
To do this, your computer sends a special message to a broadcast
address, effectively asking the question, �Who is in charge of this
WiFi?� Since your computer knows it is not the gateway itself,
it sends a broadcast message with its own serial number as the
�from� address and the broadcast address as the �to� address to
ask if there are any gateways present on the WiFi network.
From: 0f:2a:b3:1f:b3:1a
To: ff:ff:ff:ff:ff:ff
Data: Who is the MAC-Gateway
for this network?
If there is a gateway on the network, the gateway sends a message
containing its serial number back to your computer.
From: 98:2f:4e:78:c1:b4
To: 0f:2a:b3:1f:b3:1a
Data: I am the gateway
Welcome to my network
If there are no replies, your computer waits a few seconds and
then assumes there is no gateway for this network. When there
is no gateway, your computer might show a different WiFi icon or
not show the WiFi icon at all. Sometimes there can be more than
one gateway, but we will ignore that for a while because it is a
little complex and not very common.
Once your computer receives a message with the MAC address
of the gateway, it can use that address to send packets that it
wants the gateway to forward to the Internet. From that point on,
all of your computer�s packets have the actual serial number of
28 CHAPTER 3. LINK LAYER
the destination. You want to use the broadcast address as little as
possible because every computer connected to the WiFi receives
and processes any messages sent to the broadcast address to
make sure the messages were not intended for them.
3.2 Courtesy and Coordination
Because many computers are sharing the same radio frequencies,
it�s important to coordinate how they send data. When there�s a
crowd of people in a room, they can�t all talk at the same time or
everything will be garbled. The same thing happens when multiple
WiFi radios transmit at the same time on the same frequency.
So we need some way to coordinate all the radios to make best
use of the shared frequencies. We will look at the basics of technical
approaches to avoiding lost data due to transmission �collisions�.
The first technique is called �Carrier Sense�. The technique is
to first listen for a transmission, and if there is already a transmission
in progress, wait until the transmission finishes. It might
seem like you could wait for a long time, but since all messages
are broken into packets, usually your computer only has to wait
for the computer currently sending data to finish a packet, after
which your computer gets its chance to send data.
If your computer�s WiFi radio listens for data and hears silence, it
can begin transmitting. But what if another computer�s WiFi radio
that wants to send a packet listened to and heard the same silence
and decided to start transmitting at exactly the same time?
If two or more WiFi radios start transmitting at the same time, all
of the data is corrupted and both packets are lost. So once your
WiFi radio starts sending a packet it is important for it to listen to
make sure it can receive its own data. If it is not receiving the
same thing that it is sending, your WiFi radio assumes that a collision
has happened (this is called Collision Detection) and stops
transmitting, since it knows that no data will be received by the
destination WiFi radio.
We humans do a similar thing in a room full of people. When two
people start talking at the same time, they are good at noticing
that another person is talking and quickly stop talking. But the
problem is how to restart the conversation. After a long pause it
is common that both people start talking at the exact same time
again. This can happen over and over and each person says �No,
3.3. COORDINATION IN OTHER LINK LAYERS 29
you� repeatedly to attempt to figure out how to get the conversation
restarted. It can be quite comical at times.
The WiFi radios in two computers that send colliding packets are
able to solve this problem much better than people can solve
the problem. When the WiFi radios detect a collision or garbled
transmission, they compute a random amount of time to wait
before retrying the transmission. The rules for computing the
random wait are set up to make sure the two colliding stations
pick different amounts of time to wait before attempting to retransmit
the packet.
The formal name for the listen, transmit, listen, and wait and retry
if necessary is called �Carrier Sense Multiple Access with Collision
Detection� or CSMA/CD.
It might sound a little chaotic to just �give it a try� and then �give
it another try� if your transmission collides with another station�s
transmission. But in practice it works well. There is a whole category
of link layers that use this basic pattern of listen, transmit,
listen, and optionally retry. Wired Ethernet, cellular telephone
data, and even Short Message Service (SMS/Texting) all use this
�try then retry� approach.
3.3 Coordination in Other Link Layers
Sometimes when a link layer has many transmitting stations and
needs to operate at near 100% efficiency for long periods of time,
the design takes a different approach. In this approach, there is a
�token� that indicates when each station is given the opportunity
to transmit data. Stations cannot start a transmission unless they
have the token. Instead of listening for �silence� and jumping in,
they must wait for their turn to come around.
When a station receives the token and has a packet to send, it
sends the packet. Once the packet has been sent, the station
gives up the token and waits until the token comes back to it. If
none of the stations have any data to send, the token is moved
from one computer to the next computer as quickly as possible.
A group of people sitting around a meeting could communicate
without ever interrupting each other by having a small ball that
they pass around in a circle and only allowing the person who has
the ball to speak. When you get the ball and have something to
say you talk for a short period (transmit a packet of words) and
then pass the ball on.
30 CHAPTER 3. LINK LAYER
Figure 3.2: Communicating with a Token
The �try then retry� CSMA/CD approach works very well when
there is no data or when low or moderate levels of data are being
sent. But on a token-style network, if there is no data being sent
and you want to send a packet, you still have to wait for a while
before you receive the token and can start transmitting. When
you finish your packet you have to wait until the token comes
back before you can send the next packet. If you are the only
station that wants to send data, you spend a good bit of time
waiting for the token to come back to you after passing through
all of the other stations.
The token approach is best suited when using a link medium
such as as a satellite link or a undersea fiber optic link where
it might take too long or be too costly to detect a collision. The
CSMA/CD (listen-try) is best suited when the medium is inexpensive,
shorter distance, and there are a lot of stations sharing the
medium that only send data in short bursts. So that is why WiFi
(and CSMA/CD) is so effective for providing network access in a
coffee shop, home, or room in a school.
3.4 Summary
So now we have looked at the �lowest� layer in our four-layer
architecture. And we have only taken a simple look at how the
3.5. GLOSSARY 31
Link layer works. There are many other details that must be
designed into a link layer like connection distance, voltage, frequency,
speed, and many others.
A key benefit of the layered architecture is that engineers who design
and build Link layer technologies can ignore all of the issues
that are handled by the layers above the Link layer. This allows
them to focus on building the best possible solution to moving
data across a single �hop�. Modern-day link layers like WiFi, satellite,
cable modems, Ethernet, and cellular technology are very
well developed. Data moves so quickly and seamlessly that once
we get our connection we rarely have to worry about the Link
layer. It just works.
3.5 Glossary
base station: Another word for the first router that handles your
packets as they are forwarded to the Internet.
broadcast: Sending a packet in a way that all the stations connected
to a local area network will receive the packet.
gateway: A router that connects a local area network to a wider
area network such as the Internet. Computers that want to send
data outside the local network must send their packets to the
gateway for forwarding.
MAC Address: An address that is assigned to a piece of network
hardware when the device is manufactured.
token: A technique to allow many computers to share the same
physical media without collisions. Each computer must wait until
it has received the token before it can send data.
3.6 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. When using a WiFi network to talk to the Internet, where
does your computer send its packets?
a) A gateway
b) A satellite
c) A cell phone tower
32 CHAPTER 3. LINK LAYER
d) The Internet Central Office
2. How is the link/physical address for a network device assigned?
a) By the cell tower
b) By the Internet Assignment Numbers Authority (IANA)
c) By the manufacturer of the link equipment
d) By the government
3. Which of these is a link address?
a) 0f:2a:b3:1f:b3:1a
b) 192.168.3.14
c) www.khanacademy.com
d) @drchuck
4. How does your computer find the gateway on a WiFi network?
a) It has a gateway address installed by the manufacturer
b) It broadcasts a request for the address of the gateway
c) It repeatedly sends a message to all possible gateway addresses
until it finds one that works
d) The user must enter the gateway address by hand
5. When your computer wants to send data across WiFi, what
is the first thing it must do?
a) Listen to see if other computers are sending data
b) Just start sending the data
c) Send a message to the gateway asking for permission to
transmit
d) Wait until informed that it is your turn to transmit
6. What does a WiFi-connected workstation do when it tries to
send data and senses a collision has happened?
a) Keep sending the message so part of the message makes it
through
b) Wait until told by the gateway that the collision is over
c) Immediately restart transmitting the message at the beginning
3.6. QUESTIONS 33
d) Stop transmitting and wait a random amount of time before
restarting
7. When a station wants to send data across a �token�-style
network, what is the first thing it must do?
a) Listen to see if other computers are sending data
b) Just start sending the data
c) Send a message to the gateway asking for permission to
transmit
d) Wait until informed that it is your turn to transmit
34 CHAPTER 3. LINK LAYER
Chapter 4
Internetworking Layer
(IP)
Now that we can move data across a single link, it�s time to figure
out how to move it across the country or around the world.
To send data from your computer to any of a billion destinations,
the data needs to move across multiple hops and across multiple
networks. When you travel from your home to a distant destination,
you might walk from your home to a bus stop, take a train to
the city, take another train to the airport, take a plane to a different
airport, take a taxi into the city, then take a train to a smaller
town, a bus to an even smaller town, and finally walk from the bus
stop to your hotel. A packet also needs to take multiple forms of
transportation to reach its destination. For a packet taking its
�trip� to another country, the �walk�, �bus�, �train�, and �plane�
can be thought of as different link layers like WiFi, Ethernet, fiber
optic, and satellite.
At each point during the trip, you (or your packet) are being transported
using a shared medium. There might be hundreds of other
people on the same bus, train, or plane, but your trip is different
from that of every other traveller because of the decisions that
you make at the end of each of your �hops�. For instance, when
you arrive at a train station, you might get off one train, then walk
through the station and select a particular outbound train to continue
your journey. Travellers with different starting points and
destinations make a different series of choices. All of the choices
you make during your trip result in you following a series of links
(or hops) along a route that takes you from your starting point to
your destination.
As your packet travels from its starting point to its destination,
35
36 CHAPTER 4. INTERNETWORKING LAYER (IP)
Figure 4.1: Travelling Packets
it also passes through a number of �stations� where a decision
is made as to which output link your packet will be forwarded
on. For packets, we call these places �routers�. Like train stations,
routers have many incoming and outgoing links. Some
links may be fiber optic, others might be satellite, and still others
might be wireless. The job of the router is to make sure packets
move through the router and end up on the correct outbound link
layer. A typical packet passes through from five to 20 routers as
it moves from its source to its destination.
But unlike a train station where you need to look at displays to
figure out the next train you need to take, the router looks at the
destination address to decide which outbound link your packet
needs to take. It is as if a train station employee met every single
person getting off an inbound train, asked them where they were
headed, and escorted them to their next train. If you were a
packet, you would never have to look at another screen with a
list of train departures and tracks!
The router is able to quickly determine the outbound link for your
packet because every single packet is marked with its ultimate
destination address. This is called the Internet Protocol Address,
or IP Address for short. We carefully construct IP addresses to
4.1. INTERNET PROTOCOL (IP) ADDRESSES 37
make the router�s job of forwarding packets as efficient as possible.
4.1 Internet Protocol (IP) Addresses
In the previous section where we talked about Link layer addresses,
we said that link addresses were assigned when the
hardware was manufactured and stayed the same throughout
the life of a computer. We cannot use link layer addresses to
route packets across multiple networks because there is no relationship
between a link layer address and the location where that
computer is connected to the network. With portable computers
and cell phones moving constantly, the system would need to
track each individual computer as it moved from one location to
another. And with billions of computers on the network, using
the link layer address to make routing decisions would be slow
and inefficient.
Transport
Application
Internetwork
Link
Figure 4.2: The Internetwork Layer
To make this easier, we assign another address to every computer
based on where the computer is connected to the network. There
are two different versions of IP addresses. The old (classic) IPv4
addresses consist of four numbers separated by dots like this, and
look like this:
212.78.1.25
38 CHAPTER 4. INTERNETWORKING LAYER (IP)
Each of the numbers can only be from 0 through 255. We have
so many computers connected to the Internet now that we are
running out of IPv4 addresses to assign to them. IPv6 address are
longer and look like:
2001:0db8:85a3:0042:1000:8a2e:0370:7334
For this section we will focus on the classic IPv4 addresses, but all
of the ideas apply equally to IPv4 and IPv6 addresses.
The most important thing about IP addresses is that they can be
broken into two parts.1 The first part of the two-piece address is
called the �Network Number�. If we break out an IPv4 address
into two parts, we might find the following:
Network Number: 212.78
Host Identifier: 1.25
The idea is that many computers can be connected via a single
connection to the Internet. An entire college campus, school, or
business could connect using a single network number, or only a
few network numbers. In the example above, 65,536 computers
could be connected to the network using the network number of
�212.78�. Since all of the computers appear to the rest of the
Internet on a single connection, all packets with an IP address of:
212.78.*.*
can be routed to the same location.
By using this approach of a network number and a host identifier,
routers no longer have to keep track of billions of individual computers.
Instead, they need to keep track of perhaps a million or
less different network numbers.
So when your packet arrives in a router and the router needs to
decide which outbound link to send your packet to, the router
does not have to look at the entire IP address. It only needs to
look at the first part of the address to determine the best outbound
link.
1There are many points where an IP address can be broken into �Network
Number� and �Host Identifier� - for this example, we will just split the address
in half.
4.2. HOW ROUTERS DETERMINE THE ROUTES 39
4.2 How Routers Determine the Routes
While the idea of the collapsing many IP addresses into a single
network number greatly reduces the number of individual endpoints
that a router must track to properly route packets, each
router still needs a way to learn the path from itself to each of the
network numbers it might encounter.
When a new core router is connected to the Internet, it does not
know all the routes. It may know a few preconfigured routes, but
to build a picture of how to route packets it must discover routes
as it encounters packets. When a router encounters a packet that
it does not already know how to route, it queries the routers that
are its �neighbors�. The neighboring routers that know how to
route the network number send their data back to the requesting
router. Sometimes the neighboring routers need to ask their
neighbors and so on until the route is actually found and sent
back to the requesting router.
In the simplest case, a new core router can be connected to the
Internet and slowly build a map of network numbers to outbound
links so it can properly route packets based on the IP address for
each incoming packet. We call this mapping of network numbers
to outbound links the �routing table� for a particular router.
When the Internet is running normally, each router has a relatively
complete routing table and rarely encounters a new network
number. Once a router figures out the route to a new network
number the first time it sees a packet destined for that network
number, it does not need to rediscover the route for the
network number unless something changes or goes wrong. This
means that the router does a lookup on the first packet, but then
it could route the next billion packets to that network number just
by using the information it already has in its routing tables.
4.3 When Things Get Worse and Better
Sometimes the network has problems and a router must find a
way to route data around the problems. A common problem is
that one of the outbound links fails. Perhaps someone tripped
over a wire and unplugged a fiber optic cable. At this point, the
router has a bunch of network numbers that it wants to route
out on a link that has failed. The recovery when a router loses
an outbound link is surprisingly simple. The router discards all
40 CHAPTER 4. INTERNETWORKING LAYER (IP)
of the entries in its routing table that were being routed on that
link. Then as more packets arrive for those network numbers, the
router goes through the route discovery process again, this time
asking all the neighboring routers except the ones that can no
longer be contacted due to the broken link.
Figure 4.3: Dynamic Routing
Packets are routed more slowly for a while as routing tables are rebuilt
that reflect the new network configuration, but after a while
things are humming along nicely.
This is why it is important for there to always be at least two independent
paths from a source network to a destination network in
the core of the network. If there are always at least two possible
independent routes, we say that a network is a �two-connected
network�. A two-connected network can recover from any single
link outage. In places where there are a lot of network connections,
like the east coast of the United States, the network could
lose many links without ever becoming completely disconnected.
But when you are at your home or school and have only one connection,
if that connection goes down you are disconnected completely.
At some point the broken link is repaired or a new link is brought
up, and the router wants to make best use of the new links. The
4.4. DETERMINING YOUR ROUTE 41
router is always interested in improving its routing tables, and
looks for opportunities to improve its routing tables in its spare
time. When there is a lull in communication, a router will ask a
neighboring router for all or part of its routing table. The router
looks through the neighbor�s tables and if it looks like the other
router has a better route to a particular network number, it updates
its network table to forward packets for that network number
through the link to the router that has a better route.
With these approaches to outages and the exchange of routing
table information, routers can quickly react to network outages
and reroute packets from links that are down or slow to links that
are up and/or faster. All the while, each router is talking to its
neighboring routers to find ways to improve its own routing table.
Even though there is no central source of the �best route�
from any source to any destination, the routers are good at knowing
the fastest path from a source to a destination nearly all the
time. Routers are also good at detecting and dynamically routing
packets around links that are slow or temporarily overloaded.
One of the side effects of the way routers discover the structure
of the network is that the route your packets take from the source
to the destination can change over time. You can even send one
packet immediately followed by another packet and because of
how the packets are routed, the second packet might arrive at
the destination before the first packet. We don�t ask the IP layer
to worry about the order of the packets; it already has enough to
worry about.
We pour our packets with source and destination IP addresses into
the Internet much like we would send out a bunch of letters in the
mail at the post office. The packets each find their way though
the system and arrive at their destinations.
4.4 Determining Your Route
There is no place in the Internet that knows in advance the route
your packets will take from your computer to a particular destination.
Even the routers that participate in forwarding your packets
across the Internet do not know the entire route your packet will
take. They only know which link to send your packets to so they
will get closer to their final destination.
But it turns out that most computers have a network diagnostic
tool called �traceroute� (or �tracert�, depending on the operating
42 CHAPTER 4. INTERNETWORKING LAYER (IP)
system) that allows you to trace the route between your computer
and a destination computer. Given that the route between
any two computers can change from one packet to another, when
we �trace� a route, it is only a �pretty good guess� as to the actual
route packets will take.
The traceroute command does not actually �trace� your packet
at all. It takes advantage of a feature in the IP network protocol
that was designed to avoid packets becoming �trapped� in the
network and never reaching their destination. Before we take a
look at traceroute, let�s take a quick look at how a packet might
get trapped in the network forever and how the IP protocol solves
that problem.
Remember that the information in any single router is imperfect
and is only an approximation of the best outbound link for a particular
network number, and each router has no way of knowing
what any other router will do. But what if we had three routers
with routing table entries that formed an endless loop?
141.21.*.*
141.21.*.*
141.21.*.*
141.21.*.*
141.21.*.*
141.21.*.*
141.21.*.*
141.21.*.*
Figure 4.4: Routing Vortex
Each of the routers thinks it knows the best outbound link for IP
addresses that start with �212.78�. But somehow the routers are
a little confused and their routing tables form a loop. If a packet
with a prefix of �212.78� found its way into one of these routers,
it would be routed around a circle of three links forever. There
is no way out. As more packets arrived with the same prefix,
they would just be added to the �infinite packet vortex�. Pretty
soon the links would be full of traffic going round and round, the
routers would fill up with packets waiting to be sent, and all three
routers would crash. This problem is worse than having someone
4.4. DETERMINING YOUR ROUTE 43
trip over a fiber optic cable, since it can cause several routers to
crash.
To solve this problem, the Internet Protocol designers added a
number to each packet that is called the Time To Live (TTL). This
number starts out with a value of about 30. Each time an IP
packet is forwarded down a link, the router subtracts 1 from the
TTL value. So if the packet takes 15 hops to cross the Internet, it
will emerge on the far end with a TTL of 15.
But now let�s look at how the TTL functions when there is a routing
loop (or �packet vortex�) for a particular network number. Since
the packet keeps getting forwarded around the loop, eventually
the TTL reaches zero. And when the TTL reaches zero, the router
assumes that something is wrong and throws the packet away.
This approach ensures that routing loops do not bring whole areas
of the network down.
So that is a pretty cool bit of network protocol engineering. To
detect and recover from routing loops, we just put a number in,
subtract 1 from that number on each link, and when the number
goes to zero throw the packet away.
It also turns out that when the router throws a packet away, it
usually sends back a courtesy notification, something like, �Sorry
I had to throw your packet away.� The message includes the IP
address of the router that threw the packet away.
Network loops are actually pretty rare, but we can use this notification
that a packet was dropped to map the approximate route a
packet takes through the network. The traceroute program sends
packets in a tricky manner to get the routers that your packets
pass through to send it back notifications. First, traceroute sends
a packet with a TTL of 1. That packet gets to the first router and
is discarded and your computer gets a notification from the first
router. Then traceroute sends a packet with a TTL of 2. That
packet makes it through the first router and is dropped by the
second router, which sends you back a note about the discarded
packet. Then traceroute sends a packet with a TTL of 3, and continues
to increase the TTL until the packet makes it all the way to
its destination.
With this approach, traceroute builds up an approximate path that
your packets are taking across the network.
It took 14 hops to get from Ann Arbor, Michigan to Palo Alto, California.
The packets passed through Kansas, Texas, Los Angeles,
and Oakland. This might not be the best route between the two
cities if you were driving a car or taking a train, but on that day
44 CHAPTER 4. INTERNETWORKING LAYER (IP)
traceroute www.stanford.edu
traceroute to www5.stanford.edu (171.67.20.37), 64 hops max, 40 byte packets
1 141.211.203.252 (141.211.203.252) 1.390 ms 0.534 ms 0.490 ms
2 v-bin-seb.r-bin-seb.umnet.umich.edu (192.122.183.61) 0.591 ms 0.558 ms 0.570 ms
3 v-bin-seb-i2-aa.merit-aa2.umnet.umich.edu (192.12.80.33) 6.610 ms 6.545 ms 6.654 ms
4 192.122.183.30 (192.122.183.30) 7.919 ms 7.209 ms 7.122 ms
5 so-4-3-0.0.rtr.kans.net.internet2.edu (64.57.28.36) 17.672 ms 17.836 ms 17.673 ms
6 so-0-1-0.0.rtr.hous.net.internet2.edu (64.57.28.57) 31.800 ms 41.967 ms 31.787 ms
7 so-3-0-0.0.rtr.losa.net.internet2.edu (64.57.28.44) 63.478 ms 63.704 ms 63.710 ms
8 hpr-lax-hpr--i2-newnet.cenic.net (137.164.26.132) 63.093 ms 63.026 ms 63.384 ms
9 svl-hpr--lax-hpr-10ge.cenic.net (137.164.25.13) 71.242 ms 71.542 ms 76.282 ms
10 oak-hpr--svl-hpr-10ge.cenic.net (137.164.25.9) 72.744 ms 72.243 ms 72.556 ms
11 hpr-stan-ge--oak-hpr.cenic.net (137.164.27.158) 73.763 ms 73.396 ms 73.665 ms
12 bbra-rtr.Stanford.EDU (171.64.1.134) 73.577 ms 73.682 ms 73.492 ms
13 * * *
14 www5.Stanford.EDU (171.67.20.37) 77.317 ms 77.128 ms 77.648 ms
Figure 4.5: Traceroute from Michigan to Stanford
for packets between the two cities this was the best route on the
Internet.
Figure 4.6: Notifications of Dropped Packets
You can also see how long it took the packets to make it from the
source to each router, and then from the source to the destination.
A millisecond (ms) is a 1/1000 of a second. So 77.317 ms is just
under a tenth of a second. This network is pretty fast.
Sometimes a traceroute can take a little while, up to a minute or
two. Not all routers will give you the �I discarded your packet�
4.5. GETTING AN IP ADDRESS 45
message. In the example above, the router at hop 13 threw our
packet away without saying �I am sorry�. Traceroute waits for the
message and after a few seconds just gives up and increases the
TTL value so it gets past the rude router.
If you run a traceroute for a connection that includes an undersea
cable, you can see how fast data moves under the sea. Here
is a traceroute between the University of Michigan and Peking
University in China.
$ traceroute www.pku.edu.cn
traceroute to www.pku.edu.cn (162.105.129.104), 64 hops max, 40 byte packets
1 141.211.203.252 (141.211.203.252) 1.228 ms 0.584 ms 0.592 ms
2 v-bin-seb.r-bin-seb.umnet.umich.edu (192.122.183.61) 0.604 ms 0.565 ms 0.466 ms
3 v-bin-seb-i2-aa.merit-aa2.umnet.umich.edu (192.12.80.33) 7.511 ms 6.641 ms 6.588 ms
4 192.122.183.30 (192.122.183.30) 12.078 ms 6.989 ms 7.619 ms
5 192.31.99.133 (192.31.99.133) 7.666 ms 8.953 ms 17.861 ms
6 192.31.99.170 (192.31.99.170) 59.275 ms 59.273 ms 59.108 ms
7 134.75.108.209 (134.75.108.209) 173.614 ms 173.552 ms 173.333 ms
8 134.75.107.10 (134.75.107.10) 256.760 ms 134.75.107.18 (134.75.107.18) 256.574 ms
9 202.112.53.17 (202.112.53.17) 256.761 ms 256.801 ms 256.688 ms
10 202.112.61.157 (202.112.61.157) 257.416 ms 257.960 ms 257.747 ms
11 202.112.53.194 (202.112.53.194) 256.827 ms 257.068 ms 256.962 ms
12 202.112.41.202 (202.112.41.202) 256.800 ms 257.053 ms 256.933 ms
Figure 4.7: Traceroute from Michigan to Peking University
You can see when the packet is encountering a long undersea
cable in steps seven and eight. The time goes from less than
1/10 of a second to nearly 1/4 of a second. Even though 1/4 of a
second is slower than 1/10 a second, it is pretty impressive when
you consider that the packet is going nearly all of the way around
the world in that 1/4 second.
The core of our IP network is remarkable. Most of the time we
don�t really care how hard the routers are working to make sure
our packets move quickly from our computer to the various destinations
around the world. Next we will move from looking at
how the core of the network functions to how IP addresses are
managed at the edges.
4.5 Getting an IP Address
Increasingly, computers are portable or mobile. We just pointed
out how important it was for the IP layer to track large groups
of computers using network numbers instead of tracking every
single computer individually. But since these network numbers
indicate a particular physical connection to the network, when
we move a computer from one location to another, it will need a
new IP address. Remember that the link layer address is set when
46 CHAPTER 4. INTERNETWORKING LAYER (IP)
a computer is manufactured and never changes throughout the
life of the computer. If you close your laptop in one coffee shop
and reopen it using your home WiFi, your computer will need a
different IP address.
This ability for your computer to get a different IP address when
it is moved from one network to another uses a protocol called
�Dynamic Host Configuration Protocol� (or DHCP for short). DHCP
is pretty simple. Going back to the Link layer section, recall the
first thing your computer does at the link level is ask �Is there
a base station on this network?� by sending a message to a
special broadcast address. Once your computer is successfully
connected at the link layer through that base station, it sends another
broadcast message, this time asking �Is there a gateway
connected to this network that can get me to the Internet? If
there is, tell me your IP address and tell me what IP address I
should use on this network�.
When the gateway router replies, your computer is given a temporary
IP address to use on that network (for instance, while you
are at the coffee shop). After the router has not heard from your
computer for a while, it decides you are gone and loans the IP
address to another computer.
If this process of reusing a loaned IP address goes wrong, two
computers end up on the same network with the same IP address.
Perhaps you have seen a message on your computer to the effect
of, �Another computer is using 192.168.0.5, we have stopped using
this address�. Your computer sees another computer with a
link address other than its own using the IP address that your
computer thinks is assigned to it.
But most of the time this dynamic IP address assignment (DHCP)
works perfectly. You open your laptop and in a few seconds you
are connected and can use the Internet. Then you close your
laptop and go to a different location and are given a different IP
address to use at that location.
In some operating systems, when a computer connects to a network,
issues a DHCP request, and receives no answer, it decides
to assign itself an IP address anyway. Often these self-assigned
addresses start with �169. . . .�. When your computer has one of
these self-assigned IP addresses, it thinks it is connected to a network
and has an IP address, but without a gateway, it has no
possibility of getting packets routed across the local network and
onto the Internet. The best that can be done is that a few computers
can connect to a local network, find each other, and play
4.6. A DIFFERENT KIND OF ADDRESS REUSE 47
Figure 4.8: Getting an IP Address via DHCP
a networked game. There is not much else that can be done with
these self-assigned IP addresses.
4.6 A Different Kind of Address Reuse
If you know how to find the IP address on your laptop, you can do
a little experiment and look at the different IP addresses you get
at different locations. If you made a list of the different addresses
you received at the different locations, you might find that many
of the locations give out addresses with a prefix of �192.168.�.
This seems to be a violation of the rule that the network number
(IP address prefix) is tied to the place where the computer is connected
to the Internet, but a different rule applies to addresses
that start with �192.168.� (The prefix �10.� is also special).
Addresses that start with �192.168.� are called �non-routable�
addresses. This means that they will never be used as real addresses
that will route data across the core of the network. They
can be used within a single local network, but not used on the
global network.
So then how is it that your computer gets an address like
�192.168.0.5� on your home network and it works perfectly
well on the overall Internet? This is because your home
48 CHAPTER 4. INTERNETWORKING LAYER (IP)
router/gateway/base station is doing something we call �Network
Address Translation�, or �NAT�. The gateway has a single routable
IP address that it is sharing across multiple workstations that are
connected to the gateway. Your computer uses its non-routable
address like �192.168.0.5� to send its packets, but as the packets
move across the gateway, the gateway replaces the address
with its actual routable address. When packets come back to
your workstation, the router puts your workstation�s non-routable
address back into the returning packets.
This approach allows us to conserve the real routable addresses
and use the same non-routable addresses over and over for workstations
that move from one network to another.
4.7 Global IP Address Allocation
If you wanted to connect the network for a new organization
to the Internet you would need to contact an Internet Service
Provider and make a connection. Your ISP would give you a range
of IP addresses (i.e., one or more network numbers) that you
could allocate to the computers attached to your network. The
ISP assigns you network numbers by giving you a portion of the
network numbers they received from a higher-level Internet Service
Provider.
At the top level of IP address allocations are five Regional Internet
Registries (RIRs). Each of the five registries allocates IP addresses
for a major geographic area. Between the five registries, every
location in the world can be allocated a network number. The
five registries are North America (ARIN), South and Central America
(LACNIC), Europe (RIPE NCC), Asia-Pacific (APNIC) and Africa
(AFRNIC).
When the classic IPv4 addresses like �212.78.1.25� were invented,
only a few thousand computers were connected to the
Internet. We never imagined then that someday we would have
a billion computers on the Internet. But today with the expansion
of the Internet and the �Internet of things� where smart cars,
refrigerators, thermostats, and even lights will need IP addresses,
we need to connect far more than a billion computers to the
Internet. To make it possible to connect all these new computers
to the Internet, engineers have designed a new generation of the
Internet Protocol called �IPv6�. The 128-bit IPv6 addresses are
much longer than the 32-bit IPv4 addresses.
4.8. SUMMARY 49
The Regional Internet Registries (RIRs) are leading the transition
from IPv4 to IPv6. The transition from IPv4 to IPv6 will take many
years. During that time, both IPv4 and IPv6 must work seamlessly
together.
4.8 Summary
The Internetworking Protocol layer extends our network from a
single hop (Link layer) to a series of hops that result in packets
quickly and efficiently being routed from your computer to a destination
IP address and back to your computer. The IP layer is
designed to react and route around network outages and maintain
near-ideal routing paths for packets moving between billions
of computers without any kind of central routing clearinghouse.
Each router learns its position within the overall network, and by
cooperating with its neighboring routers helps move packets effectively
across the Internet.
The IP layer is not 100% reliable. Packets can be lost due to momentary
outages or because the network is momentarily �confused�
about the path that a packet needs to take across the
network. Packets that your system sends later can find a quicker
route through the network and arrive before packets that your
system sent earlier.
Itmight seemtempting to design the IP layer so that it never loses
packets and insures that packets arrive in order, but this would
make it nearly impossible for the IP layer to handle the extreme
complexities involved in connecting so many systems.
So instead of asking too much of the IP layer, we leave the problem
of packet loss and packets that arrive out of order to our next
layer up, the Transport layer.
4.9 Glossary
core router: A router that is forwarding traffic within the core of
the Internet.
DHCP: Dynamic Host Configuration Protocol. DHCP is how a
portable computer gets an IP address when it is moved to a new
location.
50 CHAPTER 4. INTERNETWORKING LAYER (IP)
edge router: A router which provides a connection between a
local network and the Internet. Equivalent to �gateway�.
Host Identifier: The portion of an IP address that is used to
identify a computer within a local area network.
IP Address: A globally assigned address that is assigned to a
computer so that it can communicate with other computers that
have IP addresses and are connected to the Internet. To simplify
routing in the core of the Internet IP addresses are broken into
Network Numbers and Host Identifiers. An example IP address
might be �212.78.1.25�.
NAT: Network Address Translation. This technique allows a single
global IP address to be shared by many computers on a single
local area network.
Network Number: The portion of an IP address that is used to
identify which local network the computer is connected to.
packet vortex: An error situation where a packet gets into an
infinite loop because of errors in routing tables.
RIR: Regional Internet Registry. The five RIRs roughly correspond
to the continents of the world and allocate IP address for the major
geographical areas of the world.
routing tables: Information maintained by each router that
keeps track of which outbound link should be used for each
network number.
Time To Live (TTL): A number that is stored in every packet
that is reduced by one as the packet passes through each router.
When the TTL reaches zero, the packet is discarded.
traceroute: A command that is available on many Linux/UNIX
systems that attempts to map the path taken by a packet as it
moves from its source to its destination. May be called �tracert�
on Windows systems.
two-connected network: A situation where there is at least two
possible paths between any pair of nodes in a network. A twoconnected
network can lose any single link without losing overall
connectivity.
4.10 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
4.10. QUESTIONS 51
1. What is the goal of the Internetworking layer?
a) Move packets across multiple hops from a source to destination
computer
b) Move packets across a single physical connection
c) Deal with web server failover
d) Deal with encryption of sensitive data
2. How many different physical links does a typical packet cross
from its source to its destination on the Internet?
a) 1
b) 4
c) 15
d) 255
3. Which of these is an IP address?
a) 0f:2a:b3:1f:b3:1a
b) 192.168.3.14
c) www.khanacademy.com
d) @drchuck
4. Why is it necessary to move from IPv4 to IPv6?
a) Because IPv6 has smaller routing tables
b) Because IPv6 reduces the number of hops a packet must go
across
c) Because we are running out of IPv4 addresses
d) Because IPv6 addresses are chosen by network hardware
manufacturers
5. What is a network number?
a) A group of IP addresses with the same prefix
b) The GPS coordinates of a particular LAN
c) The number of hops it takes for a packet to cross the network
d) The overall delay packets experience crossing the network
6. How many computers can have addresses within network
number �218.78�?
52 CHAPTER 4. INTERNETWORKING LAYER (IP)
a) 650
b) 6500
c) 65000
d) 650000
7. How do routers determine the path taken by a packet across
the Internet?
a) The routes are controlled by the IRG (Internet Routing Group)
b) Each router looks at a packet and forwards it based on its
best guess as to the correct outbound link
c) Each router sends all packets on every outbound link (flooding
algorithm)
d) Each router holds on to a packet until a packet comes in from
the destination computer
8. What is a routing table?
a) A list of IP addresses mapped to link addresses
b) A list of IP addresses mapped to GPS coordinates
c) A list of network numbers mapped to GPS coordinates
d) A list of network numbers mapped to outbound links from
the router
9. How does a newly connected router fill its routing tables?
a) By consulting the IANA (Internet Assigned Numbers Authority)
b) By downloading the routing RFC (Request for Comments)
c) By contacting the Internet Engineering Task Force (IETF)
d) By asking neighboring routers how they route packets
10. What does a router do when a physical link goes down?
a) Throws away all of the routing table entries for that link
b) Consults the Internet Map (IMAP) service
c) Does a Domain Name (DNS) looking for the IP address
d) Sends all the packets for that link back to the source computer
11. Why is it good to have at least a �two-connected� network?
a) Because routing tables are much smaller
4.10. QUESTIONS 53
b) Because it removes the need for network numbers
c) Because it supports more IPv4 addresses
d) Because it continues to function even when a single link
goes down
12. Do all packets from a message take the same route across
the Internet?
a) Yes
b) No
13. How do routers discover new routes and improve their routing
tables?
a) Each day at midnight they download a new Internet map
from IMAP
b) They periodically ask neighboring routers for their network
tables
c) They randomly discard packets to trigger error-correction
code within the Internet
d) They are given transmission speed data by destination computers
14. What is the purpose of the �Time to Live� field in a packet?
a) To make sure that packets do not end up in an �infinite loop�
b) To track how many minutes it takes for a packet to get
through the network
c) To maintain a mapping between network numbers and GPS
coordinates
d) To tell the router the correct output link for a particular
packet
15. How does the �traceroute� command work?
a) It sends a series of packets with low TTL values so it can get
a picture of where the packets get dropped
b) It loads a network route from the Internet Map (IMAP)
c) It contacts a Domain Name Server to get the route for a
particular network number
d) It asks routers to append route information to a packet as it
is routed from source to destination
54 CHAPTER 4. INTERNETWORKING LAYER (IP)
16. About how long does it take for a packet to cross the Pacific
Ocean via an undersea fiber optic cable?
a) 0.0025 Seconds
b) 0.025 Seconds
c) 0.250 Seconds
d) 2.5 Seconds
17. On a WiFi network, how does a computer get an Internetworking
(IP) address?
a) Using the DHCP protocol
b) Using the DNS protocol
c) Using the HTTP protocol
d) Using the IMAP protocol
18. What is Network Address Translation (NAT)?
a) It looks up the IP address associated with text names like
�www.dr-chuck.com�
b) It allows IPv6 traffic to go across IPv4 networks
c) It looks up the best outbound link for a particular router and
network number
d) It reuses special network numbers like �192.168� across multiple
network gateways at multiple locations
19. How are IP addresses and network numbers managed globally?
a) There are five top-level registries that manage network numbers
in five geographic areas
b) IP addresses are assigned worldwide randomly in a lottery
c) IP addresses are assigned by network equipment manufacturers
d) IP addresses are based on GPS coordinates
20. How much larger are IPv6 addresses than IPv4 addresses?
a) They are the same size
b) IPv6 addresses are 50% larger than IPv4 addresses
c) IPv6 addresses are twice as large as IPv4 addresses
d) IPv6 addresses are 10 times larger than IPv4 addresses
4.10. QUESTIONS 55
21. What does it mean when your computer receives an IP address
that starts with �169..�?
a) Your connection to the Internet supports the Multicast protocol
b) The gateway is mapping your local address to a global address
using NAT
c) There was no gateway available to forward your packets to
the Internet
d) The gateway for this network is a low-speed gateway with a
small window size
22. If you were starting an Internet Service Provider in Poland,
which Regional Internet Registry (RIR) would assign you a
block of IP addresses.
a) ARIN
b) LACNIC
c) RIPE NCC
d) APNIC
e) AFRNIC
f) United Nations
56 CHAPTER 4. INTERNETWORKING LAYER (IP)
Chapter 5
The Domain Name
System
The Domain Name System lets you access websites by their
domain name like (www.khanacademy.org), so you don�t have
to keep a list of numeric Internet Protocol (IP) addresses like
�212.78.1.25�. IP address are determined by where your computer
connects to the Internet. When you have a portable
computer and you move from one location to another, you get
a new IP address at each new location. Since no one connects
to your portable computer, it does not matter if your IP address
changes from time to time. But since so many people connect to
a web server, it would be inconvenient if the server moved to a
new location and needed to change its IP address.
When your computer makes a connection to a system using a
domain name address, the first thing your computer does is look
up the IP address that corresponds to the domain name. Then
your computer makes the connection using the IP address.
Adding the separate step of looking up the IP address for a DNS
address also makes it easier to move a server from one location
to another. The server is given a new IP address and the entry for
the domain address is updated. Once the DNS entry is updated,
new requests for the domain name are given the new IP address.
Since end users access most servers using domain names and
never see the IP address, a server can be moved to a new network
connection without affecting the end user�s ability to access the
server.
57
58 CHAPTER 5. THE DOMAIN NAME SYSTEM
5.1 Allocating Domain Names
If you recall from the previous section, IP addresses are allocated
based on where you connect a new network to the Internet. Domain
names are allocated based on organizations that �own� the
domain name. At the top of the domain name hierarchy is an organization
called the International Corporation for Assigned Network
Names and Numbers(ICANN). ICANN chooses the top-level
domains (TLDs) like .com, .edu, and .org and assigns those to
other organizations to manage. Recently a new set of TLDs like
.club and .help have been made available.
ICANN also assigns two-letter country code top-level domain
names like .us, .za, .nl, and .jp to countries around the world
We call these Country-Code Top-Level Domain Names (ccTLDs).
Countries often add second-level TLDs, like .co.uk for commercial
organizations within the UK. Policies for applying for domain
names with any particular ccTLD vary widely from one country to
another.
Figure 5.1: Domain Names
Once a domain name is assigned to an organization, the controlling
organization is allowed to assign subdomains within the
domain. As an example, the .edu top-level domain is assigned
5.2. READING DOMAIN NAMES 59
to the Educause organization. Educause assigns domains like
umich.edu to higher education institutions. Once the University
of Michigan is given control of umich.edu, it can make its own
choices for subdomains within its new domain. Domains ending
in .com and .org can be purchased by individuals. The individual
owners of those domains are allowed to manage their domain and
create subdomains under it for their own use or use by others.
5.2 Reading Domain Names
When we look at an IP address like �212.78.1.25�, the left prefix
is the �Network Number�, so in a sense we read IP addresses
from left to right, where the left part of the IP address is the most
general part of the address and right part of the address is most
specific:
212.78.1.25
Broad ----> Narrow
For domain names, we read from right to left:
drchuck.personal.si.umich.edu
Narrow <--- Broad
The most general part of this domain name is �.edu�, which
means higher education institutions. The subdomain �umich.edu�
is a particular higher education institution.
5.3 Summary
While the Domain Name System is not one of our four layers in
the model, it is an important part of making the Internet easier to
use. Domain names allow end users to use symbolic names for
servers instead of numeric Internet Protocol addresses. By adding
a service that maps domain names to IP addresses, we can move
servers from one Internet connection to another connection without
requiring users to manually change their configurations to
connect to a server.
If you would like to purchase a domain name for yourself or your
company, you can choose from any number of domain name registrars.
60 CHAPTER 5. THE DOMAIN NAME SYSTEM
5.4 Glossary
DNS: Domain Name System. A system of protocols and servers
that allow networked applications to look up domain names and
retrieve the corresponding IP address for the domain name.
domain name: A name that is assigned within a top-level domain.
For example, khanacademy.org is a domain that is assigned
within the �.org� top-level domain.
ICANN: International Corporation for Assigned Network Names
and Numbers. Assigns and manages the top-level domains for
the Internet.
registrar: A company that can register, sell, and host domain
names.
subdomain: A name that is created �below� a domain
name. For example, �umich.edu� is a domain name and
both �www.umich.edu� and �mail.umich.edu� are subdomains
within �umich.edu�.
TLD: Top Level Domain. The rightmost portion of the domain
name. Example TLDs include �.com�, �.org�, and �.ru�. Recently,
new top-level domains like �.club� and �.help� were added.
5.5 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. What does the Domain Name System accomplish?
a) It allows network-connected computers to use a textual
name for a computer and look up its IP address
b) It keeps track of the GPS coordinates of all servers
c) It allows Regional Internet Registries (RIRs) to manage IP addresses
on the various continents
d) It assigns different IP addresses to portable computers as
they move from one WiFi to another
2. What organization assigns top-level domains like �.com�,
�.org�, and �.club�?
a) IANA - Internet Assigned Numbers Authority
5.5. QUESTIONS 61
b) IETF - Internet Engineering Task Force
c) ICANN - International Corporation for Assigned Network
Names and Numbers
d) IMAP - Internet Mapping Authorization Protocol
3. Which of these is a domain address?
a) 0f:2a:b3:1f:b3:1a
b) 192.168.3.14
c) www.khanacademy.org
d) @drchuck
4. Which of these is not something a domain owner can do with
their domain?
a) Create subdomains
b) Sell subdomains
c) Create new top-level domains
d) Assign an IP address to the domain or subdomain
62 CHAPTER 5. THE DOMAIN NAME SYSTEM
Chapter 6
Transport Layer
The next layer up from the Internetworking layer is the Transport
layer. A key element of the Internetworking layer is that it does
not attempt to guarantee delivery of any particular packet. The
Internetworking layer is nearly perfect, but sometimes packets
can be lost or misrouted.
Transport
Application
Internetwork
Link
Figure 6.1: The Transport Layer
But users of the network want to reliably send entire files or messages
across the Internet. A network is not much good to us if
all it can do is send packets that are received most of the time.
For the network to be useful, all of the packets need to be reassembled
into the right order to reconstruct the message on the
receiving system. The network must also deal with packets that
arrive out of order or never arrive at all. The Transport layer is
where we handle reliability and message reconstruction on the
63
64 CHAPTER 6. TRANSPORT LAYER
destination computer.
Just like the IP layer, the Transport layer adds a small amount of
data to each packet to help solve the problems of packet reassembly
and retransmission.
6.1 Packet Headers
If you were to look at a packet going across one of many links
between its source and destination computers, you would see a
link header, an IP header, and a Transport Control Protocol (TCP)
header, along with the actual data in the packet.
From | To
Link Header
From | To | TTL
IP Header
Port | Offset
TCP Header
��.
Data Packet
Figure 6.2: Headers and Data
The link header is removed when the packet is received on one
link and a new link header is added when the packet is sent out
on the next link on its journey. The IP and TCP headers stay with
a packet as it is going across each link in its journey. Remember
that a packet may go across several types of link layers as it is
routed through the Internet.
The IP header holds the source and destination Internet Protocol
(IP) addresses as well as the Time to Live (TTL) for the packet. The
IP header is set on the source computer and is unchanged (other
than the TTL) as the packet moves through the various routers on
its journey.
The TCP headers indicate where the data in each packet belongs.
As the source computer breaks the message or file into packets,
it keeps track of the position of each packet relative to the beginning
of the message or file and places the offset in each packet
that is created and sent.
6.2. PACKET REASSEMBLY AND RETRANSMISSION 65
6.2 Packet Reassembly and Retransmis-
sion
As the destination computer receives the packets, it looks at the
offset position from the beginning of the message so it can put
the packet into the proper place in the reassembled message.
Simply by making sure to put the packet data at the correct position
relative to the beginning of the message, the Transport layer
easily handles packets that arrive out of order. If it receives a
packet further down a message, it places that packet in a buffer,
keeping track of the fact that there is now a gap in the message
that is being reconstructed. When the earlier packet arrives a
moment later, it fits perfectly into the gap in the reassembled
data.
To avoid overwhelming the network, the Transport layer in the
sending computer only sends a certain amount of data before
waiting for an acknowledgement from the Transport layer on
the destination computer that the packets were received. The
amount of data that the sending computer will send before
pausing to wait for an acknowledgment is called the �window
size�.
The sending computer keeps track of how quickly it starts to
receive acknowledgements from the receiving computer. If the
acknowledgments come back quickly, the sending computer increases
its window size, but if the acknowledgments come back
slowly, the sending computer transmits less data. By adjusting
the window size, transmitting computers can send large amounts
of data quickly over fast connections that have light loads. When
sending data over slow or heavily loaded links, they can send the
data in a way that does not overwhelm the network.
If a packet is lost, it will never arrive at the destination computer
and so the destination computer will never send an acknowledgment
for that data. Because the sending computer does not receive
an acknowledgment, it quickly reaches the point where it
has sent enough unacknowledged data to fill up the window and
stops sending new packets.
At this point, both computers are waiting. The sending computer
is waiting for an acknowledgement for a lost packet that will never
come and the receiving computer is waiting for a lost packet that
will never come. To make sure that the computers do not wait
forever, the destination computer keeps track of the amount of
time since it received the last packet of data. At some point, the
66 CHAPTER 6. TRANSPORT LAYER
receiving computer decides too much time has passed and sends
a packet to the sending computer indicating where in the stream
the receiving computer has last received data. When the sending
computer receives this message, it �backs up� and resends data
from the last position that the receiving computer had successfully
received.
Figure 6.3: Waiting for a Lost Packet
The combination of the receiving computer acknowledging received
data, not allowing the transmitting computer to get too
far ahead (window size), and the receiving computer requesting
the sending computer to �back up and restart� when it appears
that data has been lost creates a relatively simple method to reliably
send large messages or files across a network.
While the Transport layer is sending a stream of data, it continuously
monitors how quickly it receives acknowledgements and
dynamically adjusts its window size. This ensures that data is
sent rapidly when the connection between two computers is fast
and much more slowly when the connection has slow links or a
heavy load.
6.3. THE TRANSPORT LAYER IN OPERATION 67
6.3 The Transport Layer In Operation
One of the key elements of the Transport layer is that the sending
computer must hold on to all of the data it is sending until the
data has been acknowledged. Once the receiving computer acknowledges
the data, the sending computer can discard the sent
data. We can look at this graphically when a message is broken
into many packets. Here, the first ten packets of the message
have been sent and acknowledged by the destination computer
(�a�). The sending computer has sent six more packets (�S�), and
then stopped because it reached its window size.
Source
Message
Transport
Layer
Destination
Transport
Layer
PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
aaaaaaaaSSSSSS
aaaaaaaaRR R
S S S
Figure 6.4: Buffering in the Transport Layer
There are three packets that have been sent but not yet received
(�S�). Since there are many hops in the network, it is very common
for more than one packet to be enroute in the network at the
same time.
The Transport layer on the receiving computer has received and
acknowledged ten packets and delivered them to the receiving
application (�a�).1 The Transport layer on the destination computer
has received more three packets (�R�), but one packet is out
of order. Receiving a packet out of order is not a cause for concern
if the missing packet arrives in a reasonably short amount
of time. As long as all the packets are received, the receiving
Transport layer will reconstruct the message, fitting the packets
together like puzzle pieces, and deliver them to the receiving application.
1We talk about the Application layer in later material.
68 CHAPTER 6. TRANSPORT LAYER
6.4 Application Clients and Servers
The purpose of the Transport layer is to provide reliable connections
between networked applications so those applications can
send and receive streams of data. For an application, this is as
simple as asking the Transport layer to make a connection to an
application running on a remote host. We call the application that
initiates the connection on the local computer the �client� and the
application that responds to the connection request the �server�.
We call the combination of the two networked applications at the
ends of the connection a �client/server� application because the
two pieces of the application must work together.
A lot of engineering has gone into the lower three layers of our
architecture to make it easy to open a connection to a remote
computer and then send and receive data over that connection.
6.5 Server Applications and Ports
When a client application wants to make a connection to a remote
computer, it is important that the connection is made to
the correct server application on that remote computer. A remote
computer might have any number of different server applications
running at the same time. Example server applications would
include:
� Web Server
� Video Server
� Mail Server
For instance, a web client (a browser) needs to connect to the web
server running on the remote computer. So a client application
not only needs to know which remote computer to connect to, it
also needs to choose a particular application to interact with on
that remote computer.
We use a concept called �ports� to allow a client application to
choose which server application it wants to interact with. Ports
are like telephone extensions. All of the extensions have the
same phone number (IP Address) but each extension (server application)
has a different extension number (port number).
When a server application starts up, it �listens� for incoming connections
on the specified port. Once the server application has
6.5. SERVER APPLICATIONS AND PORTS 69
Figure 6.5: TCP Ports
registered that it is ready to receive incoming connections, it
waits until the first connection is made.
So that client applications know which port to connect to, there is
a list of well-known default ports for various server applications:
� Telnet (23) - Login
� SSH (22) - Secure Login
� HTTP (80) - World Wide Web
� HTTPS (443) - Secure Web
� SMTP (25) - Incoming Mail
� IMAP (143/220/993) - Mail Retrieval
� POP (109/110) - Mail Retrieval
� DNS (53) - Domain Name Resolution
� FTP (21) - File Transfer
These are the normal ports for these applications. Sometimes
servers will make applications available at non-standard ports. If
you are doing web development, you may run a web server at a
non-standard port like 3000, 8080, or 8888. If you see a URL like:
http://testing.example.com:8080/login
70 CHAPTER 6. TRANSPORT LAYER
the �8080� indicates that your browser is going to use the web
protocols to interact with the server, but connect to port 8080
instead of the default port 80.
6.6 Summary
In a sense, the purpose of the Transport layer is to compensate
for the fact that the Link and Internetworking layers might lose
data. When the two lower layers lose or reroute packets, the
Transport layer works to reassemble and/or retransmit that data.
The existence of the Transport layer makes it possible for the two
lower layers to ignore retransmission and rate-limiting issues.
Part of the goal of a layered architecture is to break an overly
complex problem into smaller subproblems. Each layer focuses
on solving part of the overall problem and assumes that the other
layers solve the problems they are supposed to solve.
6.7 Glossary
acknowledgement: When the receiving computer sends a notification
back to the source computer indicating that data has
been received.
buffering: Temporarily holding on to data that has been sent or
received until the computer is sure the data is no longer needed.
listen: When a server application is started and ready to accept
incoming connections from client applications.
port: A way to allow many different server applications to be
waiting for incoming connections on a single computer. Each
application listens on a different port. Client applications make
connections to well-known port numbers to make sure they are
talking to the correct server application.
6.8 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. What is the primary problem the Transport (TCP) layer is supposed
to solve?
6.8. QUESTIONS 71
a) Move packets across multiple hops from a source to destination
computer
b) Move packets across a single physical connection
c) Deal with lost and out-of-order packets
d) Deal with encryption of sensitive data
2. What is in the TCP header?
a) Physical address
b) IP Address and Time to Live
c) Port number and offset
d) Which document is being requested
3. Why is �window size� important for the proper functioning of
the network?
a) Because packets that are too large will clog fiber optic connections
b) It prevents a fast computer from sending too much data on
a slow connection
c) It limits the number of hops a packet can take before it is
dropped
d) It determines what part of an IP address is the network number
4. What happens when a sending computer receives an acknowledgement
from the receiving computer?
a) The sending computer resends the data to make sure it was
transmitted accurately
b) The sending computer sends more data up to the window
size
c) The sending computer sends an �acknowledgment for the
acknowledgment�
d) The sending computer sends the acknowledgement to the
Internet Map (IMAP)
5. Which of these detects and takes action when packets are
lost?
a) Sending computer
b) Network gateway
c) Core Internet routers
72 CHAPTER 6. TRANSPORT LAYER
d) Receiving computer
6. Which of these retains data packets so they can be retransmitted
if a packets lost?
a) Sending computer
b) Network gateway
c) Core Internet routers
d) Receiving computer
7. Which of these is most similar to a TCP port?
a) Train station
b) Undersea network cable
c) Apartment number
d) Sculpture garden
8. Which half of the client/server application must start first?
a) Client
b) Server
9. What is the port number for the Domain Name System?
a) 22
b) 80
c) 53
d) 143
10. What is the port number for the IMAP mail retrieval protocol?
a) 22
b) 80
c) 53
d) 143
Chapter 7
Application Layer
We have been working from the bottom to the top of our fourlayer
TCP/IP network model and we are finally at the top. The Application
layer is where the networked software like web browsers,
mail programs, video players, or networked video players operate.
We as users interact with these applications and the applications
interact with the network on our behalf.
Transport
Application
Internetwork
Link
Figure 7.1: The Application Layer
7.1 Client and Server Applications
It is important to remember that two parts are required for a networked
application to function. The architecture for a networked
73
74 CHAPTER 7. APPLICATION LAYER
application is called �client/server�. The server portion of the application
runs somewhere on the Internet and has the information
that users want to view or interact with. The client portion of the
application makes connections to the server application, retrieves
information, and shows it to the user. These applications use the
Transport layer on each of their computers to exchange data.
Figure 7.2: Client/Server Applications
To browse a web address like www.khanacademy.org, you must
have a web application running on your computer. When you type
an address into your web browser, it connects to the appropriate
web server, retrieves pages for you to view, and then shows you
the pages.
The web browser on your computer sends a request to connect
to www.khanacademy.org. Your computer looks up the domain
name to find the corresponding IP address for the server and
makes a transport connection to that IP address, then begins to
request data from the server over that network connection. When
the data is received, the web browser shows it to you. Sometimes
web browsers display a small animated icon to let you know that
the data is being retrieved across the network.
On the other end of the connection is another application called a
�web server�. The web server program is always up and waiting
for incoming connections. So when you want to see a web page,
you are connecting to a server application that is already running
and waiting for your connection.
In a sense, the Transport, Internetwork, and Link layers, along
7.2. APPLICATION LAYER PROTOCOLS 75
with the Domain Name System, are like a telephone network for
networked applications. They �dial up� different server applications
on the network and have �conversations� with those applications
to exchange data.
7.2 Application Layer Protocols
Just like people talking on telephones, each pair of network applications
needs a set of rules that govern the conversation. In most
cultures, when your phone rings and you pick up the phone you
say �Hello�. Normally the person who made the call (the client
person) is silent until the person who picked up the phone (the
server person) says �Hello�. If you have ever called someone
who does not follow this simple rule, it can be quite confusing.
You probably would assume that the connection was not working,
hang up, and retry the call.
A set of rules that govern how we communicate is called a �protocol�.
The definition of the word protocol is �a rule which describes
how an activity should be performed, especially in the field of
diplomacy.� The idea is that in formal situations, we should behave
according to a precise set of rules. We use this word to
describe networked applications, because without precise rules,
applications have no way to establish and manage a conversation.
Computers like precision.
There are many different networked applications and it is important
that each networked application have a well-documented
protocol so that all servers and clients can interoperate. Some
of these protocols are intricate and complex.
The protocol that describes how a web browser communicates
with a web server is described in a number of large documents
starting with this one:
https://tools.ietf.org/html/rfc7230
The formal name of the protocol between web clients and web
servers is the �HyperText Transport Protocol�, or HTTP for short.
When you put �http:� or �https:� on the beginning of a URL that
you type into the browser, you are indicating that you would like
to retrieve a document using the HTTP protocol.
If you were to read the above document, and go to section 5.3.2
on page 41, you find the exact text of what a web client is supposed
to send to a web server:
76 CHAPTER 7. APPLICATION LAYER
Figure 7.3: Application Protocols
GET http://www.example.org/pub/WWW/TheProject.html HTTP/1.1
One of the reasons that HTTP is so successful is that it is relatively
simple compared to most client/server protocols. Even though
the basic use of HTTP is relatively simple, there is a lot of detail
that allows web clients and servers communicate efficiently and
transfer a wide range of information and data. Six different documents
describe the HTTP protocol, in a total of 305 pages. That
might seem like a lot of detail, but the key in designing protocols
is to think through all possible uses of the protocol and describe
each scenario carefully.
7.3 Exploring the HTTP Protocol
In this section we will manually exercise the HTTP protocol by
pretending to be a web browser and sending HTTP commands to
a web server to retrieve data. To play with the HTTP protocol, we
will use one of the earliest Internet applications ever built.
The �telnet� application was first developed in 1968, and was
7.3. EXPLORING THE HTTP PROTOCOL 77
developed according to one of the earliest standards for the Internet:
https://tools.ietf.org/html/rfc15
This standard is only five pages long and at this point, you probably
can easily read and understand most of what is in the document.
The telnet client application is so old that it is effectively a
dinosaur, as it comes from �prehistoric� times in terms of the age
of the Internet. The Internet was created in 1985 by the NSFNet
project and the precursor to the NSFNet called the ARPANET was
brought up in 1969. Telnet was designed and built even before
the first TCP/IP network was in production.
Interestingly, the telnet application is still present in most modern
operating systems. You can access telnet from the terminal
(command line) in Macintosh and Linux. The telnet application
was also present in Windows 95 through Windows XP, but is not
included in later versions of Windows. If you have a later version
of Windows, you can download and install a telnet client to do the
examples in this section.
Telnet is a simple application. Run telnet from the command line
(or terminal) and type the following command:
telnet www.dr-chuck.com 80
The first parameter is a domain name (an IP address would work
here as well) and a port to connect to on that host. We use the
port to indicate which application server we would like to connect
to. Port 80 is where we typically expect to find an HTTP (web)
server application on a host. If there is no web server listening
on port 80, our connection will time out and fail. But if there is
a web server, we will be connected to that web server and whatever
we type on our keyboard will be sent directly to the server.
At this point, we need to know the HTTP protocol and type the
commands precisely as expected. If we don�t know the protocol,
the web server will not be too friendly. Here is an example of
things not going well:
telnet www.dr-chuck.com 80
Trying 198.251.66.43...
Connected to www.dr-chuck.com.
Escape character is '^]'.
HELP
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
78 CHAPTER 7. APPLICATION LAYER
<html><head>
<title>501 Method Not Implemented</title>
...
</body></html>
Connection closed by foreign host.
We type �telnet� in the terminal requesting a connection to port
80 (the web server) on the host www.dr-chuck.com. You can see
as our transport layer is looking up the domain name, finding
that the actual address is �198.251.66.43�, and then making a
successful connection to that server. Once we are connected, the
server waits for us to type a command followed by the enter or
return key. Since we don�t know the protocol, we type �HELP� and
enter. The server is not pleased, gives us an error message, and
then closes the connection. We do not get a second chance. If we
do not know the protocol, the web server does not want to talk to
us.
But let�s go back and read section 5.3.2 of the RFC-7230 document
and try again to request a document using the correct syntax:
telnet www.dr-chuck.com 80
Trying 198.251.66.43...
Connected to www.dr-chuck.com.
Escape character is '^]'.
GET http://www.dr-chuck.com/page1.htm HTTP/1.0
HTTP/1.1 200 OK
Last-Modified: Sun, 19 Jan 2014 14:25:43 GMT
Content-Length: 131
Content-Type: text/html
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
Connection closed by foreign host.
We make the connection to the web browser again using telnet,
then we send a GET command that indicates which document we
want to retrieve. We use version 1.0 of the HTTP protocol because
7.3. EXPLORING THE HTTP PROTOCOL 79
Figure 7.4: Hacking HTTP By Hand
it is simpler than HTTP 1.1. Then we send a blank line by pressing
�return� or �enter� to indicate that we are done with our request.
Since we have sent the proper request, the host responds with a
series of headers describing the document, followed by a blank
line, then it sends the actual document.
The headers communicate metadata (data about data) about the
document that we have asked to retrieve. For example, the first
line contains a status code.
In this example, the status code of �200� means that things went
well. A status code of �404� in the first line of the headers indicates
that the requested document was not found. A status
code of �301� indicates that the document has moved to a new
location.
The status codes for HTTP are grouped into ranges: 2XX codes indicate
success, 3XX codes are for redirecting, 4XX codes indicate
that the client application did something wrong, and 5xx codes
indicate that the server did something wrong.
This is a HyperText Markup Language (HTML) document, so it is
marked up with tags like <h1> and <p>. When your browser
receives the document in HTML format, it looks at the markup in
the document, interprets it, and presents you a formatted version
of the document.
80 CHAPTER 7. APPLICATION LAYER
7.4 The IMAP Protocol for Retrieving Mail
The HTTP protocol is only one of many client/server application
protocols used on the Internet. Another common protocol is used
so that a mail application running on your computer can retrieve
mail from a central server. Since your personal computer might
not be turned on at all times, when mail is sent to you it is sent to
a server and stored on that server until you turn on your computer
and retrieve any new email.
Like many application standards, the Internet Message Access
Protocol (IMAP) is described in a series of Request For Comment
(RFC) documents starting with this RFC:
https://tools.ietf.org/html/rfc3501
IMAP is a more complicated protocol than the web protocol, so
we won�t be able to use the telnet command to fake the protocol.
But if you were going to develop a mail reading application, you
could carefully read this document and develop code to have a
successful conversation with a standards-compliant IMAP server.
Here is a simple example from section 6.3.1 of the above document
showing what the client (C:) sends and how the server (S:)
responds:
C: A142 SELECT INBOX
S: * 172 EXISTS
S: * 1 RECENT
S: * OK [UNSEEN 12] Message 12 is first unseen
S: * OK [UIDVALIDITY 3857529045] UIDs valid
S: * OK [UIDNEXT 4392] Predicted next UID
S: * FLAGS (\Answered \Flagged \Deleted \Seen \Draft)
S: * OK [PERMANENTFLAGS (\Deleted \Seen \*)] Limited
S: A142 OK [READ-WRITE] SELECT completed
The messages that are sent by the client and server are not designed
to be viewed by an end user so they are not particularly
descriptive. Thesemessages are precisely formatted and are sent
in a precise order so that they can be generated and read by networked
computer applications on each end of the connection.
7.5. FLOW CONTROL 81
7.5 Flow Control
When we looked at the Transport layer, we talked about the �window
size�, which was the amount of data that the Transport layer
on the sending computer will send before pausing to wait for an
acknowledgement.
Source
Message
Transport
Layer
Destination
Transport
Layer
PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
aaaaaaaaSSSSSS
aaaaaaaaRR R
S S S
Figure 7.5: Buffering in the Transport Layer
In this figure, we see a message broken into packets, with some
of the packets sent and acknowledged. Six packets have been
sent but not yet acknowledged and the sending Transport layer
has reached the limit of the transmit window, so it is pausing until
it receives an acknowledgement from the receiving computer�s
Transport layer. The receiving computer has received three packets,
one of which is out of order.
When we were looking at this example before from the point of
view of the Transport layer, we ignored where the packets to be
sent were coming from and where the packets were going to in
the receiving computer. Now that we are looking at the Application
layer, we can add the two applications that are the source
and the destination of the stream of data.
Let�s assume the web browser has made a transport connection
to the web server and has started downloading an image file. The
web server has opened the image file and is sending the data
from the file to its Transport layer as quickly as possible. But the
Transport layer must follow the rules of window size, so it can only
send a certain amount of data at a time. When the window fills up,
the web server is paused until the Transport layer on the destination
computer has started to receive and acknowledge packets.
As the Transport layer on the destination computer starts to re82
CHAPTER 7. APPLICATION LAYER
ceive packets, reconstruct the stream of data, and acknowledge
packets, it delivers the reconstructed stream of packets to the
web browser application display on the user�s screen. Sometimes
on a slow connection you can see your browser �paint� pictures
as the data is downloaded. On a fast connection the data comes
so quickly that the pictures appear instantaneously.
If we redraw our picture of packets in the Transport layer, adding
both of the application layers where the packets are in the middle
of an image, now it looks like this:
Web Server
Application
Transport Transport
Web Browser
Application
Internetwork
and Link
Layers
..aSSSSSS ..aRR R AAAA
AAAA
AA..
...FFFFF
S S S
ACK
Figure 7.6: Buffering in the Application and Transport Layers
The web server is reading the image file (�F�) and sending it as
a stream to the web browser as quickly as it can send the data.
The source Transport layer has broken the stream into packets
and used IP to send the packets to the destination computer.
The Transport layer has sent six packets (�S�) and has stopped
sending because it has reached its window size and paused the
web server. Three of those six packets have made it to the Transport
layer on the destination computer (�R�) and three of the packets
are still making their way across the Internet (�S�).
As the destination Transport layer pieces the stream back together,
it both sends an acknowledgement (ACK) and delivers
the data to the receiving application (the web browser). The web
browser reconstructs the image (�A�) and displays it to the user
as the data is received.
A key thing to notice in this picture is that the transport layers do
not keep the packets for the entire file. They only retain packets
that are �in transit� and unacknowledged. Once packets are acknowledged
and delivered to the destination application, there is
no reason for either the source or destination Transport layer to
hold on to the packets.
When the acknowledgment flows back from the destination
computer to the source computer, the Transport layer on the
7.6. WRITING NETWORKED APPLICATIONS 83
source computer unpauses the web server application and the
web server continues to read data from the file and send it to the
source Transport layer for transmission.
This ability to start and stop the sending application to make sure
we send data as quickly as possible without sending data so fast
that they clog up the Internet is called �flow control�. The applications
are not responsible for flow control, they just try to send or
receive data as quickly as possible and the two transport layers
start and stop the applications as needed based on the speed and
reliability of the network.
7.6 Writing Networked Applications
The applications which send and receive data over the network
are written in one or more programming languages. Many programming
languages have libraries of code that make it quite
simple to write application code to send and receive data across
the network. With a good programming library, making a connection
to an application running on a server, sending data to the
server, and receiving data from the server is generally as easy as
reading and writing a file.
As an example, the code below is all it takes in the Python programming
language to make a connection to a web server and
retrieve a document:
import socket
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('www.py4inf.com', 80))
mysock.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\n\n')
while True:
data = mysock.recv(512)
if ( len(data) < 1 ) :
break
print data
mysock.close()
Figure 7.7: Programming with Sockets in Python
While you may or not know the Python programming language,
the key point is that it only takes ten lines of application code to
make and use a network connection. This code is simple because
the Transport, Internetwork, and Link layers so effectively solve
84 CHAPTER 7. APPLICATION LAYER
the problems at each of their layers that applications using the
network can ignore nearly all of the details of how the network is
implemented.
In the Python application, in this line of code
mysock.connect(('www.py4inf.com', 80))
we have specified that we are connecting to the application that
is listening for incoming connections on port 80 on the remote
computer www.py4inf.com.
By choosing port 80 we are indicating that we want to connect to
a World Wide Web server on that host and are expecting to communicate
with that server using the HyperText Transport Protocol.
7.7 Summary
The entire purpose of the lower three layers (Transport, Internetwork,
and Link) is to make it so that applications running in the
Application layer can focus the application problem that needs to
be solved and leave virtually all of the complexity of moving data
across a network to be handled by the lower layers of the network
model.
Because this approach makes it so simple to build networked
applications, we have seen a wide range of networked applications
including web browsers, mail applications, networked video
games, network-based telephony applications, and many others.
And what is even more exciting is that it is easy to experiment
and build whole new types of networked applications to solve
problems that have not yet been imagined.
7.8 Glossary
HTML: HyperText Markup Language. A textual format that
marks up text using tags surrounded by less-than and
greater-than characters. Example HTML looks like: <p>This
is <strong>nice</strong></p>.
HTTP: HyperText Transport Protocol. An Application layer protocol
that allows web browsers to retrieve web documents from web
servers.
7.9. QUESTIONS 85
IMAP: Internet Message Access Protocol. A protocol that allows
mail clients to log into and retrieve mail from IMAP-enabled mail
servers.
flow control: When a sending computer slows down to make
sure that it does not overwhelm either the network or the destination
computer. Flow control also causes the sending computer
to increase the speed at which data is sent when it is sure that
the network and destination computer can handle the faster data
rates.
socket: A software library available in many programming languages
that makes creating a network connection and exchanging
data nearly as easy as opening and reading a file on your
computer.
status code: One aspect of the HTTP protocol that indicates
the overall success or failure of a request for a document. The
most well-known HTTP status code is �404�, which is how an HTTP
server tells an HTTP client (i.e., a browser) that it the requested
document could not be found.
telnet: A simple client application that makes TCP connections
to various address/port combinations and allows typed data to be
sent across the connection. In the early days of the Internet, telnet
was used to remotely log in to a computer across the network.
web browser: A client application that you run on your computer
to retrieve and display web pages.
web server: An application that deliver (serves up) Web pages
7.9 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. Which layer is right below the Application layer?
a) Transport
b) Internetworking
c) Link Layer
d) Obtuse layer
2. What kind of document is used to describe widely used Application
layer protocols?
86 CHAPTER 7. APPLICATION LAYER
a) DHCP
b) RFC
c) APPDOC
d) ISO 9000
3. Which of these is an idea that was invented in the Application
layer?
a) 0f:2a:b3:1f:b3:1a
b) 192.168.3.14
c) www.khanacademy.com
d) http://www.dr-chuck.com/
4. Which of the following is not something that the Application
layer worries about?
a) Whether the client or server starts talking first
b) The format of the commands and responses exchanged
across a socket
c) How the window size changes as data is sent across a socket
d) How data is represented as it is sent across the network to
assure interoperability.
5. Which of these is an Application layer protocol?
a) HTTP
b) TCP
c) DHCP
d) Ethernet
6. What port would typically be used to talk to a web server?
a) 23
b) 80
c) 103
d) 143
7. What is the command that a web browser sends to a web
server to retrieve an web document?
a) RETR
b) DOCUMENT
7.9. QUESTIONS 87
c) 404
d) GET
8. What is the purpose of the �Content-type:� header when you
retrieve a document over the web protocol?
a) Tells the browser how to display the retrieved document
b) Tells the browser where to go if the document cannot be
found
c) Tells the browser whether or not the retrieved document is
empty
d) Tells the browser where the headers end and the content
starts
9. What common UNIX command can be used to send simple
commands to a web server?
a) ftp
b) ping
c) traceroute
d) telnet
10. What does an HTTP status code of �404� mean?
a) Document has moved
b) Successful document retrieval
c) Protocol error
d) Document not found
11. What characters are used to mark up HTML documents?
a) Less-than and greater-than signs < >
b) Exclamation points !
c) Square brackets [ ]
d) Curly brackets { }
12. What is a common application protocol for retrieving mail?
a) RFC
b) HTML
c) ICANN
d) IMAP
88 CHAPTER 7. APPLICATION LAYER
13. What application protocol does RFC15 describe?
a) telnet
b) ping
c) traceroute
d) www
14. What happens to a server application that is sending a large
file when the TCP layer has sent enough data to fill the window
size and has not yet received an acknowledgement?
a) The application switches its transmission to a new socket
b) The application crashes and must be restarted
c) The application is paused until the remote computer
acknowledges that it has received some of the data
d) The closest gateway router starts to discard packets that
would exceed the window size
15. What is a �socket� on the Internet?
a) A way for devices to get wireless power
b) A way for devices to get an IP address
c) An entry in a routing table
d) A two-way data connection between a pair of client and
server applications
16. What must an application know to make a socket connection
in software?
a) The address of the server and the port number on the server
b) The route between the source and destination computers
c) Which part of the IP address is the network number
d) The initial size of the TCP window during transmission
Chapter 8
Secure Transport Layer
In the early days of the Internet, networks were small and all of
the routers were in secure locations. As long as each computer
connected to the Internet protected itself from unwanted incoming
connections, it was felt that there was no need to protect data
from prying eyes while it was crossing the network.
So the Link, Internetwork, and Transport layers were focused on
the efficient movement of data and solving the problems of a
large-scale shared distributed network without worrying about
the privacy of that data.
But as the use of the Internet grew rapidly in the late 1980s and
literally exploded when the Web became mainstream in 1994, security
and privacy of network traffic became very important problems
to solve. When we began using the Internet to conduct commerce
and credit cards and bank account numbers were being
routinely sent across the network, securing data became essential.
And when we started using wireless technologies like WiFi,
security became necessary for even the simplest uses of the Internet.
There are two general approaches to securing network activity.
The first makes sure that all of the network hardware (routers
and links) is in physically secure locations so it is not possible for
someone to sneak in and monitor traffic while it is crossing the
Internet. This approach is not practical for hundreds of thousands
of network routers owned and operated by many different organizations.
While you might be able to ensure that some of the
router operators adhered to strict security procedures and policies,
sooner or later a mistake will be made. And once WiFi was
added to the mix and your packets went over radio waves, a network
attacker could just sit in a coffee shop and intercept packets
89
90 CHAPTER 8. SECURE TRANSPORT LAYER
as they passed through the air.
Under these conditions, the only reasonable solution is to encrypt
data in your computer before it is sent across its first physical
link, and then decrypt the data in the destination computer after
it arrives. Using this approach, we assume that an attacker can
see all of the packets that you send, but they cannot decrypt the
data that they have captured. The encryption also guarantees
that there is no way to alter your data while it is crossing the
Internet.
8.1 Encrypting and Decrypting Data
The concept of protecting information so it cannot be read while
it is being transported over an insecure medium is thousands of
years old. The leaders in Roman armies sent coded messages to
each other using a code called the �Caesar Cipher�. The simplest
version of this approach is to take each of the characters of the
actual message (we call this �plain text�) and shift each character
a fixed distance down the alphabet to produce the scrambled
message or �ciphertext�.
Then we send the ciphertext via the courier or other insecure
transport to the other person. The courier cannot read the message
because it appears to be random characters unless you
know the technique used to encode the message.
As long as the person receiving the message knew the number
used to shift the message, they could unshift the characters in
the encoded message to reproduce the original message.
Here is a simple example of plain text and ciphertext using a shift
of one:
Plain text: Go to the river
Cipher text: Hp up uif sjwfs
We use the word �encrypt� to describe transforming the plain text
to the ciphertext and �decrypt� to describe the reverse process.
The Caesar Cipher is very simple to defeat, but it was used to
protect important messages until about 150 years ago. Modern
encryption techniques are far more sophisticated than a simple
character shift, but all encryption systems depend on some kind
of a secret key that both parties are aware of so they can decrypt
received data.
8.2. TWO KINDS OF SECRETS 91
8.2 Two Kinds of Secrets
The traditional way to encrypt transmissions is using a shared secret
(a password, a sentence, a number) that only the sending
and receiving parties know. With the secret, it is easy to decrypt
the received data, but if you received the data without possessing
the secret, it would be effectively impossible to decrypt the
message.
Figure 8.1: Shared Versus Asymmetric Keys
In the early days of the Internet, two people could send encrypted
email to each other by one person first calling the other person
on the phone and giving them the decryption secret. This worked
well when there were only a few users on the network, but could
not scale to situations where a company might have millions of
customers and could not afford to make a phone call to each customer
to establish a shared secret before they could make a purchase.
It might seem like a good idea to distribute the shared secrets
over the Internet, but if we assume that the attackers are monitoring
and capturing all network traffic, they could also capture the
unencrypted message that contained the shared secret. At that
point it would be trivial for the attacker to use the shared secret
to decrypt a message. And even worse, the attacker could intercept
a message, delay it, then decrypt it, change and re-encrypt
it, and send the modified message back on its way. The receiving
92 CHAPTER 8. SECURE TRANSPORT LAYER
computer would decrypt the message and never know that it had
been modified by an attacker while in transit.
So shared secrets clearly would not work to solve the problem of
securing network traffic between trillions of pairs of networked
computers.
The solution to this problem came in the 1970s when the concept
of asymmetric key encryption was developed. The idea of
asymmetric key encryption is that one key is used to encrypt the
message and another key is used to decrypt it. The computer that
will be receiving the encrypted data chooses both the encryption
key and decryption key. Then the encryption key is sent to the
computer that will be sending the data. The sending computer
encrypts the data and sends it across the network. The receiving
computer uses the decryption key to decrypt the data.
We call the encryption key the �public� key because it can be
widely shared. We call the decryption key the �private� key because
it never leaves the computer where it was created. Another
name for asymmetric keys is public/private keys.
The whole process is designed so that if an attacker has the public
key (which was sent unencrypted) and the encrypted text, it is
virtually impossible to decrypt the encrypted data. There is a lot
of math with large prime numbers that makes it hard to guess the
private key from the public key and encrypted data.
So with the advent of public/private key technology, the only
question left was how to apply it in our network model.
8.3 Secure Sockets Layer (SSL)
Since network engineers decided to add security nearly 20 years
after the Internet protocols were developed, it was important not
to break any existing Internet protocols or architecture. Their solution
was to add an optional partial layer between the Transport
layer and the Application layer. They called this partial layer the
Secure Sockets Layer (SSL) or Transport Layer Security (TLS).
When an application requested that the Transport layer make a
connection to a remote host, it could request that the connection
either be encrypted or unencrypted. If an encrypted connection
was requested, the Transport layer encrypted the data before
breaking the stream into packets. This meant that the Transport
layer, Internetwork layer, and physical (link) layers could still perform
exactly the same way whether the packets were encrypted
8.4. ENCRYPTING WEB BROWSER TRAFFIC 93
Figure 8.2: Where Encryption and Decryption Happens
or non-encrypted. The applications making the connections were
also spared the details of how encryption and decryption worked.
Since encryption was a simple and transparent addition to the
Transport layer, there was no need to change the routers that
operate at the Internetwork and Link layers. There was no need
to change any Link layer hardware to make encryption work. And
applications did not need to be modified except to request that a
connection be encrypted when appropriate.
8.4 Encrypting Web Browser Traffic
Since web browsers and web servers operate at the application
layer, we barely notice whether we are using encrypted or unencrypted
connections. Web browsers use the URL convention
of replacing �http:� with �https:� to indicate that the browser is
to communicate with the web server using the Secure Transport
Layer instead of the unencrypted Transport layer. Your browser
will usually show a �lock� icon in the address bar to let you know
that you are communicating with a secure web site.
94 CHAPTER 8. SECURE TRANSPORT LAYER
There is a small overhead in setting up the https connections and
a small cost to encrypt and decrypt the data that is being sent.
Since https was slightly more costly, for a while it was used only
for pages that contained passwords, bank account numbers, or
other sensitive data.
But over time as networks have become faster and the https implementations
have gotten much more efficient, there is a trend
toward encrypting all web server interactions whenever you are
interacting with a web server where you have an account. The
current trend is towards using https for all web traffic.
8.5 Certificates and Certificate Authorities
While public/private key encryption works to allow the distribution
of encryption keys across insecure networks and the use of those
keys to encrypt transmissions, there is still a problem of knowing
if the public key that you have received when you connected to a
server is really from the organization it claims to be from.
Figure 8.3: Certificate Authorities and Public Keys
Perhaps you think you are connecting to www.amazon.com
but a rogue computer intercepts your traffic, claiming to be
8.6. SUMMARY 95
www.amazon.com and giving you a public key to use for encryption.
If your web browser trusts the key, it will use the rogue
computer�s public key to encrypt your banking information and
send it to the rogue computer. Since the rogue computer gave
you the public key, it also has the corresponding private key and
is able to decrypt and abscond with your banking information.
So your computer needs to know who the key is actually coming
from. This is achieved by sending you a public key that is digitally
signed by a Certificate Authority (CA). When your computer
or browser is initially installed, it knows about a number of wellknown
certificate authorities. If your browser is given a public
key that is signed by one of the well-known certificate authorities,
it trusts the key and uses it to encrypt and send your data. If
your computer receives a public key that is not signed by one of
its trusted certificate authorities, it will warn you before sending
your data using the key.
If you see a warning message about an untrusted certificate, you
should probably say �no� and figure out why your network traffic
is not being routed to the server that you think it is going to before
sending any sensitive data.
8.6 Summary
Since the Internet was nearly 20 years old before we needed
broadly deployed security, we had to find a way to add security to
the already existing four-layer model. The perfect place to add security
was as an option in the Transport layer. This is why we call
secure connections on the Internet �Secure Sockets Layer� (SSL)
or �Transport Layer Security� (TLS). There are subtle differences
between SSL and TLS but they both encrypt data at the Transport
layer.
The invention of public/private key encryption was well timed in
that it solved the key distribution problem of shared-secret encryption
approaches. With public/private keys, the public encryption
key can be routinely shared across insecure media. This
means we can use an unencrypted connection to exchange data
and upgrade the connection to a secure connection.
By inserting the secure layer at the top of the Transport layer, we
were able to avoid changing the Application, Internetwork, and
Link layers while still easily securing any Transport layer connection.
This approach ensures that all data being sent across a connection
is encrypted before it leaves your computer. Given that
96 CHAPTER 8. SECURE TRANSPORT LAYER
many of us use wireless connections like WiFi, which are easily
monitored by attackers, it is a good idea to encrypt data before it
is sent across WiFi.
Browsers support secure connections by changing the prefix on
the URL from �http:� to �https:�. By keeping an eye on the URL,
end users can make sure they never send sensitive data across
insecure connections. A series of trusted Certificate Authorities
will sign public keys to give you an assurance that the key you
received is indeed from the organization you expect it to be.
The design of the Secure Transport Layer provides a secure and
yet easy-to-use mechanism for secure communications across
the Internet at a scale of trillions of pairs of interacting computers.
8.7 Glossary
asymmetric key: An approach to encryption where one (public)
key is used to encrypt data prior to transmission and a different
(private) key is used to decrypt data once it is received.
certificate authority: An organization that digitally signs public
keys after verifying that the name listed in the public key is actually
the person or organization in possession of the public key.
ciphertext: A scrambled version of a message that cannot be
read without knowing the decryption key and technique.
decrypt: The act of transforming a ciphertext message to a plain
text message using a secret or key.
encrypt: The act of transforming a plain text message to a ciphertext
message using a secret or key.
plain text: A readable message that is about to be encrypted
before being sent.
private key: The portion of a key pair that is used to decrypt
transmissions.
public key: The portion of a key pair that is used to encrypt
transmissions.
shared secret: An approach to encryption that uses the same
key for encryption and decryption.
SSL: Secure Sockets Layer. An approach that allows an application
to request that a Transport layer connection is to be en8.8.
QUESTIONS 97
crypted as it crosses the network. Similar to Transport Layer Security
(TLS).
TLS: Transport Layer Security. An approach that allows an application
to request that a Transport layer connection is to be encrypted
as it crosses the network. Similar to Secure Sockets Layer
(SSL).
8.8 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. How do we indicate that we want a secure connection when
using a web browser?
a) Use https:// in the URL
b) Use a secure web browser
c) Open an incognito window
d) Manually encode the address of the server using SHA1
2. Why is a shared-secret approach not suitable for use on the
Internet?
a) Because people would lose or misplace the secret
b) It is difficult to distribute the secrets
c) Encryption and decryption with shared secrets are too easily
broken
d) Encryption and decryption with shared secrets take too
much compute power
3. What is the underlying mathematical concept that makes
public/private key encryption secure?
a) Continuous functions
b) Taylor series
c) Karnaugh Maps
d) Prime numbers
4. Which of the keys can be sent across the Internet in plain
text without compromising security?
a) Encryption key
98 CHAPTER 8. SECURE TRANSPORT LAYER
b) Decryption Key
c) Shared Secret
d) Univerally Safe Key (USK)
5. Where does the Secure Sockets Layer (SSL) fit in the fourlayer
Internet architecture?
a) Below the Link layer
b) Between the Link and Internetworking layers
c) Between the Internetworking and Transport layers
d) Between the Transport and Application layers
6. If you were properly using https in a browser over WiFi in a
cafe, which of the following is the greatest risk to your losing
credit card information when making an online purchase?
a) Someone captured the packets that were sent across the
WiFi
b) Someone captured the packets in the gateway router
c) Someone captured the packets as they passed through a
core Intenet router
d) You have a virus on your computer that is capturing
keystrokes
7. With the Secure Sockets Layer, where are packets encrypted
and decrypted?
a) They are encrypted and decrypted as they pass through the
router
b) Each physical link has its own separate encryption
c) They are encrypted in your computer and decrypted in the
server
d) They are encrypted in the WiFi gateway and decrypted in the
last router before the destination computer
8. What changes to the IP layer were needed to make secure
socket layer (SSL) work?
a) No changes were needed
b) We had to add support for Secure IP (IPSEC)
c) We needed to support longer packets in IP
d) The Time-To-Live (TTL) value needed to be encrypted
8.8. QUESTIONS 99
9. If a rogue element was able to monitor all packets going
through an undersea cable and you were using public/
private key encryption properly, which of the following
would be the most difficult for them to obtain?
a) What servers you were communicating with
b) How often you used the servers
c) How much data you retrieved from the servers
d) Which documents you retrieved from the servers
10. What is the purpose of a Certificate Authority in public/
private key encryption?
a) To make sure people do not forge badges for learning activities
b) To make sure packets get routed to the correct destination
computer
c) To assure us that a public key comes from the organization
it claims to be from
d) To choose when a particular country must switch from IPv4
to IPv6
11. The ARPANET network was in operation starting in the 1960s.
Secure Sockets Layer (SSL) was not invented util the 1980s.
How did the ARPANET insure the security of the data on its
network?
a) By using public/private keys and encrypting all transmissions
b) By using encryption at the Link layer
c) By making sure no one could access the physical links
d) By only using secure WiFi routers
12. Which of these answers is �Security is fun� encrypted with a
Caesar Cipher shift of 1.
a) Ptsjduao rt dii
b) Wentudhs di dju
c) Tfdvsjuz jt gvo
d) Asdfghjk qw zxc
13. What Caesar Cipher shift was used to encrypt �V yvxr frphevgl�?
100 CHAPTER 8. SECURE TRANSPORT LAYER
a) 1
b) 6
c) 13
d) 24
Chapter 9
The OSI Model
So far we have spent all of our time describing the four-layer
model used to design and implement the TCP/IP protocols and applications
that make up the Internet. However, the TCP/IP model
is not the only model we can use to help us understand how networks
work. The other model commonly used to make sense of
network design is called the Open System Interconnection (OSI)
model. While the TCP/IP model was designed and evolved as the
TCP/IP protocols were developed, deployed, and changed, the OSI
model was the result of a careful design process by many networking
experts who worked to develop a general approach to
network models.
In today�s networked world, the OSI model and the TCP/IP model
serve two different purposes.1 The TCP/IP model is an implementation
model, in that it provides the guidance for those who would
build TCP/IP-compatible network hardware or software. The OSI
model is more of an abstract model that can be used to understand
a wide range of network architectures.
While TCP/IP is the most widely used network technology today,
many different types of networks have been implemented and deployed
over the past 50 years. And as we continue to improve and
evolve networking, new implementation models may emerge.
The OSI model has seven layers instead of the four layers of the
TCP/IP model. Starting at the bottom (nearest the physical connections)
of the OSI model, the layers are: (1) Physical, (2) Data
Link, (3) Network, (4) Transport, (5) Session, (6) Presentation, and
1This, of course, is an oversimplification. Prior to 1990, there were operational
network implementations based on ISO specifications that followed the
OSI network model very closely. But today, those ISO/OSI network implementations
no longer are in broad use.
101
102 CHAPTER 9. THE OSI MODEL
(7) Application. We will look at each layer in the OSI model in turn,
starting with the Physical layer.
9.1 Physical (Layer 1)
The OSI Physical layer deals with the physical attributes of the
actual wired, wireless, fiber optic, or other connection that is used
to transport data across a single link. The Physical layer also
defines the shapes of the connectors and type of media which can
be used. Another problem solved at this layer is how to encode
the bits (0�s and 1�s) that make up the data being sent across the
medium.2 The �bit encoding� (or modulation) determines how
fast data can be sent across the link.
9.2 Data Link (Layer 2)
The OSI Data Link layer is concerned with how the systems using
a physical link cooperate with one another. When data is broken
into packets, the Data Link layer defines special sequences to indicate
the beginning and end of each packet. The stations communicating
using the physical connection are assigned addresses to
allow for effective use of the media. Sometimes multiple stations
are sharing the same media (as on a wireless network) and the
Data Link layer defines how those stations will share the connections
with the other systems connected to the network. Most Data
Link layers also have some form of checksum to detect and/or correct
for errors in the transmitted data.
The design problems solved in the Physical and Data Link layers
of the OSI model are addressed by the Link layer of the TCP/IP
model.
9.3 Network (Layer 3)
Like the Internetwork Layer (IP) in the TCP/IP model, the OSI Network
layer deals with the global assignment of �routable� addresses
to the various systems connected to the network. The
2�Manchester Encoding� is a common technique for encoding bits for transmission
across a wire.
9.4. TRANSPORT (LAYER 4) 103
Network layer governs how routers forward packets across multiple
hops to get from their source to their destination. Like the
IP layer, The OSI Network layer does not attempt to be error free,
as it assumes that lost data will be detected and retransmitted at
the next layer up.
9.4 Transport (Layer 4)
The Transport layer in the OSI model manages packet loss and
retransmission as well as flow control and window size. The rest
of the functionality of the TCP/IP Transport layer is handled in the
Session layer in the OSI model.
9.5 Session (Layer 5)
The OSI Session layer handles establishing connections between
applications. The Session layer deals with �ports� so that a connecting
client application can �find� the correct server application
on a particular system. Some aspects of secure transmission are
also handled in the OSI Session layer.
9.6 Presentation (Layer 6)
The Presentation layer focuses on how data is represented and
encoded for transmission across the network. As an example, the
Presentation layer would describe how to encode the pixels of an
image so that the receiving application can properly decode the
data. The Presentation layer also handles data encryption and
decryption.
9.7 Application (Layer 7)
The OSI Application Layer is very similar to the Application layer
in the TCP/IP model, in that it contains the applications themselves.
Some applications are client applications that initiate
connections, and other applications are the server applications
that respond to those connection requests. The various pairs of
applications have protocol standards that define interoperability
104 CHAPTER 9. THE OSI MODEL
between multiple clients and multiple servers from different vendors.
Transport
Application
Internetwork
Link
2??
Application
3
Transport
4??	
Data Link
Physical
Protocols
Encoding formats
Encryption/SSL
Ports
Reliability
Addressing / Routing
Addressing / Coordination
Voltages / Frequencies
Speeds / Connectors
Figure 9.1: Comparing the TCP and OSI Models
9.8 Comparing the OSI and TCP/IP Models
We can use the OSI model to provide an alternative �view� of the
TCP/IP model by comparing how the OSI model breaks network
functionality into its layers and how the TCP/IP model breaks its
functionality into layers.
9.9 Link Layer (TCP/IP)
The TCP/IP Link layer combines the Physical and Data Link layers
from the OSI model. The Physical and Data Link layers are usually
implemented in hardware. Products like Ethernet, WiFi, satellite,
or fiber optic often are implemented in a network driver card that
plugs into the back of a computer or router. The network driver
card generally implements both the physical and the data link
9.10. INTERNETWORK LAYER (TCP/IP) 105
aspects of the connection in the hardware on the card. In most
cases, the data link layers are tuned to the limitations and requirements
of their corresponding physical layers. So in real systems,
it is somewhat rare for a particular data link layer to be arbitrarily
paired with any number of physical layers. Since it can be hard
to separate the physical and data link aspects for a particular link
technology, the TCP model combines them into a single layer for
simplicity.
9.10 Internetwork Layer (TCP/IP)
One place that maps pretty cleanly between the two models is
the OSI Network and TCP/IP Internetwork layers. They perform
the same functions of creating a globally routable address space
and building routers to insure that packets properly find their way
from the source to the destination across multiple hops.
9.11 Transport Layer (TCP/IP)
The features of the Transport layer in TCP/IP are spread across the
Transport and Session layers of the OSI model. The OSI Transport
layer deals with flow control and packet retransmission, while the
OSI Presentation layer deals with multiple applications running on
multiple ports as well as session establishment and teardown.
The Secure Sockets Layer (SSL) in the TCP/IP model corresponds
to parts of the Session and Presentation layers in the OSI model.
9.12 Application Layer (TCP/IP)
The TCP/IP Application Layer combines the non-security aspects
of the OSI Presentation layer and the OSI Application layer. While
many TCP/IP applications deal with issues like encoding and decoding
various types of data, the TCP/IP model does not see data
formatting as a separate �layer�. Various data encoding and decoding
technologies are used in TCP/IP applications, but TCP/IP
tends to treat these capabilities as library code that applications
make use of as needed for the application.
106 CHAPTER 9. THE OSI MODEL
9.13 Conclusion
While the TCP/IP model described in this book is widely used to
guide the implementation of TCP/IP networks, hardware, and software,
the OSI model can help us look at and compare a wide
range of network architectures ranging from openly developed
networks to proprietary vendor-specific networks.
9.14 Glossary
abstract model: A model and set of terminology that is used to
generally understand a problem area and guide the development
of standards and implementations to solve problems.
implementation model: A model and set of terminology that is
used to guide the development of standards and an implementation
to solve a particular problem.
ISO: International Organization for Standardization. A worldwide
body that develops standards in computing, networking, and
many other areas.
OSI: Open System Interconnection. A seven-layer model used to
help organize the design of various approaches to network architecture.
9.15 Questions
You can take this quiz online at http://www.net-intro.com/quiz/
1. What is the primary value of the OSI network model?
a) OSI networks are used in the southern hemisphere
b) The OSI approach can be use to analyze many different network
models
c) OSI networks make better use of limited bandwidth
d) OSI networks are more secure
2. How many layers does the OSI model have?
a) Four
9.15. QUESTIONS 107
b) Six
c) Seven
d) Nine
3. Which of the OSI layers deals with the shape of connectors
for network connections?
a) Physical
b) Data Link
c) Network
d) Transport
4. Which of the layers is most similar between the OSI and TCP
network models?
a) TCP Link Layer and OSI Data Link Layer
b) TCP Internetwork Layer and OSI Network Layer
c) TCP Transport Layer and OSI Transport Layer
d) TCP Application Layer and OSI Session Layer
5. What layer does the TCP/IP Secure Sockets Layer map to in
the OSI network model?
a) Secure Data Link Layer (SDLL)
b) Secure Network Layer (SNL)
c) Secure Transport Layer (STL)
d) Session and Presentation Layers
6. Why does the TCP model combine the OSI Data Link and
Physical layers into a single Link layer?
a) Because the TCP model does not worry about the Physical
layer
b) Because the TCP model designers were ignored at the 1981
OSI meeting in Utrect, Netherlands
c) Because quite often the design of Data Link and Physical
layers are tightly connected for a particular technology
d) To make the TCP model easier to understand by end users
108 CHAPTER 9. THE OSI MODEL
Chapter 10
Wrap Up
It has been said that building the Internet solved the world�s most
complex engineering problem to date. The design and engineering
of the Internet started well over 50 years ago. It has been
continuously improving and evolving over the past 50 years and
will continue to evolve in the future.
The Internet now connects billions of computers using many thousands
of routers and link-level connections. The Internet is so
complex that it is never fully operational, The Internet is less
about being �perfect� and more about adapting to problems, outages,
errors, lost data, and many other unforeseen problems. The
Internet is designed to be flexible and adapt to whatever problems
are encountered.
Transport
Application
Internetwork
Link
Figure 10.1: The Four-Layer Model
In order to build an overall solution that works at scale, it was
109
110 CHAPTER 10. WRAP UP
important to break the Internet engineering problems down into
four distinct layers:
� The Link/Physical layer includes all of the complex engineering
required to move data across a single �hop�, whether the
hop is a wireless WiFi, wired Ethernet, fiber optic, or satellite
connection.
� The Internetwork Protocol (IP) layer is how data is routed
across a series of hops to get quickly and efficiently from one
of a billion source computers to any of a billion destination
computers. The IP layer dynamically adjusts and reroutes
data based on network load, link performance, or network
outages. While the IP layer is highly reliable and fast, at
times it loses or even throws away data. The IP layer is not
responsible for insuring the overall reliability of the network.
It just moves the data the best that it can.
� The Transport layer compensates for any imperfections in
the IP or Link layers. The Transport layer makes sure that
any lost packets are retransmitted and packets that arrive
out of order are put back into order before being passed on
to the receiving application. The Transport layer also acts as
flow control between the sending and receiving applications
to make sure that data is moved quickly when the network is
fast and the links are not overloaded, and to slow the transfer
of data when using slower or heavily loaded links. The
data flow and rate limitation in the Transport layer allow the
Internet to continue to function smoothly even when it is
heavily loaded.
� The other three layers make the use of the network very
simple for the Application Layer. An application can make a
network connection and send/receive data on that connection
with just a few lines of code. By making the use of the
network simple, applications can focus on solving the enduser
problems they need to solve. Because it is so easy for
applications to use the network in new and different ways,
we have seen the emergence of a wide range of highly innovative
applications that work without any changes required
to the Internet protocols.
Without breaking the problem of engineering and building the Internet
into these four distinct layers, it would be far more difficult
to build and deploy ever-improving versions of the network. And
111
if every single application needed to be fully aware of all of the
complex details required to use the Internet, it would greatly limit
the richness and diversity of the networked applications that we
have today.
It is amazing to realize what has been accomplished in building
the Internet over the past 50 years. But in a way, we have only
just begun the engineering journey of building networked applications.
It does not take much to imagine an Internet where every
light switch, lightbulb, refrigerator, table, automobile, roadway,
flying drone, and chair has an Internet address and they all
want to communicate with one another. New engineering issues
will have to be solved, and perhaps even the four-layer network
model will need to evolve to meet these new engineering challenges.
But just like brilliant engineers designed and evolved network protocols
to move from hundreds of network-connected computers
to billions of network-connected computers, our present and future
engineers will certainly solve the problems and challenges
we will face as the network evolves to connect trillions of computers.
The Python Language Reference
Python Essential Reference
Python Pocket Reference
Python Cookbook
Writing Idiomatic Python
Quick search

�2011-2020 Kenneth Reitz & Real Python. CC BY-NC-SA 3.0

Luciano Ramalho
Fluent
Python
CLEAR, CONCISE, AND EFFECTIVE PROGRAMMING
PROGRAMMING/PYTHON
Fluent Python
ISBN: 978-1-491-9-46008
US $49.99 CAN $57.99
� I am proud to have been
a tech reviewer for this
excellent book�not only
will it help many
intermediate Python
programmers on their
road towards mastery,
but it has taught me quite
a few things, too!� �Alex Martelli
Python Software Foundation Fellow
� Fluent Python is a
treasure trove full of
useful programming
tricks for intermediate to
advanced Python coders
who want to push the
boundaries of their
knowledge.� �Daniel and Audrey Roy Greenfeld
authors of Two Scoops of Django
Twitter: @oreillymedia
facebook.com/oreilly
Python�s simplicity lets you become productive quickly, but this often means
you aren�t using everything it has to offer. With this hands-on guide, you�ll learn
how to write effective, idiomatic Python code by leveraging its best�and
possibly most neglected�features. Author Luciano Ramalho takes you
through Python�s core language features and libraries, and shows you how to
make your code shorter, faster, and more readable at the same time.
Many experienced programmers try to bend Python to fit patterns they
learned from other languages, and never discover Python features outside
of their experience. With this book, those Python programmers will
thoroughly learn how to become proficient in Python 3.
This book covers:
� The Python data model: understand how special methods are
the key to the consistent behavior of objects
� Data structures: take full advantage of built-in types, and
understand the text versus bytes duality in the Unicode age
� Functions as objects: view Python functions as first-class objects,
and understand how this affects popular design patterns
� Object-oriented idioms: build classes by learning about
references, mutability, interfaces, operator overloading, and
multiple inheritance
� Control flow: leverage context managers, generators,
coroutines, and concurrency with the concurrent.futures and
asyncio packages
� Metaprogramming: understand how properties, attribute
descriptors, class decorators, and metaclasses work
Luciano Ramalho, a Python programmer since 1998, is a Python Software
Foundation fellow, co-owner of Python.pro.br�a training company in Brazil�
and cofounder of Garoa Hacker Clube, Brazil�s first hackerspace. He has led
software development teams and taught Python courses in Brazilian media,
banking, and government sectors.
Luciano Ramalho
Boston
Fluent Python
Fluent Python
by Luciano Ramalho
Copyright � 2015 Luciano Gama de Sousa Ramalho. All rights reserved.
Printed in the United States of America.
Published by O�Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O�Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles (http://safaribooksonline.com). For more information, contact our corporate/
institutional sales department: 800-998-9938 or corporate@oreilly.com.
Editors: Meghan Blanchette and Rachel Roumeliotis
Production Editor: Melanie Yarbrough
Copyeditor: Kim Cofer
Proofreader: Jasmine Kwityn
Indexer: Judy McConville
Cover Designer: Ellie Volckhausen
Interior Designer: David Futato
Illustrator: Rebecca Demarest
August 2015: First Edition
Revision History for the First Edition:
2015-07-24: First release
2015-08-21: Second release
See http://oreilly.com/catalog/errata.csp?isbn=9781491946008 for release details.
The O�Reilly logo is a registered trademark of O�Reilly Media, Inc. Fluent Python, the cover image, and
related trade dress are trademarks of O�Reilly Media, Inc.
While the publisher and author have used good faith efforts to ensure that the information and instructions
contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or
omissions, including without limitation responsibility for damages resulting from the use of or reliance on
this work. Use of the information and instructions contained in this work is at your own risk. If any code
samples or other technology this work contains or describes is subject to open source licenses or the intellectual
property rights of others, it is your responsibility to ensure that your use thereof complies with such
licenses and/or rights.
ISBN: 978-1-491-94600-8
[LSI]
Para Marta, com todo o meu amor.

Table of Contents
Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv
Part I. Prologue
1. The Python Data Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
A Pythonic Card Deck 4
How Special Methods Are Used 8
Emulating Numeric Types 9
String Representation 11
Arithmetic Operators 12
Boolean Value of a Custom Type 12
Overview of Special Methods 13
Why len Is Not a Method 14
Chapter Summary 14
Further Reading 15
Part II. Data Structures
2. An Array of Sequences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Overview of Built-In Sequences 20
List Comprehensions and Generator Expressions 21
List Comprehensions and Readability 21
Listcomps Versus map and filter 23
Cartesian Products 23
Generator Expressions 25
Tuples Are Not Just Immutable Lists 26
Tuples as Records 26
Tuple Unpacking 27
v
Nested Tuple Unpacking 29
Named Tuples 30
Tuples as Immutable Lists 32
Slicing 33
Why Slices and Range Exclude the Last Item 33
Slice Objects 34
Multidimensional Slicing and Ellipsis 35
Assigning to Slices 36
Using + and * with Sequences 36
Building Lists of Lists 37
Augmented Assignment with Sequences 38
A += Assignment Puzzler 40
list.sort and the sorted Built-In Function 42
Managing Ordered Sequences with bisect 44
Searching with bisect 44
Inserting with bisect.insort 47
When a List Is Not the Answer 48
Arrays 48
Memory Views 51
NumPy and SciPy 52
Deques and Other Queues 55
Chapter Summary 57
Further Reading 59
3. Dictionaries and Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Generic Mapping Types 64
dict Comprehensions 66
Overview of Common Mapping Methods 66
Handling Missing Keys with setdefault 68
Mappings with Flexible Key Lookup 70
defaultdict: Another Take on Missing Keys 70
The __missing__ Method 72
Variations of dict 75
Subclassing UserDict 76
Immutable Mappings 77
Set Theory 79
set Literals 80
Set Comprehensions 81
Set Operations 82
dict and set Under the Hood 85
A Performance Experiment 85
Hash Tables in Dictionaries 87
vi | Table of Contents
Practical Consequences of How dict Works 90
How Sets Work�Practical Consequences 93
Chapter Summary 93
Further Reading 94
4. Text versus Bytes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
Character Issues 98
Byte Essentials 99
Structs and Memory Views 102
Basic Encoders/Decoders 103
Understanding Encode/Decode Problems 105
Coping with UnicodeEncodeError 105
Coping with UnicodeDecodeError 106
SyntaxError When Loading Modules with Unexpected Encoding 108
How to Discover the Encoding of a Byte Sequence 109
BOM: A Useful Gremlin 110
Handling Text Files 111
Encoding Defaults: A Madhouse 114
Normalizing Unicode for Saner Comparisons 117
Case Folding 119
Utility Functions for Normalized Text Matching 120
Extreme �Normalization�: Taking Out Diacritics 121
Sorting Unicode Text 124
Sorting with the Unicode Collation Algorithm 126
The Unicode Database 127
Dual-Mode str and bytes APIs 129
str Versus bytes in Regular Expressions 129
str Versus bytes on os Functions 130
Chapter Summary 132
Further Reading 133
Part III. Functions as Objects
5. First-Class Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
Treating a Function Like an Object 140
Higher-Order Functions 141
Modern Replacements for map, filter, and reduce 142
Anonymous Functions 143
The Seven Flavors of Callable Objects 144
User-Defined Callable Types 145
Function Introspection 146
Table of Contents | vii
From Positional to Keyword-Only Parameters 148
Retrieving Information About Parameters 150
Function Annotations 154
Packages for Functional Programming 156
The operator Module 156
Freezing Arguments with functools.partial 159
Chapter Summary 161
Further Reading 162
6. Design Patterns with First-Class Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
Case Study: Refactoring Strategy 168
Classic Strategy 168
Function-Oriented Strategy 172
Choosing the Best Strategy: Simple Approach 175
Finding Strategies in a Module 176
Command 177
Chapter Summary 179
Further Reading 180
7. Function Decorators and Closures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
Decorators 101 184
When Python Executes Decorators 185
Decorator-Enhanced Strategy Pattern 187
Variable Scope Rules 189
Closures 192
The nonlocal Declaration 195
Implementing a Simple Decorator 196
How It Works 198
Decorators in the Standard Library 199
Memoization with functools.lru_cache 200
Generic Functions with Single Dispatch 202
Stacked Decorators 205
Parameterized Decorators 206
A Parameterized Registration Decorator 206
The Parameterized Clock Decorator 209
Chapter Summary 211
Further Reading 212
viii | Table of Contents
Part IV. Object-Oriented Idioms
8. Object References, Mutability, and Recycling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
Variables Are Not Boxes 220
Identity, Equality, and Aliases 221
Choosing Between == and is 223
The Relative Immutability of Tuples 224
Copies Are Shallow by Default 225
Deep and Shallow Copies of Arbitrary Objects 228
Function Parameters as References 229
Mutable Types as Parameter Defaults: Bad Idea 230
Defensive Programming with Mutable Parameters 232
del and Garbage Collection 234
Weak References 236
The WeakValueDictionary Skit 237
Limitations of Weak References 239
Tricks Python Plays with Immutables 240
Chapter Summary 242
Further Reading 243
9. A Pythonic Object. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
Object Representations 248
Vector Class Redux 248
An Alternative Constructor 251
classmethod Versus staticmethod 252
Formatted Displays 253
A Hashable Vector2d 257
Private and �Protected� Attributes in Python 262
Saving Space with the __slots__ Class Attribute 264
The Problems with __slots__ 267
Overriding Class Attributes 267
Chapter Summary 269
Further Reading 271
10. Sequence Hacking, Hashing, and Slicing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
Vector: A User-Defined Sequence Type 276
Vector Take #1: Vector2d Compatible 276
Protocols and Duck Typing 279
Vector Take #2: A Sliceable Sequence 280
How Slicing Works 281
A Slice-Aware __getitem__ 283
Vector Take #3: Dynamic Attribute Access 284
Table of Contents | ix
Vector Take #4: Hashing and a Faster == 288
Vector Take #5: Formatting 294
Chapter Summary 301
Further Reading 302
11. Interfaces: From Protocols to ABCs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
Interfaces and Protocols in Python Culture 308
Python Digs Sequences 310
Monkey-Patching to Implement a Protocol at Runtime 312
Alex Martelli�s Waterfowl 314
Subclassing an ABC 319
ABCs in the Standard Library 321
ABCs in collections.abc 321
The Numbers Tower of ABCs 323
Defining and Using an ABC 324
ABC Syntax Details 328
Subclassing the Tombola ABC 329
A Virtual Subclass of Tombola 332
How the Tombola Subclasses Were Tested 335
Usage of register in Practice 338
Geese Can Behave as Ducks 338
Chapter Summary 340
Further Reading 342
12. Inheritance: For Good or For Worse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
Subclassing Built-In Types Is Tricky 348
Multiple Inheritance and Method Resolution Order 351
Multiple Inheritance in the Real World 356
Coping with Multiple Inheritance 358
1. Distinguish Interface Inheritance from Implementation Inheritance 359
2. Make Interfaces Explicit with ABCs 359
3. Use Mixins for Code Reuse 359
4. Make Mixins Explicit by Naming 359
5. An ABC May Also Be a Mixin; The Reverse Is Not True 360
6. Don�t Subclass from More Than One Concrete Class 360
7. Provide Aggregate Classes to Users 360
8. �Favor Object Composition Over Class Inheritance.� 361
Tkinter: The Good, the Bad, and the Ugly 361
A Modern Example: Mixins in Django Generic Views 362
Chapter Summary 366
Further Reading 367
x | Table of Contents
13. Operator Overloading: Doing It Right. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
Operator Overloading 101 372
Unary Operators 372
Overloading + for Vector Addition 375
Overloading * for Scalar Multiplication 380
Rich Comparison Operators 384
Augmented Assignment Operators 388
Chapter Summary 392
Further Reading 393
Part V. Control Flow
14. Iterables, Iterators, and Generators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
Sentence Take #1: A Sequence of Words 402
Why Sequences Are Iterable: The iter Function 404
Iterables Versus Iterators 405
Sentence Take #2: A Classic Iterator 409
Making Sentence an Iterator: Bad Idea 411
Sentence Take #3: A Generator Function 412
How a Generator Function Works 413
Sentence Take #4: A Lazy Implementation 416
Sentence Take #5: A Generator Expression 417
Generator Expressions: When to Use Them 419
Another Example: Arithmetic Progression Generator 420
Arithmetic Progression with itertools 423
Generator Functions in the Standard Library 424
New Syntax in Python 3.3: yield from 433
Iterable Reducing Functions 434
A Closer Look at the iter Function 436
Case Study: Generators in a Database Conversion Utility 437
Generators as Coroutines 439
Chapter Summary 439
Further Reading 440
15. Context Managers and else Blocks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
Do This, Then That: else Blocks Beyond if 448
Context Managers and with Blocks 450
The contextlib Utilities 454
Using @contextmanager 455
Chapter Summary 459
Further Reading 459
Table of Contents | xi
16. Coroutines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
How Coroutines Evolved from Generators 464
Basic Behavior of a Generator Used as a Coroutine 465
Example: Coroutine to Compute a Running Average 468
Decorators for Coroutine Priming 469
Coroutine Termination and Exception Handling 471
Returning a Value from a Coroutine 475
Using yield from 477
The Meaning of yield from 483
Use Case: Coroutines for Discrete Event Simulation 489
About Discrete Event Simulations 489
The Taxi Fleet Simulation 490
Chapter Summary 498
Further Reading 500
17. Concurrency with Futures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
Example: Web Downloads in Three Styles 505
A Sequential Download Script 507
Downloading with concurrent.futures 509
Where Are the Futures? 511
Blocking I/O and the GIL 515
Launching Processes with concurrent.futures 515
Experimenting with Executor.map 517
Downloads with Progress Display and Error Handling 520
Error Handling in the flags2 Examples 525
Using futures.as_completed 527
Threading and Multiprocessing Alternatives 530
Chapter Summary 530
Further Reading 531
18. Concurrency with asyncio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
Thread Versus Coroutine: A Comparison 539
asyncio.Future: Nonblocking by Design 545
Yielding from Futures, Tasks, and Coroutines 546
Downloading with asyncio and aiohttp 548
Running Circling Around Blocking Calls 552
Enhancing the asyncio downloader Script 554
Using asyncio.as_completed 555
Using an Executor to Avoid Blocking the Event Loop 560
From Callbacks to Futures and Coroutines 562
Doing Multiple Requests for Each Download 564
Writing asyncio Servers 567
xii | Table of Contents
An asyncio TCP Server 568
An aiohttp Web Server 573
Smarter Clients for Better Concurrency 576
Chapter Summary 577
Further Reading 579
Part VI. Metaprogramming
19. Dynamic Attributes and Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
Data Wrangling with Dynamic Attributes 586
Exploring JSON-Like Data with Dynamic Attributes 588
The Invalid Attribute Name Problem 591
Flexible Object Creation with __new__ 592
Restructuring the OSCON Feed with shelve 594
Linked Record Retrieval with Properties 598
Using a Property for Attribute Validation 604
LineItem Take #1: Class for an Item in an Order 604
LineItem Take #2: A Validating Property 605
A Proper Look at Properties 606
Properties Override Instance Attributes 608
Property Documentation 610
Coding a Property Factory 611
Handling Attribute Deletion 614
Essential Attributes and Functions for Attribute Handling 616
Special Attributes that Affect Attribute Handling 616
Built-In Functions for Attribute Handling 616
Special Methods for Attribute Handling 617
Chapter Summary 619
Further Reading 619
20. Attribute Descriptors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625
Descriptor Example: Attribute Validation 625
LineItem Take #3: A Simple Descriptor 626
LineItem Take #4: Automatic Storage Attribute Names 631
LineItem Take #5: A New Descriptor Type 637
Overriding Versus Nonoverriding Descriptors 640
Overriding Descriptor 642
Overriding Descriptor Without __get__ 643
Nonoverriding Descriptor 644
Overwriting a Descriptor in the Class 645
Methods Are Descriptors 646
Table of Contents | xiii
Descriptor Usage Tips 648
Descriptor docstring and Overriding Deletion 650
Chapter Summary 651
Further Reading 651
21. Class Metaprogramming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
A Class Factory 656
A Class Decorator for Customizing Descriptors 659
What Happens When: Import Time Versus Runtime 661
The Evaluation Time Exercises 662
Metaclasses 101 666
The Metaclass Evaluation Time Exercise 669
A Metaclass for Customizing Descriptors 673
The Metaclass __prepare__ Special Method 675
Classes as Objects 677
Chapter Summary 678
Further Reading 679
Afterword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683
A. Support Scripts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687
Python Jargon. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 715
Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 725
xiv | Table of Contents
1. Message to the comp.lang.python Usenet group, Dec. 23, 2002: �Acrimony in c.l.p.�
Preface
Here�s the plan: when someone uses a feature you don�t understand, simply shoot them.
This is easier than learning something new, and before too long the only living coders
will be writing in an easily understood, tiny subset of Python 0.9.6 <wink>.1
� Tim Peters
Legendary core developer and author of The Zen of Python
�Python is an easy to learn, powerful programming language.� Those are the first words
of the official Python Tutorial. That is true, but there is a catch: because the language is
easy to learn and put to use, many practicing Python programmers leverage only a
fraction of its powerful features.
An experienced programmer may start writing useful Python code in a matter of hours.
As the first productive hours become weeks and months, a lot of developers go on
writing Python code with a very strong accent carried from languages learned before.
Even if Python is your first language, often in academia and in introductory books it is
presented while carefully avoiding language-specific features.
As a teacher introducing Python to programmers experienced in other languages, I see
another problem that this book tries to address: we only miss stuff we know about.
Coming from another language, anyone may guess that Python supports regular expressions,
and look that up in the docs. But if you�ve never seen tuple unpacking or
descriptors before, you will probably not search for them, and may end up not using
those features just because they are specific to Python.
This book is not an A-to-Z exhaustive reference of Python. Its emphasis is on the language
features that are either unique to Python or not found in many other popular
languages. This is also mostly a book about the core language and some of its libraries.
I will rarely talk about packages that are not in the standard library, even though the
xv
Python package index now lists more than 60,000 libraries and many of them are incredibly
useful.
Who This Book Is For
This book was written for practicing Python programmers who want to become proficient
in Python 3. If you know Python 2 but are willing to migrate to Python 3.4 or
later, you should be fine. At the time of this writing, the majority of professional Python
programmers are using Python 2, so I took special care to highlight Python 3 features
that may be new to that audience.
However, Fluent Python is about making the most of Python 3.4, and I do not spell out
the fixes needed to make the code work in earlier versions. Most examples should run
in Python 2.7 with little or no changes, but in some cases, backporting would require
significant rewriting.
Having said that, I believe this book may be useful even if you must stick with Python
2.7, because the core concepts are still the same. Python 3 is not a new language, and
most differences can be learned in an afternoon. What�s New in Python 3.0 is a good
starting point. Of course, there have been changes since Python 3.0 was released in 2009,
but none as important as those in 3.0.
If you are not sure whether you know enough Python to follow along, review the topics
of the official Python Tutorial. Topics covered in the tutorial will not be explained here,
except for some features that are new in Python 3.
Who This Book Is Not For
If you are just learning Python, this book is going to be hard to follow. Not only that, if
you read it too early in your Python journey, it may give you the impression that every
Python script should leverage special methods and metaprogramming tricks. Premature
abstraction is as bad as premature optimization.
How This Book Is Organized
The core audience for this book should not have trouble jumping directly to any chapter
in this book. However, each of the six parts forms a book within the book. I conceived
the chapters within each part to be read in sequence.
I tried to emphasize using what is available before discussing how to build your own.
For example, in Part II, Chapter 2 covers sequence types that are ready to use, including
some that don�t get a lot of attention, like collections.deque. Building user-defined
sequences is only addressed in Part IV, where we also see how to leverage the abstract
base classes (ABCs) from collections.abc. Creating your own ABCs is discussed even
xvi | Preface
later in Part IV, because I believe it�s important to be comfortable using an ABC before
writing your own.
This approach has a few advantages. First, knowing what is ready to use can save you
from reinventing the wheel. We use existing collection classes more often than we implement
our own, and we can give more attention to the advanced usage of available
tools by deferring the discussion on how to create new ones. We are also more likely to
inherit from existing ABCs than to create a new ABC from scratch. And finally, I believe
it is easier to understand the abstractions after you�ve seen them in action.
The downside of this strategy are the forward references scattered throughout the
chapters. I hope these will be easier to tolerate now that you know why I chose this path.
Here are the main topics in each part of the book:
Part I
A single chapter about the Python data model explaining how the special methods
(e.g., __repr__) are the key to the consistent behavior of objects of all types�in a
language that is admired for its consistency. Understanding various facets of the
data model is the subject of most of the rest of the book, but Chapter 1 provides a
high-level overview.
Part II
The chapters in this part cover the use of collection types: sequences, mappings,
and sets, as well as the str versus bytes split�the cause of much celebration among
Python 3 users and much pain for Python 2 users who have not yet migrated their
code bases. The main goals are to recall what is already available and to explain
some behavior that is sometimes surprising, like the reordering of dict keys when
we are not looking, or the caveats of locale-dependent Unicode string sorting. To
achieve these goals, the coverage is sometimes high level and wide (e.g., when many
variations of sequences and mappings are presented) and sometimes deep (e.g.,
when we dive into the hash tables underneath the dict and set types).
Part III
Here we talk about functions as first-class objects in the language: what that means,
how it affects some popular design patterns, and how to implement function decorators
by leveraging closures. Also covered here is the general concept of callables
in Python, function attributes, introspection, parameter annotations, and the new
nonlocal declaration in Python 3.
Part IV
Now the focus is on building classes. In Part II, the class declaration appears in
few examples; Part IV presents many classes. Like any object-oriented (OO) language,
Python has its particular set of features that may or may not be present in
the language in which you and I learned class-based programming. The chapters
explain how references work, what mutability really means, the lifecycle of instan?
Preface | xvii
ces, how to build your own collections and ABCs, how to cope with multiple inheritance,
and how to implement operator overloading�when that makes sense.
Part V
Covered in this part are the language constructs and libraries that go beyond sequential
control flow with conditionals, loops, and subroutines. We start with generators,
then visit context managers and coroutines, including the challenging but
powerful new yield from syntax. Part V closes with a high-level introduction to
modern concurrency in Python with collections.futures (using threads and
processes under the covers with the help of futures) and doing event-oriented I/O
with asyncio (leveraging futures on top of coroutines and yield from).
Part VI
This part starts with a review of techniques for building classes with attributes
created dynamically to handle semi-structured data such as JSON datasets. Next,
we cover the familiar properties mechanism, before diving into how object attribute
access works at a lower level in Python using descriptors. The relationship between
functions, methods, and descriptors is explained. Throughout Part VI, the step-bystep
implementation of a field validation library uncovers subtle issues that lead to
the use of the advanced tools of the final chapter: class decorators and metaclasses.
Hands-On Approach
Often we�ll use the interactive Python console to explore the language and libraries. I
feel it is important to emphasize the power of this learning tool, particularly for those
readers who�ve had more experience with static, compiled languages that don�t provide
a read-eval-print#loop (REPL).
One of the standard Python testing packages, doctest, works by simulating console
sessions and verifying that the expressions evaluate to the responses shown. I used
doctest to check most of the code in this book, including the console listings. You don�t
need to use or even know about doctest to follow along: the key feature of doctests is
that they look like transcripts of interactive Python console sessions, so you can easily
try out the demonstrations yourself.
Sometimes I will explain what we want to accomplish by showing a doctest before the
code that makes it pass. Firmly establishing what is to be done before thinking about
how to do it helps focus our coding effort. Writing tests first is the basis of test driven
development (TDD) and I�ve also found it helpful when teaching. If you are unfamiliar
with doctest, take a look at its documentation and this book�s source code repository.
You�ll find that you can verify the correctness of most of the code in the book by typing
python3 -m doctest example_script.py in the command shell of your OS.
xviii | Preface
Hardware Used for Timings
The book has some simple benchmarks and timings. Those tests were performed on
one or the other laptop I used to write the book: a 2011 MacBook Pro 13� with a 2.7
GHz Intel Core i7 CPU, 8GB of RAM, and a spinning hard disk, and a 2014 MacBook
Air 13� with a 1.4 GHz Intel Core i5 CPU, 4GB of RAM, and a solid-state disk. The
MacBook Air has a slower CPU and less RAM, but its RAM is faster (1600 versus 1333
MHz) and the SSD is much faster than the HD. In daily usage, I can�t tell which machine
is faster.
Soapbox: My Personal Perspective
I have been using, teaching, and debating Python since 1998, and I enjoy studying and
comparing programming languages, their design, and the theory behind them. At the
end of some chapters, I have added �Soapbox� sidebars with my own perspective about
Python and other languages. Feel free to skip these if you are not into such discussions.
Their content is completely optional.
Python Jargon
I wanted this to be a book not only about Python but also about the culture around it.
Over more than 20 years of communications, the Python community has developed its
own particular lingo and acronyms. At the end of this book, Python Jargon contains a
list of terms that have special meaning among Pythonistas.
Python Version Covered
I tested all the code in the book using Python 3.4�that is, CPython 3.4, the most popular
Python implementation written in C. There is only one exception: �The New @ Infix
Operator in Python 3.5� on page 383 shows the @ operator, which is only supported by
Python 3.5.
Almost all code in the book should work with any Python 3.x�compatible interpreter,
including PyPy3 2.4.0, which is compatible with Python 3.2.5. The notable exceptions
are the examples using yield from and asyncio, which are only available in Python
3.3 or later.
Most code should also work with Python 2.7 with minor changes, except the Unicoderelated
examples in Chapter 4, and the exceptions already noted for Python 3 versions
earlier than 3.3.
Preface | xix
Conventions Used in This Book
The following typographical conventions are used in this book:
Italic
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width
Used for program listings, as well as within paragraphs to refer to program elements
such as variable or function names, databases, data types, environment variables,
statements, and keywords.
Note that when a line break falls within a constant_width term, a hyphen is not added
�it could be misunderstood as part of the term.
Constant width bold
Shows commands or other text that should be typed literally by the user.
Constant width italic
Shows text that should be replaced with user-supplied values or by values determined
by context.
This element signifies a tip or suggestion.
This element signifies a general note.
This element indicates a warning or caution.
Using Code Examples
Every script and most code snippets that appear in the book are available in the Fluent
Python code repository on GitHub.
xx | Preface
We appreciate, but do not require, attribution. An attribution usually includes the title,
author, publisher, and ISBN. For example: �Fluent Python by Luciano Ramalho (O�Reilly).
Copyright 2015 Luciano Ramalho, 978-1-491-94600-8.�
Safari� Books Online
Safari Books Online is an on-demand digital library that
delivers expert content in both book and video form
from the world�s leading authors in technology and
business.
Technology professionals, software developers, web designers, and business and creative
professionals use Safari Books Online as their primary resource for research, problem
solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations,
government agencies, and individuals. Subscribers have access to thousands of
books, training videos, and prepublication manuscripts in one fully searchable database
from publishers like O�Reilly Media, Prentice Hall Professional, Addison-Wesley Professional,
Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John
Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT
Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course Technology,
and dozens more. For more information about Safari Books Online, please visit us
online.
How to Contact Us
Please address comments and questions concerning this book to the publisher:
O�Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
800-998-9938 (in the United States or Canada)
707-829-0515 (international or local)
707-829-0104 (fax)
We have a web page for this book, where we list errata, examples, and any additional
information. You can access this page at http://bit.ly/fluent-python.
To comment or ask technical questions about this book, send email to bookques
tions@oreilly.com.
For more information about our books, courses, conferences, and news, see our website
at http://www.oreilly.com.
Preface | xxi
Find us on Facebook: http://facebook.com/oreilly
Follow us on Twitter: http://twitter.com/oreillymedia
Watch us on YouTube: http://www.youtube.com/oreillymedia
Acknowledgments
The Bauhaus chess set by Josef Hartwig is an example of excellent design: beautiful,
simple, and clear. Guido van Rossum, son of an architect and brother of a master font
designer, created a masterpiece of language design. I love teaching Python because it is
beautiful, simple, and clear.
Alex Martelli and Anna Ravenscroft were the first people to see the outline of this book
and encouraged me to submit it to O�Reilly for publication. Their books taught me
idiomatic Python and are models of clarity, accuracy, and depth in technical writing.
Alex�s 5,000+ Stack Overflow posts are a fountain of insights about the language and its
proper use.
Martelli and Ravenscroft were also technical reviewers of this book, along with Lennart
Regebro and Leonardo Rochael. Everyone in this outstanding technical review team
has at least 15 years of Python experience, with many contributions to high-impact
Python projects in close contact with other developers in the community. Together they
sent me hundreds of corrections, suggestions, questions, and opinions, adding tremendous
value to the book. Victor Stinner kindly reviewed Chapter 18, bringing his expertise
as an asyncio maintainer to the technical review team. It was a great privilege and a
pleasure to collaborate with them over these past several months.
Editor Meghan Blanchette was an outstanding mentor, helping me improve the organization
and flow of the book, letting me know when it was boring, and keeping me
from delaying even more. Brian MacDonald edited chapters in Part III while Meghan
was away. I enjoyed working with them, and with everyone I�ve contacted at O�Reilly,
including the Atlas development and support team (Atlas is the O�Reilly book publishing
platform, which I was fortunate to use to write this book).
Mario Domenech Goulart provided numerous, detailed suggestions starting with the
first Early Release. I also received valuable feedback from Dave Pawson, Elias Dorneles,
Leonardo Alexandre Ferreira Leite, Bruce Eckel, J. S. Bueno, Rafael Goncalves, Alex
Chiaranda, Guto Maia, Lucas Vido, and Lucas Brunialti.
Over the years, a number of people urged me to become an author, but the most persuasive
were Rubens Prates, Aurelio Jargas, Ruda Moura, and Rubens Altimari. Mauricio
Bussab opened many doors for me, including my first real shot at writing a book. Renzo
Nuccitelli supported this writing project all the way, even if that meant a slow start for
our partnership at python.pro.br.
xxii | Preface
The wonderful Brazilian Python community is knowledgeable, generous, and fun. The
Python Brasil group has thousands of people and our national conferences bring together
hundreds, but the most influential in my journey as a Pythonista were Leonardo
Rochael, Adriano Petrich, Daniel Vainsencher, Rodrigo RBP Pimentel, Bruno Gola,
Leonardo Santagada, Jean Ferri, Rodrigo Senra, J. S. Bueno, David Kwast, Luiz Irber,
Osvaldo Santana, Fernando Masanori, Henrique Bastos, Gustavo Niemayer, Pedro
Werneck, Gustavo Barbieri, Lalo Martins, Danilo Bellini, and Pedro Kroger.
Dorneles Tremea was a great friend (incredibly generous with his time and knowledge),
an amazing hacker, and the most inspiring leader of the Brazilian Python Association.
He left us too early.
My students over the years taught me a lot through their questions, insights, feedback,
and creative solutions to problems. Erico Andrei and Simples Consultoria made it possible
for me to focus on being a Python teacher for the first time.
Martijn Faassen was my Grok mentor and shared invaluable insights with me about
Python and Neanderthals. His work and that of Paul Everitt, Chris McDonough, Tres
Seaver, Jim Fulton, Shane Hathaway, Lennart Regebro, Alan Runyan, Alexander Limi,
Martijn Pieters, Godefroid Chapelle, and others from the Zope, Plone, and Pyramid
planets have been decisive in my career. Thanks to Zope and surfing the first web wave,
I was able to start making a living with Python in 1998. Jose Octavio Castro Neves was
my partner in the first Python-centric software house in Brazil.
I have too many gurus in the wider Python community to list them all, but besides those
already mentioned, I am indebted to Steve Holden, Raymond Hettinger, A.M. Kuchling,
David Beazley, Fredrik Lundh, Doug Hellmann, Nick Coghlan, Mark Pilgrim, Martijn
Pieters, Bruce Eckel, Michele Simionato, Wesley Chun, Brandon Craig Rhodes, Philip
Guo, Daniel Greenfeld, Audrey Roy, and Brett Slatkin for teaching me new and better
ways to teach Python.
Most of these pages were written in my home office and in two labs: CoffeeLab and
Garoa Hacker Clube. CoffeeLab is the caffeine-geek headquarters in Vila Madalena, Sao
Paulo, Brazil. Garoa Hacker Clube is a hackerspace open to all: a community lab where
anyone can freely try out new ideas.
The Garoa community provided inspiration, infrastructure, and slack. I think Aleph
would enjoy this book.
My mother, Maria Lucia, and my father, Jairo, always supported me in every way. I wish
he was here to see the book; I am glad I can share it with her.
My wife, Marta Mello, endured 15 months of a husband who was always working, but
remained supportive and coached me through some critical moments in the project
when I feared I might drop out of the marathon.
Thank you all, for everything.
Preface | xxiii

PART I
Prologue

1. Story of Jython, written as a Foreword to Jython Essentials (O�Reilly, 2002), by Samuele Pedroni and Noel
Rappin.
CHAPTER 1
The Python Data Model
Guido�s sense of the aesthetics of language design is amazing. I�ve met many fine language
designers who could build theoretically beautiful languages that no one would ever use,
but Guido is one of those rare people who can build a language that is just slightly less
theoretically beautiful but thereby is a joy to write programs in.1
� Jim Hugunin
Creator of Jython, cocreator of AspectJ, architect of the .Net DLR
One of the best qualities of Python is its consistency. After working with Python for a
while, you are able to start making informed, correct guesses about features that are
new to you.
However, if you learned another object-oriented language before Python, you may have
found it strange to use len(collection) instead of collection.len(). This apparent
oddity is the tip of an iceberg that, when properly understood, is the key to everything
we call Pythonic. The iceberg is called the Python data model, and it describes the API
that you can use to make your own objects play well with the most idiomatic language
features.
You can think of the data model as a description of Python as a framework. It formalizes
the interfaces of the building blocks of the language itself, such as sequences, iterators,
functions, classes, context managers, and so on.
While coding with any framework, you spend a lot of time implementing methods that
are called by the framework. The same happens when you leverage the Python data
model. The Python interpreter invokes special methods to perform basic object operations,
often triggered by special syntax. The special method names are always written
with leading and trailing double underscores (i.e., __getitem__). For example, the syn?
3
2. See �Private and �Protected� Attributes in Python� on page 262.
3. I personally first heard �dunder� from Steve Holden. Wikipedia credits Mark Johnson and Tim Hochberg
for the first written records of �dunder� in responses to the question �How do you pronounce __ (double
underscore)?� in the python-list on September 26, 2002: Johnson�s message; Hochberg�s (11 minutes later).
tax obj[key] is supported by the __getitem__ special method. In order to evaluate
my_collection[key], the interpreter calls my_collection.__getitem__(key).
The special method names allow your objects to implement, support, and interact with
basic language constructs such as:
� Iteration
� Collections
� Attribute access
� Operator overloading
� Function and method invocation
� Object creation and destruction
� String representation and formatting
� Managed contexts (i.e., with blocks)
Magic and Dunder
The term magic method is slang for special method, but when
talking about a specific method like __getitem__, some Python
developers take the shortcut of saying �under-under-getitem�
which is ambiguous, because the syntax __x has another special
meaning.2 Being precise and pronouncing �under-under-getitemunder-
under� is tiresome, so I follow the lead of author and teacher
Steve Holden and say �dunder-getitem.� All experienced Pythonistas
understand that shortcut. As a result, the special methods
are also known as dunder methods.3
A Pythonic Card Deck
The following is a very simple example, but it demonstrates the power of implementing
just two special methods, __getitem__ and __len__.
Example 1-1 is a class to represent a deck of playing cards.
Example 1-1. A deck as a sequence of cards
import collections
4 | Chapter 1: The Python Data Model
Card = collections.namedtuple('Card', ['rank', 'suit'])
class FrenchDeck:
ranks = [str(n) for n in range(2, 11)] + list('JQKA')
suits = 'spades diamonds clubs hearts'.split()
def __init__(self):
self._cards = [Card(rank, suit) for suit in self.suits
for rank in self.ranks]
def __len__(self):
return len(self._cards)
def __getitem__(self, position):
return self._cards[position]
The first thing to note is the use of collections.namedtuple to construct a simple class
to represent individual cards. Since Python 2.6, namedtuple can be used to build classes
of objects that are just bundles of attributes with no custom methods, like a database
record. In the example, we use it to provide a nice representation for the cards in the
deck, as shown in the console session:
>>> beer_card = Card('7', 'diamonds')
>>> beer_card
Card(rank='7', suit='diamonds')
But the point of this example is the FrenchDeck class. It�s short, but it packs a punch.
First, like any standard Python collection, a deck responds to the len() function by
returning the number of cards in it:
>>> deck = FrenchDeck()
>>> len(deck)
52
Reading specific cards from the deck�say, the first or the last�should be as easy as
deck[0] or deck[-1], and this is what the __getitem__ method provides:
>>> deck[0]
Card(rank='2', suit='spades')
>>> deck[-1]
Card(rank='A', suit='hearts')
Should we create a method to pick a random card? No need. Python already has a
function to get a random item from a sequence: random.choice. We can just use it on
a deck instance:
>>> from random import choice
>>> choice(deck)
Card(rank='3', suit='hearts')
>>> choice(deck)
Card(rank='K', suit='spades')
A Pythonic Card Deck | 5
>>> choice(deck)
Card(rank='2', suit='clubs')
We�ve just seen two advantages of using special methods to leverage the Python data
model:
� The users of your classes don�t have to memorize arbitrary method names for standard
operations (�How to get the number of items? Is it .size(), .length(), or
what?�).
� It�s easier to benefit from the rich Python standard library and avoid reinventing
the wheel, like the random.choice function.
But it gets better.
Because our __getitem__ delegates to the [] operator of self._cards, our deck automatically
supports slicing. Here�s how we look at the top three cards from a brand new
deck, and then pick just the aces by starting on index 12 and skipping 13 cards at a time:
>>> deck[:3]
[Card(rank='2', suit='spades'), Card(rank='3', suit='spades'),
Card(rank='4', suit='spades')]
>>> deck[12::13]
[Card(rank='A', suit='spades'), Card(rank='A', suit='diamonds'),
Card(rank='A', suit='clubs'), Card(rank='A', suit='hearts')]
Just by implementing the __getitem__ special method, our deck is also iterable:
>>> for card in deck: # doctest: +ELLIPSIS
... print(card)
Card(rank='2', suit='spades')
Card(rank='3', suit='spades')
Card(rank='4', suit='spades')
...
The deck can also be iterated in reverse:
>>> for card in reversed(deck): # doctest: +ELLIPSIS
... print(card)
Card(rank='A', suit='hearts')
Card(rank='K', suit='hearts')
Card(rank='Q', suit='hearts')
...
6 | Chapter 1: The Python Data Model
4. In Python 2, you�d have to be explicit and write FrenchDeck(object), but that�s the default in Python 3.
Ellipsis in doctests
Whenever possible, the Python console listings in this book were
extracted from doctests to ensure accuracy. When the output was
too long, the elided part is marked by an ellipsis (...) like in the
last line in the preceding code. In such cases, we used the #
doctest: +ELLIPSIS directive to make the doctest pass. If you
are trying these examples in the interactive console, you may omit
the doctest directives altogether.
Iteration is often implicit. If a collection has no __contains__ method, the in operator
does a sequential scan. Case in point: in works with our FrenchDeck class because it is
iterable. Check it out:
>>> Card('Q', 'hearts') in deck
True
>>> Card('7', 'beasts') in deck
False
How about sorting? A common system of ranking cards is by rank (with aces being
highest), then by suit in the order of spades (highest), then hearts, diamonds, and clubs
(lowest). Here is a function that ranks cards by that rule, returning 0 for the 2 of clubs
and 51 for the ace of spades:
suit_values = dict(spades=3, hearts=2, diamonds=1, clubs=0)
def spades_high(card):
rank_value = FrenchDeck.ranks.index(card.rank)
return rank_value * len(suit_values) + suit_values[card.suit]
Given spades_high, we can now list our deck in order of increasing rank:
>>> for card in sorted(deck, key=spades_high): # doctest: +ELLIPSIS
... print(card)
Card(rank='2', suit='clubs')
Card(rank='2', suit='diamonds')
Card(rank='2', suit='hearts')
... (46 cards ommitted)
Card(rank='A', suit='diamonds')
Card(rank='A', suit='hearts')
Card(rank='A', suit='spades')
Although FrenchDeck implicitly inherits from object,4 its functionality is not inherited,
but comes from leveraging the data model and composition. By implementing the special
methods __len__ and __getitem__, our FrenchDeck behaves like a standard Python
sequence, allowing it to benefit from core language features (e.g., iteration and slicing)
A Pythonic Card Deck | 7
and from the standard library, as shown by the examples using random.choice,
reversed, and sorted. Thanks to composition, the __len__ and __getitem__ implementations
can hand off all the work to a list object, self._cards.
How About Shuffling?
As implemented so far, a FrenchDeck cannot be shuffled, because
it is immutable: the cards and their positions cannot be
changed, except by violating encapsulation and handling the
_cards attribute directly. In Chapter 11, that will be fixed by
adding a one-line __setitem__ method.
How Special Methods Are Used
The first thing to know about special methods is that they are meant to be called by the
Python interpreter, and not by you. You don�t write my_object.__len__(). You write
len(my_object) and, if my_object is an instance of a user-defined class, then Python
calls the __len__ instance method you implemented.
But for built-in types like list, str, bytearray, and so on, the interpreter takes a shortcut:
the CPython implementation of len() actually returns the value of the ob_size
field in the PyVarObject C struct that represents any variable-sized built-in object in
memory. This is much faster than calling a method.
More often than not, the special method call is implicit. For example, the statement for
i in x: actually causes the invocation of iter(x), which in turn may call x.__iter__()
if that is available.
Normally, your code should not have many direct calls to special methods. Unless you
are doing a lot of metaprogramming, you should be implementing special methods
more often than invoking them explicitly. The only special method that is frequently
called by user code directly is __init__, to invoke the initializer of the superclass in
your own __init__ implementation.
If you need to invoke a special method, it is usually better to call the related built-in
function (e.g., len, iter, str, etc). These built-ins call the corresponding special method,
but often provide other services and�for built-in types�are faster than method
calls. See, for example, �A Closer Look at the iter Function� on page 436 in Chapter 14.
Avoid creating arbitrary, custom attributes with the __foo__ syntax because such names
may acquire special meanings in the future, even if they are unused today.
8 | Chapter 1: The Python Data Model
Emulating Numeric Types
Several special methods allow user objects to respond to operators such as +. We will
cover that in more detail in Chapter 13, but here our goal is to further illustrate the use
of special methods through another simple example.
We will implement a class to represent two-dimensional vectors�that is Euclidean
vectors like those used in math and physics (see Figure 1-1).
Figure 1-1. Example of two-dimensional vector addition; Vector(2, 4) + Vector(2, 1) results
in Vector(4, 5).
The built-in complex type can be used to represent twodimensional
vectors, but our class can be extended to represent ndimensional
vectors. We will do that in Chapter 14.
We will start by designing the API for such a class by writing a simulated console session
that we can use later as a doctest. The following snippet tests the vector addition pictured
in Figure 1-1:
>>> v1 = Vector(2, 4)
>>> v2 = Vector(2, 1)
>>> v1 + v2
Vector(4, 5)
Note how the + operator produces a Vector result, which is displayed in a friendly
manner in the console.
How Special Methods Are Used | 9
The abs built-in function returns the absolute value of integers and floats, and the
magnitude of complex numbers, so to be consistent, our API also uses abs to calculate
the magnitude of a vector:
>>> v = Vector(3, 4)
>>> abs(v)
5.0
We can also implement the * operator to perform scalar multiplication (i.e., multiplying
a vector by a number to produce a new vector with the same direction and a multiplied
magnitude):
>>> v * 3
Vector(9, 12)
>>> abs(v * 3)
15.0
Example 1-2 is a Vector class implementing the operations just described, through the
use of the special methods __repr__, __abs__, __add__ and __mul__.
Example 1-2. A simple two-dimensional vector class
from math import hypot
class Vector:
def __init__(self, x=0, y=0):
self.x = x
self.y = y
def __repr__(self):
return 'Vector(%r, %r)' % (self.x, self.y)
def __abs__(self):
return hypot(self.x, self.y)
def __bool__(self):
return bool(abs(self))
def __add__(self, other):
x = self.x + other.x
y = self.y + other.y
return Vector(x, y)
def __mul__(self, scalar):
return Vector(self.x * scalar, self.y * scalar)
Note that although we implemented four special methods (apart from __init__), none
of them is directly called within the class or in the typical usage of the class illustrated
by the console listings. As mentioned before, the Python interpreter is the only frequent
10 | Chapter 1: The Python Data Model
caller of most special methods. In the following sections, we discuss the code for each
special method.
String Representation
The __repr__ special method is called by the repr built-in to get the string representation
of the object for inspection. If we did not implement __repr__, vector instances
would be shown in the console like <Vector object at 0x10e100070>.
The interactive console and debugger call repr on the results of the expressions evaluated,
as does the %r placeholder in classic formatting with the % operator, and the !r
conversion field in the new Format String Syntax used in the str.format method.
Speaking of the % operator and the str.format method, you will
notice I use both in this book, as does the Python community at
large. I am increasingly favoring the more powerful str.for
mat, but I am aware many Pythonistas prefer the simpler %, so
we�ll probably see both in Python source code for the foreseeable
future.
Note that in our __repr__ implementation, we used %r to obtain the standard representation
of the attributes to be displayed. This is good practice, because it shows the
crucial difference between Vector(1, 2) and Vector('1', '2')�the latter would not
work in the context of this example, because the constructor�s arguments must be numbers,
not str.
The string returned by __repr__ should be unambiguous and, if possible, match the
source code necessary to re-create the object being represented. That is why our chosen
representation looks like calling the constructor of the class (e.g., Vector(3, 4)).
Contrast __repr__ with __str__, which is called by the str() constructor and implicitly
used by the print function. __str__ should return a string suitable for display to end
users.
If you only implement one of these special methods, choose __repr__, because when
no custom __str__ is available, Python will call __repr__ as a fallback.
�Difference between __str__ and __repr__ in Python� is a Stack
Overflow question with excellent contributions from Pythonistas
Alex Martelli and Martijn Pieters.
How Special Methods Are Used | 11
Arithmetic Operators
Example 1-2 implements two operators: + and *, to show basic usage of __add__ and
__mul__. Note that in both cases, the methods create and return a new instance of
Vector, and do not modify either operand�self or other are merely read. This is the
expected behavior of infix operators: to create new objects and not touch their operands.
I will have a lot more to say about that in Chapter 13.
As implemented, Example 1-2 allows multiplying a Vector by a
number, but not a number by a Vector, which violates the commutative
property of multiplication. We will fix that with the special
method __rmul__ in Chapter 13.
Boolean Value of a Custom Type
Although Python has a bool type, it accepts any object in a boolean context, such as the
expression controlling an if or while statement, or as operands to and, or, and not. To
determine whether a value x is truthy or falsy, Python applies bool(x), which always
returns True or False.
By default, instances of user-defined classes are considered truthy, unless either
__bool__ or __len__ is implemented. Basically, bool(x) calls x.__bool__() and uses
the result. If __bool__ is not implemented, Python tries to invoke x.__len__(), and if
that returns zero, bool returns False. Otherwise bool returns True.
Our implementation of __bool__ is conceptually simple: it returns False if the magnitude
of the vector is zero, True otherwise. We convert the magnitude to a Boolean
using bool(abs(self)) because __bool__ is expected to return a boolean.
Note how the special method __bool__ allows your objects to be consistent with the
truth value testing rules defined in the �Built-in Types� chapter of The Python Standard
Library documentation.
A faster implementation of Vector.__bool__ is this:
def __bool__(self):
return bool(self.x or self.y)
This is harder to read, but avoids the trip through abs, __abs__,
the squares, and square root. The explicit conversion to bool is
needed because __bool__ must return a boolean and or returns
either operand as is: x or y evaluates to x if that is truthy, otherwise
the result is y, whatever that is.
12 | Chapter 1: The Python Data Model
Overview of Special Methods
The �Data Model� chapter of The Python Language Reference lists 83 special method
names, 47 of which are used to implement arithmetic, bitwise, and comparison operators.
As an overview of what is available, see Tables 1-1 and 1-2.
The grouping shown in the following tables is not exactly the same
as in the official documentation.
Table 1-1. Special method names (operators excluded)
Category Method names
String/bytes representation __repr__, __str__, __format__, __bytes__
Conversion to number __abs__, __bool__, __complex__, __int__, __float__, __hash__,
__index__
Emulating collections __len__, __getitem__, __setitem__, __delitem__, __contains__
Iteration __iter__, __reversed__, __next__
Emulating callables __call__
Context management __enter__, __exit__
Instance creation and destruction __new__, __init__, __del__
Attribute management __getattr__, __getattribute__, __setattr__, __delattr__, __dir__
Attribute descriptors __get__, __set__, __delete__
Class services __prepare__, __instancecheck__, __subclasscheck__
Table 1-2. Special method names for operators
Category Method names and related operators
Unary numeric operators __neg__ -, __pos__ +, __abs__ abs()
Rich comparison operators __lt__ >, __le__ <=, __eq__ ==, __ne__ !=, __gt__ >, __ge__ >=
Arithmetic operators __add__ +, __sub__ -, __mul__ *, __truediv__ /, __floordiv__ //, __mod__
%, __divmod__ divmod() , __pow__ ** or pow(), __round__ round()
Reversed arithmetic operators __radd__, __rsub__, __rmul__, __rtruediv__, __rfloordiv__, __rmod__,
__rdivmod__, __rpow__
Augmented assignment
arithmetic operators
__iadd__, __isub__, __imul__, __itruediv__, __ifloordiv__, __imod__,
__ipow__
Bitwise operators __invert__ ~, __lshift__ <<, __rshift__ >>, __and__ &, __or__ |,
__xor__ ^
Overview of Special Methods | 13
Category Method names and related operators
Reversed bitwise operators __rlshift__, __rrshift__, __rand__, __rxor__, __ror__
Augmented assignment bitwise
operators
__ilshift__, __irshift__, __iand__, __ixor__, __ior__
The reversed operators are fallbacks used when operands are
swapped (b * a instead of a * b), while augmented assignments
are shortcuts combining an infix operator with variable assignment
(a = a * b becomes a *= b). Chapter 13 explains both
reversed operators and augmented assignment in detail.
Why len Is Not a Method
I asked this question to core developer Raymond Hettinger in 2013 and the key to his
answer was a quote from The Zen of Python: �practicality beats purity.� In �How Special
Methods Are Used� on page 8, I described how len(x) runs very fast when x is an
instance of a built-in type. No method is called for the built-in objects of CPython: the
length is simply read from a field in a C struct. Getting the number of items in a collection
is a common operation and must work efficiently for such basic and diverse types as
str, list, memoryview, and so on.
In other words, len is not called as a method because it gets special treatment as part of
the Python data model, just like abs. But thanks to the special method __len__, you can
also make len work with your own custom objects. This is a fair compromise between
the need for efficient built-in objects and the consistency of the language. Also from
The Zen of Python: �Special cases aren�t special enough to break the rules.�
If you think of abs and len as unary operators, you may be more
inclined to forgive their functional look-and-feel, as opposed to
the method call syntax one might expect in an OO language. In
fact, the ABC language�a direct ancestor of Python that pioneered
many of its features�had an # operator that was the
equivalent of len (you�d write #s). When used as an infix operator,
written x#s, it counted the occurrences of x in s, which in
Python you get as s.count(x), for any sequence s.
Chapter Summary
By implementing special methods, your objects can behave like the built-in types, enabling
the expressive coding style the community considers Pythonic.
14 | Chapter 1: The Python Data Model
A basic requirement for a Python object is to provide usable string representations of
itself, one used for debugging and logging, another for presentation to end users. That
is why the special methods __repr__ and __str__ exist in the data model.
Emulating sequences, as shown with the FrenchDeck example, is one of the most widely
used applications of the special methods. Making the most of sequence types is the
subject of Chapter 2, and implementing your own sequence will be covered in Chapter
10 when we create a multidimensional extension of the Vector class.
Thanks to operator overloading, Python offers a rich selection of numeric types, from
the built-ins to decimal.Decimal and fractions.Fraction, all supporting infix arithmetic
operators. Implementing operators, including reversed operators and augmented
assignment, will be shown in Chapter 13 via enhancements of the Vector example.
The use and implementation of the majority of the remaining special methods of the
Python data model is covered throughout this book.
Further Reading
The �Data Model� chapter of The Python Language Reference is the canonical source
for the subject of this chapter and much of this book.
Python in a Nutshell, 2nd Edition (O�Reilly) by Alex Martelli has excellent coverage of
the data model. As I write this, the most recent edition of the Nutshell book is from 2006
and focuses on Python 2.5, but there have been very few changes in the data model since
then, and Martelli�s description of the mechanics of attribute access is the most authoritative
I�ve seen apart from the actual C source code of CPython. Martelli is also a prolific
contributor to Stack Overflow, with more than 5,000 answers posted. See his user profile
at Stack Overflow.
David Beazley has two books covering the data model in detail in the context of Python
3: Python Essential Reference, 4th Edition (Addison-Wesley Professional), and Python
Cookbook, 3rd Edition (O�Reilly), coauthored with Brian K. Jones.
The Art of the Metaobject Protocol (AMOP, MIT Press) by Gregor Kiczales, Jim des
Rivieres, and Daniel G. Bobrow explains the concept of a metaobject protocol (MOP),
of which the Python data model is one example.
Soapbox
Data Model or Object Model?
What the Python documentation calls the �Python data model,� most authors would say
is the �Python object model.� Alex Martelli�s Python in a Nutshell 2E, and David Beazley�s
Python Essential Reference 4E are the best books covering the �Python data model,� but
Further Reading | 15
they always refer to it as the �object model.� On Wikipedia, the first definition of object
model is �The properties of objects in general in a specific computer programming
language.� This is what the �Python data model� is about. In this book, I will use �data
model� because the documentation favors that term when referring to the Python object
model, and because it is the title of the chapter of The Python Language Reference most
relevant to our discussions.
Magic Methods
The Ruby community calls their equivalent of the special methods magic methods. Many
in the Python community adopt that term as well. I believe the special methods are
actually the opposite of magic. Python and Ruby are the same in this regard: both empower
their users with a rich metaobject protocol that is not magic, but enables users
to leverage the same tools available to core developers.
In contrast, consider JavaScript. Objects in that language have features that are magic,
in the sense that you cannot emulate them in your own user-defined objects. For example,
before JavaScript 1.8.5, you could not define read-only attributes in your Java?
Script objects, but some built-in objects always had read-only attributes. In JavaScript,
read-only attributes were �magic,� requiring supernatural powers that a user of the language
did not have until ECMAScript 5.1 came out in 2009. The metaobject protocol
of JavaScript is evolving, but historically it has been more limited than those of Python
and Ruby.
Metaobjects
The Art of the Metaobject Protocol (AMOP) is my favorite computer book title. Less
subjectively, the term metaobject protocol is useful to think about the Python data model
and similar features in other languages. The metaobject part refers to the objects that
are the building blocks of the language itself. In this context, protocol is a synonym of
interface. So a metaobject protocol is a fancy synonym for object model: an API for core
language constructs.
A rich metaobject protocol enables extending a language to support new programming
paradigms. Gregor Kiczales, the first author of the AMOP book, later became a pioneer
in aspect-oriented programming and the initial author of AspectJ, an extension of Java
implementing that paradigm. Aspect-oriented programming is much easier to implement
in a dynamic language like Python, and several frameworks do it, but the most
important is zope.interface, which is briefly discussed in �Further Reading� on page
342 of Chapter 11.
16 | Chapter 1: The Python Data Model
PART II
Data Structures

1. Leo Geurts, Lambert Meertens, and Steven Pemberton, ABC Programmer�s Handbook, p. 8.
CHAPTER 2
An Array of Sequences
As you may have noticed, several of the operations mentioned work equally for texts, lists
and tables. Texts, lists and tables together are called trains. [�] The FOR command also
works generically on trains.1
� Geurts, Meertens, and Pemberton
ABC Programmer�s Handbook
Before creating Python, Guido was a contributor to the ABC language�a 10-year research
project to design a programming environment for beginners. ABC introduced
many ideas we now consider �Pythonic�: generic operations on sequences, built-in tuple
and mapping types, structure by indentation, strong typing without variable declarations,
and more. It�s no accident that Python is so user-friendly.
Python inherited from ABC the uniform handling of sequences. Strings, lists, byte sequences,
arrays, XML elements, and database results share a rich set of common operations
including iteration, slicing, sorting, and concatenation.
Understanding the variety of sequences available in Python saves us from reinventing
the wheel, and their common interface inspires us to create APIs that properly support
and leverage existing and future sequence types.
Most of the discussion in this chapter applies to sequences in general, from the familiar
list to the str and bytes types that are new in Python 3. Specific topics on lists, tuples,
arrays, and queues are also covered here, but the focus on Unicode strings and byte
sequences is deferred to Chapter 4. Also, the idea here is to cover sequence types that
are ready to use. Creating your own sequence types is the subject of Chapter 10.
19
Overview of Built-In Sequences
The standard library offers a rich selection of sequence types implemented in C:
Container sequences
list, tuple, and collections.deque can hold items of different types.
Flat sequences
str, bytes, bytearray, memoryview, and array.array hold items of one type.
Container sequences hold references to the objects they contain, which may be of any
type, while flat sequences physically store the value of each item within its own memory
space, and not as distinct objects. Thus, flat sequences are more compact, but they are
limited to holding primitive values like characters, bytes, and numbers.
Another way of grouping sequence types is by mutability:
Mutable sequences
list, bytearray, array.array, collections.deque, and memoryview
Immutable sequences
tuple, str, and bytes
Figure 2-1 helps visualize how mutable sequences differ from immutable ones, while
also inheriting several methods from them. Note that the built-in concrete sequence
types do not actually subclass the Sequence and MutableSequence abstract base classes
(ABCs) depicted, but the ABCs are still useful as a formalization of what functionality
to expect from a full-featured sequence type.
Figure 2-1. UML class diagram for some classes from collections.abc (superclasses are
on the left; inheritance arrows point from subclasses to superclasses; names in italic are
abstract classes and abstract methods)
Keeping in mind these common traits�mutable versus immutable; container versus
flat�is helpful to extrapolate what you know about one sequence type to others.
20 | Chapter 2: An Array of Sequences
The most fundamental sequence type is the list�mutable and mixed-type. I am sure
you are comfortable handling them, so we�ll jump right into list comprehensions, a
powerful way of building lists that is somewhat underused because the syntax may be
unfamiliar. Mastering list comprehensions opens the door to generator expressions,
which�among other uses�can produce elements to fill up sequences of any type. Both
are the subject of the next section.
List Comprehensions and Generator Expressions
A quick way to build a sequence is using a list comprehension (if the target is a list)
or a generator expression (for all other kinds of sequences). If you are not using these
syntactic forms on a daily basis, I bet you are missing opportunities to write code that
is more readable and often faster at the same time.
If you doubt my claim that these constructs are �more readable,� read on. I�ll try to
convince you.
For brevity, many Python programmers refer to list comprehensions
as listcomps, and generator expressions as genexps. I will use
these words as well.
List Comprehensions and Readability
Here is a test: which do you find easier to read, Example 2-1 or Example 2-2?
Example 2-1. Build a list of Unicode codepoints from a string
>>> symbols = '$???��'
>>> codes = []
>>> for symbol in symbols:
... codes.append(ord(symbol))
...
>>> codes
[36, 162, 163, 165, 8364, 164]
Example 2-2. Build a list of Unicode codepoints from a string, take two
>>> symbols = '$???��'
>>> codes = [ord(symbol) for symbol in symbols]
>>> codes
[36, 162, 163, 165, 8364, 164]
Anybody who knows a little bit of Python can read Example 2-1. However, after learning
about listcomps, I find Example 2-2 more readable because its intent is explicit.
List Comprehensions and Generator Expressions | 21
A for loop may be used to do lots of different things: scanning a sequence to count or
pick items, computing aggregates (sums, averages), or any number of other processing
tasks. The code in Example 2-1 is building up a list. In contrast, a listcomp is meant to
do one thing only: to build a new list.
Of course, it is possible to abuse list comprehensions to write truly incomprehensible
code. I�ve seen Python code with listcomps used just to repeat a block of code for its side
effects. If you are not doing something with the produced list, you should not use that
syntax. Also, try to keep it short. If the list comprehension spans more than two lines,
it is probably best to break it apart or rewrite as a plain old for loop. Use your best
judgment: for Python as for English, there are no hard-and-fast rules for clear writing.
Syntax Tip
In Python code, line breaks are ignored inside pairs of [], {}, or
(). So you can build multiline lists, listcomps, genexps, dictionaries
and the like without using the ugly \ line continuation escape.
Listcomps No Longer Leak Their Variables
In Python 2.x, variables assigned in the for clauses in list comprehensions were set in
the surrounding scope, sometimes with tragic consequences. See the following Python
2.7 console session:
Python 2.7.6 (default, Mar 22 2014, 22:59:38)
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> x = 'my precious'
>>> dummy = [x for x in 'ABC']
>>> x
'C'
As you can see, the initial value of x was clobbered. This no longer happens in Python
3.
List comprehensions, generator expressions, and their siblings set and dict comprehensions
now have their own local scope, like functions. Variables assigned within the
expression are local, but variables in the surrounding scope can still be referenced. Even
better, the local variables do not mask the variables from the surrounding scope.
This is Python 3:
>>> x = 'ABC'
>>> dummy = [ord(x) for x in x]
>>> x
'ABC'
>>> dummy
22 | Chapter 2: An Array of Sequences
[65, 66, 67]
>>>
The value of x is preserved.
The list comprehension produces the expected list.
List comprehensions build lists from sequences or any other iterable type by filtering
and transforming items. The filter and map built-ins can be composed to do the same,
but readability suffers, as we will see next.
Listcomps Versus map and filter
Listcomps do everything the map and filter functions do, without the contortions of
the functionally challenged Python lambda. Consider Example 2-3.
Example 2-3. The same list built by a listcomp and a map/filter composition
>>> symbols = '$???��'
>>> beyond_ascii = [ord(s) for s in symbols if ord(s) > 127]
>>> beyond_ascii
[162, 163, 165, 8364, 164]
>>> beyond_ascii = list(filter(lambda c: c > 127, map(ord, symbols)))
>>> beyond_ascii
[162, 163, 165, 8364, 164]
I used to believe that map and filter were faster than the equivalent listcomps, but Alex
Martelli pointed out that�s not the case�at least not in the preceding examples. The 02-
array-seq/listcomp_speed.py script in the Fluent Python code repository is a simple speed
test comparing listcomp with filter/map.
I�ll have more to say about map and filter in Chapter 5. Now we turn to the use of
listcomps to compute Cartesian products: a list containing tuples built from all items
from two or more lists.
Cartesian Products
Listcomps can generate lists from the Cartesian product of two or more iterables. The
items that make up the cartesian product are tuples made from items from every input
iterable. The resulting list has a length equal to the lengths of the input iterables multiplied.
See Figure 2-2.
List Comprehensions and Generator Expressions | 23
Figure 2-2. The Cartesian product of a sequence of three card ranks and a sequence of
four suits results in a sequence of twelve pairings
For example, imagine you need to produce a list of T-shirts available in two colors and
three sizes. Example 2-4 shows how to produce that list using a listcomp. The result has
six items.
Example 2-4. Cartesian product using a list comprehension
>>> colors = ['black', 'white']
>>> sizes = ['S', 'M', 'L']
>>> tshirts = [(color, size) for color in colors for size in sizes]
>>> tshirts
[('black', 'S'), ('black', 'M'), ('black', 'L'), ('white', 'S'),
('white', 'M'), ('white', 'L')]
>>> for color in colors:
... for size in sizes:
... print((color, size))
...
('black', 'S')
('black', 'M')
('black', 'L')
('white', 'S')
('white', 'M')
('white', 'L')
>>> tshirts = [(color, size) for size in sizes
... for color in colors]
>>> tshirts
[('black', 'S'), ('white', 'S'), ('black', 'M'), ('white', 'M'),
('black', 'L'), ('white', 'L')]
24 | Chapter 2: An Array of Sequences
This generates a list of tuples arranged by color, then size.
Note how the resulting list is arranged as if the for loops were nested in the
same order as they appear in the listcomp.
To get items arranged by size, then color, just rearrange the for clauses; adding
a line break to the listcomp makes it easy to see how the result will be ordered.
In Example 1-1 (Chapter 1), the following expression was used to initialize a card deck
with a list made of 52 cards from all 13 ranks of each of the 4 suits, grouped by suit:
self._cards = [Card(rank, suit) for suit in self.suits
for rank in self.ranks]
Listcomps are a one-trick pony: they build lists. To fill up other sequence types, a genexp
is the way to go. The next section is a brief look at genexps in the context of building
nonlist sequences.
Generator Expressions
To initialize tuples, arrays, and other types of sequences, you could also start from a
listcomp, but a genexp saves memory because it yields items one by one using the iterator
protocol instead of building a whole list just to feed another constructor.
Genexps use the same syntax as listcomps, but are enclosed in parentheses rather than
brackets.
Example 2-5 shows basic usage of genexps to build a tuple and an array.
Example 2-5. Initializing a tuple and an array from a generator expression
>>> symbols = '$???��'
>>> tuple(ord(symbol) for symbol in symbols)
(36, 162, 163, 165, 8364, 164)
>>> import array
>>> array.array('I', (ord(symbol) for symbol in symbols))
array('I', [36, 162, 163, 165, 8364, 164])
If the generator expression is the single argument in a function call, there is no
need to duplicate the enclosing parentheses.
The array constructor takes two arguments, so the parentheses around the
generator expression are mandatory. The first argument of the array constructor
defines the storage type used for the numbers in the array, as we�ll see in �Arrays�
on page 48.
Example 2-6 uses a genexp with a Cartesian product to print out a roster of T-shirts of
two colors in three sizes. In contrast with Example 2-4, here the six-item list of T-shirts
is never built in memory: the generator expression feeds the for loop producing one
List Comprehensions and Generator Expressions | 25
item at a time. If the two lists used in the Cartesian product had 1,000 items each, using
a generator expression would save the expense of building a list with a million items
just to feed the for loop.
Example 2-6. Cartesian product in a generator expression
>>> colors = ['black', 'white']
>>> sizes = ['S', 'M', 'L']
>>> for tshirt in ('%s %s' % (c, s) for c in colors for s in sizes):
... print(tshirt)
...
black S
black M
black L
white S
white M
white L
The generator expression yields items one by one; a list with all six T-shirt
variations is never produced in this example.
Chapter 14 is devoted to explaining how generators work in detail. Here the idea was
just to show the use of generator expressions to initialize sequences other than lists, or
to produce output that you don�t need to keep in memory.
Now we move on to the other fundamental sequence type in Python: the tuple.
Tuples Are Not Just Immutable Lists
Some introductory texts about Python present tuples as �immutable lists,� but that is
short selling them. Tuples do double duty: they can be used as immutable lists and also
as records with no field names. This use is sometimes overlooked, so we will start with
that.
Tuples as Records
Tuples hold records: each item in the tuple holds the data for one field and the position
of the item gives its meaning.
If you think of a tuple just as an immutable list, the quantity and the order of the items
may or may not be important, depending on the context. But when using a tuple as a
collection of fields, the number of items is often fixed and their order is always vital.
Example 2-7 shows tuples being used as records. Note that in every expression, sorting
the tuple would destroy the information because the meaning of each data item is given
by its position in the tuple.
26 | Chapter 2: An Array of Sequences
Example 2-7. Tuples used as records
>>> lax_coordinates = (33.9425, -118.408056)
>>> city, year, pop, chg, area = ('Tokyo', 2003, 32450, 0.66, 8014)
>>> traveler_ids = [('USA', '31195855'), ('BRA', 'CE342567'),
... ('ESP', 'XDA205856')]
>>> for passport in sorted(traveler_ids):
... print('%s/%s' % passport)
...
BRA/CE342567
ESP/XDA205856
USA/31195855
>>> for country, _ in traveler_ids:
... print(country)
...
USA
BRA
ESP
Latitude and longitude of the Los Angeles International Airport.
Data about Tokyo: name, year, population (millions), population change (%),
area (km?).
A list of tuples of the form (country_code, passport_number).
As we iterate over the list, passport is bound to each tuple.
The % formatting operator understands tuples and treats each item as a separate
field.
The for loop knows how to retrieve the items of a tuple separately�this is called
�unpacking.� Here we are not interested in the second item, so it�s assigned to
_, a dummy variable.
Tuples work well as records because of the tuple unpacking mechanism�our next subject.
Tuple Unpacking
In Example 2-7, we assigned ('Tokyo', 2003, 32450, 0.66, 8014) to city, year,
pop, chg, area in a single statement. Then, in the last line, the % operator assigned
each item in the passport tuple to one slot in the format string in the print argument.
Those are two examples of tuple unpacking.
Tuples Are Not Just Immutable Lists | 27
Tuple unpacking works with any iterable object. The only requirement
is that the iterable yields exactly one item per variable in the
receiving tuple, unless you use a star (*) to capture excess items as
explained in �Using * to grab excess items� on page 29. The term
tuple unpacking is widely used by Pythonistas, but iterable unpacking
is gaining traction, as in the title of PEP 3132 � Extended
Iterable Unpacking.
The most visible form of tuple unpacking is parallel assignment; that is, assigning items
from an iterable to a tuple of variables, as you can see in this example:
>>> lax_coordinates = (33.9425, -118.408056)
>>> latitude, longitude = lax_coordinates # tuple unpacking
>>> latitude
33.9425
>>> longitude
-118.408056
An elegant application of tuple unpacking is swapping the values of variables without
using a temporary variable:
>>> b, a = a, b
Another example of tuple unpacking is prefixing an argument with a star when calling
a function:
>>> divmod(20, 8)
(2, 4)
>>> t = (20, 8)
>>> divmod(*t)
(2, 4)
>>> quotient, remainder = divmod(*t)
>>> quotient, remainder
(2, 4)
The preceding code also shows a further use of tuple unpacking: enabling functions to
return multiple values in a way that is convenient to the caller. For example, the
os.path.split() function builds a tuple (path, last_part) from a filesystem path:
>>> import os
>>> _, filename = os.path.split('/home/luciano/.ssh/idrsa.pub')
>>> filename
'idrsa.pub'
Sometimes when we only care about certain parts of a tuple when unpacking, a dummy
variable like _ is used as placeholder, as in the preceding example.
28 | Chapter 2: An Array of Sequences
If you write internationalized software, _ is not a good dummy
variable because it is traditionally used as an alias to the get
text.gettext function, as recommended in the gettext module
documentation. Otherwise, it�s a nice name for placeholder variable.
Another way of focusing on just some of the items when unpacking a tuple is to use the
*, as we�ll see right away.
Using * to grab excess items
Defining function parameters with *args to grab arbitrary excess arguments is a classic
Python feature.
In Python 3, this idea was extended to apply to parallel assignment as well:
>>> a, b, *rest = range(5)
>>> a, b, rest
(0, 1, [2, 3, 4])
>>> a, b, *rest = range(3)
>>> a, b, rest
(0, 1, [2])
>>> a, b, *rest = range(2)
>>> a, b, rest
(0, 1, [])
In the context of parallel assignment, the * prefix can be applied to exactly one variable,
but it can appear in any position:
>>> a, *body, c, d = range(5)
>>> a, body, c, d
(0, [1, 2], 3, 4)
>>> *head, b, c, d = range(5)
>>> head, b, c, d
([0, 1], 2, 3, 4)
Finally, a powerful feature of tuple unpacking is that it works with nested structures.
Nested Tuple Unpacking
The tuple to receive an expression to unpack can have nested tuples, like (a, b, (c,
d)), and Python will do the right thing if the expression matches the nesting structure.
Example 2-8 shows nested tuple unpacking in action.
Example 2-8. Unpacking nested tuples to access the longitude
metro_areas = [
('Tokyo', 'JP', 36.933, (35.689722, 139.691667)), #
('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),
('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),
Tuples Are Not Just Immutable Lists | 29
('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),
('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)),
]
print('{:15} | {:^9} | {:^9}'.format('', 'lat.', 'long.'))
fmt = '{:15} | {:9.4f} | {:9.4f}'
for name, cc, pop, (latitude, longitude) in metro_areas: #
if longitude <= 0: #
print(fmt.format(name, latitude, longitude))
Each tuple holds a record with four fields, the last of which is a coordinate pair.
By assigning the last field to a tuple, we unpack the coordinates.
if longitude <= 0: limits the output to metropolitan areas in the Western
hemisphere.
The output of Example 2-8 is:
| lat. | long.
Mexico City | 19.4333 | -99.1333
New York-Newark | 40.8086 | -74.0204
Sao Paulo | -23.5478 | -46.6358
Before Python 3, it was possible to define functions with nested
tuples in the formal parameters (e.g., def fn(a, (b, c), d):).
This is no longer supported in Python 3 function definitions, for
practical reasons explained in PEP 3113 � Removal of Tuple Parameter
Unpacking. To be clear: nothing changed from the perspective
of users calling a function. The restriction applies only to
the definition of functions.
As designed, tuples are very handy. But there is a missing feature when using them as
records: sometimes it is desirable to name the fields. That is why the namedtuple function
was invented. Read on.
Named Tuples
The collections.namedtuple function is a factory that produces subclasses of tuple
enhanced with field names and a class name�which helps debugging.
Instances of a class that you build with namedtuple take exactly the
same amount of memory as tuples because the field names are
stored in the class. They use less memory than a regular object
because they don�t store attributes in a per-instance __dict__.
30 | Chapter 2: An Array of Sequences
Recall how we built the Card class in Example 1-1 in Chapter 1:
Card = collections.namedtuple('Card', ['rank', 'suit'])
Example 2-9 shows how we could define a named tuple to hold information about a
city.
Example 2-9. Defining and using a named tuple type
>>> from collections import namedtuple
>>> City = namedtuple('City', 'name country population coordinates')
>>> tokyo = City('Tokyo', 'JP', 36.933, (35.689722, 139.691667))
>>> tokyo
City(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722,
139.691667))
>>> tokyo.population
36.933
>>> tokyo.coordinates
(35.689722, 139.691667)
>>> tokyo[1]
'JP'
Two parameters are required to create a named tuple: a class name and a list of
field names, which can be given as an iterable of strings or as a single spacedelimited
string.
Data must be passed as positional arguments to the constructor (in contrast, the
tuple constructor takes a single iterable).
You can access the fields by name or position.
A named tuple type has a few attributes in addition to those inherited from tuple.
Example 2-10 shows the most useful: the _fields class attribute, the class method
_make(iterable), and the _asdict() instance method.
Example 2-10. Named tuple attributes and methods (continued from the previous example)
>>> City._fields
('name', 'country', 'population', 'coordinates')
>>> LatLong = namedtuple('LatLong', 'lat long')
>>> delhi_data = ('Delhi NCR', 'IN', 21.935, LatLong(28.613889, 77.208889))
>>> delhi = City._make(delhi_data)
>>> delhi._asdict()
OrderedDict([('name', 'Delhi NCR'), ('country', 'IN'), ('population',
21.935), ('coordinates', LatLong(lat=28.613889, long=77.208889))])
>>> for key, value in delhi._asdict().items():
print(key + ':', value)
name: Delhi NCR
country: IN
population: 21.935
Tuples Are Not Just Immutable Lists | 31
coordinates: LatLong(lat=28.613889, long=77.208889)
>>>
_fields is a tuple with the field names of the class.
_make() allow you to instantiate a named tuple from an iterable; City(*del
hi_data) would do the same.
_asdict() returns a collections.OrderedDict built from the named tuple
instance. That can be used to produce a nice display of city data.
Now that we�ve explored the power of tuples as records, we can consider their second
role as an immutable variant of the list type.
Tuples as Immutable Lists
When using a tuple as an immutable variation of list, it helps to know how similar
they actually are. As you can see in Table 2-1, tuple supports all list methods that do
not involve adding or removing items, with one exception�tuple lacks the __re
versed__ method. However, that is just for optimization; reversed(my_tuple) works
without it.
Table 2-1. Methods and attributes found in list or tuple (methods implemented by object
are omitted for brevity)
list tuple
s.__add__(s2) ? ? s + s2�concatenation
s.__iadd__(s2) ? s += s2�in-place concatenation
s.append(e) ? Append one element after last
s.clear() ? Delete all items
s.__contains__(e) ? ? e in s
s.copy() ? Shallow copy of the list
s.count(e) ? ? Count occurrences of an element
s.__delitem__(p) ? Remove item at position p
s.extend(it) ? Append items from iterable it
s.__getitem__(p) ? ? s[p]�get item at position
s.__getnewargs__() ? Support for optimized serialization with pickle
s.index(e) ? ? Find position of first occurrence of e
s.insert(p, e) ? Insert element e before the item at position p
s.__iter__() ? ? Get iterator
s.__len__() ? ? len(s)�number of items
s.__mul__(n) ? ? s * n�repeated concatenation
32 | Chapter 2: An Array of Sequences
list tuple
s.__imul__(n) ? s *= n�in-place repeated concatenation
s.__rmul__(n) ? ? n * s�reversed repeated concatenationa
s.pop([p]) ? Remove and return last item or item at optional position p
s.remove(e) ? Remove first occurrence of element e by value
s.reverse() ? Reverse the order of the items in place
s.__reversed__() ? Get iterator to scan items from last to first
s.__setitem__(p, e) ? s[p] = e�put e in position p, overwriting existing item
s.sort([key], [reverse]) ? Sort items in place with optional keyword arguments key and reverse
a Reversed operators are explained in Chapter 13.
Every Python programmer knows that sequences can be sliced using the s[a:b] syntax.
We now turn to some less well-known facts about slicing.
Slicing
A common feature of list, tuple, str, and all sequence types in Python is the support
of slicing operations, which are more powerful than most people realize.
In this section, we describe the use of these advanced forms of slicing. Their implementation
in a user-defined class will be covered in Chapter 10, in keeping with our
philosophy of covering ready-to-use classes in this part of the book, and creating new
classes in Part IV.
Why Slices and Range Exclude the Last Item
The Pythonic convention of excluding the last item in slices and ranges works well with
the zero-based indexing used in Python, C, and many other languages. Some convenient
features of the convention are:
� It�s easy to see the length of a slice or range when only the stop position is given:
range(3) and my_list[:3] both produce three items.
� It�s easy to compute the length of a slice or range when start and stop are given: just
subtract stop - start.
� It�s easy to split a sequence in two parts at any index x, without overlapping: simply
get my_list[:x] and my_list[x:]. For example:
>>> l = [10, 20, 30, 40, 50, 60]
>>> l[:2] # split at 2
[10, 20]
>>> l[2:]
[30, 40, 50, 60]
Slicing | 33
>>> l[:3] # split at 3
[10, 20, 30]
>>> l[3:]
[40, 50, 60]
But the best arguments for this convention were written by the Dutch computer scientist
Edsger W. Dijkstra (see the last reference in �Further Reading� on page 59).
Now let�s take a close look at how Python interprets slice notation.
Slice Objects
This is no secret, but worth repeating just in case: s[a:b:c] can be used to specify a
stride or step c, causing the resulting slice to skip items. The stride can also be negative,
returning items in reverse. Three examples make this clear:
>>> s = 'bicycle'
>>> s[::3]
'bye'
>>> s[::-1]
'elcycib'
>>> s[::-2]
'eccb'
Another example was shown in Chapter 1 when we used deck[12::13] to get all the
aces in the unshuffled deck:
>>> deck[12::13]
[Card(rank='A', suit='spades'), Card(rank='A', suit='diamonds'),
Card(rank='A', suit='clubs'), Card(rank='A', suit='hearts')]
The notation a:b:c is only valid within [] when used as the indexing or subscript
operator, and it produces a slice object: slice(a, b, c). As we will see in �How Slicing
Works� on page 281, to evaluate the expression seq[start:stop:step], Python calls
seq.__getitem__(slice(start, stop, step)). Even if you are not implementing
your own sequence types, knowing about slice objects is useful because it lets you assign
names to slices, just like spreadsheets allow naming of cell ranges.
Suppose you need to parse flat-file data like the invoice shown in Example 2-11. Instead
of filling your code with hardcoded slices, you can name them. See how readable this
makes the for loop at the end of the example.
Example 2-11. Line items from a flat-file invoice
>>> invoice = """
... 0.....6.................................40........52...55........
... 1909 Pimoroni PiBrella $17.50 3 $52.50
... 1489 6mm Tactile Switch x20 $4.95 2 $9.90
... 1510 Panavise Jr. - PV-201 $28.00 1 $28.00
... 1601 PiTFT Mini Kit 320x240 $34.95 1 $34.95
... """
34 | Chapter 2: An Array of Sequences
2. No, I did not get this backwards: the ellipsis class name is really all lowercase and the instance is a builtin
named Ellipsis, just like bool is lowercase but its instances are True and False.
>>> SKU = slice(0, 6)
>>> DESCRIPTION = slice(6, 40)
>>> UNIT_PRICE = slice(40, 52)
>>> QUANTITY = slice(52, 55)
>>> ITEM_TOTAL = slice(55, None)
>>> line_items = invoice.split('\n')[2:]
>>> for item in line_items:
... print(item[UNIT_PRICE], item[DESCRIPTION])
...
$17.50 Pimoroni PiBrella
$4.95 6mm Tactile Switch x20
$28.00 Panavise Jr. - PV-201
$34.95 PiTFT Mini Kit 320x240
We�ll come back to slice objects when we discuss creating your own collections in
�Vector Take #2: A Sliceable Sequence� on page 280. Meanwhile, from a user perspective,
slicing includes additional features such as multidimensional slices and ellipsis (...)
notation. Read on.
Multidimensional Slicing and Ellipsis
The [] operator can also take multiple indexes or slices separated by commas. This is
used, for instance, in the external NumPy package, where items of a two-dimensional
numpy.ndarray can be fetched using the syntax a[i, j] and a two-dimensional slice
obtained with an expression like a[m:n, k:l]. Example 2-22 later in this chapter shows
the use of this notation. The __getitem__ and __setitem__ special methods that handle
the [] operator simply receive the indices in a[i, j] as a tuple. In other words, to
evaluate a[i, j], Python calls a.__getitem__((i, j)).
The built-in sequence types in Python are one-dimensional, so they support only one
index or slice, and not a tuple of them.
The ellipsis�written with three full stops (...) and not � (Unicode U+2026)�is recognized
as a token by the Python parser. It is an alias to the Ellipsis object, the single
instance of the ellipsis class.2 As such, it can be passed as an argument to functions
and as part of a slice specification, as in f(a, ..., z) or a[i:...]. NumPy uses ...
as a shortcut when slicing arrays of many dimensions; for example, if x is a fourdimensional
array, x[i, ...] is a shortcut for x[i, :, :, :,]. See the Tentative
NumPy Tutorial to learn more about this.
At the time of this writing, I am unaware of uses of Ellipsis or multidimensional
indexes and slices in the Python standard library. If you spot one, let me know. These
syntactic features exist to support user-defined types and extensions such as NumPy.
Slicing | 35
Slices are not just useful to extract information from sequences; they can also be used
to change mutable sequences in place�that is, without rebuilding them from scratch.
Assigning to Slices
Mutable sequences can be grafted, excised, and otherwise modified in place using slice
notation on the left side of an assignment statement or as the target of a del statement.
The next few examples give an idea of the power of this notation:
>>> l = list(range(10))
>>> l
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> l[2:5] = [20, 30]
>>> l
[0, 1, 20, 30, 5, 6, 7, 8, 9]
>>> del l[5:7]
>>> l
[0, 1, 20, 30, 5, 8, 9]
>>> l[3::2] = [11, 22]
>>> l
[0, 1, 20, 11, 5, 22, 9]
>>> l[2:5] = 100
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: can only assign an iterable
>>> l[2:5] = [100]
>>> l
[0, 1, 100, 22, 9]
When the target of the assignment is a slice, the right side must be an iterable
object, even if it has just one item.
Everybody knows that concatenation is a common operation with sequences of any
type. Any introductory Python text explains the use of + and * for that purpose, but
there are some subtle details on how they work, which we cover next.
Using + and * with Sequences
Python programmers expect that sequences support + and *. Usually both operands of
+ must be of the same sequence type, and neither of them is modified but a new sequence
of the same type is created as result of the concatenation.
To concatenate multiple copies of the same sequence, multiply it by an integer. Again,
a new sequence is created:
>>> l = [1, 2, 3]
>>> l * 5
[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]
36 | Chapter 2: An Array of Sequences
>>> 5 * 'abcd'
'abcdabcdabcdabcdabcd'
Both + and * always create a new object, and never change their operands.
Beware of expressions like a * n when a is a sequence containing
mutable items because the result may surprise you. For example,
trying to initialize a list of lists as my_list = [[]] * 3 will
result in a list with three references to the same inner list, which is
probably not what you want.
The next section covers the pitfalls of trying to use * to initialize a list of lists.
Building Lists of Lists
Sometimes we need to initialize a list with a certain number of nested lists�for example,
to distribute students in a list of teams or to represent squares on a game board. The
best way of doing so is with a list comprehension, as in Example 2-12.
Example 2-12. A list with three lists of length 3 can represent a tic-tac-toe board
>>> board = [['_'] * 3 for i in range(3)]
>>> board
[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]
>>> board[1][2] = 'X'
>>> board
[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']]
Create a list of three lists of three items each. Inspect the structure.
Place a mark in row 1, column 2, and check the result.
A tempting but wrong shortcut is doing it like Example 2-13.
Example 2-13. A list with three references to the same list is useless
>>> weird_board = [['_'] * 3] * 3
>>> weird_board
[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]
>>> weird_board[1][2] = 'O'
>>> weird_board
[['_', '_', 'O'], ['_', '_', 'O'], ['_', '_', 'O']]
The outer list is made of three references to the same inner list. While it is
unchanged, all seems right.
Placing a mark in row 1, column 2, reveals that all rows are aliases referring to
the same object.
Using + and * with Sequences | 37
The problem with Example 2-13 is that, in essence, it behaves like this code:
row = ['_'] * 3
board = []
for i in range(3):
board.append(row)
The same row is appended three times to board.
On the other hand, the list comprehension from Example 2-12 is equivalent to this code:
>>> board = []
>>> for i in range(3):
... row = ['_'] * 3 #
... board.append(row)
...
>>> board
[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]
>>> board[2][0] = 'X'
>>> board #
[['_', '_', '_'], ['_', '_', '_'], ['X', '_', '_']]
Each iteration builds a new row and appends it to board.
Only row 2 is changed, as expected.
If either the problem or the solution in this section are not clear
to you, relax. Chapter 8 was written to clarify the mechanics and
pitfalls of references and mutable objects.
So far we have discussed the use of the plain + and * operators with sequences, but there
are also the += and *= operators, which produce very different results depending on the
mutability of the target sequence. The following section explains how that works.
Augmented Assignment with Sequences
The augmented assignment operators += and *= behave very differently depending on
the first operand. To simplify the discussion, we will focus on augmented addition first
(+=), but the concepts also apply to *= and to other augmented assignment operators.
The special method that makes += work is __iadd__ (for �in-place addition�). However,
if __iadd__ is not implemented, Python falls back to calling __add__. Consider this
simple expression:
>>> a += b
38 | Chapter 2: An Array of Sequences
3. str is an exception to this description. Because string building with += in loops is so common in the wild,
CPython is optimized for this use case. str instances are allocated in memory with room to spare, so that
concatenation does not require copying the whole string every time.
If a implements __iadd__, that will be called. In the case of mutable sequences (e.g.,
list, bytearray, array.array), a will be changed in place (i.e., the effect will be similar
to a.extend(b)). However, when a does not implement __iadd__, the expression a +=
b has the same effect as a = a + b: the expression a + b is evaluated first, producing a
new object, which is then bound to a. In other words, the identity of the object bound
to a may or may not change, depending on the availability of __iadd__.
In general, for mutable sequences, it is a good bet that __iadd__ is implemented and
that += happens in place. For immutable sequences, clearly there is no way for that to
happen.
What I just wrote about += also applies to *=, which is implemented via __imul__. The
__iadd__ and __imul__ special methods are discussed in Chapter 13.
Here is a demonstration of *= with a mutable sequence and then an immutable one:
>>> l = [1, 2, 3]
>>> id(l)
4311953800
>>> l *= 2
>>> l
[1, 2, 3, 1, 2, 3]
>>> id(l)
4311953800
>>> t = (1, 2, 3)
>>> id(t)
4312681568
>>> t *= 2
>>> id(t)
4301348296
ID of the initial list
After multiplication, the list is the same object, with new items appended
ID of the initial tuple
After multiplication, a new tuple was created
Repeated concatenation of immutable sequences is inefficient, because instead of just
appending new items, the interpreter has to copy the whole target sequence to create a
new one with the new items concatenated.3
We�ve seen common use cases for +=. The next section shows an intriguing corner case
that highlights what �immutable� really means in the context of tuples.
Augmented Assignment with Sequences | 39
4. Thanks to Leonardo Rochael and Cesar Kawakami for sharing this riddle at the 2013 PythonBrasil Conference.
5. A reader suggested that the operation in the example can be performed with t[2].extend([50,60]),
without errors. We�re aware of that, but the intent of the example is to discuss the odd behavior of the +=
operator.
A += Assignment Puzzler
Try to answer without using the console: what is the result of evaluating the two expressions
in Example 2-14?4
Example 2-14. A riddle
>>> t = (1, 2, [30, 40])
>>> t[2] += [50, 60]
What happens next? Choose the best answer:
a. t becomes (1, 2, [30, 40, 50, 60]).
b. TypeError is raised with the message 'tuple' object does not support item
assignment.
c. Neither.
d. Both a and b.
When I saw this, I was pretty sure the answer was b, but it�s actually d, �Both a and b.�!
Example 2-15 is the actual output from a Python 3.4 console (actually the result is the
same in a Python 2.7 console).5
Example 2-15. The unexpected result: item t2 is changed and an exception is raised
>>> t = (1, 2, [30, 40])
>>> t[2] += [50, 60]
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: 'tuple' object does not support item assignment
>>> t
(1, 2, [30, 40, 50, 60])
Online Python Tutor is an awesome online tool to visualize how Python works in detail.
Figure 2-3 is a composite of two screenshots showing the initial and final states of the
tuple t from Example 2-15.
40 | Chapter 2: An Array of Sequences
Figure 2-3. Initial and final state of the tuple assignment puzzler (diagram generated
by Online Python Tutor)
If you look at the bytecode Python generates for the expression s[a] += b
(Example 2-16), it becomes clear how that happens.
Example 2-16. Bytecode for the expression s[a] += b
>>> dis.dis('s[a] += b')
1 0 LOAD_NAME 0 (s)
3 LOAD_NAME 1 (a)
6 DUP_TOP_TWO
7 BINARY_SUBSCR
8 LOAD_NAME 2 (b)
11 INPLACE_ADD
12 ROT_THREE
13 STORE_SUBSCR
14 LOAD_CONST 0 (None)
17 RETURN_VALUE
Put the value of s[a] on TOS (Top Of Stack).
Perform TOS += b. This succeeds if TOS refers to a mutable object (it�s a list, in
Example 2-15).
Assign s[a] = TOS. This fails if s is immutable (the t tuple in Example 2-15).
This example is quite a corner case�in 15 years of using Python, I have never seen this
strange behavior actually bite somebody.
I take three lessons from this:
� Putting mutable items in tuples is not a good idea.
Augmented Assignment with Sequences | 41
� Augmented assignment is not an atomic operation�we just saw it throwing an
exception after doing part of its job.
� Inspecting Python bytecode is not too difficult, and is often helpful to see what is
going on under the hood.
After witnessing the subtleties of using + and * for concatenation, we can change the
subject to another essential operation with sequences: sorting.
list.sort and the sorted Built-In Function
The list.sort method sorts a list in place�that is, without making a copy. It returns
None to remind us that it changes the target object, and does not create a new list. This
is an important Python API convention: functions or methods that change an object in
place should return None to make it clear to the caller that the object itself was changed,
and no new object was created. The same behavior can be seen, for example, in the
random.shuffle function.
The convention of returning None to signal in-place changes has
a drawback: you cannot cascade calls to those methods. In contrast,
methods that return new objects (e.g., all str methods) can
be cascaded in the fluent interface style. See Wikipedia�s Wikipedia�s
�Fluent interface� entry for further description of this topic.
In contrast, the built-in function sorted creates a new list and returns it. In fact, it
accepts any iterable object as an argument, including immutable sequences and generators
(see Chapter 14). Regardless of the type of iterable given to sorted, it always returns
a newly created list.
Both list.sort and sorted take two optional, keyword-only arguments:
reverse
If True, the items are returned in descending order (i.e., by reversing the comparison
of the items). The default is False.
key
A one-argument function that will be applied to each item to produce its sorting
key. For example, when sorting a list of strings, key=str.lower can be used to
perform a case-insensitive sort, and key=len will sort the strings by character
length. The default is the identity function (i.e., the items themselves are compared).
42 | Chapter 2: An Array of Sequences
6. The examples also demonstrate that Timsort�the sorting algorithm used in Python�is stable (i.e., it preserves
the relative ordering of items that compare equal). Timsort is discussed further in the �Soapbox� sidebar
at the end of this chapter.
The key optional keyword parameter can also be used with the
min() and max() built-ins and with other functions from the standard
library (e.g., itertools.groupby() and heapq.nlargest()).
Here are a few examples to clarify the use of these functions and keyword arguments6:
>>> fruits = ['grape', 'raspberry', 'apple', 'banana']
>>> sorted(fruits)
['apple', 'banana', 'grape', 'raspberry']
>>> fruits
['grape', 'raspberry', 'apple', 'banana']
>>> sorted(fruits, reverse=True)
['raspberry', 'grape', 'banana', 'apple']
>>> sorted(fruits, key=len)
['grape', 'apple', 'banana', 'raspberry']
>>> sorted(fruits, key=len, reverse=True)
['raspberry', 'banana', 'grape', 'apple']
>>> fruits
['grape', 'raspberry', 'apple', 'banana']
>>> fruits.sort()
>>> fruits
['apple', 'banana', 'grape', 'raspberry']
This produces a new list of strings sorted alphabetically.
Inspecting the original list, we see it is unchanged.
This is simply reverse alphabetical ordering.
A new list of strings, now sorted by length. Because the sorting algorithm is
stable, �grape� and �apple,� both of length 5, are in the original order.
These are the strings sorted in descending order of length. It is not the reverse
of the previous result because the sorting is stable, so again �grape� appears
before �apple.�
So far, the ordering of the original fruits list has not changed.
This sorts the list in place, and returns None (which the console omits).
Now fruits is sorted.
Once your sequences are sorted, they can be very efficiently searched. Fortunately, the
standard binary search algorithm is already provided in the bisect module of the
Python standard library. We discuss its essential features next, including the convenient
list.sort and the sorted Built-In Function | 43
bisect.insort function, which you can use to make sure that your sorted sequences
stay sorted.
Managing Ordered Sequences with bisect
The bisect module offers two main functions�bisect and insort�that use the binary
search algorithm to quickly find and insert items in any sorted sequence.
Searching with bisect
bisect(haystack, needle) does a binary search for needle in haystack�which must
be a sorted sequence�to locate the position where needle can be inserted while maintaining
haystack in ascending order. In other words, all items appearing up to that
position are less than or equal to needle. You could use the result of bisect(haystack,
needle) as the index argument to haystack.insert(index, needle)�however, using
insort does both steps, and is faster.
Raymond Hettinger�a prolific Python contributor�has a Sorted
Collection recipe that leverages the bisect module but is easier
to use than these standalone functions.
Example 2-17 uses a carefully chosen set of �needles� to demonstrate the insert positions
returned by bisect. Its output is in Figure 2-4.
Example 2-17. bisect finds insertion points for items in a sorted sequence
import bisect
import sys
HAYSTACK = [1, 4, 5, 6, 8, 12, 15, 20, 21, 23, 23, 26, 29, 30]
NEEDLES = [0, 1, 2, 5, 8, 10, 22, 23, 29, 30, 31]
ROW_FMT = '{0:2d} @ {1:2d} {2}{0:<2d}'
def demo(bisect_fn):
for needle in reversed(NEEDLES):
position = bisect_fn(HAYSTACK, needle)
offset = position * ' |'
print(ROW_FMT.format(needle, position, offset))
if __name__ == '__main__':
if sys.argv[-1] == 'left':
bisect_fn = bisect.bisect_left
else:
44 | Chapter 2: An Array of Sequences
bisect_fn = bisect.bisect
print('DEMO:', bisect_fn.__name__)
print('haystack ->', ' '.join('%2d' % n for n in HAYSTACK))
demo(bisect_fn)
Use the chosen bisect function to get the insertion point.
Build a pattern of vertical bars proportional to the offset.
Print formatted row showing needle and insertion point.
Choose the bisect function to use according to the last command-line
argument.
Print header with name of function selected.
Figure 2-4. Output of Example 2-17 with bisect in use�each row starts with the notation
needle @ position and the needle value appears again below its insertion point in
the haystack
The behavior of bisect can be fine-tuned in two ways.
First, a pair of optional arguments, lo and hi, allow narrowing the region in the sequence
to be searched when inserting. lo defaults to 0 and hi to the len() of the sequence.
Second, bisect is actually an alias for bisect_right, and there is a sister function called
bisect_left. Their difference is apparent only when the needle compares equal to an
item in the list: bisect_right returns an insertion point after the existing item, and
bisect_left returns the position of the existing item, so insertion would occur before
Managing Ordered Sequences with bisect | 45
it. With simple types like int this makes no difference, but if the sequence contains
objects that are distinct yet compare equal, then it may be relevant. For example, 1 and
1.0 are distinct, but 1 == 1.0 is True. Figure 2-5 shows the result of using bisect_left.
Figure 2-5. Output of Example 2-17 with bisect_left in use (compare with Figure 2-4
and note the insertion points for the values 1, 8, 23, 29, and 30 to the left of the same
numbers in the haystack).
An interesting application of bisect is to perform table lookups by numeric values�
for example, to convert test scores to letter grades, as in Example 2-18.
Example 2-18. Given a test score, grade returns the corresponding letter grade
>>> def grade(score, breakpoints=[60, 70, 80, 90], grades='FDCBA'):
... i = bisect.bisect(breakpoints, score)
... return grades[i]
...
>>> [grade(score) for score in [33, 99, 77, 70, 89, 90, 100]]
['F', 'A', 'C', 'C', 'B', 'A', 'A']
The code in Example 2-18 is from the bisect module documentation, which also lists
functions to use bisect as a faster replacement for the index method when searching
through long ordered sequences of numbers.
These functions are not only used for searching, but also for inserting items in sorted
sequences, as the following section shows.
46 | Chapter 2: An Array of Sequences
Inserting with bisect.insort
Sorting is expensive, so once you have a sorted sequence, it�s good to keep it that way.
That is why bisect.insort was created.
insort(seq, item) inserts item into seq so as to keep seq in ascending order. See
Example 2-19 and its output in Figure 2-6.
Example 2-19. Insort keeps a sorted sequence always sorted
import bisect
import random
SIZE = 7
random.seed(1729)
my_list = []
for i in range(SIZE):
new_item = random.randrange(SIZE*2)
bisect.insort(my_list, new_item)
print('%2d ->' % new_item, my_list)
Figure 2-6. Output of Example 2-19
Like bisect, insort takes optional lo, hi arguments to limit the search to a subsequence.
There is also an insort_left variation that uses bisect_left to find insertion
points.
Much of what we have seen so far in this chapter applies to sequences in general, not
just lists or tuples. Python programmers sometimes overuse the list type because it is
so handy�I know I�ve done it. If you are handling lists of numbers, arrays are the way
to go. The remainder of the chapter is devoted to them.
Managing Ordered Sequences with bisect | 47
When a List Is Not the Answer
The list type is flexible and easy to use, but depending on specific requirements, there
are better options. For example, if you need to store 10 million floating-point values, an
array is much more efficient, because an array does not actually hold full-fledged float
objects, but only the packed bytes representing their machine values�just like an array
in the C language. On the other hand, if you are constantly adding and removing items
from the ends of a list as a FIFO or LIFO data structure, a deque (double-ended queue)
works faster.
If your code does a lot of containment checks (e.g., item in
my_collection), consider using a set for my_collection, especially
if it holds a large number of items. Sets are optimized for fast
membership checking. But they are not sequences (their content
is unordered). We cover them in Chapter 3.
For the remainder of this chapter, we discuss mutable sequence types that can replace
lists in many cases, starting with arrays.
Arrays
If the list will only contain numbers, an array.array is more efficient than a list: it
supports all mutable sequence operations (including .pop, .insert, and .extend), and
additional methods for fast loading and saving such as .frombytes and .tofile.
A Python array is as lean as a C array. When creating an array, you provide a typecode,
a letter to determine the underlying C type used to store each item in the array. For
example, b is the typecode for signed char. If you create an array('b'), then each item
will be stored in a single byte and interpreted as an integer from �128 to 127. For large
sequences of numbers, this saves a lot of memory. And Python will not let you put any
number that does not match the type for the array.
Example 2-20 shows creating, saving, and loading an array of 10 million floating-point
random numbers.
Example 2-20. Creating, saving, and loading a large array of floats
>>> from array import array
>>> from random import random
>>> floats = array('d', (random() for i in range(10**7)))
>>> floats[-1]
0.07802343889111107
>>> fp = open('floats.bin', 'wb')
>>> floats.tofile(fp)
>>> fp.close()
>>> floats2 = array('d')
48 | Chapter 2: An Array of Sequences
>>> fp = open('floats.bin', 'rb')
>>> floats2.fromfile(fp, 10**7)
>>> fp.close()
>>> floats2[-1]
0.07802343889111107
>>> floats2 == floats
True
Import the array type.
Create an array of double-precision floats (typecode 'd') from any iterable
object�in this case, a generator expression.
Inspect the last number in the array.
Save the array to a binary file.
Create an empty array of doubles.
Read 10 million numbers from the binary file.
Inspect the last number in the array.
Verify that the contents of the arrays match.
As you can see, array.tofile and array.fromfile are easy to use. If you try the example,
you�ll notice they are also very fast. A quick experiment show that it takes about
0.1s for array.fromfile to load 10 million double-precision floats from a binary file
created with array.tofile. That is nearly 60 times faster than reading the numbers
from a text file, which also involves parsing each line with the float built-in. Saving
with array.tofile is about 7 times faster than writing one float per line in a text file.
In addition, the size of the binary file with 10 million doubles is 80,000,000 bytes (8
bytes per double, zero overhead), while the text file has 181,515,739 bytes, for the same
data.
Another fast and more flexible way of saving numeric data is the
pickle module for object serialization. Saving an array of floats
with pickle.dump is almost as fast as with array.tofile�however,
pickle handles almost all built-in types, including complex
numbers, nested collections, and even instances of user-defined
classes automatically (if they are not too tricky in their implementation).
For the specific case of numeric arrays representing binary data, such as raster images,
Python has the bytes and bytearray types discussed in Chapter 4.
We wrap up this section on arrays with Table 2-2, comparing the features of list and
array.array.
When a List Is Not the Answer | 49
Table 2-2. Methods and attributes found in list or array (deprecated array methods
and those also implemented by object were omitted for brevity)
list array
s.__add__(s2) ? ? s + s2�concatenation
s.__iadd__(s2) ? ? s += s2�in-place concatenation
s.append(e) ? ? Append one element after last
s.byteswap() ? Swap bytes of all items in array for endianess conversion
s.clear() ? Delete all items
s.__contains__(e) ? ? e in s
s.copy() ? Shallow copy of the list
s.__copy__() ? Support for copy.copy
s.count(e) ? ? Count occurrences of an element
s.__deepcopy__() ? Optimized support for copy.deepcopy
s.__delitem__(p) ? ? Remove item at position p
s.extend(it) ? ? Append items from iterable it
s.frombytes(b) ? Append items from byte sequence interpreted as packed machine values
s.fromfile(f, n) ? Append n items from binary file f interpreted as packed machine values
s.fromlist(l) ? Append items from list; if one causes TypeError, none are appended
s.__getitem__(p) ? ? s[p]�get item at position
s.index(e) ? ? Find position of first occurrence of e
s.insert(p, e) ? ? Insert element e before the item at position p
s.itemsize ? Length in bytes of each array item
s.__iter__() ? ? Get iterator
s.__len__() ? ? len(s)�number of items
s.__mul__(n) ? ? s * n�repeated concatenation
s.__imul__(n) ? ? s *= n�in-place repeated concatenation
s.__rmul__(n) ? ? n * s�reversed repeated concatenationa
s.pop([p]) ? ? Remove and return item at position p (default: last)
s.remove(e) ? ? Remove first occurrence of element e by value
s.reverse() ? ? Reverse the order of the items in place
s.__reversed__() ? Get iterator to scan items from last to first
s.__setitem__(p, e) ? ? s[p] = e�put e in position p, overwriting existing item
s.sort([key], [reverse]) ? Sort items in place with optional keyword arguments key and reverse
s.tobytes() ? Return items as packed machine values in a bytes object
s.tofile(f) ? Save items as packed machine values to binary file f
s.tolist() ? Return items as numeric objects in a list
50 | Chapter 2: An Array of Sequences
list array
s.typecode ? One-character string identifying the C type of the items
a Reversed operators are explained in Chapter 13.
As of Python 3.4, the array type does not have an in-place sort
method like list.sort(). If you need to sort an array, use the
sorted function to rebuild it sorted:
a = array.array(a.typecode, sorted(a))
To keep a sorted array sorted while adding items to it, use the
bisect.insort function (as seen in �Inserting with bisect.insort�
on page 47).
If you do a lot of work with arrays and don�t know about memoryview, you�re missing
out. See the next topic.
Memory Views
The built-in memorview class is a shared-memory sequence type that lets you handle
slices of arrays without copying bytes. It was inspired by the NumPy library (which we�ll
discuss shortly in �NumPy and SciPy� on page 52). Travis Oliphant, lead author of Num?
Py, answers When should a memoryview be used? like this:
A memoryview is essentially a generalized NumPy array structure in Python itself
(without the math). It allows you to share memory between data-structures (things like
PIL images, SQLlite databases, NumPy arrays, etc.) without first copying. This is very
important for large data sets.
Using notation similar to the array module, the memoryview.cast method lets you
change the way multiple bytes are read or written as units without moving bits around
(just like the C cast operator). memoryview.cast returns yet another memoryview object,
always sharing the same memory.
See Example 2-21 for an example of changing a single byte of an array of 16-bit integers.
Example 2-21. Changing the value of an array item by poking one of its bytes
>>> numbers = array.array('h', [-2, -1, 0, 1, 2])
>>> memv = memoryview(numbers)
>>> len(memv)
5
>>> memv[0]
-2
>>> memv_oct = memv.cast('B')
>>> memv_oct.tolist()
[254, 255, 255, 255, 0, 0, 1, 0, 2, 0]
>>> memv_oct[5] = 4
When a List Is Not the Answer | 51
>>> numbers
array('h', [-2, -1, 1024, 1, 2])
Build memoryview from array of 5 short signed integers (typecode 'h').
memv sees the same 5 items in the array.
Create memv_oct by casting the elements of memv to typecode 'B' (unsigned
char).
Export elements of memv_oct as a list, for inspection.
Assign value 4 to byte offset 5.
Note change to numbers: a 4 in the most significant byte of a 2-byte unsigned
integer is 1024.
We�ll see another short example with memoryview in the context of binary sequence
manipulations with struct (Chapter 4, Example 4-4).
Meanwhile, if you are doing advanced numeric processing in arrays, you should be using
the NumPy and SciPy libraries. We�ll take a brief look at them right away.
NumPy and SciPy
Throughout this book, I make a point of highlighting what is already in the Python
standard library so you can make the most of it. But NumPy and SciPy are so awesome
that a detour is warranted.
For advanced array and matrix operations, NumPy and SciPy are the reason why Python
became mainstream in scientific computing applications. NumPy implements multidimensional,
homogeneous arrays and matrix types that hold not only numbers but
also user-defined records, and provides efficient elementwise operations.
SciPy is a library, written on top of NumPy, offering many scientific computing algorithms
from linear algebra, numerical calculus, and statistics. SciPy is fast and reliable
because it leverages the widely used C and Fortran code base from the Netlib Repository.
In other words, SciPy gives scientists the best of both worlds: an interactive prompt
and high-level Python APIs, together with industrial-strength number-crunching functions
optimized in C and Fortran.
As a very brief demo, Example 2-22 shows some basic operations with two-dimensional
arrays in NumPy.
Example 2-22. Basic operations with rows and columns in a numpy.ndarray
>>> import numpy
>>> a = numpy.arange(12)
>>> a
array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
52 | Chapter 2: An Array of Sequences
>>> type(a)
<class 'numpy.ndarray'>
>>> a.shape
(12,)
>>> a.shape = 3, 4
>>> a
array([[ 0, 1, 2, 3],
[ 4, 5, 6, 7],
[ 8, 9, 10, 11]])
>>> a[2]
array([ 8, 9, 10, 11])
>>> a[2, 1]
9
>>> a[:, 1]
array([1, 5, 9])
>>> a.transpose()
array([[ 0, 4, 8],
[ 1, 5, 9],
[ 2, 6, 10],
[ 3, 7, 11]])
Import Numpy, after installing (it�s not in the Python standard library).
Build and inspect a numpy.ndarray with integers 0 to 11.
Inspect the dimensions of the array: this is a one-dimensional, 12-element array.
Change the shape of the array, adding one dimension, then inspecting the result.
Get row at index 2.
Get element at index 2, 1.
Get column at index 1.
Create a new array by transposing (swapping columns with rows).
NumPy also supports high-level operations for loading, saving, and operating on all
elements of a numpy.ndarray:
>>> import numpy
>>> floats = numpy.loadtxt('floats-10M-lines.txt')
>>> floats[-3:]
array([ 3016362.69195522, 535281.10514262, 4566560.44373946])
>>> floats *= .5
>>> floats[-3:]
array([ 1508181.34597761, 267640.55257131, 2283280.22186973])
>>> from time import perf_counter as pc
>>> t0 = pc(); floats /= 3; pc() - t0
0.03690556302899495
>>> numpy.save('floats-10M', floats)
>>> floats2 = numpy.load('floats-10M.npy', 'r+')
>>> floats2 *= 6
When a List Is Not the Answer | 53
>>> floats2[-3:]
memmap([ 3016362.69195522, 535281.10514262, 4566560.44373946])
Load 10 million floating-point numbers from a text file.
Use sequence slicing notation to inspect the last three numbers.
Multiply every element in the floats array by .5 and inspect the last three
elements again.
Import the high-resolution performance measurement timer (available since
Python 3.3).
Divide every element by 3; the elapsed time for 10 million floats is less than 40
milliseconds.
Save the array in a .npy binary file.
Load the data as a memory-mapped file into another array; this allows efficient
processing of slices of the array even if it does not fit entirely in memory.
Inspect the last three elements after multiplying every element by 6.
Installing NumPy and SciPy from source is not a breeze. The Installing
the SciPy Stack page on SciPy.org recommends using special
scientific Python distributions such as Anaconda, Enthought
Canopy, and WinPython, among others. These are large downloads,
but come ready to use. Users of popular GNU/Linux distributions
can usually find NumPy and SciPy in the standard package
repositories. For example, installing them on Debian or Ubuntu
is as easy as:
$ sudo apt-get install python-numpy python-scipy
This was just an appetizer. NumPy and SciPy are formidable libraries, and are the foundation
of other awesome tools such as the Pandas and Blaze data analysis libraries, which
provide efficient array types that can hold nonnumeric data as well as import/export
functions compatible with many different formats (e.g., .csv, .xls, SQL dumps, HDF5,
etc.). These packages deserve entire books about them. This is not one of those books.
But no overview of Python sequences would be complete without at least a quick look
at NumPy arrays.
Having looked at flat sequences�standard arrays and NumPy arrays�we now turn to
a completely different set of replacements for the plain old list: queues.
54 | Chapter 2: An Array of Sequences
Deques and Other Queues
The .append and .pop methods make a list usable as a stack or a queue (if you
use .append and .pop(0), you get LIFO behavior). But inserting and removing from
the left of a list (the 0-index end) is costly because the entire list must be shifted.
The class collections.deque is a thread-safe double-ended queue designed for fast
inserting and removing from both ends. It is also the way to go if you need to keep a list
of �last seen items� or something like that, because a deque can be bounded�i.e., created
with a maximum length�and then, when it is full, it discards items from the opposite
end when you append new ones. Example 2-23 shows some typical operations performed
on a deque.
Example 2-23. Working with a deque
>>> from collections import deque
>>> dq = deque(range(10), maxlen=10)
>>> dq
deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)
>>> dq.rotate(3)
>>> dq
deque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)
>>> dq.rotate(-4)
>>> dq
deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)
>>> dq.appendleft(-1)
>>> dq
deque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)
>>> dq.extend([11, 22, 33])
>>> dq
deque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)
>>> dq.extendleft([10, 20, 30, 40])
>>> dq
deque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10)
The optional maxlen argument sets the maximum number of items allowed in
this instance of deque; this sets a read-only maxlen instance attribute.
Rotating with n > 0 takes items from the right end and prepends them to the
left; when n < 0 items are taken from left and appended to the right.
Appending to a deque that is full (len(d) == d.maxlen) discards items from
the other end; note in the next line that the 0 is dropped.
Adding three items to the right pushes out the leftmost -1, 1, and 2.
Note that extendleft(iter) works by appending each successive item of the
iter argument to the left of the deque, therefore the final position of the items
is reversed.
When a List Is Not the Answer | 55
Table 2-3 compares the methods that are specific to list and deque (removing those
that also appear in object).
Note that deque implements most of the list methods, and adds a few specific to its
design, like popleft and rotate. But there is a hidden cost: removing items from the
middle of a deque is not as fast. It is really optimized for appending and popping from
the ends.
The append and popleft operations are atomic, so deque is safe to use as a LIFO queue
in multithreaded applications without the need for using locks.
Table 2-3. Methods implemented in list or deque (those that are also implemented by
object were omitted for brevity)
list deque
s.__add__(s2) ? s + s2�concatenation
s.__iadd__(s2) ? ? s += s2�in-place concatenation
s.append(e) ? ? Append one element to the right (after last)
s.appendleft(e) ? Append one element to the left (before first)
s.clear() ? ? Delete all items
s.__contains__(e) ? e in s
s.copy() ? Shallow copy of the list
s.__copy__() ? Support for copy.copy (shallow copy)
s.count(e) ? ? Count occurrences of an element
s.__delitem__(p) ? ? Remove item at position p
s.extend(i) ? ? Append items from iterable i to the right
s.extendleft(i) ? Append items from iterable i to the left
s.__getitem__(p) ? ? s[p]�get item at position
s.index(e) ? Find position of first occurrence of e
s.insert(p, e) ? Insert element e before the item at position p
s.__iter__() ? ? Get iterator
s.__len__() ? ? len(s)�number of items
s.__mul__(n) ? s * n�repeated concatenation
s.__imul__(n) ? s *= n�in-place repeated concatenation
s.__rmul__(n) ? n * s�reversed repeated concatenationa
s.pop() ? ? Remove and return last itemb
s.popleft() ? Remove and return first item
s.remove(e) ? ? Remove first occurrence of element e by value
s.reverse() ? ? Reverse the order of the items in place
s.__reversed__() ? ? Get iterator to scan items from last to first
56 | Chapter 2: An Array of Sequences
list deque
s.rotate(n) ? Move n items from one end to the other
s.__setitem__(p, e) ? ? s[p] = e�put e in position p, overwriting existing item
s.sort([key], [reverse]) ? Sort items in place with optional keyword arguments key and reverse
a Reversed operators are explained in Chapter 13.
b a_list.pop(p) allows removing from position p but deque does not support that option.
Besides deque, other Python standard library packages implement queues:
queue
This provides the synchronized (i.e., thread-safe) classes Queue, LifoQueue, and
PriorityQueue. These are used for safe communication between threads. All three
classes can be bounded by providing a maxsize argument greater than 0 to the
constructor. However, they don�t discard items to make room as deque does. Instead,
when the queue is full the insertion of a new item blocks�i.e., it waits until
some other thread makes room by taking an item from the queue, which is useful
to throttle the number of live threads.
multiprocessing
Implements its own bounded Queue, very similar to queue.Queue but designed for
interprocess communication. A specialized multiprocessing.JoinableQueue is
also available for easier task management.
asyncio
Newly added to Python 3.4, asyncio provides Queue, LifoQueue, PriorityQueue,
and JoinableQueue with APIs inspired by the classes contained in the queue and
multiprocessing modules, but adapted for managing tasks in asynchronous programming.
heapq
In contrast to the previous three modules, heapq does not implement a queue class,
but provides functions like heappush and heappop that let you use a mutable sequence
as a heap queue or priority queue.
This ends our overview of alternatives to the list type, and also our exploration of
sequence types in general�except for the particulars of str and binary sequences,
which have their own chapter (Chapter 4).
Chapter Summary
Mastering the standard library sequence types is a prerequisite for writing concise,
effective, and idiomatic Python code.
Chapter Summary | 57
Python sequences are often categorized as mutable or immutable, but it is also useful
to consider a different axis: flat sequences and container sequences. The former are more
compact, faster, and easier to use, but are limited to storing atomic data such as numbers,
characters, and bytes. Container sequences are more flexible, but may surprise you when
they hold mutable objects, so you need to be careful to use them correctly with nested
data structures.
List comprehensions and generator expressions are powerful notations to build and
initialize sequences. If you are not yet comfortable with them, take the time to master
their basic usage. It is not hard, and soon you will be hooked.
Tuples in Python play two roles: as records with unnamed fields and as immutable lists.
When a tuple is used as a record, tuple unpacking is the safest, most readable way of
getting at the fields. The new * syntax makes tuple unpacking even better by making it
easier to ignore some fields and to deal with optional fields. Named tuples are not so
new, but deserve more attention: like tuples, they have very little overhead per instance,
yet provide convenient access to the fields by name and a handy ._asdict() to export
the record as an OrderedDict.
Sequence slicing is a favorite Python syntax feature, and it is even more powerful than
many realize. Multidimensional slicing and ellipsis (...) notation, as used in NumPy,
may also be supported by user-defined sequences. Assigning to slices is a very expressive
way of editing mutable sequences.
Repeated concatenation as in seq * n is convenient and, with care, can be used to
initialize lists of lists containing immutable items. Augmented assignment with += and
*= behaves differently for mutable and immutable sequences. In the latter case, these
operators necessarily build new sequences. But if the target sequence is mutable, it is
usually changed in place�but not always, depending on how the sequence is implemented.
The sort method and the sorted built-in function are easy to use and flexible, thanks
to the key optional argument they accept, with a function to calculate the ordering
criterion. By the way, key can also be used with the min and max built-in functions. To
keep a sorted sequence in order, always insert items into it using bisect.insort; to
search it efficiently, use bisect.bisect.
Beyond lists and tuples, the Python standard library provides array.array. Although
NumPy and SciPy are not part of the standard library, if you do any kind of numerical
processing on large sets of data, studying even a small part of these libraries can take
you a long way.
We closed by visiting the versatile and thread-safe collections.deque, comparing its
API with that of list in Table 2-3 and mentioning other queue implementations in the
standard library.
58 | Chapter 2: An Array of Sequences
Further Reading
Chapter 1, �Data Structures� of Python Cookbook, 3rd Edition (O�Reilly) by David
Beazley and Brian K. Jones has many recipes focusing on sequences, including �Recipe
1.11. Naming a Slice,� from which I learned the trick of assigning slices to variables to
improve readability, illustrated in our Example 2-11.
The second edition of Python Cookbook was written for Python 2.4, but much of its
code works with Python 3, and a lot of the recipes in Chapters 5 and 6 deal with sequences.
The book was edited by Alex Martelli, Anna Martelli Ravenscroft, and David
Ascher, and it includes contributions by dozens of Pythonistas. The third edition was
rewritten from scratch, and focuses more on the semantics of the language�particularly
what has changed in Python 3�while the older volume emphasizes pragmatics (i.e.,
how to apply the language to real-world problems). Even though some of the second
edition solutions are no longer the best approach, I honestly think it is worthwhile to
have both editions of Python Cookbook on hand.
The official Python Sorting HOW TO has several examples of advanced tricks for using
sorted and list.sort.
PEP 3132 � Extended Iterable Unpacking is the canonical source to read about the new
use of *extra as a target in parallel assignments. If you�d like a glimpse of Python evolving,
Missing *-unpacking generalizations is a bug tracker issue proposing even wider
use of iterable unpacking notation. PEP 448 � Additional Unpacking Generalizations
resulted from the discussions in that issue. At the time of this writing, it seems likely
the proposed changes will be merged to Python, perhaps in version 3.5.
Eli Bendersky�s blog post �Less Copies in Python with the Buffer Protocol and memoryviews
includes a short tutorial on memoryview.
There are numerous books covering NumPy in the market, even some that don�t mention
�NumPy� in the title. Wes McKinney�s Python for Data Analysis (O�Reilly) is one
such title.
Scientists love the combination of an interactive prompt with the power of NumPy and
SciPy so much that they developed IPython, an incredibly powerful replacement for the
Python console that also provides a GUI, integrated inline graph plotting, literate programming
support (interleaving text with code), and rendering to PDF. Interactive,
multimedia IPython sessions can even be shared over HTTP as IPython notebooks. See
screenshots and video at The IPython Notebook. IPython is so hot that in 2012 its core
developers, most of whom are researchers at UC Berkeley, received a $1.15 million grant
from the Sloan Foundation for enhancements to be implemented over the 2013�2014
period.
In The Python Standard Library, 8.3. collections � Container datatypes includes short
examples and practical recipes using deque (and other collections).
Further Reading | 59
The best defense of the Python convention of excluding the last item in ranges and slices
was written by Edsger W. Dijkstra himself, in a short memo titled �Why Numbering
Should Start at Zero�. The subject of the memo is mathematical notation, but it�s relevant
to Python because Prof. Dijkstra explains with rigor and humor why the sequence 2, 3,
�, 12 should always be expressed as 2 ? i < 13. All other reasonable conventions are
refuted, as is the idea of letting each user choose a convention. The title refers to zerobased
indexing, but the memo is really about why it is desirable that 'ABCDE'[1:3]
means 'BC' and not 'BCD' and why it makes perfect sense to write 2, 3, �, 12 as
range(2, 13). (By the way, the memo is a handwritten note, but it�s beautiful and totally
readable. Somebody should create a Dijkstra font�I�d buy it.)
Soapbox
The Nature of Tuples
In 2012, I presented a poster about the ABC language at PyCon US. Before creating
Python, Guido had worked on the ABC interpreter, so he came to see my poster. Among
other things, we talked about the ABC compounds, which are clearly the predecessors
of Python tuples. Compounds also support parallel assignment and are used as composite
keys in dictionaries (or tables, in ABC parlance). However, compounds are not
sequences. They are not iterable and you cannot retrieve a field by index, much less slice
them. You either handle the compound as whole or extract the individual fields using
parallel assignment, that�s all.
I told Guido that these limitations make the main purpose of compounds very clear:
they are just records without field names. His response: �Making tuples behave as sequences
was a hack.�
This illustrates the pragmatic approach that makes Python so much better and more
successful than ABC. From a language implementer perspective, making tuples behave
as sequences costs little. As a result, tuples may not be as �conceptually pure� as compounds,
but we have many more ways of using them. They can even be used as immutable
lists, of all things!
It is really useful to have immutable lists in the language, even if their type is not called
frozenlist but is really tuple behaving as a sequence.
�Elegance Begets Simplicity�
The use of the syntax *extra to assign multiple items to a parameter started with function
definitions a long time ago (I have a book about Python 1.4 from 1996 that covers
that). Starting with Python 1.6, the form *extra can be used in the context of function
calls to unpack an iterable into multiple arguments, a complementary operation. This
is elegant, makes intuitive sense, and made the apply function redundant (it�s now gone).
Now, with Python 3, the *extra notation also works on the left of parallel assignments
to grab excess items, enhancing what was already a handy language feature.
60 | Chapter 2: An Array of Sequences
With each of these changes, the language became more flexible, more consistent, and
simpler at the same time. �Elegance begets simplicity� is the motto on my favorite PyCon
T-shirt from Chicago, 2009. It is decorated with a painting by Bruce Eckel depicting
hexagram 22 from the I Ching, ? (bi), �Adorning,� sometimes translated as �Grace� or
�Beauty.�
Flat Versus Container Sequences
To highlight the different memory models of the sequence types, I used the terms
container sequence and flat sequence. The �container� word is from the Data Model
documentation:
Some objects contain references to other objects; these are called containers.
I used the term �container sequence� to be specific, because there are containers in
Python that are not sequences, like dict and set. Container sequences can be nested
because they may contain objects of any type, including their own type.
On the other hand, flat sequences are sequence types that cannot be nested because they
only hold simple atomic types like integers, floats, or characters.
I adopted the term flat sequence because I needed something to contrast with �container
sequence.� I can�t cite a reference to support the use of flat sequence in this specific
context: as the category of Python sequence types that are not containers. On Wikipedia,
this usage would be tagged �original research.� I prefer to call it �our term,� hoping you�ll
find it useful and adopt it too.
Mixed Bag Lists
Introductory Python texts emphasize that lists can contain objects of mixed types, but
in practice that feature is not very useful: we put items in a list to process them later,
which implies that all items should support at least some operation in common (i.e.,
they should all �quack� whether or not they are genetically 100% ducks). For example,
you can�t sort a list in Python 3 unless the items in it are comparable:
>>> l = [28, 14, '28', 5, '9', '1', 0, 6, '23', 19]
>>> sorted(l)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: unorderable types: str() < int()
Unlike lists, tuples often hold items of different types. That is natural, considering that
each item in a tuple is really a field, and each field type is independent of the others.
Key Is Brilliant
The key optional argument of list.sort, sorted, max, and min is a great idea. Other
languages force you to provide a two-argument comparison function like the deprecated
cmp(a, b) function in Python 2. Using key is both simpler and more efficient. It�s simpler
because you just define a one-argument function that retrieves or calculates whatever
criterion you want to use to sort your objects; this is easier than writing a two-argument
Further Reading | 61
function to return �1, 0, 1. It is also more efficient because the key function is invoked
only once per item, while the two-argument comparison is called every time the sorting
algorithm needs to compare two items. Of course, Python also has to compare the keys
while sorting, but that comparison is done in optimized C code and not in a Python
function that you wrote.
By the way, using key actually lets us sort a mixed bag of numbers and number-like
strings. You just need to decide whether you want to treat all items as integers or strings:
>>> l = [28, 14, '28', 5, '9', '1', 0, 6, '23', 19]
>>> sorted(l, key=int)
[0, '1', 5, 6, '9', 14, 19, '23', 28, '28']
>>> sorted(l, key=str)
[0, '1', 14, 19, '23', 28, '28', 5, 6, '9']
Oracle, Google, and the Timbot Conspiracy
The sorting algorithm used in sorted and list.sort is Timsort, an adaptive algorithm
that switches from insertion sort to merge sort strategies, depending on how ordered
the data is. This is efficient because real-world data tends to have runs of sorted items.
There is a Wikipedia article about it.
Timsort was first deployed in CPython, in 2002. Since 2009, Timsort is also used to sort
arrays in both standard Java and Android, a fact that became widely known when Oracle
used some of the code related to Timsort as evidence of Google infringement of Sun�s
intellectual property. See Oracle v. Google - Day 14 Filings.
Timsort was invented by Tim Peters, a Python core developer so prolific that he is believed
to be an AI, the Timbot. You can read about that conspiracy theory in Python
Humor. Tim also wrote The Zen of Python: import this.
62 | Chapter 2: An Array of Sequences
CHAPTER 3
Dictionaries and Sets
Any running Python program has many dictionaries active at the same time, even if the
user�s program code doesn�t explicitly use a dictionary.
� A. M. Kuchling
Chapter 18, �Python�s Dictionary Implementation
The dict type is not only widely used in our programs but also a fundamental part of
the Python implementation. Module namespaces, class and instance attributes, and
function keyword arguments are some of the fundamental constructs where dictionaries
are deployed. The built-in functions live in __builtins__.__dict__.
Because of their crucial role, Python dicts are highly optimized. Hash tables are the
engines behind Python�s high-performance dicts.
We also cover sets in this chapter because they are implemented with hash tables as well.
Knowing how a hash table works is key to making the most of dictionaries and sets.
Here is a brief outline of this chapter:
� Common dictionary methods
� Special handling for missing keys
� Variations of dict in the standard library
� The set and frozenset types
� How hash tables work
� Implications of hash tables (key type limitations, unpredictable ordering, etc.)
63
Generic Mapping Types
The collections.abc module provides the Mapping and MutableMapping ABCs to
formalize the interfaces of dict and similar types (in Python 2.6 to 3.2, these classes are
imported from the collections module, and not from collections.abc). See
Figure 3-1.
Figure 3-1. UML class diagram for the MutableMapping and its superclasses from collections.
abc (inheritance arrows point from subclasses to superclasses; names in italic
are abstract classes and abstract methods)
Implementations of specialized mappings often extend dict or collections.User
Dict, instead of these ABCs. The main value of the ABCs is documenting and formalizing
the minimal interfaces for mappings, and serving as criteria for isinstance tests
in code that needs to support mappings in a broad sense:
>>> my_dict = {}
>>> isinstance(my_dict, abc.Mapping)
True
Using isinstance is better than checking whether a function argument is of dict type,
because then alternative mapping types can be used.
All mapping types in the standard library use the basic dict in their implementation,
so they share the limitation that the keys must be hashable (the values need not be
hashable, only the keys).
64 | Chapter 3: Dictionaries and Sets
What Is Hashable?
Here is part of the definition of hashable from the Python Glossary:
An object is hashable if it has a hash value which never changes during its lifetime (it
needs a __hash__() method), and can be compared to other objects (it needs an
__eq__() method). Hashable objects which compare equal must have the same hash
value. [�]
The atomic immutable types (str, bytes, numeric types) are all hashable. A frozen
set is always hashable, because its elements must be hashable by definition. A tuple is
hashable only if all its items are hashable. See tuples tt, tl, and tf:
>>> tt = (1, 2, (30, 40))
>>> hash(tt)
8027212646858338501
>>> tl = (1, 2, [30, 40])
>>> hash(tl)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: unhashable type: 'list'
>>> tf = (1, 2, frozenset([30, 40]))
>>> hash(tf)
-4118419923444501110
At the time of this writing, the Python Glossary states: �All of
Python�s immutable built-in objects are hashable� but that is
inaccurate because a tuple is immutable, yet it may contain
references to unhashable objects.
User-defined types are hashable by default because their hash value is their id() and
they all compare not equal. If an object implements a custom __eq__ that takes into
account its internal state, it may be hashable only if all its attributes are immutable.
Given these ground rules, you can build dictionaries in several ways. The Built-in
Types page in the Library Reference has this example to show the various means of
building a dict:
>>> a = dict(one=1, two=2, three=3)
>>> b = {'one': 1, 'two': 2, 'three': 3}
>>> c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))
>>> d = dict([('two', 2), ('one', 1), ('three', 3)])
>>> e = dict({'three': 3, 'one': 1, 'two': 2})
>>> a == b == c == d == e
True
Generic Mapping Types | 65
In addition to the literal syntax and the flexible dict constructor, we can use dict comprehensions
to build dictionaries. See the next section.
dict Comprehensions
Since Python 2.7, the syntax of listcomps and genexps was applied to dict comprehensions
(and set comprehensions as well, which we�ll soon visit). A dictcomp builds a dict
instance by producing key:value pair from any iterable. Example 3-1 shows the use of
dict comprehensions to build two dictionaries from the same list of tuples.
Example 3-1. Examples of dict comprehensions
>>> DIAL_CODES = [
... (86, 'China'),
... (91, 'India'),
... (1, 'United States'),
... (62, 'Indonesia'),
... (55, 'Brazil'),
... (92, 'Pakistan'),
... (880, 'Bangladesh'),
... (234, 'Nigeria'),
... (7, 'Russia'),
... (81, 'Japan'),
... ]
>>> country_code = {country: code for code, country in DIAL_CODES}
>>> country_code
{'China': 86, 'India': 91, 'Bangladesh': 880, 'United States': 1,
'Pakistan': 92, 'Japan': 81, 'Russia': 7, 'Brazil': 55, 'Nigeria':
234, 'Indonesia': 62}
>>> {code: country.upper() for country, code in country_code.items()
... if code < 66}
{1: 'UNITED STATES', 55: 'BRAZIL', 62: 'INDONESIA', 7: 'RUSSIA'}
A list of pairs can be used directly with the dict constructor.
Here the pairs are reversed: country is the key, and code is the value.
Reversing the pairs again, values uppercased and items filtered by code < 66.
If you�re used to liscomps, dictcomps are a natural next step. If you aren�t, the spread of
the listcomp syntax means it�s now more profitable than ever to become fluent in it.
We now move to a panoramic view of the API for mappings.
Overview of Common Mapping Methods
The basic API for mappings is quite rich. Table 3-1 shows the methods implemented
by dict and two of its most useful variations: defaultdict and OrderedDict, both
defined in the collections module.
66 | Chapter 3: Dictionaries and Sets
Table 3-1. Methods of the mapping types dict, collections.defaultdict, and collections.
OrderedDict (common object methods omitted for brevity); optional arguments
are enclosed in [�]
dict defaultdict OrderedDict
d.clear() ? ? ? Remove all items
d.__contains__(k) ? ? ? k in d
d.copy() ? ? ? Shallow copy
d.__copy__() ? Support for copy.copy
d.default_factory ? Callable invoked by __missing__ to set
missing valuesa
d.__delitem__(k) ? ? ? del d[k]�remove item with key k
d.fromkeys(it, [initial]) ? ? ? New mapping from keys in iterable, with optional
initial value (defaults to None)
d.get(k, [default]) ? ? ? Get item with key k, return default or None
if missing
d.__getitem__(k) ? ? ? d[k]�get item with key k
d.items() ? ? ? Get view over items�(key, value) pairs
d.__iter__() ? ? ? Get iterator over keys
d.keys() ? ? ? Get view over keys
d.__len__() ? ? ? len(d)�number of items
d.__missing__(k) ? Called when __getitem__ cannot find the key
d.move_to_end(k, [last]) ? Move k first or last position (last is True by
default)
d.pop(k, [default]) ? ? ? Remove and return value at k, or default or
None if missing
d.popitem() ? ? ? Remove and return an arbitrary (key, val
ue) itemb
d.__reversed__() ? Get iterator for keys from last to first inserted
d.setdefault(k, [default]) ? ? ? If k in d, return d[k]; else set d[k] =
default and return it
d.__setitem__(k, v) ? ? ? d[k] = v�put v at k
d.update(m, [**kargs]) ? ? ? Update d with items from mapping or iterable of
(key, value) pairs
d.values() ? ? ? Get view over values
a default_factory is not a method, but a callable instance attribute set by the end user when defaultdict is instantiated.
b OrderedDict.popitem() removes the first item inserted (FIFO); an optional last argument, if set to True, pops the
last item (LIFO).
Overview of Common Mapping Methods | 67
1. The original script appears in slide 41 of Martelli�s �Re-learning Python� presentation. His script is actually
a demonstration of dict.setdefault, as shown in our Example 3-4.
The way update handles its first argument m is a prime example of duck typing: it first
checks whether m has a keys method and, if it does, assumes it is a mapping. Otherwise,
update falls back to iterating over m, assuming its items are (key, value) pairs. The
constructor for most Python mappings uses the logic of update internally, which means
they can be initialized from other mappings or from any iterable object producing (key,
value) pairs.
A subtle mapping method is setdefault. We don�t always need it, but when we do, it
provides a significant speedup by avoiding redundant key lookups. If you are not comfortable
using it, the following section explains how, through a practical example.
Handling Missing Keys with setdefault
In line with the fail-fast philosophy, dict access with d[k] raises an error when k is not
an existing key. Every Pythonista knows that d.get(k, default) is an alternative to
d[k] whenever a default value is more convenient than handling KeyError. However,
when updating the value found (if it is mutable), using either __getitem__ or get is
awkward and inefficient. Consider Example 3-2, a suboptimal script written just to show
one case where dict.get is not the best way to handle a missing key.
Example 3-2 is adapted from an example by Alex Martelli,1 which generates an index
like that in Example 3-3.
Example 3-2. index0.py uses dict.get to fetch and update a list of word occurrences from
the index (a better solution is in Example 3-4)
"""Build an index mapping word -> list of occurrences"""
import sys
import re
WORD_RE = re.compile('\w+')
index = {}
with open(sys.argv[1], encoding='utf-8') as fp:
for line_no, line in enumerate(fp, 1):
for match in WORD_RE.finditer(line):
word = match.group()
column_no = match.start()+1
location = (line_no, column_no)
# this is ugly; coded like this to make a point
occurrences = index.get(word, [])
occurrences.append(location)
index[word] = occurrences
68 | Chapter 3: Dictionaries and Sets
2. This is an example of using a method as a first-class function, the subject of Chapter 5.
# print in alphabetical order
for word in sorted(index, key=str.upper):
print(word, index[word])
Get the list of occurrences for word, or [] if not found.
Append new location to occurrences.
Put changed occurrences into index dict; this entails a second search through
the index.
In the key= argument of sorted I am not calling str.upper, just passing a
reference to that method so the sorted function can use it to normalize the
words for sorting.2
Example 3-3. Partial output from Example 3-2 processing the Zen of Python; each line
shows a word and a list of occurrences coded as pairs: (line-number, column-number)
$ python3 index0.py ../../data/zen.txt
a [(19, 48), (20, 53)]
Although [(11, 1), (16, 1), (18, 1)]
ambiguity [(14, 16)]
and [(15, 23)]
are [(21, 12)]
aren [(10, 15)]
at [(16, 38)]
bad [(19, 50)]
be [(15, 14), (16, 27), (20, 50)]
beats [(11, 23)]
Beautiful [(3, 1)]
better [(3, 14), (4, 13), (5, 11), (6, 12), (7, 9), (8, 11),
(17, 8), (18, 25)]
...
The three lines dealing with occurrences in Example 3-2 can be replaced by a single
line using dict.setdefault. Example 3-4 is closer to Alex Martelli�s original example.
Example 3-4. index.py uses dict.setdefault to fetch and update a list of word occurrences
from the index in a single line; contrast with Example 3-2
"""Build an index mapping word -> list of occurrences"""
import sys
import re
WORD_RE = re.compile('\w+')
index = {}
Overview of Common Mapping Methods | 69
with open(sys.argv[1], encoding='utf-8') as fp:
for line_no, line in enumerate(fp, 1):
for match in WORD_RE.finditer(line):
word = match.group()
column_no = match.start()+1
location = (line_no, column_no)
index.setdefault(word, []).append(location)
# print in alphabetical order
for word in sorted(index, key=str.upper):
print(word, index[word])
Get the list of occurrences for word, or set it to [] if not found; setdefault
returns the value, so it can be updated without requiring a second search.
In other words, the end result of this line�
my_dict.setdefault(key, []).append(new_value)
�is the same as running�
if key not in my_dict:
my_dict[key] = []
my_dict[key].append(new_value)
�except that the latter code performs at least two searches for key�three if it�s not
found�while setdefault does it all with a single lookup.
A related issue, handling missing keys on any lookup (and not only when inserting), is
the subject of the next section.
Mappings with Flexible Key Lookup
Sometimes it is convenient to have mappings that return some made-up value when a
missing key is searched. There are two main approaches to this: one is to use a default
dict instead of a plain dict. The other is to subclass dict or any other mapping type
and add a __missing__ method. Both solutions are covered next.
defaultdict: Another Take on Missing Keys
Example 3-5 uses collections.defaultdict to provide another elegant solution to the
problem in Example 3-4. A defaultdict is configured to create items on demand
whenever a missing key is searched.
Here is how it works: when instantiating a defaultdict, you provide a callable that is
used to produce a default value whenever __getitem__ is passed a nonexistent key
argument.
70 | Chapter 3: Dictionaries and Sets
For example, given an empty defaultdict created as dd = defaultdict(list), if
'new-key' is not in dd, the expression dd['new-key'] does the following steps:
1. Calls list() to create a new list.
2. Inserts the list into dd using 'new-key' as key.
3. Returns a reference to that list.
The callable that produces the default values is held in an instance attribute called
default_factory.
Example 3-5. index_default.py: using an instance of defaultdict instead of the setdefault
method
"""Build an index mapping word -> list of occurrences"""
import sys
import re
import collections
WORD_RE = re.compile('\w+')
index = collections.defaultdict(list)
with open(sys.argv[1], encoding='utf-8') as fp:
for line_no, line in enumerate(fp, 1):
for match in WORD_RE.finditer(line):
word = match.group()
column_no = match.start()+1
location = (line_no, column_no)
index[word].append(location)
# print in alphabetical order
for word in sorted(index, key=str.upper):
print(word, index[word])
Create a defaultdict with the list constructor as default_factory.
If word is not initially in the index, the default_factory is called to produce
the missing value, which in this case is an empty list that is then assigned to
index[word] and returned, so the .append(location) operation always
succeeds.
If no default_factory is provided, the usual KeyError is raised for missing keys.
Mappings with Flexible Key Lookup | 71
The default_factory of a defaultdict is only invoked to provide
default values for __getitem__ calls, and not for the other
methods. For example, if dd is a defaultdict, and k is a missing
key, dd[k] will call the default_factory to create a default value,
but dd.get(k) still returns None.
The mechanism that makes defaultdict work by calling default_factory is actually
the __missing__ special method, a feature supported by all standard mapping types
that we discuss next.
The __missing__ Method
Underlying the way mappings deal with missing keys is the aptly named __missing__
method. This method is not defined in the base dict class, but dict is aware of it: if you
subclass dict and provide a __missing__ method, the standard dict.__getitem__ will
call it whenever a key is not found, instead of raising KeyError.
The __missing__ method is just called by __getitem__ (i.e., for
the d[k] operator). The presence of a __missing__ method has no
effect on the behavior of other methods that look up keys, such as
get or __contains__ (which implements the in operator). This is
why the default_factory of defaultdict works only with
__getitem__, as noted in the warning at the end of the previous
section.
Suppose you�d like a mapping where keys are converted to str when looked up. A
concrete use case is the Pingo.io project, where a programmable board with GPIO pins
(e.g., the Raspberry Pi or the Arduino) is represented by a board object with a
board.pins attribute, which is a mapping of physical pin locations to pin objects, and
the physical location may be just a number or a string like "A0" or "P9_12". For consistency,
it is desirable that all keys in board.pins are strings, but it is also convenient
that looking up my_arduino.pin[13] works as well, so beginners are not tripped when
they want to blink the LED on pin 13 of their Arduinos. Example 3-6 shows how such
a mapping would work.
Example 3-6. When searching for a nonstring key, StrKeyDict0 converts it to str when it
is not found
Tests for item retrieval using `d[key]` notation::
>>> d = StrKeyDict0([('2', 'two'), ('4', 'four')])
>>> d['2']
'two'
72 | Chapter 3: Dictionaries and Sets
>>> d[4]
'four'
>>> d[1]
Traceback (most recent call last):
...
KeyError: '1'
Tests for item retrieval using `d.get(key)` notation::
>>> d.get('2')
'two'
>>> d.get(4)
'four'
>>> d.get(1, 'N/A')
'N/A'
Tests for the `in` operator::
>>> 2 in d
True
>>> 1 in d
False
Example 3-7 implements a class StrKeyDict0 that passes the preceding tests.
A better way to create a user-defined mapping type is to subclass
collections.UserDict instead of dict (as we�ll do in Example
3-8). Here we subclass dict just to show that __miss
ing__ is supported by the built-in dict.__getitem__ method.
Example 3-7. StrKeyDict0 converts nonstring keys to str on lookup (see tests in
Example 3-6)
class StrKeyDict0(dict):
def __missing__(self, key):
if isinstance(key, str):
raise KeyError(key)
return self[str(key)]
def get(self, key, default=None):
try:
return self[key]
except KeyError:
return default
def __contains__(self, key):
return key in self.keys() or str(key) in self.keys()
Mappings with Flexible Key Lookup | 73
StrKeyDict0 inherits from dict.
Check whether key is already a str. If it is, and it�s missing, raise KeyError.
Build str from key and look it up.
The get method delegates to __getitem__ by using the self[key] notation; that
gives the opportunity for our __missing__ to act.
If a KeyError was raised, __missing__ already failed, so we return the default.
Search for unmodified key (the instance may contain non-str keys), then for a
str built from the key.
Take a moment to consider why the test isinstance(key, str) is necessary in the
__missing__ implementation.
Without that test, our __missing__ method would work OK for any key k�str or not
str�whenever str(k) produced an existing key. But if str(k) is not an existing key,
we�d have an infinite recursion. The last line, self[str(key)] would call __geti
tem__ passing that str key, which in turn would call __missing__ again.
The __contains__ method is also needed for consistent behavior in this example, because
the operation k in d calls it, but the method inherited from dict does not fall
back to invoking __missing__. There is a subtle detail in our implementation of __con
tains__: we do not check for the key in the usual Pythonic way�k in my_dict�because
str(key) in self would recursively call __contains__. We avoid this by explicitly
looking up the key in self.keys().
A search like k in my_dict.keys() is efficient in Python 3 even
for very large mappings because dict.keys() returns a view,
which is similar to a set, and containment checks in sets are as
fast as in dictionaries. Details are documented in the �Dictionary�
view objects section of the documentation. In Python 2,
dict.keys() returns a list, so our solution also works there, but
it is not efficient for large dictionaries, because k in my_list
must scan the list.
The check for the unmodified key�key in self.keys()�is necessary for correctness
because StrKeyDict0 does not enforce that all keys in the dictionary must be of type
str. Our only goal with this simple example is to make searching �friendlier� and not
enforce types.
So far we have covered the dict and defaultdict mapping types, but the standard
library comes with other mapping implementations, which we discuss next.
74 | Chapter 3: Dictionaries and Sets
Variations of dict
In this section, we summarize the various mapping types included in the collec
tions module of the standard library, besides defaultdict:
collections.OrderedDict
Maintains keys in insertion order, allowing iteration over items in a predictable
order. The popitem method of an OrderedDict pops the first item by default, but
if called as my_odict.popitem(last=True), it pops the last item added.
collections.ChainMap
Holds a list of mappings that can be searched as one. The lookup is performed on
each mapping in order, and succeeds if the key is found in any of them. This is useful
to interpreters for languages with nested scopes, where each mapping represents a
scope context. The �ChainMap objects� section of the collections docs has several
examples of ChainMap usage, including this snippet inspired by the basic rules of
variable lookup in Python:
import builtins
pylookup = ChainMap(locals(), globals(), vars(builtins))
collections.Counter
A mapping that holds an integer count for each key. Updating an existing key adds
to its count. This can be used to count instances of hashable objects (the keys) or
as a multiset�a set that can hold several occurrences of each element. Counter
implements the + and - operators to combine tallies, and other useful methods such
as most_common([n]), which returns an ordered list of tuples with the n most common
items and their counts; see the documentation. Here is Counter used to count
letters in words:
>>> ct = collections.Counter('abracadabra')
>>> ct
Counter({'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1})
>>> ct.update('aaaaazzz')
>>> ct
Counter({'a': 10, 'z': 3, 'b': 2, 'r': 2, 'c': 1, 'd': 1})
>>> ct.most_common(2)
[('a', 10), ('z', 3)]
collections.UserDict
A pure Python implementation of a mapping that works like a standard dict.
While OrderedDict, ChainMap, and Counter come ready to use, UserDict is designed
to be subclassed, as we�ll do next.
Variations of dict | 75
3. The exact problem with subclassing dict and other built-ins is covered in �Subclassing Built-In Types Is
Tricky� on page 348.
Subclassing UserDict
It�s almost always easier to create a new mapping type by extending UserDict rather
than dict. Its value can be appreciated as we extend our StrKeyDict0 from Example 3-7
to make sure that any keys added to the mapping are stored as str.
The main reason why it�s preferable to subclass from UserDict rather than from dict
is that the built-in has some implementation shortcuts that end up forcing us to override
methods that we can just inherit from UserDict with no problems.3
Note that UserDict does not inherit from dict, but has an internal dict instance, called
data, which holds the actual items. This avoids undesired recursion when coding special
methods like __setitem__, and simplifies the coding of __contains__, compared to
Example 3-7.
Thanks to UserDict, StrKeyDict (Example 3-8) is actually shorter than StrKeyDict0
(Example 3-7), but it does more: it stores all keys as str, avoiding unpleasant surprises
if the instance is built or updated with data containing nonstring keys.
Example 3-8. StrKeyDict always converts non-string keys to str�on insertion, update,
and lookup
import collections
class StrKeyDict(collections.UserDict):
def __missing__(self, key):
if isinstance(key, str):
raise KeyError(key)
return self[str(key)]
def __contains__(self, key):
return str(key) in self.data
def __setitem__(self, key, item):
self.data[str(key)] = item
StrKeyDict extends UserDict.
__missing__ is exactly as in Example 3-7.
__contains__ is simpler: we can assume all stored keys are str and we can check
on self.data instead of invoking self.keys() as we did in StrKeyDict0.
76 | Chapter 3: Dictionaries and Sets
__setitem__ converts any key to a str. This method is easier to overwrite when
we can delegate to the self.data attribute.
Because UserDict subclasses MutableMapping, the remaining methods that make
StrKeyDict a full-fledged mapping are inherited from UserDict, MutableMapping, or
Mapping. The latter have several useful concrete methods, in spite of being abstract base
classes (ABCs). The following methods are worth noting:
MutableMapping.update
This powerful method can be called directly but is also used by __init__ to load
the instance from other mappings, from iterables of (key, value) pairs, and keyword
arguments. Because it uses self[key] = value to add items, it ends up calling
our implementation of __setitem__.
Mapping.get
In StrKeyDict0 (Example 3-7), we had to code our own get to obtain results consistent
with __getitem__, but in Example 3-8 we inherited Mapping.get, which is
implemented exactly like StrKeyDict0.get (see Python source code).
After I wrote StrKeyDict, I discovered that Antoine Pitrou authored
PEP 455 � Adding a key-transforming dictionary to collections
and a patch to enhance the collections module with a
TransformDict. The patch is attached to issue18986 and may land
in Python 3.5. To experiment with TransformDict, I extracted it
into a standalone module (03-dict-set/transformdict.py in the Fluent
Python code repository). TransformDict is more general than
StrKeyDict, and is complicated by the requirement to preserve the
keys as they were originally inserted.
We know there are several immutable sequence types, but how about an immutable
dictionary? Well, there isn�t a real one in the standard library, but a stand-in is available.
Read on.
Immutable Mappings
The mapping types provided by the standard library are all mutable, but you may need
to guarantee that a user cannot change a mapping by mistake. A concrete use case can
be found, again, in the Pingo.io project I described in �The __missing__ Method� on
page 72: the board.pins mapping represents the physical GPIO pins on the device. As
such, it�s nice to prevent inadvertent updates to board.pins because the hardware can�t
possibly be changed via software, so any change in the mapping would make it inconsistent
with the physical reality of the device.
Immutable Mappings | 77
4. We are not actually using MappingProxyType in Pingo.io because it is new in Python 3.3 and we need to
support Python 2.7 at this time.
Since Python 3.3, the types module provides a wrapper class called MappingProxy
Type, which, given a mapping, returns a mappingproxy instance that is a read-only but
dynamic view of the original mapping. This means that updates to the original mapping
can be seen in the mappingproxy, but changes cannot be made through it. See
Example 3-9 for a brief demonstration.
Example 3-9. MappingProxyType builds a read-only mappingproxy instance from a
dict
>>> from types import MappingProxyType
>>> d = {1: 'A'}
>>> d_proxy = MappingProxyType(d)
>>> d_proxy
mappingproxy({1: 'A'})
>>> d_proxy[1]
'A'
>>> d_proxy[2] = 'x'
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: 'mappingproxy' object does not support item assignment
>>> d[2] = 'B'
>>> d_proxy
mappingproxy({1: 'A', 2: 'B'})
>>> d_proxy[2]
'B'
>>>
Items in d can be seen through d_proxy.
Changes cannot be made through d_proxy.
d_proxy is dynamic: any change in d is reflected.
Here is how this could be used in practice in the Pingo.io scenario: the constructor in
a concrete Board subclass would fill a private mapping with the pin objects, and expose
it to clients of the API via a public .pins attribute implemented as a mappingproxy. That
way the clients would not be able to add, remove, or change pins by accident.4
Now that we�ve covered most mapping types in the standard library and when to use
them, we will move to the set types.
78 | Chapter 3: Dictionaries and Sets
Set Theory
Sets are a relatively new addition in the history of Python, and somewhat underused.
The set type and its immutable sibling frozenset first appeared in a module in Python
2.3 and were promoted to built-ins in Python 2.6.
In this book, the word �set� is used to refer both to set and
frozenset. When talking specifically about the set class, its name
appears in the constant width font used for source code: set.
A set is a collection of unique objects. A basic use case is removing duplication:
>>> l = ['spam', 'spam', 'eggs', 'spam']
>>> set(l)
{'eggs', 'spam'}
>>> list(set(l))
['eggs', 'spam']
Set elements must be hashable. The set type is not hashable, but frozenset is, so you
can have frozenset elements inside a set.
In addition to guaranteeing uniqueness, the set types implement the essential set operations
as infix operators, so, given two sets a and b, a | b returns their union, a & b
computes the intersection, and a - b the difference. Smart use of set operations can
reduce both the line count and the runtime of Python programs, at the same time making
code easier to read and reason about�by removing loops and lots of conditional logic.
For example, imagine you have a large set of email addresses (the haystack) and a
smaller set of addresses (the needles) and you need to count how many needles occur
in the haystack. Thanks to set intersection (the & operator) you can code that in a
simple line (see Example 3-10).
Example 3-10. Count occurrences of needles in a haystack, both of type set
found = len(needles & haystack)
Without the intersection operator, you�d have write Example 3-11 to accomplish the
same task as Example 3-10.
Example 3-11. Count occurrences of needles in a haystack (same end result as
Example 3-10)
found = 0
for n in needles:
if n in haystack:
found += 1
Set Theory | 79
Example 3-10 runs slightly faster than Example 3-11. On the other hand, Example 3-11
works for any iterable objects needles and haystack, while Example 3-10 requires that
both be sets. But, if you don�t have sets on hand, you can always build them on the fly,
as shown in Example 3-12.
Example 3-12. Count occurrences of needles in a haystack; these lines work for any
iterable types
found = len(set(needles) & set(haystack))
# another way:
found = len(set(needles).intersection(haystack))
Of course, there is an extra cost involved in building the sets in Example 3-12, but if
either the needles or the haystack is already a set, the alternatives in Example 3-12 may
be cheaper than Example 3-11.
Any one of the preceding examples are capable of searching 1,000 values in a hay
stack of 10,000,000 items in a little over 3 milliseconds�that�s about 3 microseconds
per needle.
Besides the extremely fast membership test (thanks to the underlying hash table), the
set and frozenset built-in types provide a rich selection of operations to create new
sets or, in the case of set, to change existing ones. We will discuss the operations shortly,
but first a note about syntax.
set Literals
The syntax of set literals�{1}, {1, 2}, etc.�looks exactly like the math notation, with
one important exception: there�s no literal notation for the empty set, so we must remember
to write set().
Syntax Quirk
Don�t forget: to create an empty set, you should use the constructor
without an argument: set(). If you write {}, you�re creating an
empty dict�this hasn�t changed.
In Python 3, the standard string representation of sets always uses the {...} notation,
except for the empty set:
>>> s = {1}
>>> type(s)
<class 'set'>
>>> s
{1}
>>> s.pop()
1
80 | Chapter 3: Dictionaries and Sets
>>> s
set()
Literal set syntax like {1, 2, 3} is both faster and more readable than calling the
constructor (e.g., set([1, 2, 3])). The latter form is slower because, to evaluate it,
Python has to look up the set name to fetch the constructor, then build a list, and finally
pass it to the constructor. In contrast, to process a literal like {1, 2, 3}, Python runs
a specialized BUILD_SET bytecode.
Take a look at the bytecode for the two operations, as output by dis.dis (the disassembler
function):
>>> from dis import dis
>>> dis('{1}')
1 0 LOAD_CONST 0 (1)
3 BUILD_SET 1
6 RETURN_VALUE
>>> dis('set([1])')
1 0 LOAD_NAME 0 (set)
3 LOAD_CONST 0 (1)
6 BUILD_LIST 1
9 CALL_FUNCTION 1 (1 positional, 0 keyword pair)
12 RETURN_VALUE
Disassemble bytecode for literal expression {1}.
Special BUILD_SET bytecode does almost all the work.
Bytecode for set([1]).
Three operations instead of BUILD_SET: LOAD_NAME, BUILD_LIST, and
CALL_FUNCTION.
There is no special syntax to represent frozenset literals�they must be created by
calling the constructor. The standard string representation in Python 3 looks like a
frozenset constructor call. Note the output in the console session:
>>> frozenset(range(10))
frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9})
Speaking of syntax, the familiar shape of listcomps was adapted to build sets as well.
Set Comprehensions
Set comprehensions (setcomps) were added in Python 2.7, together with the dictcomps
that we saw in �dict Comprehensions� on page 66. Example 3-13 is a simple example.
Set Theory | 81
Example 3-13. Build a set of Latin-1 characters that have the word �SIGN� in their
Unicode names
>>> from unicodedata import name
>>> {chr(i) for i in range(32, 256) if 'SIGN' in name(chr(i),'')}
{'�', '=', '?', '#', '�', '<', '?', '?', '?', '$', '�', '?', '�',
'�', '+', '?', '�', '>', '�', '�', '%'}
Import name function from unicodedata to obtain character names.
Build set of characters with codes from 32 to 255 that have the word 'SIGN' in
their names.
Syntax matters aside, let�s now review the rich assortment of operations provided by
sets.
Set Operations
Figure 3-2 gives an overview of the methods you can expect from mutable and immutable
sets. Many of them are special methods for operator overloading. Table 3-2 shows
the math set operators that have corresponding operators or methods in Python. Note
that some operators and methods perform in-place changes on the target set (e.g., &=,
difference_update, etc.). Such operations make no sense in the ideal world of mathematical
sets, and are not implemented in frozenset.
Figure 3-2. UML class diagram for MutableSet and its superclasses from collections.abc
(names in italic are abstract classes and abstract methods; reverse operator methods
omitted for brevity)
82 | Chapter 3: Dictionaries and Sets
The infix operators in Table 3-2 require that both operands be sets,
but all other methods take one or more iterable arguments. For
example, to produce the union of four collections, a, b, c, and d,
you can call a.union(b, c, d), where a must be a set, but b, c,
and d can be iterables of any type.
Table 3-2. Mathematical set operations: these methods either produce a new set or update
the target set in place, if it�s mutable
Math
symbol
Python operator Method Description
S ? Z s & z s.__and__(z) Intersection of s and z
z & s s.__rand__(z) Reversed & operator
s.intersection(it, �) Intersection of s and all sets built from iterables it, etc.
s &= z s.__iand__(z) s updated with intersection of s and z
s.intersection_up
date(it, �)
s updated with intersection of s and all sets built from
iterables it, etc.
S ? Z s | z s.__or__(z) Union of s and z
z | s s.__ror__(z) Reversed |
s.union(it, �) Union of s and all sets built from iterables it, etc.
s |= z s.__ior__(z) s updated with union of s and z
s.update(it, �) s updated with union of s and all sets built from iterables
it, etc.
S \ Z s - z s.__sub__(z) Relative complement or difference between s and z
z - s s.__rsub__(z) Reversed - operator
s.difference(it, �) Difference between s and all sets built from iterables it,
etc.
s -= z s.__isub__(z) s updated with difference between s and z
s.difference_up
date(it, �)
s updated with difference between s and all sets built
from iterables it, etc.
s.symmetric_differ
ence(it)
Complement of s & set(it)
S ? Z s ^ z s.__xor__(z) Symmetric difference (the complement of the intersection
s & z)
z ^ s s.__rxor__(z) Reversed ^ operator
s.symmetric_differ
ence_update(it, �)
s updated with symmetric difference of s and all sets built
from iterables it, etc.
s ^= z s.__ixor__(z) s updated with symmetric difference of s and z
Set Theory | 83
As I write this, there is a Python bug report�(issue 8743)�that
says: �The set() operators (or, and, sub, xor, and their in-place
counterparts) require that the parameter also be an instance of
set().�, with the undesired side effect that these operators don�t work
with collections.abc.Set subclasses. The bug is already fixed in
trunk for Python 2.7 and 3.4, and should be history by the time
you read this.
Table 3-3 lists set predicates: operators and methods that return True or False.
Table 3-3. Set comparison operators and methods that return a bool
Math symbol Python operator Method Description
s.isdisjoint(z) s and z are disjoint (have no elements in common)
e ? S e in s s.__contains__(e) Element e is a member of s
S ? Z s <= z s.__le__(z) s is a subset of the z set
s.issubset(it) s is a subset of the set built from the iterable it
S ? Z s < z s.__lt__(z) s is a proper subset of the z set
S ? Z s >= z s.__ge__(z) s is a superset of the z set
s.issuperset(it) s is a superset of the set built from the iterable it
S ? Z s > z s.__gt__(z) s is a proper superset of the z set
In addition to the operators and methods derived from math set theory, the set types
implement other methods of practical use, summarized in Table 3-4.
Table 3-4. Additional set methods
set frozenset
s.add(e) ? Add element e to s
s.clear() ? Remove all elements of s
s.copy() ? ? Shallow copy of s
s.discard(e) ? Remove element e from s if it is present
s.__iter__() ? ? Get iterator over s
s.__len__() ? ? len(s)
s.pop() ? Remove and return an element from s, raising KeyError if s is empty
s.remove(e) ? Remove element e from s, raising KeyError if e not in s
This completes our overview of the features of sets.
We now change gears to discuss how dictionaries and sets are implemented with hash
tables. After reading the rest of this chapter, you will no longer be surprised by the
84 | Chapter 3: Dictionaries and Sets
apparently unpredictable behavior sometimes exhibited by dict, set, and their
brethren.
dict and set Under the Hood
Understanding how Python dictionaries and sets are implemented using hash tables is
helpful to make sense of their strengths and limitations.
Here are some questions this section will answer:
� How efficient are Python dict and set?
� Why are they unordered?
� Why can�t we use any Python object as a dict key or set element?
� Why does the order of the dict keys or set elements depend on insertion order,
and may change during the lifetime of the structure?
� Why is it bad to add items to a dict or set while iterating through it?
To motivate the study of hash tables, we start by showcasing the amazing performance
of dict and set with a simple test involving millions of items.
A Performance Experiment
From experience, all Pythonistas know that dicts and sets are fast. We�ll confirm that
with a controlled experiment.
To see how the size of a dict, set, or list affects the performance of search using the
in operator, I generated an array of 10 million distinct double-precision floats, the
�haystack.� I then generated an array of needles: 1,000 floats, with 500 picked from the
haystack and 500 verified not to be in it.
For the dict benchmark, I used dict.fromkeys() to create a dict named haystack
with 1,000 floats. This was the setup for the dict test. The actual code I clocked with
the timeit module is Example 3-14 (like Example 3-11).
Example 3-14. Search for needles in haystack and count those found
found = 0
for n in needles:
if n in haystack:
found += 1
The benchmark was repeated another four times, each time increasing tenfold the size
of haystack, to reach a size of 10,000,000 in the last test. The result of the dict performance
test is in Table 3-5.
dict and set Under the Hood | 85
Table 3-5. Total time for using in operator to search for 1,000 needles in haystack dicts
of five sizes on a Core i7 laptop running Python 3.4.0 (tests timed the loop in
Example 3-14)
len of haystack Factor dict time Factor
1,000 1x 0.000202s 1.00x
10,000 10x 0.000140s 0.69x
100,000 100x 0.000228s 1.13x
1,000,000 1,000x 0.000290s 1.44x
10,000,000 10,000x 0.000337s 1.67x
In concrete terms, to check for the presence of 1,000 floating-point keys in a dictionary
with 1,000 items, the processing time on my laptop was 0.000202s, and the same search
in a dict with 10,000,000 items took 0.000337s. In other words, the time per search in
the haystack with 10 million items was 0.337?s for each needle�yes, that is about one
third of a microsecond per needle.
To compare, I repeated the benchmark, with the same haystacks of increasing size, but
storing the haystack as a set or as list. For the set tests, in addition to timing the for
loop in Example 3-14, I also timed the one-liner in Example 3-15, which produces the
same result: count the number of elements from needles that are also in haystack.
Example 3-15. Use set intersection to count the needles that occur in haystack
found = len(needles & haystack)
Table 3-6 shows the tests side by side. The best times are in the �set& time� column,
which displays results for the set & operator using the code from Example 3-15. The
worst times are�as expected�in the �list time� column, because there is no hash table
to support searches with the in operator on a list, so a full scan must be made, resulting
in times that grow linearly with the size of the haystack.
Table 3-6. Total time for using in operator to search for 1,000 keys in haystacks of 5
sizes, stored as dicts, sets, and lists on a Core i7 laptop running Python 3.4.0 (tests
timed the loop in Example 3-14 except the set&, which uses Example 3-15)
len of haystack Factor dict time Factor set time Factor set& time Factor list time Factor
1,000 1x 0.000202s 1.00x 0.000143s 1.00x 0.000087s 1.00x 0.010556s 1.00x
10,000 10x 0.000140s 0.69x 0.000147s 1.03x 0.000092s 1.06x 0.086586s 8.20x
100,000 100x 0.000228s 1.13x 0.000241s 1.69x 0.000163s 1.87x 0.871560s 82.57x
1,000,000 1,000x 0.000290s 1.44x 0.000332s 2.32x 0.000250s 2.87x 9.189616s 870.56x
10,000,000 10,000x 0.000337s 1.67x 0.000387s 2.71x 0.000314s 3.61x 97.948056s 9,278.90x
86 | Chapter 3: Dictionaries and Sets
5. The source code for the CPython dictobject.c module is rich in comments. See also the reference for the
Beautiful Code book in �Further Reading� on page 94.
6. Because we just mentioned int, here is a CPython implementation detail: the hash value of an int that fits
in a machine word is the value of the int itself.
If your program does any kind of I/O, the lookup time for keys in dicts or sets is negligible,
regardless of the dict or set size (as long as it does fit in RAM). See the code used
to generate Table 3-6 and accompanying discussion in Appendix A, Example A-1.
Now that we have concrete evidence of the speed of dicts and sets, let�s explore how that
is achieved. The discussion of the hash table internals explains, for example, why the
key ordering is apparently random and unstable.
Hash Tables in Dictionaries
This is a high-level view of how Python uses a hash table to implement a dict. Many
details are omitted�the CPython code has some optimization tricks5�but the overall
description is accurate.
To simplify the ensuing presentation, we will focus on the internals
of dict first, and later transfer the concepts to sets.
A hash table is a sparse array (i.e., an array that always has empty cells). In standard data
structure texts, the cells in a hash table are often called �buckets.� In a dict hash table,
there is a bucket for each item, and it contains two fields: a reference to the key and a
reference to the value of the item. Because all buckets have the same size, access to an
individual bucket is done by offset.
Python tries to keep at least 1/3 of the buckets empty; if the hash table becomes too
crowded, it is copied to a new location with room for more buckets.
To put an item in a hash table, the first step is to calculate the hash value of the item key,
which is done with the hash() built-in function, explained next.
Hashes and equality
The hash() built-in function works directly with built-in types and falls back to calling
__hash__ for user-defined types. If two objects compare equal, their hash values must
also be equal, otherwise the hash table algorithm does not work. For example, because
1 == 1.0 is true, hash(1) == hash(1.0) must also be true, even though the internal
representation of an int and a float are very different.6
dict and set Under the Hood | 87
Also, to be effective as hash table indexes, hash values should scatter around the index
space as much as possible. This means that, ideally, objects that are similar but not equal
should have hash values that differ widely. Example 3-16 is the output of a script to
compare the bit patterns of hash values. Note how the hashes of 1 and 1.0 are the same,
but those of 1.0001, 1.0002, and 1.0003 are very different.
Example 3-16. Comparing hash bit patterns of 1, 1.0001, 1.0002, and 1.0003 on a 32-
bit build of Python (bits that are different in the hashes above and below are highlighted
with ! and the right column shows the number of bits that differ)
32-bit Python build
1 00000000000000000000000000000001
!= 0
1.0 00000000000000000000000000000001
------------------------------------------------
1.0 00000000000000000000000000000001
! !!! ! !! ! ! ! ! !! !!! != 16
1.0001 00101110101101010000101011011101
------------------------------------------------
1.0001 00101110101101010000101011011101
!!! !!!! !!!!! !!!!! !! ! != 20
1.0002 01011101011010100001010110111001
------------------------------------------------
1.0002 01011101011010100001010110111001
! ! ! !!! ! ! !! ! ! ! !!!! != 17
1.0003 00001100000111110010000010010110
------------------------------------------------
The code to produce Example 3-16 is in Appendix A. Most of it deals with formatting
the output, but it is listed as Example A-3 for completeness.
Starting with Python 3.3, a random salt value is added to the
hashes of str, bytes, and datetime objects. The salt value is
constant within a Python process but varies between interpreter
runs. The random salt is a security measure to prevent a DOS
attack. Details are in a note in the documentation for the __hash__
special method.
With this basic understanding of object hashes, we are ready to dive into the algorithm
that makes hash tables operate.
The hash table algorithm
To fetch the value at my_dict[search_key], Python calls hash(search_key) to obtain
the hash value of search_key and uses the least significant bits of that number as an
offset to look up a bucket in the hash table (the number of bits used depends on the
current size of the table). If the found bucket is empty, KeyError is raised. Otherwise,
88 | Chapter 3: Dictionaries and Sets
7. The C function that shuffles the hash bits in case of collision has a curious name: perturb. For all the details,
see dictobject.c in the CPython source code.
the found bucket has an item�a found_key:found_value pair�and then Python
checks whether search_key == found_key. If they match, that was the item sought:
found_value is returned.
However, if search_key and found_key do not match, this is a hash collision. This happens
because a hash function maps arbitrary objects to a small number of bits, and�in
addition�the hash table is indexed with a subset of those bits. In order to resolve the
collision, the algorithm then takes different bits in the hash, massages them in a particular
way, and uses the result as an offset to look up a different bucket.7 If that is empty,
KeyError is raised; if not, either the keys match and the item value is returned, or the
collision resolution process is repeated. See Figure 3-3 for a diagram of this algorithm.
Figure 3-3. Flowchart for retrieving an item from a dict; given a key, this procedure either
returns a value or raises KeyError
The process to insert or update an item is the same, except that when an empty bucket
is located, the new item is put there, and when a bucket with a matching key is found,
the value in that bucket is overwritten with the new value.
Additionally, when inserting items, Python may determine that the hash table is too
crowded and rebuild it to a new location with more room. As the hash table grows, so
does the number of hash bits used as bucket offsets, and this keeps the rate of collisions
low.
dict and set Under the Hood | 89
This implementation may seem like a lot of work, but even with millions of items in a
dict, many searches happen with no collisions, and the average number of collisions
per search is between one and two. Under normal usage, even the unluckiest keys can
be found after a handful of collisions are resolved.
Knowing the internals of the dict implementation we can explain the strengths and
limitations of this data structure and all the others derived from it in Python. We are
now ready to consider why Python dicts behave as they do.
Practical Consequences of How dict Works
In the following subsections, we�ll discuss the limitations and benefits that the underlying
hash table implementation brings to dict usage.
Keys must be hashable objects
An object is hashable if all of these requirements are met:
1. It supports the hash() function via a hash() method that always returns the same
value over the lifetime of the object.
2. It supports equality via an eq() method.
3. If a == b is True then hash(a) == hash(b) must also be True.
User-defined types are hashable by default because their hash value is their id() and
they all compare not equal.
If you implement a class with a custom __eq__ method, you must
also implement a suitable __hash__, because you must always make
sure that if a == b is True then hash(a) == hash(b) is also True.
Otherwise you are breaking an invariant of the hash table algorithm,
with the grave consequence that dicts and sets will not deal
reliably with your objects. If a custom __eq__ depends on mutable
state, then __hash__ must raise TypeError with a message like
unhashable type: 'MyClass'.
dicts have significant memory overhead
Because a dict uses a hash table internally, and hash tables must be sparse to work, they
are not space efficient. For example, if you are handling a large quantity of records, it
makes sense to store them in a list of tuples or named tuples instead of using a list of
dictionaries in JSON style, with one dict per record. Replacing dicts with tuples reduces
the memory usage in two ways: by removing the overhead of one hash table per record
and by not storing the field names again with each record.
90 | Chapter 3: Dictionaries and Sets
For user-defined types, the __slots__ class attribute changes the storage of instance
attributes from a dict to a tuple in each instance. This will be discussed in �Saving Space
with the __slots__ Class Attribute� on page 264 (Chapter 9).
Keep in mind we are talking about space optimizations. If you are dealing with a few
million objects and your machine has gigabytes of RAM, you should postpone such
optimizations until they are actually warranted. Optimization is the altar where maintainability
is sacrificed.
Key search is very fast
The dict implementation is an example of trading space for time: dictionaries have
significant memory overhead, but they provide fast access regardless of the size of the
dictionary�as long as it fits in memory. As Table 3-5 shows, when we increased the size
of a dict from 1,000 to 10,000,000 elements, the time to search grew by a factor of 2.8,
from 0.000163s to 0.000456s. The latter figure means we could search more than 2
million keys per second in a dict with 10 million items.
Key ordering depends on insertion order
When a hash collision happens, the second key ends up in a position that it would not
normally occupy if it had been inserted first. So, a dict built as dict([(key1, value1),
(key2, value2)]) compares equal to dict([(key2, value2), (key1, value1)]),
but their key ordering may not be the same if the hashes of key1 and key2 collide.
Example 3-17 demonstrates the effect of loading three dicts with the same data, just in
different order. The resulting dictionaries all compare equal, even if their order is not
the same.
Example 3-17. dialcodes.py fills three dictionaries with the same data sorted in different
ways
# dial codes of the top 10 most populous countries
DIAL_CODES = [
(86, 'China'),
(91, 'India'),
(1, 'United States'),
(62, 'Indonesia'),
(55, 'Brazil'),
(92, 'Pakistan'),
(880, 'Bangladesh'),
(234, 'Nigeria'),
(7, 'Russia'),
(81, 'Japan'),
]
d1 = dict(DIAL_CODES)
print('d1:', d1.keys())
d2 = dict(sorted(DIAL_CODES))
dict and set Under the Hood | 91
print('d2:', d2.keys())
d3 = dict(sorted(DIAL_CODES, key=lambda x:x[1]))
print('d3:', d3.keys())
assert d1 == d2 and d2 == d3
d1: built from the tuples in descending order of country population.
d2: filled with tuples sorted by dial code.
d3: loaded with tuples sorted by country name.
The dictionaries compare equal, because they hold the same key:value pairs.
Example 3-18 shows the output.
Example 3-18. Output from dialcodes.py shows three distinct key orderings
d1: dict_keys([880, 1, 86, 55, 7, 234, 91, 92, 62, 81])
d2: dict_keys([880, 1, 91, 86, 81, 55, 234, 7, 92, 62])
d3: dict_keys([880, 81, 1, 86, 55, 7, 234, 91, 92, 62])
Adding items to a dict may change the order of existing keys
Whenever you add a new item to a dict, the Python interpreter may decide that the
hash table of that dictionary needs to grow. This entails building a new, bigger hash
table, and adding all current items to the new table. During this process, new (but
different) hash collisions may happen, with the result that the keys are likely to be ordered
differently in the new hash table. All of this is implementation-dependent, so you
cannot reliably predict when it will happen. If you are iterating over the dictionary keys
and changing them at the same time, your loop may not scan all the items as expected
�not even the items that were already in the dictionary before you added to it.
This is why modifying the contents of a dict while iterating through it is a bad idea. If
you need to scan and add items to a dictionary, do it in two steps: read the dict from
start to finish and collect the needed additions in a second dict. Then update the first
one with it.
In Python 3, the .keys(), .items(), and .values() methods return
dictionary views, which behave more like sets than the lists
returned by these methods in Python 2. Such views are also dynamic:
they do not replicate the contents of the dict, and they
immediately reflect any changes to the dict.
We can now apply what we know about hash tables to sets.
92 | Chapter 3: Dictionaries and Sets
How Sets Work�Practical Consequences
The set and frozenset types are also implemented with a hash table, except that each
bucket holds only a reference to the element (as if it were a key in a dict, but without
a value to go with it). In fact, before set was added to the language, we often used
dictionaries with dummy values just to perform fast membership tests on the keys.
Everything said in �Practical Consequences of How dict Works� on page 90 about how
the underlying hash table determines the behavior of a dict applies to a set. Without
repeating the previous section, we can summarize it for sets with just a few words:
� Set elements must be hashable objects.
� Sets have a significant memory overhead.
� Membership testing is very efficient.
� Element ordering depends on insertion order.
� Adding elements to a set may change the order of other elements.
Chapter Summary
Dictionaries are a keystone of Python. Beyond the basic dict, the standard library offers
handy, ready-to-use specialized mappings like defaultdict, OrderedDict, ChainMap,
and Counter, all defined in the collections module. The same module also provides
the easy-to-extend UserDict class.
Two powerful methods available in most mappings are setdefault and update. The
setdefault method is used to update items holding mutable values, for example, in a
dict of list values, to avoid redundant searches for the same key. The update method
allows bulk insertion or overwriting of items from any other mapping, from iterables
providing (key, value) pairs and from keyword arguments. Mapping constructors
also use update internally, allowing instances to be initialized from mappings, iterables,
or keyword arguments.
A clever hook in the mapping API is the __missing__ method, which lets you customize
what happens when a key is not found.
The collections.abc module provides the Mapping and MutableMapping abstract base
classes for reference and type checking. The little-known MappingProxyType from the
types module creates immutable mappings. There are also ABCs for Set and Mutable
Set.
Chapter Summary | 93
The hash table implementation underlying dict and set is extremely fast. Understanding
its logic explains why items are apparently unordered and may even be reordered
behind our backs. There is a price to pay for all this speed, and the price is in memory.
Further Reading
In The Python Standard Library, 8.3. collections � Container datatypes includes examples
and practical recipes with several mapping types. The Python source code for
the module Lib/collections/init.py is a great reference for anyone who wants to create a
new mapping type or grok the logic of the existing ones.
Chapter 1 of Python Cookbook, Third edition (O�Reilly) by David Beazley and Brian K.
Jones has 20 handy and insightful recipes with data structures�the majority using dict
in clever ways.
Written by A.M. Kuchling�a Python core contributor and author of many pages of the
official Python docs and how-tos�Chapter 18, �Python�s Dictionary Implementation:
Being All Things to All People, in the book Beautiful Code (O�Reilly) includes a detailed
explanation of the inner workings of the Python dict. Also, there are lots of comments
in the source code of the dictobject.cCPython module. Brandon Craig Rhodes� presentation
The Mighty Dictionary is excellent and shows how hash tables work by using
lots of slides with� tables!
The rationale for adding sets to the language is documented in PEP 218 � Adding a
Built-In Set Object Type. When PEP 218 was approved, no special literal syntax was
adopted for sets. The set literals were created for Python 3 and backported to Python
2.7, along with dict and set comprehensions. PEP 274 � Dict Comprehensions is the
birth certificate of dictcomps. I could not find a PEP for setcomps; apparently they were
adopted because they get along well with their siblings�a jolly good reason.
Soapbox
My friend Geraldo Cohen once remarked that Python is �simple and correct.�
The dict type is an example of simplicity and correctness. It�s highly optimized to do
one thing well: retrieve arbitrary keys. It�s fast and robust enough to be used all over the
Python interpreter itself. If you need predictable ordering, use OrderedDict. That is not
a requirement in most uses of mappings, so it makes sense to keep the core implementation
simple and offer variations in the standard library.
Contrast with PHP, where arrays are described like this in the official PHP Manual:
An array in PHP is actually an ordered map. A map is a type that associates values to
keys. This type is optimized for several different uses; it can be treated as an array, list
94 | Chapter 3: Dictionaries and Sets
(vector), hash table (an implementation of a map), dictionary, collection, stack, queue,
and probably more.
From that description, I don�t know what is the real cost of using PHP�s list/Ordered
Dict hybrid.
The goal of this and the previous chapter in this book was to showcase the Python
collection types optimized for particular uses. I made the point that beyond the trusty
list and dict there are specialized alternatives for different use cases.
Before finding Python, I had done web programming using Perl, PHP, and JavaScript.
I really enjoyed having a literal syntax for mappings in these languages, and I badly miss
it whenever I have to use Java or C. A good literal syntax for mappings makes it easy to
do configuration, table-driven implementations, and to hold data for prototyping and
testing. The lack of it pushed the Java community to adopt the verbose and overly complex
XML as a data format.
JSON was proposed as �The Fat-Free Alternative to XML� and became a huge success,
replacing XML in many contexts. A concise syntax for lists and dictionaries makes an
excellent data interchange format.
PHP and Ruby imitated the hash syntax from Perl, using => to link keys to values.
JavaScript followed the lead of Python and uses :. Of course, JSON came from JavaScript,
but it also happens to be an almost exact subset of Python syntax. JSON is compatible
with Python except for the spelling of the values true, false, and null. The syntax
everybody now uses for exchanging data is the Python dict and list syntax.
Simple and correct.
Further Reading | 95

1. Slide 12 of PyCon 2014 talk �Character Encoding and Unicode in Python� (slides, video).
CHAPTER 4
Text versus Bytes
Humans use text. Computers speak bytes.1
� Esther Nam and Travis Fischer
Character Encoding and Unicode in Python
Python 3 introduced a sharp distinction between strings of human text and sequences
of raw bytes. Implicit conversion of byte sequences to Unicode text is a thing of the past.
This chapter deals with Unicode strings, binary sequences, and the encodings used to
convert between them.
Depending on your Python programming context, a deeper understanding of Unicode
may or may not be of vital importance to you. In the end, most of the issues covered in
this chapter do not affect programmers who deal only with ASCII text. But even if that
is your case, there is no escaping the str versus byte divide. As a bonus, you�ll find that
the specialized binary sequence types provide features that the �all-purpose� Python 2
str type does not have.
In this chapter, we will visit the following topics:
� Characters, code points, and byte representations
� Unique features of binary sequences: bytes, bytearray, and memoryview
� Codecs for full Unicode and legacy character sets
� Avoiding and dealing with encoding errors
� Best practices when handling text files
� The default encoding trap and standard I/O issues
� Safe Unicode text comparisons with normalization
97
� Utility functions for normalization, case folding, and brute-force diacritic removal
� Proper sorting of Unicode text with locale and the PyUCA library
� Character metadata in the Unicode database
� Dual-mode APIs that handle str and bytes
Let�s start with the characters, code points, and bytes.
Character Issues
The concept of �string� is simple enough: a string is a sequence of characters. The problem
lies in the definition of �character.�
In 2015, the best definition of �character� we have is a Unicode character. Accordingly,
the items you get out of a Python 3 str are Unicode characters, just like the items of a
unicode object in Python 2�and not the raw bytes you get from a Python 2 str.
The Unicode standard explicitly separates the identity of characters from specific byte
representations:
� The identity of a character�its code point�is a number from 0 to 1,114,111 (base
10), shown in the Unicode standard as 4 to 6 hexadecimal digits with a �U+� prefix.
For example, the code point for the letter A is U+0041, the Euro sign is U+20AC,
and the musical symbol G clef is assigned to code point U+1D11E. About 10% of
the valid code points have characters assigned to them in Unicode 6.3, the standard
used in Python 3.4.
� The actual bytes that represent a character depend on the encoding in use. An encoding
is an algorithm that converts code points to byte sequences and vice versa.
The code point for A (U+0041) is encoded as the single byte \x41 in the UTF-8
encoding, or as the bytes \x41\x00 in UTF-16LE encoding. As another example,
the Euro sign (U+20AC) becomes three bytes in UTF-8�\xe2\x82\xac�but in
UTF-16LE it is encoded as two bytes: \xac\x20.
Converting from code points to bytes is encoding; converting from bytes to code points
is decoding. See Example 4-1.
Example 4-1. Encoding and decoding
>>> s = 'cafe'
>>> len(s) #
4
>>> b = s.encode('utf8') #
>>> b
b'caf\xc3\xa9' #
>>> len(b) #
5
98 | Chapter 4: Text versus Bytes
>>> b.decode('utf8') #
'cafe'
The str 'cafe' has four Unicode characters.
Encode str to bytes using UTF-8 encoding.
bytes literals start with a b prefix.
bytes b has five bytes (the code point for �e� is encoded as two bytes in UTF-8).
Decode bytes to str using UTF-8 encoding.
If you need a memory aid to help distinguish .decode()
from .encode(), convince yourself that byte sequences can be
cryptic machine core dumps while Unicode str objects are �human�
text. Therefore, it makes sense that we decode bytes to str
to get human-readable text, and we encode str to bytes for storage
or transmission.
Although the Python 3 str is pretty much the Python 2 unicode type with a new name,
the Python 3 bytes is not simply the old str renamed, and there is also the closely
related bytearray type. So it is worthwhile to take a look at the binary sequence types
before advancing to encoding/decoding issues.
Byte Essentials
The new binary sequence types are unlike the Python 2 str in many regards. The first
thing to know is that there are two basic built-in types for binary sequences: the immutable
bytes type introduced in Python 3 and the mutable bytearray, added in
Python 2.6. (Python 2.6 also introduced bytes, but it�s just an alias to the str type, and
does not behave like the Python 3 bytes type.)
Each item in bytes or bytearray is an integer from 0 to 255, and not a one-character
string like in the Python 2 str. However, a slice of a binary sequence always produces
a binary sequence of the same type�including slices of length 1. See Example 4-2.
Example 4-2. A five-byte sequence as bytes and as bytearray
>>> cafe = bytes('cafe', encoding='utf_8')
>>> cafe
b'caf\xc3\xa9'
>>> cafe[0]
99
>>> cafe[:1]
b'c'
>>> cafe_arr = bytearray(cafe)
>>> cafe_arr
Byte Essentials | 99
bytearray(b'caf\xc3\xa9')
>>> cafe_arr[-1:]
bytearray(b'\xa9')
bytes can be built from a str, given an encoding.
Each item is an integer in range(256).
Slices of bytes are also bytes�even slices of a single byte.
There is no literal syntax for bytearray: they are shown as bytearray() with a
bytes literal as argument.
A slice of bytearray is also a bytearray.
The fact that my_bytes[0] retrieves an int but my_bytes[:1]
returns a bytes object of length 1 should not be surprising. The
only sequence type where s[0] == s[:1] is the str type. Although
practical, this behavior of str is exceptional. For every
other sequence, s[i] returns one item, and s[i:i+1] returns a
sequence of the same type with the s[1] item inside it.
Although binary sequences are really sequences of integers, their literal notation reflects
the fact that ASCII text is often embedded in them. Therefore, three different displays
are used, depending on each byte value:
� For bytes in the printable ASCII range�from space to ~�the ASCII character itself
is used.
� For bytes corresponding to tab, newline, carriage return, and \, the escape sequences
\t, \n, \r, and \\ are used.
� For every other byte value, a hexadecimal escape sequence is used (e.g., \x00 is the
null byte).
That is why in Example 4-2 you see b'caf\xc3\xa9': the first three bytes b'caf' are in
the printable ASCII range, the last two are not.
Both bytes and bytearray support every str method except those that do formatting
(format, format_map) and a few others that depend on Unicode data, including case
fold, isdecimal, isidentifier, isnumeric, isprintable, and encode. This means that
you can use familiar string methods like endswith, replace, strip, translate, upper,
and dozens of others with binary sequences�only using bytes and not str arguments.
In addition, the regular expression functions in the re module also work on binary
sequences, if the regex is compiled from a binary sequence instead of a str. The %
operator does not work with binary sequences in Python 3.0 to 3.4, but should be sup?
100 | Chapter 4: Text versus Bytes
ported in version 3.5 according to PEP 461 � Adding % formatting to bytes and bytearray.
Binary sequences have a class method that str doesn�t have, called fromhex, which builds
a binary sequence by parsing pairs of hex digits optionally separated by spaces:
>>> bytes.fromhex('31 4B CE A9')
b'1K\xce\xa9'
The other ways of building bytes or bytearray instances are calling their constructors
with:
� A str and an encoding keyword argument.
� An iterable providing items with values from 0 to 255.
� A single integer, to create a binary sequence of that size initialized with null bytes.
(This signature will be deprecated in Python 3.5 and removed in Python 3.6. See
PEP 467 � Minor API improvements for binary sequences.)
� An object that implements the buffer protocol (e.g., bytes, bytearray, memory
view, array.array); this copies the bytes from the source object to the newly created
binary sequence.
Building a binary sequence from a buffer-like object is a low-level operation that may
involve type casting. See a demonstration in Example 4-3.
Example 4-3. Initializing bytes from the raw data of an array
>>> import array
>>> numbers = array.array('h', [-2, -1, 0, 1, 2])
>>> octets = bytes(numbers)
>>> octets
b'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00'
Typecode 'h' creates an array of short integers (16 bits).
octets holds a copy of the bytes that make up numbers.
These are the 10 bytes that represent the five short integers.
Creating a bytes or bytearray object from any buffer-like source will always copy the
bytes. In contrast, memoryview objects let you share memory between binary data structures.
To extract structured information from binary sequences, the struct module is
invaluable. We�ll see it working along with bytes and memoryview in the next section.
Byte Essentials | 101
2. Pillow is PIL�s most active fork.
Structs and Memory Views
The struct module provides functions to parse packed bytes into a tuple of fields of
different types and to perform the opposite conversion, from a tuple into packed bytes.
struct is used with bytes, bytearray, and memoryview objects.
As we�ve seen in �Memory Views� on page 51, the memoryview class does not let you
create or store byte sequences, but provides shared memory access to slices of data from
other binary sequences, packed arrays, and buffers such as Python Imaging Library
(PIL) images,2 without copying the bytes.
Example 4-4 shows the use of memoryview and struct together to extract the width and
height of a GIF image.
Example 4-4. Using memoryview and struct to inspect a GIF image header
>>> import struct
>>> fmt = '<3s3sHH' #
>>> with open('filter.gif', 'rb') as fp:
... img = memoryview(fp.read()) #
...
>>> header = img[:10] #
>>> bytes(header) #
b'GIF89a+\x02\xe6\x00'
>>> struct.unpack(fmt, header) #
(b'GIF', b'89a', 555, 230)
>>> del header #
>>> del img
struct format: < little-endian; 3s3s two sequences of 3 bytes; HH two 16-bit
integers.
Create memoryview from file contents in memory�
�then another memoryview by slicing the first one; no bytes are copied here.
Convert to bytes for display only; 10 bytes are copied here.
Unpack memoryview into tuple of: type, version, width, and height.
Delete references to release the memory associated with the memoryview
instances.
Note that slicing a memoryview returns a new memoryview, without copying bytes (Leonardo
Rochael�one of the technical reviewers�pointed out that even less byte copying
would happen if I used the mmap module to open the image as a memory-mapped file.
102 | Chapter 4: Text versus Bytes
I will not cover mmap in this book, but if you read and change binary files frequently,
learning more about mmap � Memory-mapped file support will be very fruitful).
We will not go deeper into memoryview or the struct module in this book, but if you
work with binary data, you�ll find it worthwhile to study their docs: Built-in Types �
Memory Views and struct � Interpret bytes as packed binary data.
After this brief exploration of binary sequence types in Python, let�s see how they are
converted to/from strings.
Basic Encoders/Decoders
The Python distribution bundles more than 100 codecs (encoder/decoder) for text to
byte conversion and vice versa. Each codec has a name, like 'utf_8', and often aliases,
such as 'utf8', 'utf-8', and 'U8', which you can use as the encoding argument in
functions like open(), str.encode(), bytes.decode(), and so on. Example 4-5 shows
the same text encoded as three different byte sequences.
Example 4-5. The string �El Nino� encoded with three codecs producing very different
byte sequences
>>> for codec in ['latin_1', 'utf_8', 'utf_16']:
... print(codec, 'El Nino'.encode(codec), sep='\t')
...
latin_1 b'El Ni\xf1o'
utf_8 b'El Ni\xc3\xb1o'
utf_16 b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'
Figure 4-1 demonstrates a variety of codecs generating bytes from characters like the
letter �A� through the G-clef musical symbol. Note that the last three encodings are
variable-length, multibyte encodings.
Basic Encoders/Decoders | 103
Figure 4-1. Twelve characters, their code points, and their byte representation (in hex)
in seven different encodings (asterisks indicate that the character cannot be represented
in that encoding)
All those asterisks in Figure 4-1 make clear that some encodings, like ASCII and even
the multibyte GB2312, cannot represent every Unicode character. The UTF encodings,
however, are designed to handle every Unicode code point.
The encodings shown in Figure 4-1 were chosen as a representative sample:
latin1 a.k.a. iso8859_1
Important because it is the basis for other encodings, such as cp1252 and Unicode
itself (note how the latin1 byte values appear in the cp1252 bytes and even in the
code points).
cp1252
A latin1 superset by Microsoft, adding useful symbols like curly quotes and the �
(euro); some Windows apps call it �ANSI,� but it was never a real ANSI standard.
cp437
The original character set of the IBM PC, with box drawing characters. Incompatible
with latin1, which appeared later.
gb2312
Legacy standard to encode the simplified Chinese ideographs used in mainland
China; one of several widely deployed multibyte encodings for Asian languages.
104 | Chapter 4: Text versus Bytes
3. As of September, 2014, W3Techs: Usage of Character Encodings for Websites claims that 81.4% of sites use
UTF-8, while Built With: Encoding Usage Statistics estimates 79.4%.
utf-8
The most common 8-bit encoding on the Web, by far;3 backward-compatible with
ASCII (pure ASCII text is valid UTF-8).
utf-16le
One form of the UTF-16 16-bit encoding scheme; all UTF-16 encodings support
code points beyond U+FFFF through escape sequences called �surrogate pairs.�
UTF-16 superseded the original 16-bit Unicode 1.0 encoding�
UCS-2�way back in 1996. UCS-2 is still deployed in many systems,
but it only supports code points up to U+FFFF. As of Unicode
6.3, more than 50% of the allocated code points are above U
+10000, including the increasingly popular emoji pictographs.
With this overview of common encodings now complete, we move to handling issues
in encoding and decoding operations.
Understanding Encode/Decode Problems
Although there is a generic UnicodeError exception, the error reported is almost always
more specific: either a UnicodeEncodeError (when converting str to binary sequences)
or a UnicodeDecodeError (when reading binary sequences into str). Loading Python
modules may also generate a SyntaxError when the source encoding is unexpected.
We�ll show how to handle all of these errors in the next sections.
The first thing to note when you get a Unicode error is the exact
type of the exception. Is it a UnicodeEncodeError, a UnicodeDeco
deError, or some other error (e.g., SyntaxError) that mentions an
encoding problem? To solve the problem, you have to understand
it first.
Coping with UnicodeEncodeError
Most non-UTF codecs handle only a small subset of the Unicode characters. When
converting text to bytes, if a character is not defined in the target encoding, UnicodeEn
codeError will be raised, unless special handling is provided by passing an errors
argument to the encoding method or function. The behavior of the error handlers is
shown in Example 4-6.
Understanding Encode/Decode Problems | 105
Example 4-6. Encoding to bytes: success and error handling
>>> city = 'Sao Paulo'
>>> city.encode('utf_8')
b'S\xc3\xa3o Paulo'
>>> city.encode('utf_16')
b'\xff\xfeS\x00\xe3\x00o\x00 \x00P\x00a\x00u\x00l\x00o\x00'
>>> city.encode('iso8859_1')
b'S\xe3o Paulo'
>>> city.encode('cp437')
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "/.../lib/python3.4/encodings/cp437.py", line 12, in encode
return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character '\xe3' in
position 1: character maps to <undefined>
>>> city.encode('cp437', errors='ignore')
b'So Paulo'
>>> city.encode('cp437', errors='replace')
b'S?o Paulo'
>>> city.encode('cp437', errors='xmlcharrefreplace')
b'S&#227;o Paulo'
The 'utf_?' encodings handle any str.
'iso8859_1' also works for the 'Sao Paulo' str.
'cp437' can�t encode the 'a' (�a� with tilde). The default error handler
�'strict'�raises UnicodeEncodeError.
The error='ignore' handler silently skips characters that cannot be encoded;
this is usually a very bad idea.
When encoding, error='replace' substitutes unencodable characters with '?';
data is lost, but users will know something is amiss.
'xmlcharrefreplace' replaces unencodable characters with an XML entity.
The codecs error handling is extensible. You may register extra
strings for the errors argument by passing a name and an error
handling function to the codecs.register_error function. See
the codecs.register_error documentation.
Coping with UnicodeDecodeError
Not every byte holds a valid ASCII character, and not every byte sequence is valid UTF-8
or UTF-16; therefore, when you assume one of these encodings while converting a
binary sequence to text, you will get a UnicodeDecodeError if unexpected bytes are
found.
106 | Chapter 4: Text versus Bytes
On the other hand, many legacy 8-bit encodings like 'cp1252', 'iso8859_1', and
'koi8_r' are able to decode any stream of bytes, including random noise, without
generating errors. Therefore, if your program assumes the wrong 8-bit encoding, it will
silently decode garbage.
Garbled characters are known as gremlins or mojibake (????
�Japanese for �transformed text�).
Example 4-7 illustrates how using the wrong codec may produce gremlins or a Unico
deDecodeError.
Example 4-7. Decoding from str to bytes: success and error handling
>>> octets = b'Montr\xe9al'
>>> octets.decode('cp1252')
'Montreal'
>>> octets.decode('iso8859_7')
'Montr?al'
>>> octets.decode('koi8_r')
'Montr�al'
>>> octets.decode('utf_8')
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 5:
invalid continuation byte
>>> octets.decode('utf_8', errors='replace')
'Montr%al'
These bytes are the characters for �Montreal� encoded as latin1; '\xe9' is the
byte for �e�.
Decoding with 'cp1252' (Windows 1252) works because it is a proper superset
of latin1.
ISO-8859-7 is intended for Greek, so the '\xe9' byte is misinterpreted, and no
error is issued.
KOI8-R is for Russian. Now '\xe9' stands for the Cyrillic letter �Ȕ.
The 'utf_8' codec detects that octets is not valid UTF-8, and raises Unicode
DecodeError.
Using 'replace' error handling, the \xe9 is replaced by �??� (code point U
+FFFD), the official Unicode REPLACEMENT CHARACTER intended to represent
unknown characters.
Understanding Encode/Decode Problems | 107
SyntaxError When Loading Modules with Unexpected Encoding
UTF-8 is the default source encoding for Python 3, just as ASCII was the default for
Python 2 (starting with 2.5). If you load a .py module containing non-UTF-8 data and
no encoding declaration, you get a message like this:
SyntaxError: Non-UTF-8 code starting with '\xe1' in file ola.py on line
1, but no encoding declared; see http://python.org/dev/peps/pep-0263/
for details
Because UTF-8 is widely deployed in GNU/Linux and OSX systems, a likely scenario
is opening a .py file created on Windows with cp1252. Note that this error happens even
in Python for Windows, because the default encoding for Python 3 is UTF-8 across all
platforms.
To fix this problem, add a magic coding comment at the top of the file, as shown in
Example 4-8.
Example 4-8. ola.py: �Hello, World!� in Portuguese
# coding: cp1252
print('Ola, Mundo!')
Now that Python 3 source code is no longer limited to ASCII and
defaults to the excellent UTF-8 encoding, the best �fix� for source
code in legacy encodings like 'cp1252' is to convert them to UTF-8
already, and not bother with the coding comments. If your editor
does not support UTF-8, it�s time to switch.
Non-ASCII Names in Source Code: Should You Use Them?
Python 3 allows non-ASCII identifiers in source code:
>>> acao = 'PBR' # acao = stock
>>> ? = 10**-6 # ? = epsilon
Some people dislike the idea. The most common argument to stick with ASCII identifiers
is to make it easy for everyone to read and edit code. That argument misses the
point: you want your source code to be readable and editable by its intended audience,
and that may not be �everyone.� If the code belongs to a multinational corporation or is
open source and you want contributors from around the world, the identifiers should
be in English, and then all you need is ASCII.
But if you are a teacher in Brazil, your students will find it easier to read code that uses
Portuguese variable and function names, correctly spelled. And they will have no difficulty
typing the cedillas and accented vowels on their localized keyboards.
108 | Chapter 4: Text versus Bytes
Now that Python can parse Unicode names and UTF-8 is the default source encoding,
I see no point in coding identifiers in Portuguese without accents, as we used to do in
Python 2 out of necessity�unless you need the code to run on Python 2 also. If the
names are in Portuguese, leaving out the accents won�t make the code more readable to
anyone.
This is my point of view as a Portuguese-speaking Brazilian, but I believe it applies across
borders and cultures: choose the human language that makes the code easier to read by
the team, then use the characters needed for correct spelling.
Suppose you have a text file, be it source code or poetry, but you don�t know its encoding.
How do you detect the actual encoding? The next section answers that with a library
recommendation.
How to Discover the Encoding of a Byte Sequence
How do you find the encoding of a byte sequence? Short answer: you can�t. You must
be told.
Some communication protocols and file formats, like HTTP and XML, contain headers
that explicitly tell us how the content is encoded. You can be sure that some byte streams
are not ASCII because they contain byte values over 127, and the way UTF-8 and UTF-16
are built also limits the possible byte sequences. But even then, you can never be 100%
positive that a binary file is ASCII or UTF-8 just because certain bit patterns are not
there.
However, considering that human languages also have their rules and restrictions, once
you assume that a stream of bytes is human plain text it may be possible to sniff out its
encoding using heuristics and statistics. For example, if b'\x00' bytes are common, it
is probably a 16- or 32-bit encoding, and not an 8-bit scheme, because null characters
in plain text are bugs; when the byte sequence b'\x20\x00' appears often, it is likely to
be the space character (U+0020) in a UTF-16LE encoding, rather than the obscure U
+2000 EN QUAD character�whatever that is.
That is how the package Chardet � The Universal Character Encoding Detector works
to identify one of 30 supported encodings. Chardet is a Python library that you can use
in your programs, but also includes a command-line utility, chardetect. Here is what
it reports on the source file for this chapter:
$ chardetect 04-text-byte.asciidoc
04-text-byte.asciidoc: utf-8 with confidence 0.99
Although binary sequences of encoded text usually don�t carry explicit hints of their
encoding, the UTF formats may prepend a byte order mark to the textual content. That
is explained next.
Understanding Encode/Decode Problems | 109
BOM: A Useful Gremlin
In Example 4-5, you may have noticed a couple of extra bytes at the beginning of a
UTF-16 encoded sequence. Here they are again:
>>> u16 = 'El Nino'.encode('utf_16')
>>> u16
b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'
The bytes are b'\xff\xfe'. That is a BOM�byte-order mark�denoting the �littleendian�
byte ordering of the Intel CPU where the encoding was performed.
On a little-endian machine, for each code point the least significant byte comes first:
the letter 'E', code point U+0045 (decimal 69), is encoded in byte offsets 2 and 3 as 69
and 0:
>>> list(u16)
[255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]
On a big-endian CPU, the encoding would be reversed; 'E' would be encoded as 0 and
69.
To avoid confusion, the UTF-16 encoding prepends the text to be encoded with the
special character ZERO WIDTH NO-BREAK SPACE (U+FEFF), which is invisible. On a littleendian
system, that is encoded as b'\xff\xfe' (decimal 255, 254). Because, by design,
there is no U+FFFE character, the byte sequence b'\xff\xfe' must mean the ZERO
WIDTH NO-BREAK SPACE on a little-endian encoding, so the codec knows which byte
ordering to use.
There is a variant of UTF-16�UTF-16LE�that is explicitly little-endian, and another
one explicitly big-endian, UTF-16BE. If you use them, a BOM is not generated:
>>> u16le = 'El Nino'.encode('utf_16le')
>>> list(u16le)
[69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]
>>> u16be = 'El Nino'.encode('utf_16be')
>>> list(u16be)
[0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]
If present, the BOM is supposed to be filtered by the UTF-16 codec, so that you only
get the actual text contents of the file without the leading ZERO WIDTH NO-BREAK
SPACE. The standard says that if a file is UTF-16 and has no BOM, it should be assumed
to be UTF-16BE (big-endian). However, the Intel x86 architecture is little-endian, so
there is plenty of little-endian UTF-16 with no BOM in the wild.
This whole issue of endianness only affects encodings that use words of more than one
byte, like UTF-16 and UTF-32. One big advantage of UTF-8 is that it produces the same
byte sequence regardless of machine endianness, so no BOM is needed. Nevertheless,
some Windows applications (notably Notepad) add the BOM to UTF-8 files anyway�
and Excel depends on the BOM to detect a UTF-8 file, otherwise it assumes the content
110 | Chapter 4: Text versus Bytes
4. I first saw the term �Unicode sandwich� in Ned Batchelder�s excellent �Pragmatic Unicode� talk at US PyCon
2012.
5. Python 2.6 or 2.7 users have to use io.open() to get automatic decoding/encoding when reading/writing.
is encoded with a Windows codepage. The character U+FEFF encoded in UTF-8 is the
three-byte sequence b'\xef\xbb\xbf'. So if a file starts with those three bytes, it is likely
to be a UTF-8 file with a BOM. However, Python does not automatically assume a file
is UTF-8 just because it starts with b'\xef\xbb\xbf'.
We now move on to handling text files in Python 3.
Handling Text Files
The best practice for handling text is the �Unicode sandwich� (Figure 4-2).4 This means
that bytes should be decoded to str as early as possible on input (e.g., when opening
a file for reading). The �meat� of the sandwich is the business logic of your program,
where text handling is done exclusively on str objects. You should never be encoding
or decoding in the middle of other processing. On output, the str are encoded to bytes
as late as possible. Most web frameworks work like that, and we rarely touch bytes when
using them. In Django, for example, your views should output Unicode str; Django
itself takes care of encoding the response to bytes, using UTF-8 by default.
Figure 4-2. Unicode sandwich: current best practice for text processing
Python 3 makes it easier to follow the advice of the Unicode sandwich, because the open
built-in does the necessary decoding when reading and encoding when writing files in
text mode, so all you get from my_file.read() and pass to my_file.write(text) are
str objects.5
Handling Text Files | 111
Therefore, using text files is simple. But if you rely on default encodings you will get
bitten.
Consider the console session in Example 4-9. Can you spot the bug?
Example 4-9. A platform encoding issue (if you try this on your machine, you may or
may not see the problem)
>>> open('cafe.txt', 'w', encoding='utf_8').write('cafe')
4
>>> open('cafe.txt').read()
'cafA�'
The bug: I specified UTF-8 encoding when writing the file but failed to do so when
reading it, so Python assumed the system default encoding�Windows 1252�and the
trailing bytes in the file were decoded as characters 'A�' instead of 'e'.
I ran Example 4-9 on a Windows 7 machine. The same statements running on recent
GNU/Linux or Mac OSX work perfectly well because their default encoding is UTF-8,
giving the false impression that everything is fine. If the encoding argument was omitted
when opening the file to write, the locale default encoding would be used, and we�d read
the file correctly using the same encoding. But then this script would generate files with
different byte contents depending on the platform or even depending on locale settings
in the same platform, creating compatibility problems.
Code that has to run on multiple machines or on multiple occasions
should never depend on encoding defaults. Always pass an
explicit encoding= argument when opening text files, because the
default may change from one machine to the next, or from one day
to the next.
A curious detail in Example 4-9 is that the write function in the first statement reports
that four characters were written, but in the next line five characters are read.
Example 4-10 is an extended version of Example 4-9, explaining that and other details.
Example 4-10. Closer inspection of Example 4-9 running on Windows reveals the bug
and how to fix it
>>> fp = open('cafe.txt', 'w', encoding='utf_8')
>>> fp
<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'>
>>> fp.write('cafe')
4
>>> fp.close()
>>> import os
>>> os.stat('cafe.txt').st_size
5
>>> fp2 = open('cafe.txt')
112 | Chapter 4: Text versus Bytes
>>> fp2
<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'>
>>> fp2.encoding
'cp1252'
>>> fp2.read()
'cafA�'
>>> fp3 = open('cafe.txt', encoding='utf_8')
>>> fp3
<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'>
>>> fp3.read()
'cafe'
>>> fp4 = open('cafe.txt', 'rb')
>>> fp4
<_io.BufferedReader name='cafe.txt'>
>>> fp4.read()
b'caf\xc3\xa9'
By default, open operates in text mode and returns a TextIOWrapper object.
The write method on a TextIOWrapper returns the number of Unicode
characters written.
os.stat reports that the file holds 5 bytes; UTF-8 encodes 'e' as 2 bytes, 0xc3
and 0xa9.
Opening a text file with no explicit encoding returns a TextIOWrapper with the
encoding set to a default from the locale.
A TextIOWrapper object has an encoding attribute that you can inspect: cp1252
in this case.
In the Windows cp1252 encoding, the byte 0xc3 is an �A� (A with tilde) and 0xa9
is the copyright sign.
Opening the same file with the correct encoding.
The expected result: the same four Unicode characters for 'cafe'.
The 'rb' flag opens a file for reading in binary mode.
The returned object is a BufferedReader and not a TextIOWrapper.
Reading that returns bytes, as expected.
Do not open text files in binary mode unless you need to analyze
the file contents to determine the encoding�even then, you should
be using Chardet instead of reinventing the wheel (see �How to
Discover the Encoding of a Byte Sequence� on page 109). Ordinary
code should only use binary mode to open binary files, like
raster images.
Handling Text Files | 113
The problem in Example 4-10 has to do with relying on a default setting while opening
a text file. There are several sources for such defaults, as the next section shows.
Encoding Defaults: A Madhouse
Several settings affect the encoding defaults for I/O in Python. See the default_encodings.
py script in Example 4-11.
Example 4-11. Exploring encoding defaults
import sys, locale
expressions = """
locale.getpreferredencoding()
type(my_file)
my_file.encoding
sys.stdout.isatty()
sys.stdout.encoding
sys.stdin.isatty()
sys.stdin.encoding
sys.stderr.isatty()
sys.stderr.encoding
sys.getdefaultencoding()
sys.getfilesystemencoding()
"""
my_file = open('dummy', 'w')
for expression in expressions.split():
value = eval(expression)
print(expression.rjust(30), '->', repr(value))
The output of Example 4-11 on GNU/Linux (Ubuntu 14.04) and OSX (Mavericks 10.9)
is identical, showing that UTF-8 is used everywhere in these systems:
$ python3 default_encodings.py
locale.getpreferredencoding() -> 'UTF-8'
type(my_file) -> <class '_io.TextIOWrapper'>
my_file.encoding -> 'UTF-8'
sys.stdout.isatty() -> True
sys.stdout.encoding -> 'UTF-8'
sys.stdin.isatty() -> True
sys.stdin.encoding -> 'UTF-8'
sys.stderr.isatty() -> True
sys.stderr.encoding -> 'UTF-8'
sys.getdefaultencoding() -> 'utf-8'
sys.getfilesystemencoding() -> 'utf-8'
On Windows, however, the output is Example 4-12.
114 | Chapter 4: Text versus Bytes
Example 4-12. Default encodings on Windows 7 (SP 1) cmd.exe localized for Brazil;
PowerShell gives same result
Z:\>chcp
Pagina de codigo ativa: 850
Z:\>python default_encodings.py
locale.getpreferredencoding() -> 'cp1252'
type(my_file) -> <class '_io.TextIOWrapper'>
my_file.encoding -> 'cp1252'
sys.stdout.isatty() -> True
sys.stdout.encoding -> 'cp850'
sys.stdin.isatty() -> True
sys.stdin.encoding -> 'cp850'
sys.stderr.isatty() -> True
sys.stderr.encoding -> 'cp850'
sys.getdefaultencoding() -> 'utf-8'
sys.getfilesystemencoding() -> 'mbcs'
chcp shows the active codepage for the console: 850.
Running default_encodings.py with output to console.
locale.getpreferredencoding() is the most important setting.
Text files use locale.getpreferredencoding() by default.
The output is going to the console, so sys.stdout.isatty() is True.
Therefore, sys.stdout.encoding is the same as the console encoding.
If the output is redirected to a file, like this:
Z:\>python default_encodings.py > encodings.log
The value of sys.stdout.isatty() becomes False, and sys.stdout.encoding is set
by locale.getpreferredencoding(), 'cp1252' in that machine.
Note that there are four different encodings in Example 4-12:
� If you omit the encoding argument when opening a file, the default is given by
locale.getpreferredencoding() ('cp1252' in Example 4-12).
� The encoding of sys.stdout/stdin/stderr is given by the PYTHONIOENCODING
environment variable, if present, otherwise it is either inherited from the console
or defined by locale.getpreferredencoding() if the output/input is redirected
to/from a file.
Handling Text Files | 115
6. While researching this subject, I did not find a list of situations when Python 3 internally converts bytes to
str. Python core developer Antoine Pitrou says on the comp.python.devel list that CPython internal functions
that depend on such conversions �don�t get a lot of use in py3k.�
7. The Python 2 sys.setdefaultencoding function was misused and is no longer documented in Python 3.
It was intended for use by the core developers when the internal default encoding of Python was still undecided.
In the same comp.python.devel thread, Marc-Andre Lemburg states that the sys.setdefaulten
coding must never be called by user code and the only values supported by CPython are 'ascii' in Python
2 and 'utf-8' in Python 3.
� sys.getdefaultencoding() is used internally by Python to convert binary data to/
from str; this happens less often in Python 3, but still happens.6 Changing this
setting is not supported.7
� sys.getfilesystemencoding() is used to encode/decode filenames (not file contents).
It is used when open() gets a str argument for the filename; if the filename
is given as a bytes argument, it is passed unchanged to the OS API. The Python
Unicode HOWTO says: �on Windows, Python uses the name mbcs to refer to whatever
the currently configured encoding is.� The acronym MBCS stands for Multi
Byte Character Set, which for Microsoft are the legacy variable-width encodings
like gb2312 or Shift_JIS, but not UTF-8. (On this topic, a useful answer on Stack?
Overflow is �Difference between MBCS and UTF-8 on Windows�.)
On GNU/Linux and OSX all of these encodings are set to UTF-8
by default, and have been for several years, so I/O handles all
Unicode characters. On Windows, not only are different encodings
used in the same system, but they are usually codepages like
'cp850' or 'cp1252' that support only ASCII with 127 additional
characters that are not the same from one encoding to the other.
Therefore, Windows users are far more likely to face encoding
errors unless they are extra careful.
To summarize, the most important encoding setting is that returned by locale.get
preferredencoding(): it is the default for opening text files and for sys.stdout/stdin/
stderr when they are redirected to files. However, the documentation reads (in part):
locale.getpreferredencoding(do_setlocale=True)
Return the encoding used for text data, according to user preferences. User preferences
are expressed differently on different systems, and might not be available programmatically
on some systems, so this function only returns a guess. [�]
Therefore, the best advice about encoding defaults is: do not rely on them.
If you follow the advice of the Unicode sandwich and always are explicit about the
encodings in your programs, you will avoid a lot of pain. Unfortunately, Unicode is
116 | Chapter 4: Text versus Bytes
painful even if you get your bytes correctly converted to str. The next two sections
cover subjects that are simple in ASCII-land, but get quite complex on planet Unicode:
text normalization (i.e., converting text to a uniform representation for comparisons)
and sorting.
Normalizing Unicode for Saner Comparisons
String comparisons are complicated by the fact that Unicode has combining characters:
diacritics and other marks that attach to the preceding character, appearing as one when
printed.
For example, the word �cafe� may be composed in two ways, using four or five code
points, but the result looks exactly the same:
>>> s1 = 'cafe'
>>> s2 = 'cafe\u0301'
>>> s1, s2
('cafe', 'cafe')
>>> len(s1), len(s2)
(4, 5)
>>> s1 == s2
False
The code point U+0301 is the COMBINING ACUTE ACCENT. Using it after �e� renders �e�.
In the Unicode standard, sequences like 'e' and 'e\u0301' are called �canonical equivalents,�
and applications are supposed to treat them as the same. But Python sees two
different sequences of code points, and considers them not equal.
The solution is to use Unicode normalization, provided by the unicodedata.normal
ize function. The first argument to that function is one of four strings: 'NFC', 'NFD',
'NFKC', and 'NFKD'. Let�s start with the first two.
Normalization Form C (NFC) composes the code points to produce the shortest equivalent
string, while NFD decomposes, expanding composed characters into base characters
and separate combining characters. Both of these normalizations make comparisons
work as expected:
>>> from unicodedata import normalize
>>> s1 = 'cafe' # composed "e" with acute accent
>>> s2 = 'cafe\u0301' # decomposed "e" and acute accent
>>> len(s1), len(s2)
(4, 5)
>>> len(normalize('NFC', s1)), len(normalize('NFC', s2))
(4, 4)
>>> len(normalize('NFD', s1)), len(normalize('NFD', s2))
(5, 5)
>>> normalize('NFC', s1) == normalize('NFC', s2)
True
Normalizing Unicode for Saner Comparisons | 117
8. Curiously, the micro sign is considered a �compatibility character� but the ohm symbol is not. The end result
is that NFC doesn�t touch the micro sign but changes the ohm symbol to capital omega, while NFKC and
NFKD change both the ohm and the micro into other characters.
>>> normalize('NFD', s1) == normalize('NFD', s2)
True
Western keyboards usually generate composed characters, so text typed by users will be
in NFC by default. However, to be safe, it may be good to sanitize strings with normal
ize('NFC', user_text) before saving. NFC is also the normalization form recommended
by the W3C in Character Model for the World Wide Web: String Matching and
Searching.
Some single characters are normalized by NFC into another single character. The symbol
for the ohm (?) unit of electrical resistance is normalized to the Greek uppercase
omega. They are visually identical, but they compare unequal so it is essential to normalize
to avoid surprises:
>>> from unicodedata import normalize, name
>>> ohm = '\u2126'
>>> name(ohm)
'OHM SIGN'
>>> ohm_c = normalize('NFC', ohm)
>>> name(ohm_c)
'GREEK CAPITAL LETTER OMEGA'
>>> ohm == ohm_c
False
>>> normalize('NFC', ohm) == normalize('NFC', ohm_c)
True
In the acronyms for the other two normalization forms�NFKC and NFKD�the letter
K stands for �compatibility.� These are stronger forms of normalization, affecting the
so-called �compatibility characters.� Although one goal of Unicode is to have a single
�canonical� code point for each character, some characters appear more than once for
compatibility with preexisting standards. For example, the micro sign, '?' (U+00B5),
was added to Unicode to support round-trip conversion to latin1, even though the
same character is part of the Greek alphabet with code point U+03BC (GREEK SMALL
LETTER MU). So, the micro sign is considered a �compatibility character.�
In the NFKC and NFKD forms, each compatibility character is replaced by a �compatibility
decomposition� of one or more characters that are considered a �preferred� representation,
even if there is some formatting loss�ideally, the formatting should be the
responsibility of external markup, not part of Unicode. To exemplify, the compatibility
decomposition of the one half fraction '?' (U+00BD) is the sequence of three characters
'1/2', and the compatibility decomposition of the micro sign '?' (U+00B5) is the lowercase
mu '?' (U+03BC).8
118 | Chapter 4: Text versus Bytes
Here is how the NFKC works in practice:
>>> from unicodedata import normalize, name
>>> half = '?'
>>> normalize('NFKC', half)
'1?2'
>>> four_squared = '4?'
>>> normalize('NFKC', four_squared)
'42'
>>> micro = '?'
>>> micro_kc = normalize('NFKC', micro)
>>> micro, micro_kc
('?', '?')
>>> ord(micro), ord(micro_kc)
(181, 956)
>>> name(micro), name(micro_kc)
('MICRO SIGN', 'GREEK SMALL LETTER MU')
Although '1?2' is a reasonable substitute for '?', and the micro sign is really a lowercase
Greek mu, converting '4?' to '42' changes the meaning. An application could store
'4?' as '4<sup>2</sup>', but the normalize function knows nothing about formatting.
Therefore, NFKC or NFKD may lose or distort information, but they can produce
convenient intermediate representations for searching and indexing: users may be
pleased that a search for '1?2 inch' also finds documents containing '? inch'.
NFKC and NFKD normalization should be applied with care and
only in special cases�e.g., search and indexing�and not for permanent
storage, because these transformations cause data loss.
When preparing text for searching or indexing, another operation is useful: case folding,
our next subject.
Case Folding
Case folding is essentially converting all text to lowercase, with some additional transformations.
It is supported by the str.casefold() method (new in Python 3.3).
For any string s containing only latin1 characters, s.casefold() produces the same
result as s.lower(), with only two exceptions�the micro sign '?' is changed to the
Greek lowercase mu (which looks the same in most fonts) and the German Eszett or
�sharp s� (?) becomes �ss�:
>>> micro = '?'
>>> name(micro)
'MICRO SIGN'
>>> micro_cf = micro.casefold()
>>> name(micro_cf)
Normalizing Unicode for Saner Comparisons | 119
'GREEK SMALL LETTER MU'
>>> micro, micro_cf
('?', '?')
>>> eszett = '?'
>>> name(eszett)
'LATIN SMALL LETTER SHARP S'
>>> eszett_cf = eszett.casefold()
>>> eszett, eszett_cf
('?', 'ss')
As of Python 3.4, there are 116 code points for which str.casefold() and str.low
er() return different results. That�s 0.11% of a total of 110,122 named characters in
Unicode 6.3.
As usual with anything related to Unicode, case folding is a complicated issue with plenty
of linguistic special cases, but the Python core team made an effort to provide a solution
that hopefully works for most users.
In the next couple of sections, we�ll put our normalization knowledge to use developing
utility functions.
Utility Functions for Normalized Text Matching
As we�ve seen, NFC and NFD are safe to use and allow sensible comparisons between
Unicode strings. NFC is the best normalized form for most applications. str.case
fold() is the way to go for case-insensitive comparisons.
If you work with text in many languages, a pair of functions like nfc_equal and
fold_equal in Example 4-13 are useful additions to your toolbox.
Example 4-13. normeq.py: normalized Unicode string comparison
"""
Utility functions for normalized Unicode string comparison.
Using Normal Form C, case sensitive:
>>> s1 = 'cafe'
>>> s2 = 'cafe\u0301'
>>> s1 == s2
False
>>> nfc_equal(s1, s2)
True
>>> nfc_equal('A', 'a')
False
Using Normal Form C with case folding:
>>> s3 = 'Stra?e'
>>> s4 = 'strasse'
>>> s3 == s4
120 | Chapter 4: Text versus Bytes
False
>>> nfc_equal(s3, s4)
False
>>> fold_equal(s3, s4)
True
>>> fold_equal(s1, s2)
True
>>> fold_equal('A', 'a')
True
"""
from unicodedata import normalize
def nfc_equal(str1, str2):
return normalize('NFC', str1) == normalize('NFC', str2)
def fold_equal(str1, str2):
return (normalize('NFC', str1).casefold() ==
normalize('NFC', str2).casefold())
Beyond Unicode normalization and case folding�which are both part of the Unicode
standard�sometimes it makes sense to apply deeper transformations, like changing
'cafe' into 'cafe'. We�ll see when and how in the next section.
Extreme �Normalization�: Taking Out Diacritics
The Google Search secret sauce involves many tricks, but one of them apparently is
ignoring diacritics (e.g., accents, cedillas, etc.), at least in some contexts. Removing
diacritics is not a proper form of normalization because it often changes the meaning
of words and may produce false positives when searching. But it helps coping with some
facts of life: people sometimes are lazy or ignorant about the correct use of diacritics,
and spelling rules change over time, meaning that accents come and go in living languages.
Outside of searching, getting rid of diacritics also makes for more readable URLs, at
least in Latin-based languages. Take a look at the URL for the Wikipedia article about
the city of Sao Paulo:
http://en.wikipedia.org/wiki/S%C3%A3o_Paulo
The %C3%A3 part is the URL-escaped, UTF-8 rendering of the single letter �a� (�a� with
tilde). The following is much friendlier, even if it is not the right spelling:
http://en.wikipedia.org/wiki/Sao_Paulo
To remove all diacritics from a str, you can use a function like Example 4-14.
Normalizing Unicode for Saner Comparisons | 121
Example 4-14. Function to remove all combining marks (module sanitize.py)
import unicodedata
import string
def shave_marks(txt):
"""Remove all diacritic marks"""
norm_txt = unicodedata.normalize('NFD', txt)
shaved = ''.join(c for c in norm_txt
if not unicodedata.combining(c))
return unicodedata.normalize('NFC', shaved)
Decompose all characters into base characters and combining marks.
Filter out all combining marks.
Recompose all characters.
Example 4-15 shows a couple of uses of shave_marks.
Example 4-15. Two examples using shave_marks from Example 4-14
>>> order = '�Herr Vo?: � ? cup of OEtker� caffe latte � bowl of acai.�'
>>> shave_marks(order)
'�Herr Vo?: � ? cup of OEtker� caffe latte � bowl of acai.�'
>>> Greek = '???????, Zefiro'
>>> shave_marks(Greek)
'???????, Zefiro'
Only the letters �e�, �c�, and �i� were replaced.
Both �?� and �e� were replaced.
The function shave_marks from Example 4-14 works all right, but maybe it goes too
far. Often the reason to remove diacritics is to change Latin text to pure ASCII, but
shave_marks also changes non-Latin characters�like Greek letters�which will never
become ASCII just by losing their accents. So it makes sense to analyze each base character
and to remove attached marks only if the base character is a letter from the Latin
alphabet. This is what Example 4-16 does.
Example 4-16. Function to remove combining marks from Latin characters (import
statements are omitted as this is part of the sanitize.py module from Example 4-14)
def shave_marks_latin(txt):
"""Remove all diacritic marks from Latin base characters"""
norm_txt = unicodedata.normalize('NFD', txt)
latin_base = False
keepers = []
for c in norm_txt:
if unicodedata.combining(c) and latin_base:
continue # ignore diacritic on Latin base char
122 | Chapter 4: Text versus Bytes
keepers.append(c)
# if it isn't combining char, it's a new base char
if not unicodedata.combining(c):
latin_base = c in string.ascii_letters
shaved = ''.join(keepers)
return unicodedata.normalize('NFC', shaved)
Decompose all characters into base characters and combining marks.
Skip over combining marks when base character is Latin.
Otherwise, keep current character.
Detect new base character and determine if it�s Latin.
Recompose all characters.
An even more radical step would be to replace common symbols in Western texts (e.g.,
curly quotes, em dashes, bullets, etc.) into ASCII equivalents. This is what the function
asciize does in Example 4-17.
Example 4-17. Transform some Western typographical symbols into ASCII (this snippet
is also part of sanitize.py from Example 4-14)
single_map = str.maketrans("""�?��?��������?�""",
"""'f"*^<''""---~>""")
multi_map = str.maketrans({
'�': '<euro>',
'�': '...',
'OE': 'OE',
'�': '(TM)',
'oe': 'oe',
'�': '<per mille>',
'�': '**',
})
multi_map.update(single_map)
def dewinize(txt):
"""Replace Win1252 symbols with ASCII chars or sequences"""
return txt.translate(multi_map)
def asciize(txt):
no_marks = shave_marks_latin(dewinize(txt))
no_marks = no_marks.replace('?', 'ss')
return unicodedata.normalize('NFKC', no_marks)
Build mapping table for char-to-char replacement.
Build mapping table for char-to-string replacement.
Normalizing Unicode for Saner Comparisons | 123
Merge mapping tables.
dewinize does not affect ASCII or latin1 text, only the Microsoft additions in
to latin1 in cp1252.
Apply dewinize and remove diacritical marks.
Replace the Eszett with �ss� (we are not using case fold here because we want to
preserve the case).
Apply NFKC normalization to compose characters with their compatibility code
points.
Example 4-18 shows asciize in use.
Example 4-18. Two examples using asciize from Example 4-17
>>> order = '�Herr Vo?: � ? cup of OEtker� caffe latte � bowl of acai.�'
>>> dewinize(order)
'"Herr Vo?: - ? cup of OEtker(TM) caffe latte - bowl of acai."'
>>> asciize(order)
'"Herr Voss: - 1?2 cup of OEtker(TM) caffe latte - bowl of acai."'
dewinize replaces curly quotes, bullets, and � (trademark symbol).
asciize applies dewinize, drops diacritics, and replaces the '?'.
Different languages have their own rules for removing diacritics.
For example, Germans change the 'u' into 'ue'. Our asciize
function is not as refined, so it may or not be suitable for your
language. It works acceptably for Portuguese, though.
To summarize, the functions in sanitize.py go way beyond standard normalization and
perform deep surgery on the text, with a good chance of changing its meaning. Only
you can decide whether to go so far, knowing the target language, your users, and how
the transformed text will be used.
This wraps up our discussion of normalizing Unicode text.
The next Unicode matter to sort out is� sorting.
Sorting Unicode Text
Python sorts sequences of any type by comparing the items in each sequence one by
one. For strings, this means comparing the code points. Unfortunately, this produces
unacceptable results for anyone who uses non-ASCII characters.
Consider sorting a list of fruits grown in Brazil:
124 | Chapter 4: Text versus Bytes
9. Diacritics affect sorting only in the rare case when they are the only difference between two words�in that
case, the word with a diacritic is sorted after the plain word.
>>> fruits = ['caju', 'atemoia', 'caja', 'acai', 'acerola']
>>> sorted(fruits)
['acerola', 'atemoia', 'acai', 'caju', 'caja']
Sorting rules vary for different locales, but in Portuguese and many languages that use
the Latin alphabet, accents and cedillas rarely make a difference when sorting.9 So �caja�
is sorted as �caja,� and must come before �caju.�
The sorted fruits list should be:
['acai', 'acerola', 'atemoia', 'caja', 'caju']
The standard way to sort non-ASCII text in Python is to use the locale.strxfrm
function which, according to the locale module docs, �transforms a string to one that
can be used in locale-aware comparisons.�
To enable locale.strxfrm, you must first set a suitable locale for your application, and
pray that the OS supports it. On GNU/Linux (Ubuntu 14.04) with the pt_BR locale, the
sequence of commands in Example 4-19 works.
Example 4-19. Using the locale.strxfrm function as sort key
>>> import locale
>>> locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')
'pt_BR.UTF-8'
>>> fruits = ['caju', 'atemoia', 'caja', 'acai', 'acerola']
>>> sorted_fruits = sorted(fruits, key=locale.strxfrm)
>>> sorted_fruits
['acai', 'acerola', 'atemoia', 'caja', 'caju']
So you need to call setlocale(LC_COLLATE, �your_locale�) before using
locale.strxfrm as the key when sorting.
There are a few caveats, though:
� Because locale settings are global, calling setlocale in a library is not recommended.
Your application or framework should set the locale when the process
starts, and should not change it afterwards.
� The locale must be installed on the OS, otherwise setlocale raises a locale.Error:
unsupported locale setting exception.
� You must know how to spell the locale name. They are pretty much standardized
in the Unix derivatives as 'language_code.encoding', but on Windows the syntax
is more complicated: Language Name-Language Variant_Region Name.code
page>. Note that the Language Name, Language Variant, and Region Name parts
can have spaces inside them, but the parts after the first are prefixed with special
Sorting Unicode Text | 125
10. Thanks to Leonardo Rachael who went beyond his duties as tech reviewer and researched these Windows
details, even though he is a GNU/Linux user himself.
11. Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one
of the tech reviewers, had no problem using setlocale and locale.strxfrm on his Mac with OSX 10.9. In
summary: your mileage may vary.
different characters: a hyphen, an underline character, and a dot. All parts seem to
be optional except the language name. For example: English_United States.
850 means Language Name �English�, region �United States�, and codepage �850�.
The language and region names Windows understands are listed in the MSDN
article Language Identifier Constants and Strings, while Code Page Identifiers lists
the numbers for the last part.10
� The locale must be correctly implemented by the makers of the OS. I was successful
on Ubuntu 14.04, but not on OSX (Mavericks 10.9). On two different Macs, the call
setlocale(LC_COLLATE, 'pt_BR.UTF-8') returns the string 'pt_BR.UTF-8' with
no complaints. But sorted(fruits, key=locale.strxfrm) produced the same
incorrect result as sorted(fruits) did. I also tried the fr_FR, es_ES, and de_DE
locales on OSX, but locale.strxfrm never did its job.11
So the standard library solution to internationalized sorting works, but seems to be well
supported only on GNU/Linux (perhaps also on Windows, if you are an expert). Even
then, it depends on locale settings, creating deployment headaches.
Fortunately, there is a simpler solution: the PyUCA library, available on PyPI.
Sorting with the Unicode Collation Algorithm
James Tauber, prolific Django contributor, must have felt the pain and created
PyUCA, a pure-Python implementation of the Unicode Collation Algorithm (UCA).
Example 4-20 shows how easy it is to use.
Example 4-20. Using the pyuca.Collator.sort_key method
>>> import pyuca
>>> coll = pyuca.Collator()
>>> fruits = ['caju', 'atemoia', 'caja', 'acai', 'acerola']
>>> sorted_fruits = sorted(fruits, key=coll.sort_key)
>>> sorted_fruits
['acai', 'acerola', 'atemoia', 'caja', 'caju']
This is friendly and just works. I tested it on GNU/Linux, OSX, and Windows. Only
Python 3.X is supported at this time.
PyUCA does not take the locale into account. If you need to customize the sorting, you
can provide the path to a custom collation table to the Collator() constructor. Out of
126 | Chapter 4: Text versus Bytes
the box, it uses allkeys.txt, which is bundled with the project. That�s just a copy of
the Default Unicode Collation Element Table from Unicode 6.3.0.
By the way, that table is one of the many that comprise the Unicode database, our next
subject.
The Unicode Database
The Unicode standard provides an entire database�in the form of numerous structured
text files�that includes not only the table mapping code points to character names, but
also metadata about the individual characters and how they are related. For example,
the Unicode database records whether a character is printable, is a letter, is a decimal
digit, or is some other numeric symbol. That�s how the str methods isidentifier,
isprintable, isdecimal, and isnumeric work. str.casefold also uses information
from a Unicode table.
The unicodedata module has functions that return character metadata; for instance,
its official name in the standard, whether it is a combining character (e.g., diacritic like
a combining tilde), and the numeric value of the symbol for humans (not its code point).
Example 4-21 shows the use of unicodedata.name() and unicodedata.numeric()
along with the .isdecimal() and .isnumeric() methods of str.
Example 4-21. Demo of Unicode database numerical character metadata (callouts describe
each column in the output)
import unicodedata
import re
re_digit = re.compile(r'\d')
sample = '1\xbc\xb2\u0969\u136b\u216b\u2466\u2480\u3285'
for char in sample:
print('U+%04x' % ord(char),
char.center(6),
're_dig' if re_digit.match(char) else '-',
'isdig' if char.isdigit() else '-',
'isnum' if char.isnumeric() else '-',
format(unicodedata.numeric(char), '5.2f'),
unicodedata.name(char),
sep='\t')
Code point in U+0000 format.
Character centralized in a str of length 6.
Show re_dig if character matches the r'\d' regex.
Show isdig if char.isdigit() is True.
The Unicode Database | 127
12. Although it was not better than re at identifying digits in this particular sample.
Show isnum if char.isnumeric() is True.
Numeric value formated with width 5 and 2 decimal places.
Unicode character name.
Running Example 4-21 gives you the result in Figure 4-3.
Figure 4-3. Nine numeric characters and metadata about them; re_dig means the character
matches the regular expression r'\d�;
The sixth column of Figure 4-3 is the result of calling unicodedata.numeric(char) on
the character. It shows that Unicode knows the numeric value of symbols that represent
numbers. So if you want to create a spreadsheet application that supports Tamil digits
or Roman numerals, go for it!
Figure 4-3 shows that the regular expression r'\d' matches the digit �1� and the Devanagari
digit 3, but not some other characters that are considered digits by the isdi
git function. The re module is not as savvy about Unicode as it could be. The new
regex module available in PyPI was designed to eventually replace re and provides
better Unicode support.12 We�ll come back to the re module in the next section.
Throughout this chapter we�ve used several unicodedata functions, but there are many
more we did not cover. See the standard library documentation for the unicodedata
module.
We will wrap up our tour of str versus bytes with a quick look at a new trend: dualmode
APIs offering functions that accept str or bytes arguments with special handling
depending on the type.
128 | Chapter 4: Text versus Bytes
Dual-Mode str and bytes APIs
The standard library has functions that accept str or bytes arguments and behave
differently depending on the type. Some examples are in the re and os modules.
str Versus bytes in Regular Expressions
If you build a regular expression with bytes, patterns such as \d and \w only match
ASCII characters; in contrast, if these patterns are given as str, they match Unicode
digits or letters beyond ASCII. Example 4-22 and Figure 4-4 compare how letters, ASCII
digits, superscripts, and Tamil digits are matched by str and bytes patterns.
Example 4-22. ramanujan.py: compare behavior of simple str and bytes regular expressions
import re
re_numbers_str = re.compile(r'\d+')
re_words_str = re.compile(r'\w+')
re_numbers_bytes = re.compile(rb'\d+')
re_words_bytes = re.compile(rb'\w+')
text_str = ("Ramanujan saw \u0be7\u0bed\u0be8\u0bef"
" as 1729 = 1? + 12? = 9? + 10?.")
text_bytes = text_str.encode('utf_8')
print('Text', repr(text_str), sep='\n ')
print('Numbers')
print(' str :', re_numbers_str.findall(text_str))
print(' bytes:', re_numbers_bytes.findall(text_bytes))
print('Words')
print(' str :', re_words_str.findall(text_str))
print(' bytes:', re_words_bytes.findall(text_bytes))
The first two regular expressions are of the str type.
The last two are of the bytes type.
Unicode text to search, containing the Tamil digits for 1729 (the logical line
continues until the right parenthesis token).
This string is joined to the previous one at compile time (see �2.4.2. String literal
concatenation� in The Python Language Reference).
A bytes string is needed to search with the bytes regular expressions.
The str pattern r'\d+' matches the Tamil and ASCII digits.
The bytes pattern rb'\d+' matches only the ASCII bytes for digits.
Dual-Mode str and bytes APIs | 129
The str pattern r'\w+' matches the letters, superscripts, Tamil, and ASCII
digits.
The bytes pattern rb'\w+' matches only the ASCII bytes for letters and digits.
Figure 4-4. Screenshot of running ramanujan.py from Example 4-22
Example 4-22 is a trivial example to make one point: you can use regular expressions
on str and bytes, but in the second case bytes outside of the ASCII range are treated
as nondigits and nonword characters.
For str regular expressions, there is a re.ASCII flag that makes \w, \W, \b, \B, \d, \D,
\s, and \S perform ASCII-only matching. See the documentation of the re module for
full details.
Another important dual-mode module is os.
str Versus bytes on os Functions
The GNU/Linux kernel is not Unicode savvy, so in the real world you may find filenames
made of byte sequences that are not valid in any sensible encoding scheme, and cannot
be decoded to str. File servers with clients using a variety of OSes are particularly prone
to this problem.
In order to work around this issue, all os module functions that accept filenames or
pathnames take arguments as str or bytes. If one such function is called with a str
argument, the argument will be automatically converted using the codec named by
sys.getfilesystemencoding(), and the OS response will be decoded with the same
codec. This is almost always what you want, in keeping with the Unicode sandwich best
practice.
But if you must deal with (and perhaps fix) filenames that cannot be handled in that
way, you can pass bytes arguments to the os functions to get bytes return values. This
130 | Chapter 4: Text versus Bytes
feature lets you deal with any file or pathname, no matter how many gremlins you may
find. See Example 4-23.
Example 4-23. listdir with str and bytes arguments and results
>>> os.listdir('.') #
['abc.txt', 'digits-of-?.txt']
>>> os.listdir(b'.') #
[b'abc.txt', b'digits-of-\xcf\x80.txt']
The second filename is �digits-of-?.txt� (with the Greek letter pi).
Given a byte argument, listdir returns filenames as bytes: b'\xcf\x80' is the
UTF-8 encoding of the Greek letter pi).
To help with manual handling of str or bytes sequences that are file or pathnames, the
os module provides special encoding and decoding functions:
fsencode(filename)
Encodes filename (can be str or bytes) to bytes using the codec named by
sys.getfilesystemencoding() if filename is of type str, otherwise returns the
filename bytes unchanged.
fsdecode(filename)
Decodes filename (can be str or bytes) to str using the codec named by sys.get
filesystemencoding() if filename is of type bytes, otherwise returns the file
name str unchanged.
On Unix-derived platforms, these functions use the surrogateescape error handler
(see the sidebar that follows) to avoid choking on unexpected bytes. On Windows, the
strict error handler is used.
Using surrogateescape to Deal with Gremlins
A trick to deal with unexpected bytes or unknown encodings is the surrogateescape
codec error handler described in PEP 383 � Non-decodable Bytes in System Character
Interfaces introduced in Python 3.1.
The idea of this error handler is to replace each nondecodable byte with a code point in
the Unicode range from U+DC00 to U+DCFF that lies in the so-called �Low Surrogate
Area� of the standard�a code space with no characters assigned, reserved for internal
use in applications. On encoding, such code points are converted back to the byte values
they replaced. See Example 4-24.
Example 4-24. Using surrogatescape error handling
>>> os.listdir('.')
['abc.txt', 'digits-of-?.txt']
Dual-Mode str and bytes APIs | 131
>>> os.listdir(b'.')
[b'abc.txt', b'digits-of-\xcf\x80.txt']
>>> pi_name_bytes = os.listdir(b'.')[1]
>>> pi_name_str = pi_name_bytes.decode('ascii', 'surrogateescape')
>>> pi_name_str
'digits-of-\udccf\udc80.txt'
>>> pi_name_str.encode('ascii', 'surrogateescape')
b'digits-of-\xcf\x80.txt'
List directory with a non-ASCII filename.
Let�s pretend we don�t know the encoding and get filenames as bytes.
pi_names_bytes is the filename with the pi character.
Decode it to str using the 'ascii' codec with 'surrogateescape'.
Each non-ASCII byte is replaced by a surrogate code point: '\xcf\x80'
becomes '\udccf\udc80'.
Encode back to ASCII bytes: each surrogate code point is replaced by the byte
it replaced.
This ends our exploration of str and bytes. If you are still with me, congratulations!
Chapter Summary
We started the chapter by dismissing the notion that 1 character == 1 byte. As the
world adopts Unicode (80% of websites already use UTF-8), we need to keep the concept
of text strings separated from the binary sequences that represent them in files, and
Python 3 enforces this separation.
After a brief overview of the binary sequence data types�bytes, bytearray, and memo
ryview�we jumped into encoding and decoding, with a sampling of important codecs,
followed by approaches to prevent or deal with the infamous UnicodeEncodeError,
UnicodeDecodeError, and the SyntaxError caused by wrong encoding in Python
source files.
While on the subject of source code, I presented my position on the debate about non-
ASCII identifiers: if the maintainers of the code base want to use a human language that
has non-ASCII characters, the identifiers should follow suit�unless the code needs to
run on Python 2 as well. But if the project aims to attract an international contributor
base, identifiers should be made from English words, and then ASCII suffices.
We then considered the theory and practice of encoding detection in the absence of
metadata: in theory, it can�t be done, but in practice the Chardet package pulls it off
pretty well for a number of popular encodings. Byte order marks were then presented
132 | Chapter 4: Text versus Bytes
as the only encoding hint commonly found in UTF-16 and UTF-32 files�sometimes
in UTF-8 files as well.
In the next section, we demonstrated opening text files, an easy task except for one
pitfall: the encoding= keyword argument is not mandatory when you open a text file,
but it should be. If you fail to specify the encoding, you end up with a program that
manages to generate �plain text� that is incompatible across platforms, due to conflicting
default encodings. We then exposed the different encoding settings that Python uses as
defaults and how to detect them: locale.getpreferredencoding(), sys.getfilesys
temencoding(), sys.getdefaultencoding(), and the encodings for the standard I/O
files (e.g., sys.stdout.encoding). A sad realization for Windows users is that these
settings often have distinct values within the same machine, and the values are mutually
incompatible; GNU/Linux and OSX users, in contrast, live in a happier place where
UTF-8 is the default pretty much everywhere.
Text comparisons are surprisingly complicated because Unicode provides multiple ways
of representing some characters, so normalizing is a prerequisite to text matching. In
addition to explaining normalization and case folding, we presented some utility functions
that you may adapt to your needs, including drastic transformations like removing
all accents. We then saw how to sort Unicode text correctly by leveraging the standard
locale module�with some caveats�and an alternative that does not depend on tricky
locale configurations: the external PyUCA package.
Finally, we glanced at the Unicode database (a source of metadata about every character),
and wrapped up with brief discussion of dual-mode APIs (e.g., the re and os modules,
where some functions can be called with str or bytes arguments, prompting different
yet fitting results).
Further Reading
Ned Batchelder�s 2012 PyCon US talk �Pragmatic Unicode � or � How Do I Stop the
Pain?� was outstanding. Ned is so professional that he provides a full transcript of the
talk along with the slides and video. Esther Nam and Travis Fischer gave an excellent
PyCon 2014 talk �Character encoding and Unicode in Python: How to (?�?�)??
??? with dignity� (slides, video), from which I quoted this chapter�s short and sweet
epigraph: �Humans use text. Computers speak bytes.� Lennart Regebro�one of this
book�s technical reviewers�presents his �Useful Mental Model of Unicode (UMMU)�
in the short post �Unconfusing Unicode: What Is Unicode?�. Unicode is a complex
standard, so Lennart�s UMMU is a really useful starting point.
The official Unicode HOWTO in the Python docs approaches the subject from several
different angles, from a good historic intro to syntax details, codecs, regular expressions,
filenames, and best practices for Unicode-aware I/O (i.e., the Unicode sandwich), with
plenty of additional reference links from each section. Chapter 4, �Strings�, of Mark
Further Reading | 133
Pilgrim�s awesome book Dive into Python 3 also provides a very good intro to Unicode
support in Python 3. In the same book, Chapter 15 describes how the Chardet library
was ported from Python 2 to Python 3, a valuable case study given that the switch from
the old str to the new bytes is the cause of most migration pains, and that is a central
concern in a library designed to detect encodings.
If you know Python 2 but are new to Python 3, Guido van Rossum�s What�s New in
Python 3.0 has 15 bullet points that summarize what changed, with lots of links. Guido
starts with the blunt statement: �Everything you thought you knew about binary data
and Unicode has changed.� Armin Ronacher�s blog post �The Updated Guide to Unicode
on Python� is deep and highlights some of the pitfalls of Unicode in Python 3 (Armin
is not a big fan of Python 3).
Chapter 2, �Strings and Text,� of the Python Cookbook, Third Edition (O�Reilly), by David
Beazley and Brian K. Jones, has several recipes dealing with Unicode normalization,
sanitizing text, and performing text-oriented operations on byte sequences. Chapter 5
covers files and I/O, and it includes �Recipe 5.17. Writing Bytes to a Text File,� showing
that underlying any text file there is always a binary stream that may be accessed directly
when needed. Later in the cookbook, the struct module is put to use in �Recipe 6.11.
Reading and Writing Binary Arrays of Structures.�
Nick Coghlan�s Python Notes blog has two posts very relevant to this chapter: �Python
3 and ASCII Compatible Binary Protocols� and �Processing Text Files in Python 3�.
Highly recommended.
Binary sequences are about to gain new constructors and methods in Python 3.5, with
one of the current constructor signatures being deprecated (see PEP 467 � Minor API
improvements for binary sequences). Python 3.5 should also see the implementation of
PEP 461 � Adding % formatting to bytes and bytearray.
A list of encodings supported by Python is available at Standard Encodings in the codecs
module documentation. If you need to get that list programmatically, see how it�s done
in the /Tools/unicode/listcodecs.py script that comes with the CPython source code.
Martijn Faassen�s �Changing the Python Default Encoding Considered Harmful� and
Tarek Ziade�s �sys.setdefaultencoding Is Evil� explain why the default encoding you get
from sys.getdefaultencoding() should never be changed, even if you discover how.
The books Unicode Explained by Jukka K. Korpela (O�Reilly) and Unicode Demystified
by Richard Gillam (Addison-Wesley) are not Python-specific but were very helpful
as I studied Unicode concepts. Programming with Unicode by Victor Stinner is a free,
self-published book (Creative Commons BY-SA) covering Unicode in general as well
as tools and APIs in the context of the main operating systems and a few programming
languages, including Python.
134 | Chapter 4: Text versus Bytes
The W3C pages Case Folding: An Introduction and Character Model for the World
Wide Web: String Matching and Searching cover normalization concepts, with the former
being a gentle introduction and the latter a working draft written in dry standardspeak�
the same tone of the Unicode Standard Annex #15 � Unicode Normalization
Forms. The Frequently Asked Questions / Normalization from Unicode.org is more
readable, as is the NFC FAQ by Mark Davis�author of several Unicode algorithms and
president of the Unicode Consortium at the time of this writing.
Soapbox
What Is �Plain Text�?
For anyone who deals with non-English text on a daily basis, �plain text� does not imply
�ASCII.� The Unicode Glossary defines plain text like this:
Computer-encoded text that consists only of a sequence of code points from a given
standard, with no other formatting or structural information.
That definition starts very well, but I don�t agree with the part after the comma. HTML
is a great example of a plain-text format that carries formatting and structural information.
But it�s still plain text because every byte in such a file is there to represent a text
character, usually using UTF-8. There are no bytes with nontext meaning, as you can
find in a .png or .xls document where most bytes represent packed binary values like
RGB values and floating-point numbers. In plain text, numbers are represented as sequences
of digit characters.
I am writing this book in a plain-text format called�ironically�AsciiDoc, which is part
of the toolchain of O�Reilly�s excellent Atlas book publishing platform. AsciiDoc source
files are plain text, but they are UTF-8, not ASCII. Otherwise, writing this chapter would
have been really painful. Despite the name, AsciiDoc is just great.
The world of Unicode is constantly expanding and, at the edges, tool support is not
always there. That�s why I had to use images for Figures 4-1, 4-3, and 4-4: not all characters
I wanted to show were available in the fonts used to render the book. On the other
hand, the Ubuntu 14.04 and OSX 10.9 terminals display them perfectly well�including
the Japanese characters for the word �mojibake�: ????.
Unicode Riddles
Imprecise qualifiers such as �often,� �most,� and �usually� seem to pop up whenever I
write about Unicode normalization. I regret the lack of more definitive advice, but there
are so many exceptions to the rules in Unicode that it is hard to be absolutely positive.
For example, the ? (micro sign) is considered a �compatibility character� but the ?
(ohm) and A (Angstrom) symbols are not. The difference has practical consequences:
NFC normalization�recommended for text matching�replaces the ? (ohm) by ?
(uppercase Grek omega) and the A (Angstrom) by A (uppercase A with ring above).
But as a �compatibility character� the ? (micro sign) is not replaced by the visually
Further Reading | 135
identical ? (lowercase Greek mu), except when the stronger NFKC or NFKD normalizations
are applied, and these transformations are lossy.
I understand the ? (micro sign) is in Unicode because it appears in the latin1 encoding
and replacing it with the Greek mu would break round-trip conversion. After all, that�s
why the micro sign is a �compatibility character.� But if the ohm and Angstrom symbols
are not in Unicode for compatibility reasons, then why have them at all? There are
already code points for the GREEK CAPITAL LETTER OMEGA and the LATIN CAPITAL LET
TER A WITH RING ABOVE, which look the same and replace them on NFC normalization.
Go figure.
My take after many hours studying Unicode: it is hugely complex and full of special
cases, reflecting the wonderful variety of human languages and the politics of industry
standards.
How Are str Represented in RAM?
The official Python docs avoid the issue of how the code points of a str are stored in
memory. This is, after all, an implementation detail. In theory, it doesn�t matter: whatever
the internal representation, every str must be encoded to bytes on output.
In memory, Python 3 stores each str as a sequence of code points using a fixed number
of bytes per code point, to allow efficient direct access to any character or slice.
Before Python 3.3, CPython could be compiled to use either 16 or 32 bits per code point
in RAM; the former was a �narrow build,� and the latter a �wide build.� To know which
you have, check the value of sys.maxunicode: 65535 implies a �narrow build� that can�t
handle code points above U+FFFF transparently. A �wide build� doesn�t have this limitation,
but consumes a lot of memory: 4 bytes per character, even while the vast majority
of code points for Chinese ideographs fit in 2 bytes. Neither option was great, so you
had to choose depending on your needs.
Since Python 3.3, when creating a new str object, the interpreter checks the characters
in it and chooses the most economic memory layout that is suitable for that particular
str: if there are only characters in the latin1 range, that str will use just one byte per
code point. Otherwise, 2 or 4 bytes per code point may be used, depending on the str.
This is a simplification; for the full details, look up PEP 393 � Flexible String Representation.
The flexible string representation is similar to the way the int type works in Python 3:
if the integer fits in a machine word, it is stored in one machine word. Otherwise, the
interpreter switches to a variable-length representation like that of the Python 2 long
type. It is nice to see the spread of good ideas.
136 | Chapter 4: Text versus Bytes
PART III
Functions as Objects

1. �Origins of Python�s Functional Features�, from Guido�s The History of Python blog.
CHAPTER 5
First-Class Functions
I have never considered Python to be heavily influenced by functional languages, no
matter what people say or think. I was much more familiar with imperative languages
such as C and Algol 68 and although I had made functions first-class objects, I didn�t view
Python as a functional programming language.1
� Guido van Rossum
Python BDFL
Functions in Python are first-class objects. Programming language theorists define a
�first-class object� as a program entity that can be:
� Created at runtime
� Assigned to a variable or element in a data structure
� Passed as an argument to a function
� Returned as the result of a function
Integers, strings, and dictionaries are other examples of first-class objects in Python�
nothing fancy here. But if you came to Python from a language where functions are not
first-class citizens, this chapter and the rest of Part III of the book focuses on the implications
and practical applications of treating functions as objects.
The term �first-class functions� is widely used as shorthand for
�functions as first-class objects.� It�s not perfect because it seems
to imply an �elite� among functions. In Python, all functions are
first-class.
139
Treating a Function Like an Object
The console session in Example 5-1 shows that Python functions are objects. Here we
create a function, call it, read its __doc__ attribute, and check that the function object
itself is an instance of the function class.
Example 5-1. Create and test a function, then read its __doc__ and check its type
>>> def factorial(n):
... '''returns n!'''
... return 1 if n < 2 else n * factorial(n-1)
...
>>> factorial(42)
1405006117752879898543142606244511569936384000000000
>>> factorial.__doc__
'returns n!'
>>> type(factorial)
<class 'function'>
This is a console session, so we�re creating a function in �runtime.�
__doc__ is one of several attributes of function objects.
factorial is an instance of the function class.
The __doc__ attribute is used to generate the help text of an object. In the Python
interactive console, the command help(factorial) will display a screen like that in
Figure 5-1.
Figure 5-1. Help screen for the factorial function; the text is from the __doc__ attribute
of the function object
Example 5-2 shows the �first class� nature of a function object. We can assign it a variable
fact and call it through that name. We can also pass factorial as an argument to
map. The map function returns an iterable where each item is the result of the application
of the first argument (a function) to succesive elements of the second argument (an
iterable), range(10) in this example.
140 | Chapter 5: First-Class Functions
Example 5-2. Use function through a different name, and pass function as argument
>>> fact = factorial
>>> fact
<function factorial at 0x...>
>>> fact(5)
120
>>> map(factorial, range(11))
<map object at 0x...>
>>> list(map(fact, range(11)))
[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]
Having first-class functions enables programming in a functional style. One of the hallmarks
of functional programming is the use of higher-order functions, our next topic.
Higher-Order Functions
A function that takes a function as argument or returns a function as the result is a
higher-order function. One example is map, shown in Example 5-2. Another is the builtin
function sorted: an optional key argument lets you provide a function to be applied
to each item for sorting, as seen in �list.sort and the sorted Built-In Function� on page 42.
For example, to sort a list of words by length, simply pass the len function as the key,
as in Example 5-3.
Example 5-3. Sorting a list of words by length
>>> fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']
>>> sorted(fruits, key=len)
['fig', 'apple', 'cherry', 'banana', 'raspberry', 'strawberry']
>>>
Any one-argument function can be used as the key. For example, to create a rhyme
dictionary it might be useful to sort each word spelled backward. In Example 5-4, note
that the words in the list are not changed at all; only their reversed spelling is used as
the sort criterion, so that the berries appear together.
Example 5-4. Sorting a list of words by their reversed spelling
>>> def reverse(word):
... return word[::-1]
>>> reverse('testing')
'gnitset'
>>> sorted(fruits, key=reverse)
['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']
>>>
In the functional programming paradigm, some of the best known higher-order functions
are map, filter, reduce, and apply. The apply function was deprecated in Python
2.3 and removed in Python 3 because it�s no longer necessary. If you need to call a
Higher-Order Functions | 141
function with a dynamic set of arguments, you can just write fn(*args, **key
words) instead of apply(fn, args, kwargs).
The map, filter, and reduce higher-order functions are still around, but better alternatives
are available for most of their use cases, as the next section shows.
Modern Replacements for map, filter, and reduce
Functional languages commonly offer the map, filter, and reduce higher-order functions
(sometimes with different names). The map and filter functions are still builtins
in Python 3, but since the introduction of list comprehensions and generator expressions,
they are not as important. A listcomp or a genexp does the job of map and
filter combined, but is more readable. Consider Example 5-5.
Example 5-5. Lists of factorials produced with map and filter compared to alternatives
coded as list comprehensions
>>> list(map(fact, range(6)))
[1, 1, 2, 6, 24, 120]
>>> [fact(n) for n in range(6)]
[1, 1, 2, 6, 24, 120]
>>> list(map(factorial, filter(lambda n: n % 2, range(6))))
[1, 6, 120]
>>> [factorial(n) for n in range(6) if n % 2]
[1, 6, 120]
>>>
Build a list of factorials from 0! to 5!.
Same operation, with a list comprehension.
List of factorials of odd numbers up to 5!, using both map and filter.
List comprehension does the same job, replacing map and filter, and making
lambda unnecessary.
In Python 3, map and filter return generators�a form of iterator�so their direct
substitute is now a generator expression (in Python 2, these functions returned lists,
therefore their closest alternative is a listcomp).
The reduce function was demoted from a built-in in Python 2 to the functools module
in Python 3. Its most common use case, summation, is better served by the sum builtin
available since Python 2.3 was released in 2003. This is a big win in terms of readability
and performance (see Example 5-6).
Example 5-6. Sum of integers up to 99 performed with reduce and sum
>>> from functools import reduce
>>> from operator import add
>>> reduce(add, range(100))
142 | Chapter 5: First-Class Functions
4950
>>> sum(range(100))
4950
>>>
Starting with Python 3.0, reduce is not a built-in.
Import add to avoid creating a function just to add two numbers.
Sum integers up to 99.
Same task using sum; import or adding function not needed.
The common idea of sum and reduce is to apply some operation to successive items in
a sequence, accumulating previous results, thus reducing a sequence of values to a single
value.
Other reducing built-ins are all and any:
all(iterable)
Returns True if every element of the iterable is truthy; all([]) returns True.
any(iterable)
Returns True if any element of the iterable is truthy; any([]) returns False.
I give a fuller explanation of reduce in �Vector Take #4: Hashing and a Faster ==� on
page 288 where an ongoing example provides a meaningful context for the use of this
function. The reducing functions are summarized later in the book when iterables are
in focus, in �Iterable Reducing Functions� on page 434.
To use a higher-order function, sometimes it is convenient to create a small, one-off
function. That is why anonymous functions exist. We�ll cover them next.
Anonymous Functions
The lambda keyword creates an anonymous function within a Python expression.
However, the simple syntax of Python limits the body of lambda functions to be pure
expressions. In other words, the body of a lambda cannot make assignments or use any
other Python statement such as while, try, etc.
The best use of anonymous functions is in the context of an argument list. For example,
Example 5-7 is the rhyme index example from Example 5-4 rewritten with lambda,
without defining a reverse function.
Example 5-7. Sorting a list of words by their reversed spelling using lambda
>>> fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']
>>> sorted(fruits, key=lambda word: word[::-1])
Anonymous Functions | 143
['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']
>>>
Outside the limited context of arguments to higher-order functions, anonymous functions
are rarely useful in Python. The syntactic restrictions tend to make nontrivial
lambdas either unreadable or unworkable.
Lundh�s lambda Refactoring Recipe
If you find a piece of code hard to understand because of a lambda, Fredrik Lundh
suggests this refactoring procedure:
1. Write a comment explaining what the heck that lambda does.
2. Study the comment for a while, and think of a name that captures the essence of
the comment.
3. Convert the lambda to a def statement, using that name.
4. Remove the comment.
These steps are quoted from the Functional Programming HOWTO, a must read.
The lambda syntax is just syntactic sugar: a lambda expression creates a function object
just like the def statement. That is just one of several kinds of callable objects in Python.
The following section reviews all of them.
The Seven Flavors of Callable Objects
The call operator (i.e., ()) may be applied to other objects beyond user-defined functions.
To determine whether an object is callable, use the callable() built-in function.
The Python Data Model documentation lists seven callable types:
User-defined functions
Created with def statements or lambda expressions.
Built-in functions
A function implemented in C (for CPython), like len or time.strftime.
Built-in methods
Methods implemented in C, like dict.get.
Methods
Functions defined in the body of a class.
144 | Chapter 5: First-Class Functions
Classes
When invoked, a class runs its __new__ method to create an instance, then __in
it__ to initialize it, and finally the instance is returned to the caller. Because there
is no new operator in Python, calling a class is like calling a function. (Usually calling
a class creates an instance of the same class, but other behaviors are possible by
overriding __new__. We�ll see an example of this in �Flexible Object Creation with
__new__� on page 592.)
Class instances
If a class defines a __call__ method, then its instances may be invoked as functions.
See �User-Defined Callable Types� on page 145.
Generator functions
Functions or methods that use the yield keyword. When called, generator functions
return a generator object.
Generator functions are unlike other callables in many respects. Chapter 14 is devoted
to them. They can also be used as coroutines, which are covered in Chapter 16.
Given the variety of existing callable types in Python, the safest way
to determine whether an object is callable is to use the calla
ble() built-in:
>>> abs, str, 13
(<built-in function abs>, <class 'str'>, 13)
>>> [callable(obj) for obj in (abs, str, 13)]
[True, True, False]
We now move on to building class instances that work as callable objects.
User-Defined Callable Types
Not only are Python functions real objects, but arbitrary Python objects may also be
made to behave like functions. Implementing a __call__ instance method is all it takes.
Example 5-8 implements a BingoCage class. An instance is built from any iterable, and
stores an internal list of items, in random order. Calling the instance pops an item.
Example 5-8. bingocall.py: A BingoCage does one thing: picks items from a shuffled list
import random
class BingoCage:
def __init__(self, items):
self._items = list(items)
random.shuffle(self._items)
User-Defined Callable Types | 145
def pick(self):
try:
return self._items.pop()
except IndexError:
raise LookupError('pick from empty BingoCage')
def __call__(self):
return self.pick()
__init__ accepts any iterable; building a local copy prevents unexpected side
effects on any list passed as an argument.
shuffle is guaranteed to work because self._items is a list.
The main method.
Raise exception with custom message if self._items is empty.
Shortcut to bingo.pick(): bingo().
Here is a simple demo of Example 5-8. Note how a bingo instance can be invoked as a
function, and the callable(�) built-in recognizes it as a callable object:
>>> bingo = BingoCage(range(3))
>>> bingo.pick()
1
>>> bingo()
0
>>> callable(bingo)
True
A class implementing __call__ is an easy way to create function-like objects that have
some internal state that must be kept across invocations, like the remaining items in the
BingoCage. An example is a decorator. Decorators must be functions, but it is sometimes
convenient to be able to �remember� something between calls of the decorator (e.g., for
memoization�caching the results of expensive computations for later use).
A totally different approach to creating functions with internal state is to use closures.
Closures, as well as decorators, are the subject of Chapter 7.
We now move on to another aspect of handling functions as objects: runtime introspection.
Function Introspection
Function objects have many attributes beyond __doc__. See what the dir function reveals
about our factorial:
>>> dir(factorial)
['__annotations__', '__call__', '__class__', '__closure__', '__code__',
'__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__',
146 | Chapter 5: First-Class Functions
'__format__', '__ge__', '__get__', '__getattribute__', '__globals__',
'__gt__', '__hash__', '__init__', '__kwdefaults__', '__le__', '__lt__',
'__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__',
'__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__',
'__subclasshook__']
>>>
Most of these attributes are common to Python objects in general. In this section, we
cover those that are especially relevant to treating functions as objects, starting with
__dict__.
Like the instances of a plain user-defined class, a function uses the __dict__ attribute
to store user attributes assigned to it. This is useful as a primitive form of annotation.
Assigning arbitrary attributes to functions is not a very common practice in general,
but Django is one framework that uses it. See, for example, the short_description,
boolean, and allow_tags attributes described in The Django admin site documentation.
In the Django docs, this example shows attaching a short_description to a
method, to determine the description that will appear in record listings in the Django
admin when that method is used:
def upper_case_name(obj):
return ("%s %s" % (obj.first_name, obj.last_name)).upper()
upper_case_name.short_description = 'Customer name'
Now let us focus on the attributes that are specific to functions and are not found in a
generic Python user-defined object. Computing the difference of two sets quickly gives
us a list of the function-specific attributes (see Example 5-9).
Example 5-9. Listing attributes of functions that don�t exist in plain instances
>>> class C: pass #
>>> obj = C() #
>>> def func(): pass #
>>> sorted(set(dir(func)) - set(dir(obj))) #
['__annotations__', '__call__', '__closure__', '__code__', '__defaults__',
'__get__', '__globals__', '__kwdefaults__', '__name__', '__qualname__']
>>>
Create bare user-defined class.
Make an instance of it.
Create a bare function.
Using set difference, generate a sorted list of the attributes that exist in a function
but not in an instance of a bare class.
Table 5-1 shows a summary of the attributes listed by Example 5-9.
Function Introspection | 147
Table 5-1. Attributes of user-defined functions
Name Type Description
__annotations__ dict Parameter and return annotations
__call__ method-wrapper Implementation of the () operator; a.k.a. the callable object protocol
__closure__ tuple The function closure, i.e., bindings for free variables (often is None)
__code__ code Function metadata and function body compiled into bytecode
__defaults__ tuple Default values for the formal parameters
__get__ method-wrapper Implementation of the read-only descriptor protocol (see Chapter 20)
__globals__ dict Global variables of the module where the function is defined
__kwdefaults__ dict Default values for the keyword-only formal parameters
__name__ str The function name
__qualname__ str The qualified function name, e.g., Random.choice (see PEP-3155)
We will discuss the __defaults__, __code__, and __annotations__ functions, used by
IDEs and frameworks to extract information about function signatures, in later sections.
But to fully appreciate these attributes, we will make a detour to explore the powerful
syntax Python offers to declare function parameters and to pass arguments into them.
From Positional to Keyword-Only Parameters
One of the best features of Python functions is the extremely flexible parameter handling
mechanism, enhanced with keyword-only arguments in Python 3. Closely related are
the use of * and ** to �explode� iterables and mappings into separate arguments when
we call a function. To see these features in action, see the code for Example 5-10 and
tests showing its use in Example 5-11.
Example 5-10. tag generates HTML; a keyword-only argument cls is used to pass
�class� attributes as a workaround because class is a keyword in Python
def tag(name, *content, cls=None, **attrs):
"""Generate one or more HTML tags"""
if cls is not None:
attrs['class'] = cls
if attrs:
attr_str = ''.join(' %s="%s"' % (attr, value)
for attr, value
in sorted(attrs.items()))
else:
attr_str = ''
if content:
return '\n'.join('<%s%s>%s</%s>' %
(name, attr_str, c, name) for c in content)
else:
return '<%s%s />' % (name, attr_str)
148 | Chapter 5: First-Class Functions
The tag function can be invoked in many ways, as Example 5-11 shows.
Example 5-11. Some of the many ways of calling the tag function from Example 5-10
>>> tag('br')
'<br />'
>>> tag('p', 'hello')
'<p>hello</p>'
>>> print(tag('p', 'hello', 'world'))
<p>hello</p>
<p>world</p>
>>> tag('p', 'hello', id=33)
'<p id="33">hello</p>'
>>> print(tag('p', 'hello', 'world', cls='sidebar'))
<p class="sidebar">hello</p>
<p class="sidebar">world</p>
>>> tag(content='testing', name="img")
'<img content="testing" />'
>>> my_tag = {'name': 'img', 'title': 'Sunset Boulevard',
... 'src': 'sunset.jpg', 'cls': 'framed'}
>>> tag(**my_tag)
'<img class="framed" src="sunset.jpg" title="Sunset Boulevard" />'
A single positional argument produces an empty tag with that name.
Any number of arguments after the first are captured by *content as a tuple.
Keyword arguments not explicitly named in the tag signature are captured by
**attrs as a dict.
The cls parameter can only be passed as a keyword argument.
Even the first positional argument can be passed as a keyword when tag is called.
Prefixing the my_tag dict with ** passes all its items as separate arguments,
which are then bound to the named parameters, with the remaining caught by
**attrs.
Keyword-only arguments are a new feature in Python 3. In Example 5-10, the cls
parameter can only be given as a keyword argument�it will never capture unnamed
positional arguments. To specify keyword-only arguments when defining a function,
name them after the argument prefixed with *. If you don�t want to support variable
positional arguments but still want keyword-only arguments, put a * by itself in the
signature, like this:
>>> def f(a, *, b):
... return a, b
...
>>> f(1, b=2)
(1, 2)
From Positional to Keyword-Only Parameters | 149
Note that keyword-only arguments do not need to have a default value: they can be
mandatory, like b in the preceding example.
We now move on to the introspection of function parameters, starting with a motivating
example from a web framework, and on through introspection techniques.
Retrieving Information About Parameters
An interesting application of function introspection can be found in the Bobo HTTP
micro-framework. To see that in action, consider a variation of the Bobo tutorial �Hello
world� application in Example 5-12.
Example 5-12. Bobo knows that hello requires a person argument, and retrieves it from
the HTTP request
import bobo
@bobo.query('/')
def hello(person):
return 'Hello %s!' % person
The bobo.query decorator integrates a plain function such as hello with the request
handling machinery of the framework. We�ll cover decorators in Chapter 7�that�s not
the point of this example here. The point is that Bobo introspects the hello function
and finds out it needs one parameter named person to work, and it will retrieve a
parameter with that name from the request and pass it to hello, so the programmer
does not need to touch the request object at all.
If you install Bobo and point its development server to the script in Example 5-12 (e.g.,
bobo -f hello.py), a hit on the URL http://localhost:8080/ will produce the message
�Missing form variable person� with a 403 HTTP code. This happens because Bobo
understands that the person argument is required to call hello, but no such name was
found in the request. Example 5-13 is a shell session using curl to show this behavior.
Example 5-13. Bobo issues a 403 forbidden response if there are missing function arguments
in the request; curl -i is used to dump the headers to standard output
$ curl -i http://localhost:8080/
HTTP/1.0 403 Forbidden
Date: Thu, 21 Aug 2014 21:39:44 GMT
Server: WSGIServer/0.2 CPython/3.4.1
Content-Type: text/html; charset=UTF-8
Content-Length: 103
<html>
<head><title>Missing parameter</title></head>
<body>Missing form variable person</body>
</html>
150 | Chapter 5: First-Class Functions
However, if you get http://localhost:8080/?person=Jim, the response will be the
string 'Hello Jim!'. See Example 5-14.
Example 5-14. Passing the person parameter is required for an OK response
$ curl -i http://localhost:8080/?person=Jim
HTTP/1.0 200 OK
Date: Thu, 21 Aug 2014 21:42:32 GMT
Server: WSGIServer/0.2 CPython/3.4.1
Content-Type: text/html; charset=UTF-8
Content-Length: 10
Hello Jim!
How does Bobo know which parameter names are required by the function, and whether
they have default values or not?
Within a function object, the __defaults__ attribute holds a tuple with the default
values of positional and keyword arguments. The defaults for keyword-only arguments
appear in __kwdefaults__. The names of the arguments, however, are found within the
__code__ attribute, which is a reference to a code object with many attributes of its own.
To demonstrate the use of these attributes, we will inspect the function clip in a module
clip.py, listed in Example 5-15.
Example 5-15. Function to shorten a string by clipping at a space near the desired
length
def clip(text, max_len=80):
"""Return text clipped at the last space before or after max_len
"""
end = None
if len(text) > max_len:
space_before = text.rfind(' ', 0, max_len)
if space_before >= 0:
end = space_before
else:
space_after = text.rfind(' ', max_len)
if space_after >= 0:
end = space_after
if end is None: # no spaces were found
end = len(text)
return text[:end].rstrip()
Example 5-16 shows the values of __defaults__, __code__.co_varnames, and
__code__.co_argcount for the clip function listed in Example 5-15.
Example 5-16. Extracting information about the function arguments
>>> from clip import clip
>>> clip.__defaults__
Retrieving Information About Parameters | 151
(80,)
>>> clip.__code__ # doctest: +ELLIPSIS
<code object clip at 0x...>
>>> clip.__code__.co_varnames
('text', 'max_len', 'end', 'space_before', 'space_after')
>>> clip.__code__.co_argcount
2
As you can see, this is not the most convenient arrangement of information. The argument
names appear in __code__.co_varnames, but that also includes the names of the
local variables created in the body of the function. Therefore, the argument names are
the first N strings, where N is given by __code__.co_argcount which�by the way�
does not include any variable arguments prefixed with * or **. The default values are
identified only by their position in the __defaults__ tuple, so to link each with the
respective argument, you have to scan from last to first. In the example, we have two
arguments, text and max_len, and one default, 80, so it must belong to the last argument,
max_len. This is awkward.
Fortunately, there is a better way: the inspect module.
Take a look at Example 5-17.
Example 5-17. Extracting the function signature
>>> from clip import clip
>>> from inspect import signature
>>> sig = signature(clip)
>>> sig # doctest: +ELLIPSIS
<inspect.Signature object at 0x...>
>>> str(sig)
'(text, max_len=80)'
>>> for name, param in sig.parameters.items():
... print(param.kind, ':', name, '=', param.default)
...
POSITIONAL_OR_KEYWORD : text = <class 'inspect._empty'>
POSITIONAL_OR_KEYWORD : max_len = 80
This is much better. inspect.signature returns an inspect.Signature object, which
has a parameters attribute that lets you read an ordered mapping of names to in
spect.Parameter objects. Each Parameter instance has attributes such as name, de
fault, and kind. The special value inspect._empty denotes parameters with no default,
which makes sense considering that None is a valid�and popular�default value.
The kind attribute holds one of five possible values from the _ParameterKind class:
POSITIONAL_OR_KEYWORD
A parameter that may be passed as a positional or as a keyword argument (most
Python function parameters are of this kind).
152 | Chapter 5: First-Class Functions
VAR_POSITIONAL
A tuple of positional parameters.
VAR_KEYWORD
A dict of keyword parameters.
KEYWORD_ONLY
A keyword-only parameter (new in Python 3).
POSITIONAL_ONLY
A positional-only parameter; currently unsupported by Python function declaration
syntax, but exemplified by existing functions implemented in C�like divmod
�that do not accept parameters passed by keyword.
Besides name, default, and kind, inspect.Parameter objects have an annotation
attribute that is usually inspect._empty but may contain function signature metadata
provided via the new annotations syntax in Python 3 (annotations are covered in the
next section).
An inspect.Signature object has a bind method that takes any number of arguments
and binds them to the parameters in the signature, applying the usual rules for matching
actual arguments to formal parameters. This can be used by a framework to validate
arguments prior to the actual function invocation. Example 5-18 shows how.
Example 5-18. Binding the function signature from the tag function in Example 5-10 to
a dict of arguments
>>> import inspect
>>> sig = inspect.signature(tag)
>>> my_tag = {'name': 'img', 'title': 'Sunset Boulevard',
... 'src': 'sunset.jpg', 'cls': 'framed'}
>>> bound_args = sig.bind(**my_tag)
>>> bound_args
<inspect.BoundArguments object at 0x...>
>>> for name, value in bound_args.arguments.items():
... print(name, '=', value)
...
name = img
cls = framed
attrs = {'title': 'Sunset Boulevard', 'src': 'sunset.jpg'}
>>> del my_tag['name']
>>> bound_args = sig.bind(**my_tag)
Traceback (most recent call last):
...
TypeError: 'name' parameter lacking default value
Get the signature from tag function in Example 5-10.
Pass a dict of arguments to .bind().
Retrieving Information About Parameters | 153
An inspect.BoundArguments object is produced.
Iterate over the items in bound_args.arguments, which is an OrderedDict, to
display the names and values of the arguments.
Remove the mandatory argument name from my_tag.
Calling sig.bind(**my_tag) raises a TypeError complaining of the missing
name parameter.
This example shows how the Python data model, with the help of inspect, exposes the
same machinery the interpreter uses to bind arguments to formal parameters in function
calls.
Frameworks and tools like IDEs can use this information to validate code. Another
feature of Python 3, function annotations, enhances the possible uses of this, as we will
see next.
Function Annotations
Python 3 provides syntax to attach metadata to the parameters of a function declaration
and its return value. Example 5-19 is an annotated version of Example 5-15. The only
differences are in the first line.
Example 5-19. Annotated clip function
def clip(text:str, max_len:'int > 0'=80) -> str:
"""Return text clipped at the last space before or after max_len
"""
end = None
if len(text) > max_len:
space_before = text.rfind(' ', 0, max_len)
if space_before >= 0:
end = space_before
else:
space_after = text.rfind(' ', max_len)
if space_after >= 0:
end = space_after
if end is None: # no spaces were found
end = len(text)
return text[:end].rstrip()
The annotated function declaration.
Each argument in the function declaration may have an annotation expression preceded
by :. If there is a default value, the annotation goes between the argument name and
the = sign. To annotate the return value, add -> and another expression between the )
and the : at the tail of the function declaration. The expressions may be of any type. The
154 | Chapter 5: First-Class Functions
most common types used in annotations are classes, like str or int, or strings, like 'int
> 0', as seen in the annotation for max_len in Example 5-19.
No processing is done with the annotations. They are merely stored in the __annota
tions__ attribute of the function, a dict:
>>> from clip_annot import clip
>>> clip.__annotations__
{'text': <class 'str'>, 'max_len': 'int > 0', 'return': <class 'str'>}
The item with key 'return' holds the return value annotation marked with -> in the
function declaration in Example 5-19.
The only thing Python does with annotations is to store them in the __annota
tions__ attribute of the function. Nothing else: no checks, enforcement, validation, or
any other action is performed. In other words, annotations have no meaning to the
Python interpreter. They are just metadata that may be used by tools, such as IDEs,
frameworks, and decorators. At this writing no tools that use this metadata exist in the
standard library, except that inspect.signature() knows how to extract the annotations,
as Example 5-20 shows.
Example 5-20. Extracting annotations from the function signature
>>> from clip_annot import clip
>>> from inspect import signature
>>> sig = signature(clip)
>>> sig.return_annotation
<class 'str'>
>>> for param in sig.parameters.values():
... note = repr(param.annotation).ljust(13)
... print(note, ':', param.name, '=', param.default)
<class 'str'> : text = <class 'inspect._empty'>
'int > 0' : max_len = 80
The signature function returns a Signature object, which has a return_annotation
attribute and a parameters dictionary mapping parameter names to Parameter objects.
Each Parameter object has its own annotation attribute. That�s how Example 5-20
works.
In the future, frameworks such as Bobo could support annotations to further automate
request processing. For example, an argument annotated as price:float may be automatically
converted from a query string to the float expected by the function; a string
annotation like quantity:'int > 0' might be parsed to perform conversion and validation
of a parameter.
The biggest impact of function annotations will probably not be dynamic settings such
as Bobo, but in providing optional type information for static type checking in tools like
IDEs and linters.
Function Annotations | 155
After this deep dive into the anatomy of functions, the remainder of this chapter covers
the most useful packages in the standard library that support functional programming.
Packages for Functional Programming
Although Guido makes it clear that Python does not aim to be a functional programming
language, a functional coding style can be used to good extent, thanks to the
support of packages like operator and functools, which we cover in the next two
sections.
The operator Module
Often in functional programming it is convenient to use an arithmetic operator as a
function. For example, suppose you want to multiply a sequence of numbers to calculate
factorials without using recursion. To perform summation, you can use sum, but there
is no equivalent function for multiplication. You could use reduce�as we saw in
�Modern Replacements for map, filter, and reduce� on page 142�but this requires a
function to multiply two items of the sequence. Example 5-21 shows how to solve this
using lambda.
Example 5-21. Factorial implemented with reduce and an anonymous function
from functools import reduce
def fact(n):
return reduce(lambda a, b: a*b, range(1, n+1))
To save you the trouble of writing trivial anonymous functions like lambda a, b:
a*b, the operator module provides function equivalents for dozens of arithmetic operators.
With it, we can rewrite Example 5-21 as Example 5-22.
Example 5-22. Factorial implemented with reduce and operator.mul
from functools import reduce
from operator import mul
def fact(n):
return reduce(mul, range(1, n+1))
Another group of one-trick lambdas that operator replaces are functions to pick items
from sequences or read attributes from objects: itemgetter and attrgetter actually
build custom functions to do that.
Example 5-23 shows a common use of itemgetter: sorting a list of tuples by the value
of one field. In the example, the cities are printed sorted by country code (field 1).
Essentially, itemgetter(1) does the same as lambda fields: fields[1]: create a
function that, given a collection, returns the item at index 1.
156 | Chapter 5: First-Class Functions
Example 5-23. Demo of itemgetter to sort a list of tuples (data from Example 2-8)
>>> metro_data = [
... ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),
... ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),
... ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),
... ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),
... ('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)),
... ]
>>>
>>> from operator import itemgetter
>>> for city in sorted(metro_data, key=itemgetter(1)):
... print(city)
...
('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833))
('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))
('Tokyo', 'JP', 36.933, (35.689722, 139.691667))
('Mexico City', 'MX', 20.142, (19.433333, -99.133333))
('New York-Newark', 'US', 20.104, (40.808611, -74.020386))
If you pass multiple index arguments to itemgetter, the function it builds will return
tuples with the extracted values:
>>> cc_name = itemgetter(1, 0)
>>> for city in metro_data:
... print(cc_name(city))
...
('JP', 'Tokyo')
('IN', 'Delhi NCR')
('MX', 'Mexico City')
('US', 'New York-Newark')
('BR', 'Sao Paulo')
>>>
Because itemgetter uses the [] operator, it supports not only sequences but also mappings
and any class that implements __getitem__.
A sibling of itemgetter is attrgetter, which creates functions to extract object attributes
by name. If you pass attrgetter several attribute names as arguments, it also
returns a tuple of values. In addition, if any argument name contains a . (dot), attrget
ter navigates through nested objects to retrieve the attribute. These behaviors are shown
in Example 5-24. This is not the shortest console session because we need to build a
nested structure to showcase the handling of dotted attributes by attrgetter.
Example 5-24. Demo of attrgetter to process a previously defined list of namedtuple
called metro_data (the same list that appears in Example 5-23)
>>> from collections import namedtuple
>>> LatLong = namedtuple('LatLong', 'lat long') #
>>> Metropolis = namedtuple('Metropolis', 'name cc pop coord') #
Packages for Functional Programming | 157
>>> metro_areas = [Metropolis(name, cc, pop, LatLong(lat, long)) #
... for name, cc, pop, (lat, long) in metro_data]
>>> metro_areas[0]
Metropolis(name='Tokyo', cc='JP', pop=36.933, coord=LatLong(lat=35.689722,
long=139.691667))
>>> metro_areas[0].coord.lat #
35.689722
>>> from operator import attrgetter
>>> name_lat = attrgetter('name', 'coord.lat') #
>>>
>>> for city in sorted(metro_areas, key=attrgetter('coord.lat')): #
... print(name_lat(city)) #
...
('Sao Paulo', -23.547778)
('Mexico City', 19.433333)
('Delhi NCR', 28.613889)
('Tokyo', 35.689722)
('New York-Newark', 40.808611)
Use namedtuple to define LatLong.
Also define Metropolis.
Build metro_areas list with Metropolis instances; note the nested tuple
unpacking to extract (lat, long) and use them to build the LatLong for the
coord attribute of Metropolis.
Reach into element metro_areas[0] to get its latitude.
Define an attrgetter to retrieve the name and the coord.lat nested attribute.
Use attrgetter again to sort list of cities by latitude.
Use the attrgetter defined in to show only city name and latitude.
Here is a partial list of functions defined in operator (names starting with _ are omitted,
because they are mostly implementation details):
>>> [name for name in dir(operator) if not name.startswith('_')]
['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains',
'countOf', 'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt',
'iadd', 'iand', 'iconcat', 'ifloordiv', 'ilshift', 'imod', 'imul',
'index', 'indexOf', 'inv', 'invert', 'ior', 'ipow', 'irshift',
'is_', 'is_not', 'isub', 'itemgetter', 'itruediv', 'ixor', 'le',
'length_hint', 'lshift', 'lt', 'methodcaller', 'mod', 'mul', 'ne',
'neg', 'not_', 'or_', 'pos', 'pow', 'rshift', 'setitem', 'sub',
'truediv', 'truth', 'xor']
Most of the 52 names listed are self-evident. The group of names prefixed with i and
the name of another operator�e.g., iadd, iand, etc.�correspond to the augmented
assignment operators�e.g., +=, &=, etc. These change their first argument in place, if it
158 | Chapter 5: First-Class Functions
is mutable; if not, the function works like the one without the i prefix: it simply returns
the result of the operation.
Of the remaining operator functions, methodcaller is the last we will cover. It is somewhat
similar to attrgetter and itemgetter in that it creates a function on the fly. The
function it creates calls a method by name on the object given as argument, as shown
in Example 5-25.
Example 5-25. Demo of methodcaller: second test shows the binding of extra arguments
>>> from operator import methodcaller
>>> s = 'The time has come'
>>> upcase = methodcaller('upper')
>>> upcase(s)
'THE TIME HAS COME'
>>> hiphenate = methodcaller('replace', ' ', '-')
>>> hiphenate(s)
'The-time-has-come'
The first test in Example 5-25 is there just to show methodcaller at work, but if you
need to use the str.upper as a function, you can just call it on the str class and pass a
string as argument, like this:
>>> str.upper(s)
'THE TIME HAS COME'
The second test in Example 5-25 shows that methodcaller can also do a partial application
to freeze some arguments, like the functools.partial function does. That is
our next subject.
Freezing Arguments with functools.partial
The functools module brings together a handful of higher-order functions. The best
known of them is probably reduce, which was covered in �Modern Replacements for
map, filter, and reduce� on page 142. Of the remaining functions in functools, the most
useful is partial and its variation, partialmethod.
functools.partial is a higher-order function that allows partial application of a function.
Given a function, a partial application produces a new callable with some of the
arguments of the original function fixed. This is useful to adapt a function that takes
one or more arguments to an API that requires a callback with fewer arguments.
Example 5-26 is a trivial demonstration.
Example 5-26. Using partial to use a two-argument function where a one-argument
callable is required
>>> from operator import mul
>>> from functools import partial
>>> triple = partial(mul, 3)
Packages for Functional Programming | 159
>>> triple(7)
21
>>> list(map(triple, range(1, 10)))
[3, 6, 9, 12, 15, 18, 21, 24, 27]
Create new triple function from mul, binding first positional argument to 3.
Test it.
Use triple with map; mul would not work with map in this example.
A more useful example involves the unicode.normalize function that we saw in �Normalizing
Unicode for Saner Comparisons� on page 117. If you work with text from
many languages, you may want to apply unicode.normalize('NFC', s) to any string
s before comparing or storing it. If you do that often, it�s handy to have an nfc function
to do so, as in Example 5-27.
Example 5-27. Building a convenient Unicode normalizing function with partial
>>> import unicodedata, functools
>>> nfc = functools.partial(unicodedata.normalize, 'NFC')
>>> s1 = 'cafe'
>>> s2 = 'cafe\u0301'
>>> s1, s2
('cafe', 'cafe')
>>> s1 == s2
False
>>> nfc(s1) == nfc(s2)
True
partial takes a callable as first argument, followed by an arbitrary number of positional
and keyword arguments to bind.
Example 5-28 shows the use of partial with the tag function from Example 5-10, to
freeze one positional argument and one keyword argument.
Example 5-28. Demo of partial applied to the function tag from Example 5-10
>>> from tagger import tag
>>> tag
<function tag at 0x10206d1e0>
>>> from functools import partial
>>> picture = partial(tag, 'img', cls='pic-frame')
>>> picture(src='wumpus.jpeg')
'<img class="pic-frame" src="wumpus.jpeg" />'
>>> picture
functools.partial(<function tag at 0x10206d1e0>, 'img', cls='pic-frame')
>>> picture.func
<function tag at 0x10206d1e0>
>>> picture.args
('img',)
160 | Chapter 5: First-Class Functions
2. The source code for functools.py reveals that the functools.partial class is implemented in C and is
used by default. If that is not available, a pure-Python implementation of partial is available since Python
3.4.in the functools module.
>>> picture.keywords
{'cls': 'pic-frame'}
Import tag from Example 5-10 and show its ID.
Create picture function from tag by fixing the first positional argument with
'img' and the cls keyword argument with 'pic-frame'.
picture works as expected.
partial() returns a functools.partial object.2
A functools.partial object has attributes providing access to the original
function and the fixed arguments.
The functools.partialmethod function (new in Python 3.4) does the same job as
partial, but is designed to work with methods.
An impressive functools function is lru_cache, which does memoization�a form of
automatic optimization that works by storing the results of function calls to avoid expensive
recalculations. We will cover it in Chapter 7, where decorators are explained,
along with other higher-order functions designed to be used as decorators: singledis
patch and wraps.
Chapter Summary
The goal of this chapter was to explore the first-class nature of functions in Python. The
main ideas are that you can assign functions to variables, pass them to other functions,
store them in data structures, and access function attributes, allowing frameworks and
tools to act on that information. Higher-order functions, a staple of functional programming,
are common in Python�even if the use of map, filter, and reduce is not
as frequent as it was�thanks to list comprehensions (and similar constructs like generator
expressions) and the appearance of reducing built-ins like sum, all, and any. The
sorted, min, max built-ins, and functools.partial are examples of commonly used
higher-order functions in the language.
Callables come in seven different flavors in Python, from the simple functions created
with lambda to instances of classes implementing __call__. They can all be detected by
the callable() built-in. Every callable supports the same rich syntax for declaring
formal parameters, including keyword-only parameters and annotations�both new
features introduced with Python 3.
Chapter Summary | 161
Python functions and their annotations have a rich set of attributes that can be read
with the help of the inspect module, which includes the Signature.bind method to
apply the flexible rules that Python uses to bind actual arguments to declared parameters.
Lastly, we covered some functions from the operator module and functools.parti
al, which facilitate functional programming by minimizing the need for the functionally
challenged lambda syntax.
Further Reading
The next two chapters continue our exploration of programming with function objects.
Chapter 6 shows how first-class functions can simplify some classic object-oriented
design patterns, while Chapter 7 dives into function decorators�a special kind of
higher-order function�and the closure mechanism that makes them work.
Chapter 7 of the Python Cookbook, Third Edition (O�Reilly), by David Beazley and Brian
K. Jones, is an excellent complement to the current chapter as well as Chapter 7 of this
book, covering mostly the same concepts with a different approach.
In The Python Language Reference, �3.2. The standard type hierarchy� presents the seven
callable types, along with all the other built-in types.
The Python-3-only features discussed in this chapter have their own PEPs: PEP 3102
� Keyword-Only Arguments and PEP 3107 � Function Annotations.
For more about the current (as of mid-2014) use of annotations, two Stack Overflow
questions are worth reading: �What are good uses for Python3�s �Function Annotations��
has a practical answer and insightful comments by Raymond Hettinger, and the
answer for �What good are Python function annotations?� quotes extensively from
Guido van Rossum.
PEP 362 � Function Signature Object is worth reading if you intend to use the in
spect module that implements that feature.
A great introduction to functional programming in Python is A. M. Kuchling�s Python
Functional Programming HOWTO. The main focus of that text, however, is on the use
of iterators and generators, which are the subject of Chapter 14.
fn.py is a package to support functional programming in Python 2 and 3. According
to its author, Alexey Kachayev, fn.py provides �implementation of missing features to
enjoy FP� in Python. It includes a @recur.tco decorator that implements tail-call optimization
for unlimited recursion in Python, among many other functions, data structures,
and recipes.
162 | Chapter 5: First-Class Functions
The StackOverflow question �Python: Why is functools.partial necessary?� has a highly
informative (and funny) reply by Alex Martelli, author of the classic Python in a Nutshell.
Jim Fulton�s Bobo was probably the first web framework that deserved to be called
object-oriented. If you were intrigued by it and want to learn more about its modern
rewrite, start at its Introduction. A little of the early history of Bobo appears in a comment
by Phillip J. Eby in a discussion at Joel Spolsky�s blog.
Soapbox
About Bobo
I owe my Python career to Bobo. I used it in my first Python web project in 1998. I
discovered Bobo while looking for an object-oriented way to code web applications,
after trying Perl and Java alternatives.
In 1997, Bobo had pioneered the object publishing concept: direct mapping from URLs
to a hierarchy of objects, with no need to configure routes. I was hooked when I saw the
beauty of this. Bobo also featured automatic HTTP query handling based on analysis
of the signatures of the methods or functions used to handle requests.
Bobo was created by Jim Fulton, known as �The Zope Pope� thanks to his leading role
in the development of the Zope framework, the foundation of the Plone CMS, School?
Tool, ERP5, and other large-scale Python projects. Jim is also the creator of ZODB�the
Zope Object Database�a transactional object database that provides ACID (atomicity,
consistency, isolation, and durability), designed for ease of use from Python.
Jim has since rewritten Bobo from scratch to support WSGI and modern Python (including
Python 3). As of this writing, Bobo uses the six library to do the function
introspection, in order to be compatible with Python 2 and Python 3 in spite of the
changes in function objects and related APIs.
Is Python a Functional Language?
Around the year 2000, I was at a training in the United States when Guido van Rossum
dropped by the classroom (he was not the instructor). In the Q&A that followed, somebody
asked him which features of Python were borrowed from other languages. His
answer: �Everything that is good in Python was stolen from other languages.�
Shriram Krishnamurthi, professor of Computer Science at Brown University, starts his
�Teaching Programming Languages in a Post-Linnaean Age� paper with this:
Programming language �paradigms� are a moribund and tedious legacy of a bygone
age. Modern language designers pay them no respect, so why do our courses slavishly
adhere to them?
In that paper, Python is mentioned by name in this passage:
Further Reading | 163
3. There also the problem of lost indentation when pasting code to Web forums, but I digress.
What else to make of a language like Python, Ruby, or Perl? Their designers have no
patience for the niceties of these Linnaean hierarchies; they borrow features as they
wish, creating melanges that utterly defy characterization.
Krishnamurthi submits that instead of trying to classify languages in some taxonomy,
it�s more useful to consider them as aggregations of features.
Even if it was not Guido�s goal, endowing Python with first-class functions opened the
door to functional programming. In his post �Origins of Python�s Functional Features�,
he says that map, filter, and reduce were the motivation for adding lambda to
Python in the first place. All of these features were contributed together by Amrit Prem
for Python 1.0 in 1994 (according to Misc/HISTORY in the CPython source code).
lambda, map, filter, and reduce first appeared in Lisp, the original functional language.
However, Lisp does not limit what can be done inside a lambda, because everything in
Lisp is an expression. Python uses a statement-oriented syntax in which expressions
cannot contain statements, and many language constructs are statements�including
try/catch, which is what I miss most often when writing lambdas. This is the price to
pay for Python�s highly readable syntax.3 Lisp has many strengths, but readability is not
one of them.
Ironically, stealing the list comprehension syntax from another functional language�
Haskell�significantly diminished the need for map and filter, and also for lambda.
Besides the limited anonymous function syntax, the biggest obstacle to wider adoption
of functional programming idioms in Python is the lack of tail-recursion elimination,
an optimization that allows memory-efficient computation of a function that makes a
recursive call at the �tail� of its body. In another blog post, �Tail Recursion Elimination�,
Guido gives several reasons why such optimization is not a good fit for Python.
That post is a great read for the technical arguments, but even more so because the first
three and most important reasons given are usability issues. It is no accident that Python
is a pleasure to use, learn, and teach. Guido made it so.
So there you have it: Python is, by design, not a functional language�whatever that
means. Python just borrows a few good ideas from functional languages.
The Problem with Anonymous Functions
Beyond the Python-specific syntax constraints, anonymous functions have a serious
drawback in every language: they have no name.
I am only half joking here. Stack traces are easier to read when functions have names.
Anonymous functions are a handy shortcut, people have fun coding with them, but
sometimes they get carried away�especially if the language and environment encourage
deep nesting of anonymous functions, like JavaScript on Node.js. Lots of nested anonymous
functions make debugging and error handling hard. Asynchronous programming
164 | Chapter 5: First-Class Functions
in Python is more structured, perhaps because the limited lambda demands it. I promise
to write more about asynchronous programming in the future, but this subject must be
deferred to Chapter 18. By the way, promises, futures, and deferreds are concepts used
in modern asynchronous APIs. Along with coroutines, they provide an escape from the
so-called �callback hell.� We�ll see how callback-free asynchronous programming works
in �From Callbacks to Futures and Coroutines� on page 562.
Further Reading | 165

1. From a slide in the talk �Root Cause Analysis of Some Faults in Design Patterns,� presented by Ralph Johnson
at IME/CCSL, Universidade de Sao Paulo, Nov. 15, 2014.
2. Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, Design Patterns: Elements of Reusable
Object-Oriented Software (Addison-Wesley, 1995), p. 4.
CHAPTER 6
Design Patterns with First-Class Functions
Conformity to patterns is not a measure of goodness.1
� Ralph Johnson
Coauthor of the Design Patterns classic
Although design patterns are language-independent, that does not mean every pattern
applies to every language. In his 1996 presentation, �Design Patterns in Dynamic Languages�,
Peter Norvig states that 16 out of the 23 patterns in the original Design Patterns
book by Gamma et al. become either �invisible or simpler� in a dynamic language (slide
9). He was talking about Lisp and Dylan, but many of the relevant dynamic features are
also present in Python.
The authors of Design Patterns acknowledge in their Introduction that the implementation
language determines which patterns are relevant:
The choice of programming language is important because it influences one�s point of
view. Our patterns assume Smalltalk/C++-level language features, and that choice determines
what can and cannot be implemented easily. If we assumed procedural languages,
we might have included design patterns called �Inheritance,� �Encapsulation,�
and �Polymorphism.� Similarly, some of our patterns are supported directly by the less
common object-oriented languages. CLOS has multi-methods, for example, which lessen
the need for a pattern such as Visitor.2
In particular, in the context of languages with first-class functions, Norvig suggests
rethinking the Strategy, Command, Template Method, and Visitor patterns. The general
idea is: you can replace instances of some participant class in these patterns with simple
functions, reducing a lot of boilerplate code. In this chapter, we will refactor Strategy
167
using function objects, and discuss a similar approach to simplifying the Command
pattern.
Case Study: Refactoring Strategy
Strategy is a good example of a design pattern that can be simpler in Python if you
leverage functions as first-class objects. In the following section, we describe and implement
Strategy using the �classic� structure described in Design Patterns. If you are
familiar with the classic pattern, you can skip to �Function-Oriented Strategy� on page
172 where we refactor the code using functions, significantly reducing the line count.
Classic Strategy
The UML class diagram in Figure 6-1 depicts an arrangement of classes that exemplifies
the Strategy pattern.
Figure 6-1. UML class diagram for order discount processing implemented with the
Strategy design pattern
The Strategy pattern is summarized like this in Design Patterns:
Define a family of algorithms, encapsulate each one, and make them interchangeable.
Strategy lets the algorithm vary independently from clients that use it.
168 | Chapter 6: Design Patterns with First-Class Functions
A clear example of Strategy applied in the ecommerce domain is computing discounts
to orders according to the attributes of the customer or inspection of the ordered items.
Consider an online store with these discount rules:
� Customers with 1,000 or more fidelity points get a global 5% discount per order.
� A 10% discount is applied to each line item with 20 or more units in the same order.
� Orders with at least 10 distinct items get a 7% global discount.
For brevity, let�s assume that only one discount may be applied to an order.
The UML class diagram for the Strategy pattern is depicted in Figure 6-1. Its participants
are:
Context
Provides a service by delegating some computation to interchangeable components
that implement alternative algorithms. In the ecommerce example, the context is
an Order, which is configured to apply a promotional discount according to one of
several algorithms.
Strategy
The interface common to the components that implement the different algorithms.
In our example, this role is played by an abstract class called Promotion.
Concrete Strategy
One of the concrete subclasses of Strategy. FidelityPromo, BulkPromo, and Large
OrderPromo are the three concrete strategies implemented.
The code in Example 6-1 follows the blueprint in Figure 6-1. As described in Design
Patterns, the concrete strategy is chosen by the client of the context class. In our example,
before instantiating an order, the system would somehow select a promotional discount
strategy and pass it to the Order constructor. The selection of the strategy is outside of
the scope of the pattern.
Example 6-1. Implementation Order class with pluggable discount strategies
from abc import ABC, abstractmethod
from collections import namedtuple
Customer = namedtuple('Customer', 'name fidelity')
class LineItem:
def __init__(self, product, quantity, price):
self.product = product
self.quantity = quantity
self.price = price
Case Study: Refactoring Strategy | 169
def total(self):
return self.price * self.quantity
class Order: # the Context
def __init__(self, customer, cart, promotion=None):
self.customer = customer
self.cart = list(cart)
self.promotion = promotion
def total(self):
if not hasattr(self, '__total'):
self.__total = sum(item.total() for item in self.cart)
return self.__total
def due(self):
if self.promotion is None:
discount = 0
else:
discount = self.promotion.discount(self)
return self.total() - discount
def __repr__(self):
fmt = '<Order total: {:.2f} due: {:.2f}>'
return fmt.format(self.total(), self.due())
class Promotion(ABC): # the Strategy: an abstract base class
@abstractmethod
def discount(self, order):
"""Return discount as a positive dollar amount"""
class FidelityPromo(Promotion): # first Concrete Strategy
"""5% discount for customers with 1000 or more fidelity points"""
def discount(self, order):
return order.total() * .05 if order.customer.fidelity >= 1000 else 0
class BulkItemPromo(Promotion): # second Concrete Strategy
"""10% discount for each LineItem with 20 or more units"""
def discount(self, order):
discount = 0
for item in order.cart:
if item.quantity >= 20:
discount += item.total() * .1
return discount
170 | Chapter 6: Design Patterns with First-Class Functions
class LargeOrderPromo(Promotion): # third Concrete Strategy
"""7% discount for orders with 10 or more distinct items"""
def discount(self, order):
distinct_items = {item.product for item in order.cart}
if len(distinct_items) >= 10:
return order.total() * .07
return 0
Note that in Example 6-1, I coded Promotion as an abstract base class (ABC), to be able
to use the @abstractmethod decorator, thus making the pattern more explicit.
In Python 3.4, the simplest way to declare an ABC is to subclass
abc.ABC, as I did in Example 6-1. From Python 3.0 to 3.3, you must
use the metaclass= keyword in the class statement (e.g., class
Promotion(metaclass=ABCMeta):).
Example 6-2 shows doctests used to demonstrate and verify the operation of a module
implementing the rules described earlier.
Example 6-2. Sample usage of Order class with different promotions applied
>>> joe = Customer('John Doe', 0)
>>> ann = Customer('Ann Smith', 1100)
>>> cart = [LineItem('banana', 4, .5),
... LineItem('apple', 10, 1.5),
... LineItem('watermellon', 5, 5.0)]
>>> Order(joe, cart, FidelityPromo())
<Order total: 42.00 due: 42.00>
>>> Order(ann, cart, FidelityPromo())
<Order total: 42.00 due: 39.90>
>>> banana_cart = [LineItem('banana', 30, .5),
... LineItem('apple', 10, 1.5)]
>>> Order(joe, banana_cart, BulkItemPromo())
<Order total: 30.00 due: 28.50>
>>> long_order = [LineItem(str(item_code), 1, 1.0)
... for item_code in range(10)]
>>> Order(joe, long_order, LargeOrderPromo())
<Order total: 10.00 due: 9.30>
>>> Order(joe, cart, LargeOrderPromo())
<Order total: 42.00 due: 42.00>
Two customers: joe has 0 fidelity points, ann has 1,100.
One shopping cart with three line items.
The FidelityPromo promotion gives no discount to joe.
Case Study: Refactoring Strategy | 171
ann gets a 5% discount because she has at least 1,000 points.
The banana_cart has 30 units of the "banana" product and 10 apples.
Thanks to the BulkItemPromo, joe gets a $1.50 discount on the bananas.
long_order has 10 different items at $1.00 each.
joe gets a 7% discount on the whole order because of LargerOrderPromo.
Example 6-1 works perfectly well, but the same functionality can be implemented with
less code in Python by using functions as objects. The next section shows how.
Function-Oriented Strategy
Each concrete strategy in Example 6-1 is a class with a single method, discount. Furthermore,
the strategy instances have no state (no instance attributes). You could say
they look a lot like plain functions, and you would be right. Example 6-3 is a refactoring
of Example 6-1, replacing the concrete strategies with simple functions and removing
the Promo abstract class.
Example 6-3. Order class with discount strategies implemented as functions
from collections import namedtuple
Customer = namedtuple('Customer', 'name fidelity')
class LineItem:
def __init__(self, product, quantity, price):
self.product = product
self.quantity = quantity
self.price = price
def total(self):
return self.price * self.quantity
class Order: # the Context
def __init__(self, customer, cart, promotion=None):
self.customer = customer
self.cart = list(cart)
self.promotion = promotion
def total(self):
if not hasattr(self, '__total'):
self.__total = sum(item.total() for item in self.cart)
return self.__total
def due(self):
172 | Chapter 6: Design Patterns with First-Class Functions
if self.promotion is None:
discount = 0
else:
discount = self.promotion(self)
return self.total() - discount
def __repr__(self):
fmt = '<Order total: {:.2f} due: {:.2f}>'
return fmt.format(self.total(), self.due())
def fidelity_promo(order):
"""5% discount for customers with 1000 or more fidelity points"""
return order.total() * .05 if order.customer.fidelity >= 1000 else 0
def bulk_item_promo(order):
"""10% discount for each LineItem with 20 or more units"""
discount = 0
for item in order.cart:
if item.quantity >= 20:
discount += item.total() * .1
return discount
def large_order_promo(order):
"""7% discount for orders with 10 or more distinct items"""
distinct_items = {item.product for item in order.cart}
if len(distinct_items) >= 10:
return order.total() * .07
return 0
To compute a discount, just call the self.promotion() function.
No abstract class.
Each strategy is a function.
The code in Example 6-3 is 12 lines shorter than Example 6-1. Using the new Order is
also a bit simpler, as shown in the Example 6-4 doctests.
Example 6-4. Sample usage of Order class with promotions as functions
>>> joe = Customer('John Doe', 0)
>>> ann = Customer('Ann Smith', 1100)
>>> cart = [LineItem('banana', 4, .5),
... LineItem('apple', 10, 1.5),
... LineItem('watermellon', 5, 5.0)]
>>> Order(joe, cart, fidelity_promo)
<Order total: 42.00 due: 42.00>
>>> Order(ann, cart, fidelity_promo)
<Order total: 42.00 due: 39.90>
Case Study: Refactoring Strategy | 173
3. See page 323 of Design Patterns.
4. idem, p. 196
>>> banana_cart = [LineItem('banana', 30, .5),
... LineItem('apple', 10, 1.5)]
>>> Order(joe, banana_cart, bulk_item_promo)
<Order total: 30.00 due: 28.50>
>>> long_order = [LineItem(str(item_code), 1, 1.0)
... for item_code in range(10)]
>>> Order(joe, long_order, large_order_promo)
<Order total: 10.00 due: 9.30>
>>> Order(joe, cart, large_order_promo)
<Order total: 42.00 due: 42.00>
Same test fixtures as Example 6-1.
To apply a discount strategy to an Order, just pass the promotion function as an
argument.
A different promotion function is used here and in the next test.
Note the callouts in Example 6-4: there is no need to instantiate a new promotion object
with each new order: the functions are ready to use.
It is interesting to note that in Design Patterns the authors suggest: �Strategy objects
often make good flyweights.�3 A definition of the Flyweight in another part of that work
states: �A flyweight is a shared object that can be used in multiple contexts simultaneously.�
4 The sharing is recommended to reduce the cost of creating a new concrete
strategy object when the same strategy is applied over and over again with every new
context�with every new Order instance, in our example. So, to overcome a drawback
of the Strategy pattern�its runtime cost�the authors recommend applying yet another
pattern. Meanwhile, the line count and maintenance cost of your code are piling up.
A thornier use case, with complex concrete strategies holding internal state, may require
all the pieces of the Strategy and Flyweight design patterns combined. But often concrete
strategies have no internal state; they only deal with data from the context. If that is the
case, then by all means use plain old functions instead of coding single-method classes
implementing a single-method interface declared in yet another class. A function is
more lightweight than an instance of a user-defined class, and there is no need for
Flyweight because each strategy function is created just once by Python when it compiles
the module. A plain function is also �a shared object that can be used in multiple contexts
simultaneously.�
Now that we have implemented the Strategy pattern with functions, other possibilities
emerge. Suppose you want to create a �meta-strategy� that selects the best available
discount for a given Order. In the following sections, we present additional refactorings
174 | Chapter 6: Design Patterns with First-Class Functions
that implement this requirement using a variety of approaches that leverage functions
and modules as objects.
Choosing the Best Strategy: Simple Approach
Given the same customers and shopping carts from the tests in Example 6-4, we now
add three additional tests in Example 6-5.
Example 6-5. The best_promo function applies all discounts and returns the largest
>>> Order(joe, long_order, best_promo)
<Order total: 10.00 due: 9.30>
>>> Order(joe, banana_cart, best_promo)
<Order total: 30.00 due: 28.50>
>>> Order(ann, cart, best_promo)
<Order total: 42.00 due: 39.90>
best_promo selected the larger_order_promo for customer joe.
Here joe got the discount from bulk_item_promo for ordering lots of bananas.
Checking out with a simple cart, best_promo gave loyal customer ann the
discount for the fidelity_promo.
The implementation of best_promo is very simple. See Example 6-6.
Example 6-6. best_promo finds the maximum discount iterating over a list of functions
promos = [fidelity_promo, bulk_item_promo, large_order_promo]
def best_promo(order):
"""Select best discount available
"""
return max(promo(order) for promo in promos)
promos: list of the strategies implemented as functions.
best_promo takes an instance of Order as argument, as do the other *_promo
functions.
Using a generator expression, we apply each of the functions from promos to the
order, and return the maximum discount computed.
Example 6-6 is straightforward: promos is a list of functions. Once you get used to the
idea that functions are first-class objects, it naturally follows that building data structures
holding functions often makes sense.
Although Example 6-6 works and is easy to read, there is some duplication that could
lead to a subtle bug: to add a new promotion strategy, we need to code the function and
Case Study: Refactoring Strategy | 175
remember to add it to the promos list, or else the new promotion will work when explicitly
passed as an argument to Order, but will not be considered by best_promotion.
Read on for a couple of solutions to this issue.
Finding Strategies in a Module
Modules in Python are also first-class objects, and the standard library provides several
functions to handle them. The built-in globals is described as follows in the Python
docs:
globals()
Return a dictionary representing the current global symbol table. This is always the
dictionary of the current module (inside a function or method, this is the module
where it is defined, not the module from which it is called).
Example 6-7 is a somewhat hackish way of using globals to help best_promo automatically
find the other available *_promo functions.
Example 6-7. The promos list is built by introspection of the module global namespace
promos = [globals()[name] for name in globals()
if name.endswith('_promo')
and name != 'best_promo']
def best_promo(order):
"""Select best discount available
"""
return max(promo(order) for promo in promos)
Iterate over each name in the dictionary returned by globals().
Select only names that end with the _promo suffix.
Filter out best_promo itself, to avoid an infinite recursion.
No changes inside best_promo.
Another way of collecting the available promotions would be to create a module and
put all the strategy functions there, except for best_promo.
In Example 6-8, the only significant change is that the list of strategy functions is built
by introspection of a separate module called promotions. Note that Example 6-8 depends
on importing the promotions module as well as inspect, which provides highlevel
introspection functions (the imports are not shown for brevity, because they would
normally be at the top of the file).
176 | Chapter 6: Design Patterns with First-Class Functions
Example 6-8. The promos list is built by introspection of a new promotions module
promos = [func for name, func in
inspect.getmembers(promotions, inspect.isfunction)]
def best_promo(order):
"""Select best discount available
"""
return max(promo(order) for promo in promos)
The function inspect.getmembers returns the attributes of an object�in this case, the
promotions module�optionally filtered by a predicate (a boolean function). We use
inspect.isfunction to get only the functions from the module.
Example 6-8 works regardless of the names given to the functions; all that matters is
that the promotions module contains only functions that calculate discounts given orders.
Of course, this is an implicit assumption of the code. If someone were to create a
function with a different signature in the promotions module, then best_promo would
break while trying to apply it to an order.
We could add more stringent tests to filter the functions, by inspecting their arguments
for instance. The point of Example 6-8 is not to offer a complete solution, but to highlight
one possible use of module introspection.
A more explicit alternative for dynamically collecting promotional discount functions
would be to use a simple decorator. We�ll show yet another version of our ecommerce
Strategy example in Chapter 7, which deals with function decorators.
In the next section, we discuss Command�another design pattern that is sometimes
implemented via single-method classes when plain functions would do.
Command
Command is another design pattern that can be simplified by the use of functions passed
as arguments. Figure 6-2 shows the arrangement of classes in the Command pattern.
Command | 177
Figure 6-2. UML class diagram for menu-driven text editor implemented with the
Command design pattern. Each command may have a different receiver: the object that
implements the action. For PasteCommand, the receiver is the Document. For Open?
Command, the receiver is the application.
The goal of Command is to decouple an object that invokes an operation (the Invoker)
from the provider object that implements it (the Receiver). In the example from Design
Patterns, each invoker is a menu item in a graphical application, and the receivers are
the document being edited or the application itself.
The idea is to put a Command object between the two, implementing an interface with a
single method, execute, which calls some method in the Receiver to perform the desired
operation. That way the Invoker does not need to know the interface of the Receiver,
and different receivers can be adapted through different Command subclasses. The Invoker
is configured with a concrete command and calls its execute method to operate
it. Note in Figure 6-2 that MacroCommand may store a sequence of commands; its
execute() method calls the same method in each command stored.
Quoting from Gamma et al., �Commands are an object-oriented replacement for callbacks.�
The question is: do we need an object-oriented replacement for callbacks?
Sometimes yes, but not always.
Instead of giving the Invoker a Command instance, we can simply give it a function.
Instead of calling command.execute(), the Invoker can just call command(). The Macro
Command can be implemented with a class implementing __call__. Instances of Macro
Command would be callables, each holding a list of functions for future invocation, as
implemented in Example 6-9.
178 | Chapter 6: Design Patterns with First-Class Functions
Example 6-9. Each instance of MacroCommand has an internal list of commands
class MacroCommand:
"""A command that executes a list of commands"""
def __init__(self, commands):
self.commands = list(commands) #
def __call__(self):
for command in self.commands: #
command()
Building a list from the commands arguments ensures that it is iterable and keeps
a local copy of the command references in each MacroCommand instance.
When an instance of MacroCommand is invoked, each command in self.com
mands is called in sequence.
More advanced uses of the Command pattern�to support undo, for example�may
require more than a simple callback function. Even then, Python provides a couple of
alternatives that deserve consideration:
� A callable instance like MacroCommand in Example 6-9 can keep whatever state is
necessary, and provide extra methods in addition to __call__.
� A closure can be used to hold the internal state of a function between calls.
This concludes our rethinking of the Command pattern with first-class functions. At a
high level, the approach here was similar to the one we applied to Strategy: replacing
with callables the instances of a participant class that implemented a single-method
interface. After all, every Python callable implements a single-method interface, and
that method is named __call__.
Chapter Summary
As Peter Norvig pointed out a couple of years after the classic Design Patterns book
appeared, �16 of 23 patterns have qualitatively simpler implementation in Lisp or Dylan
than in C++ for at least some uses of each pattern� (slide 9 of Norvig�s �Design Patterns
in Dynamic Languages� presentation). Python shares some of the dynamic features of
the Lisp and Dylan languages, in particular first-class functions, our focus in this part
of the book.
From the same talk quoted at the start of this chapter, in reflecting on the 20th anniversary
of Design Patterns: Elements of Reusable Object-Oriented Software, Ralph Johnson
has stated that one of the failings of the book is �Too much emphasis on patterns
Chapter Summary | 179
5. From the same talk quoted at the start of this chapter: �Root Cause Analysis of Some Faults in Design Patterns,�
presented by Johnson at IME-USP, November 15, 2014.
as end-points instead of steps in the design patterns.�5 In this chapter, we used the Strategy
pattern as a starting point: a working solution that we could simplify using firstclass
functions.
In many cases, functions or callable objects provide a more natural way of implementing
callbacks in Python than mimicking the Strategy or the Command patterns as described
by Gamma, Helm, Johnson, and Vlissides. The refactoring of Strategy and the discussion
of Command in this chapter are examples of a more general insight: sometimes you
may encounter a design pattern or an API that requires that components implement an
interface with a single method, and that method has a generic-sounding name such as
�execute�, �run�, or �doIt�. Such patterns or APIs often can be implemented with less
boilerplate code in Python using first-class functions or other callables.
The message from Peter Norvig�s design patterns slides is that the Command and Strategy
patterns�along with Template Method and Visitor�can be made simpler or even
�invisible� with first-class functions, at least for some applications of these patterns.
Further Reading
Our discussion of Strategy ended with a suggestion that function decorators could be
used to improve on Example 6-8. We also mentioned the use of closures a couple of
times in this chapter. Decorators as well as closures are the focus of Chapter 7. That
chapter starts with a refactoring of the ecommerce example using a decorator to register
available promotions.
�Recipe 8.21. Implementing the Visitor Pattern,� in the Python Cookbook, Third Edition
(O�Reilly), by David Beazley and Brian K. Jones, presents an elegant implementation
of the Visitor pattern in which a NodeVisitor class handles methods as first-class objects.
On the general topic of design patterns, the choice of readings for the Python programmer
is not as broad as what is available to other language communities.
As far as I know, Learning Python Design Patterns, by Gennadiy Zlobin (Packt), is the
only book entirely devoted to patterns in Python�as of June 2014. But Zlobin�s work is
quite short (100 pages) and covers eight of the original 23 design patterns.
Expert Python Programming by Tarek Ziade (Packt) is one of the best intermediate-level
Python books in the market, and its final chapter, �Useful Design Patterns,� presents
seven of the classic patterns from a Pythonic perspective.
180 | Chapter 6: Design Patterns with First-Class Functions
Alex Martelli has given several talks about Python Design Patterns. There is a video of
his EuroPython 2011 presentation and a set of slides on his personal website. I�ve found
different slide decks and videos over the years, of varying lengths, so it is worthwhile to
do a thorough search for his name with the words �Python Design Patterns.�
Around 2008, Bruce Eckel�author of the excellent Thinking in Java (Prentice Hall)�
started a book titled Python 3 Patterns, Recipes and Idioms. It was to be written by a
community of contributors led by Eckel, but six years later it�s still incomplete and
apparently stalled (as I write this, the last change to the repository is two years old).
There are many books about design patterns in the context of Java, but among them the
one I like most is Head First Design Patterns by Eric Freeman, Bert Bates, Kathy Sierra,
and Elisabeth Robson (O�Reilly). It explains 16 of the 23 classic patterns. If you like the
wacky style of the Head First series and need an introduction to this topic, you will love
that work. However, it is Java-centric.
For a fresh look at patterns from the point of view of a dynamic language with duck
typing and first-class functions, Design Patterns in Ruby by Russ Olsen (Addison-
Wesley) has many insights that are also applicable to Python. In spite of many the syntactic
differences, at the semantic level Python and Ruby are closer to each other than
to Java or C++.
In Design Patterns in Dynamic Languages (slides), Peter Norvig shows how first-class
functions (and other dynamic features) make several of the original design patterns
either simpler or unnecessary.
Of course, the original Design Patterns book by Gamma et al. is mandatory reading if
you are serious about this subject. The Introduction by itself is worth the price. That is
the source of the often quoted design principles �Program to an interface, not an implementation�
and �Favor object composition over class inheritance.�
Soapbox
Python has first-class functions and first-class types, features that Norvig claims affect
10 of the 23 patterns (slide 10 of Design Patterns in Dynamic Languages). In the next
chapter, we�ll see that Python also has generic functions (�Generic Functions with Single
Dispatch� on page 202), similar to the CLOS multimethods that Gamma et al. suggest as
a simpler way to implement the classic Visitor pattern. Norvig, on the other hand, says
that multimethods simplify the Builder pattern (slide 10). Matching design patterns to
language features is not an exact science.
In classrooms around the world, design patterns are frequently taught using Java examples.
I�ve heard more than one student claim that they were led to believe that the
original design patterns are useful in any implementation language. It turns out that the
�classic� 23 patterns from the Gamma et al. book apply to �classic� Java very well in spite
of being originally presented mostly in the context of C++�a few have Smalltalk ex?
Further Reading | 181
amples in the book. But that does not mean every one of those patterns applies equally
well in any language. The authors are explicit right at the beginning of their book that
�some of our patterns are supported directly by the less common object-oriented languages�
(recall full quote on first page of this chapter).
The Python bibliography about design patterns is very thin, compared to that of Java,
C++, or Ruby. In �Further Reading� on page 180 I mentioned Learning Python Design
Patterns by Gennadiy Zlobin, which was published as recently as November 2013. In
contrast, Russ Olsen�s Design Patterns in Ruby was published in 2007 and has 384 pages
�284 more than Zlobin�s work.
Now that Python is becoming increasingly popular in academia, let�s hope more will be
written about design patterns in the context of this language. Also, Java 8 introduced
method references and anonymous functions, and those highly anticipated features are
likely to prompt fresh approaches to patterns in Java�recognizing that as languages
evolve, so must our understanding of how to apply the classic design patterns.
182 | Chapter 6: Design Patterns with First-Class Functions
1. That�s the 1995 Design Patterns book by the so-called Gang of Four.
CHAPTER 7
Function Decorators and Closures
There�s been a number of complaints about the choice of the name �decorator� for this
feature. The major one is that the name is not consistent with its use in the GoF book.1
The name decorator probably owes more to its use in the compiler area�a syntax tree is
walked and annotated.
� PEP 318 � Decorators for Functions and Methods
Function decorators let us �mark� functions in the source code to enhance their behavior
in some way. This is powerful stuff, but mastering it requires understanding closures.
One of the newest reserved keywords in Python is nonlocal, introduced in Python 3.0.
You can have a profitable life as a Python programmer without ever using it if you adhere
to a strict regimen of class-centered object orientation. However, if you want to implement
your own function decorators, you must know closures inside out, and then the
need for nonlocal becomes obvious.
Aside from their application in decorators, closures are also essential for effective asynchronous
programming with callbacks, and for coding in a functional style whenever
it makes sense.
The end goal of this chapter is to explain exactly how function decorators work, from
the simplest registration decorators to the rather more complicated parameterized ones.
However, before we reach that goal we need to cover:
� How Python evaluates decorator syntax
� How Python decides whether a variable is local
� Why closures exist and how they work
183
2. Python also supports class decorators. They are covered in Chapter 21.
� What problem is solved by nonlocal
With this grounding, we can tackle further decorator topics:
� Implementing a well-behaved decorator
� Interesting decorators in the standard library
� Implementing a parameterized decorator
We start with a very basic introduction to decorators, and then proceed with the rest of
the items listed here.
Decorators 101
A decorator is a callable that takes another function as argument (the decorated function).
2 The decorator may perform some processing with the decorated function, and
returns it or replaces it with another function or callable object.
In other words, assuming an existing decorator named decorate, this code:
@decorate
def target():
print('running target()')
Has the same effect as writing this:
def target():
print('running target()')
target = decorate(target)
The end result is the same: at the end of either of these snippets, the target name does
not necessarily refer to the original target function, but to whatever function is returned
by decorate(target).
To confirm that the decorated function is replaced, see the console session in
Example 7-1.
Example 7-1. A decorator usually replaces a function with a different one
>>> def deco(func):
... def inner():
... print('running inner()')
... return inner
...
>>> @deco
... def target():
184 | Chapter 7: Function Decorators and Closures
... print('running target()')
...
>>> target()
running inner()
>>> target
<function deco.<locals>.inner at 0x10063b598>
deco returns its inner function object.
target is decorated by deco.
Invoking the decorated target actually runs inner.
Inspection reveals that target is a now a reference to inner.
Strictly speaking, decorators are just syntactic sugar. As we just saw, you can always
simply call a decorator like any regular callable, passing another function. Sometimes
that is actually convenient, especially when doing metaprogramming�changing program
behavior at runtime.
To summarize: the first crucial fact about decorators is that they have the power to
replace the decorated function with a different one. The second crucial fact is that they
are executed immediately when a module is loaded. This is explained next.
When Python Executes Decorators
A key feature of decorators is that they run right after the decorated function is defined.
That is usually at import time (i.e., when a module is loaded by Python). Consider
registration.py in Example 7-2.
Example 7-2. The registration.py module
registry = []
def register(func):
print('running register(%s)' % func)
registry.append(func)
return func
@register
def f1():
print('running f1()')
@register
def f2():
print('running f2()')
def f3():
print('running f3()')
def main():
When Python Executes Decorators | 185
print('running main()')
print('registry ->', registry)
f1()
f2()
f3()
if __name__=='__main__':
main()
registry will hold references to functions decorated by @register.
register takes a function as argument.
Display what function is being decorated, for demonstration.
Include func in registry.
Return func: we must return a function; here we return the same received as
argument.
f1 and f2 are decorated by @register.
f3 is not decorated.
main displays the registry, then calls f1(), f2(), and f3().
main() is only invoked if registration.py runs as a script.
The output of running registration.py as a script looks like this:
$ python3 registration.py
running register(<function f1 at 0x100631bf8>)
running register(<function f2 at 0x100631c80>)
running main()
registry -> [<function f1 at 0x100631bf8>, <function f2 at 0x100631c80>]
running f1()
running f2()
running f3()
Note that register runs (twice) before any other function in the module. When reg
ister is called, it receives as an argument the function object being decorated�for
example, <function f1 at 0x100631bf8>.
After the module is loaded, the registry holds references to the two decorated functions:
f1 and f2. These functions, as well as f3, are only executed when explicitly called
by main.
If registration.py is imported (and not run as a script), the output is this:
>>> import registration
running register(<function f1 at 0x10063b1e0>)
running register(<function f2 at 0x10063b268>)
At this time, if you look at the registry, here is what you get:
186 | Chapter 7: Function Decorators and Closures
>>> registration.registry
[<function f1 at 0x10063b1e0>, <function f2 at 0x10063b268>]
The main point of Example 7-2 is to emphasize that function decorators are executed
as soon as the module is imported, but the decorated functions only run when they are
explicitly invoked. This highlights the difference between what Pythonistas call import
time and runtime.
Considering how decorators are commonly employed in real code, Example 7-2 is unusual
in two ways:
� The decorator function is defined in the same module as the decorated functions.
A real decorator is usually defined in one module and applied to functions in other
modules.
� The register decorator returns the same function passed as argument. In practice,
most decorators define an inner function and return it.
Even though the register decorator in Example 7-2 returns the decorated function
unchanged, that technique is not useless. Similar decorators are used in many Python
web frameworks to add functions to some central registry�for example, a registry
mapping URL patterns to functions that generate HTTP responses. Such registration
decorators may or may not change the decorated function. The next section shows a
practical example.
Decorator-Enhanced Strategy Pattern
A registration decorator is a good enhancement to the ecommerce promotional discount
from �Case Study: Refactoring Strategy� on page 168.
Recall that our main issue with Example 6-6 is the repetition of the function names in
their definitions and then in the promos list used by the best_promo function to determine
the highest discount applicable. The repetition is problematic because someone
may add a new promotional strategy function and forget to manually add it to the promos
list�in which case, best_promo will silently ignore the new strategy, introducing a subtle
bug in the system. Example 7-3 solves this problem with a registration decorator.
Example 7-3. The promos list is filled by the promotion decorator
promos = []
def promotion(promo_func):
promos.append(promo_func)
return promo_func
@promotion
def fidelity(order):
"""5% discount for customers with 1000 or more fidelity points"""
Decorator-Enhanced Strategy Pattern | 187
return order.total() * .05 if order.customer.fidelity >= 1000 else 0
@promotion
def bulk_item(order):
"""10% discount for each LineItem with 20 or more units"""
discount = 0
for item in order.cart:
if item.quantity >= 20:
discount += item.total() * .1
return discount
@promotion
def large_order(order):
"""7% discount for orders with 10 or more distinct items"""
distinct_items = {item.product for item in order.cart}
if len(distinct_items) >= 10:
return order.total() * .07
return 0
def best_promo(order):
"""Select best discount available
"""
return max(promo(order) for promo in promos)
The promos list starts empty.
promotion decorator returns promo_func unchanged, after adding it to the
promos list.
Any function decorated by @promotion will be added to promos.
No changes needed to best_promos, because it relies on the promos list.
This solution has several advantages over the others presented in �Case Study: Refactoring
Strategy� on page 168:
� The promotion strategy functions don�t have to use special names (i.e., they don�t
need to use the _promo suffix).
� The @promotion decorator highlights the purpose of the decorated function, and
also makes it easy to temporarily disable a promotion: just comment out the decorator.
� Promotional discount strategies may be defined in other modules, anywhere in the
system, as long as the @promotion decorator is applied to them.
Most decorators do change the decorated function. They usually do it by defining an
inner function and returning it to replace the decorated function. Code that uses inner
functions almost always depends on closures to operate correctly. To understand clo?
188 | Chapter 7: Function Decorators and Closures
sures, we need to take a step back a have a close look at how variable scopes work in
Python.
Variable Scope Rules
In Example 7-4, we define and test a function that reads two variables: a local variable
a, defined as function parameter, and variable b that is not defined anywhere in the
function.
Example 7-4. Function reading a local and a global variable
>>> def f1(a):
... print(a)
... print(b)
...
>>> f1(3)
3
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 3, in f1
NameError: global name 'b' is not defined
The error we got is not surprising. Continuing from Example 7-4, if we assign a value
to a global b and then call f1, it works:
>>> b = 6
>>> f1(3)
3
6
Now, let�s see an example that may surprise you.
Take a look at the f2 function in Example 7-5. Its first two lines are the same as f1 in
Example 7-4, then it makes an assignment to b, and prints its value. But it fails at the
second print, before the assignment is made.
Example 7-5. Variable b is local, because it is assigned a value in the body of the function
>>> b = 6
>>> def f2(a):
... print(a)
... print(b)
... b = 9
...
>>> f2(3)
3
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 3, in f2
UnboundLocalError: local variable 'b' referenced before assignment
Variable Scope Rules | 189
Note that the output starts with 3, which proves that the print(a) statement was executed.
But the second one, print(b), never runs. When I first saw this I was surprised,
thinking that 6 should be printed, because there is a global variable b and the assignment
to the local b is made after print(b).
But the fact is, when Python compiles the body of the function, it decides that b is a local
variable because it is assigned within the function. The generated bytecode reflects this
decision and will try to fetch b from the local environment. Later, when the call f2(3)
is made, the body of f2 fetches and prints the value of the local variable a, but when
trying to fetch the value of local variable b it discovers that b is unbound.
This is not a bug, but a design choice: Python does not require you to declare variables,
but assumes that a variable assigned in the body of a function is local. This is much
better than the behavior of JavaScript, which does not require variable declarations
either, but if you do forget to declare that a variable is local (with var), you may clobber
a global variable without knowing.
If we want the interpreter to treat b as a global variable in spite of the assignment within
the function, we use the global declaration:
>>> def f3(a):
... global b
... print(a)
... print(b)
... b = 9
...
>>> f3(3)
3
6
>>> b
9
>>> f3(3)
a = 3
b = 8
b = 30
>>> b
30
>>>
After this closer look at how variable scopes work in Python, we can tackle closures in
the next section, �Closures� on page 192. If you are curious about the bytecode differences
between the functions in Examples 7-4 and 7-5, see the following sidebar.
190 | Chapter 7: Function Decorators and Closures
Comparing Bytecodes
The dis module provides an easy way to disassemble the bytecode of Python functions.
Read Examples 7-6 and 7-7 to see the bytecodes for f1 and f2 from Examples 7-4 and 7-5.
Example 7-6. Disassembly of the f1 function from Example 7-4
>>> from dis import dis
>>> dis(f1)
2 0 LOAD_GLOBAL 0 (print)
3 LOAD_FAST 0 (a)
6 CALL_FUNCTION 1 (1 positional, 0 keyword pair)
9 POP_TOP
3 10 LOAD_GLOBAL 0 (print)
13 LOAD_GLOBAL 1 (b)
16 CALL_FUNCTION 1 (1 positional, 0 keyword pair)
19 POP_TOP
20 LOAD_CONST 0 (None)
23 RETURN_VALUE
Load global name print.
Load local name a.
Load global name b.
Contrast the bytecode for f1 shown in Example 7-6 with the bytecode for f2 in
Example 7-7.
Example 7-7. Disassembly of the f2 function from Example 7-5
>>> dis(f2)
2 0 LOAD_GLOBAL 0 (print)
3 LOAD_FAST 0 (a)
6 CALL_FUNCTION 1 (1 positional, 0 keyword pair)
9 POP_TOP
3 10 LOAD_GLOBAL 0 (print)
13 LOAD_FAST 1 (b)
16 CALL_FUNCTION 1 (1 positional, 0 keyword pair)
19 POP_TOP
4 20 LOAD_CONST 1 (9)
23 STORE_FAST 1 (b)
26 LOAD_CONST 0 (None)
29 RETURN_VALUE
Variable Scope Rules | 191
Load local name b. This shows that the compiler considers b a local variable,
even if the assignment to b occurs later, because the nature of the variable�
whether it is local or not�cannot change the body of the function.
The CPython VM that runs the bytecode is a stack machine, so the operations LOAD and
POP refer to the stack. It is beyond the scope of this book to further describe the Python
opcodes, but they are documented along with the dis module in dis � Disassembler
for Python bytecode.
Closures
In the blogosphere, closures are sometimes confused with anonymous functions. The
reason why many confuse them is historic: defining functions inside functions is not so
common, until you start using anonymous functions. And closures only matter when
you have nested functions. So a lot of people learn both concepts at the same time.
Actually, a closure is a function with an extended scope that encompasses nonglobal
variables referenced in the body of the function but not defined there. It does not matter
whether the function is anonymous or not; what matters is that it can access nonglobal
variables that are defined outside of its body.
This is a challenging concept to grasp, and is better approached through an example.
Consider an avg function to compute the mean of an ever-increasing series of values;
for example, the average closing price of a commodity over its entire history. Every day
a new price is added, and the average is computed taking into account all prices so far.
Starting with a clean slate, this is how avg could be used:
>>> avg(10)
10.0
>>> avg(11)
10.5
>>> avg(12)
11.0
Where does avg come from, and where does it keep the history of previous values?
For starters, Example 7-8 is a class-based implementation.
Example 7-8. average_oo.py: A class to calculate a running average
class Averager():
def __init__(self):
self.series = []
def __call__(self, new_value):
192 | Chapter 7: Function Decorators and Closures
self.series.append(new_value)
total = sum(self.series)
return total/len(self.series)
The Averager class creates instances that are callable:
>>> avg = Averager()
>>> avg(10)
10.0
>>> avg(11)
10.5
>>> avg(12)
11.0
Now, Example 7-9 is a functional implementation, using the higher-order function
make_averager.
Example 7-9. average.py: A higher-order function to calculate a running average
def make_averager():
series = []
def averager(new_value):
series.append(new_value)
total = sum(series)
return total/len(series)
return averager
When invoked, make_averager returns an averager function object. Each time an
averager is called, it appends the passed argument to the series, and computes the
current average, as shown in Example 7-10.
Example 7-10. Testing Example 7-9
>>> avg = make_averager()
>>> avg(10)
10.0
>>> avg(11)
10.5
>>> avg(12)
11.0
Note the similarities of the examples: we call Averager() or make_averager() to get a
callable object avg that will update the historical series and calculate the current mean.
In Example 7-8, avg is an instance of Averager, and in Example 7-9 it is the inner
function, averager. Either way, we just call avg(n) to include n in the series and get the
updated mean.
Closures | 193
It�s obvious where the avg of the Averager class keeps the history: the self.series
instance attribute. But where does the avg function in the second example find the
series?
Note that series is a local variable of make_averager because the initialization series
= [] happens in the body of that function. But when avg(10) is called, make_averager
has already returned, and its local scope is long gone.
Within averager, series is a free variable. This is a technical term meaning a variable
that is not bound in the local scope. See Figure 7-1.
Figure 7-1. The closure for averager extends the scope of that function to include the
binding for the free variable series.
Inspecting the returned averager object shows how Python keeps the names of local
and free variables in the __code__ attribute that represents the compiled body of the
function. Example 7-11 demonstrates.
Example 7-11. Inspecting the function created by make_averager in Example 7-9
>>> avg.__code__.co_varnames
('new_value', 'total')
>>> avg.__code__.co_freevars
('series',)
The binding for series is kept in the __closure__ attribute of the returned function
avg. Each item in avg.__closure__ corresponds to a name in avg.__code__.co_free
vars. These items are cells, and they have an attribute called cell_contents where
the actual value can be found. Example 7-12 shows these attributes.
Example 7-12. Continuing from Example 7-10
>>> avg.__code__.co_freevars
('series',)
>>> avg.__closure__
(<cell at 0x107a44f78: list object at 0x107a91a48>,)
>>> avg.__closure__[0].cell_contents
[10, 11, 12]
194 | Chapter 7: Function Decorators and Closures
To summarize: a closure is a function that retains the bindings of the free variables that
exist when the function is defined, so that they can be used later when the function is
invoked and the defining scope is no longer available.
Note that the only situation in which a function may need to deal with external variables
that are nonglobal is when it is nested in another function.
The nonlocal Declaration
Our previous implementation of make_averager was not efficient. In Example 7-9, we
stored all the values in the historical series and computed their sum every time averager
was called. A better implementation would just store the total and the number of items
so far, and compute the mean from these two numbers.
Example 7-13 is a broken implementation, just to make a point. Can you see where it
breaks?
Example 7-13. A broken higher-order function to calculate a running average without
keeping all history
def make_averager():
count = 0
total = 0
def averager(new_value):
count += 1
total += new_value
return total / count
return averager
If you try Example 7-13, here is what you get:
>>> avg = make_averager()
>>> avg(10)
Traceback (most recent call last):
...
UnboundLocalError: local variable 'count' referenced before assignment
>>>
The problem is that the statement count += 1 actually means the same as count =
count + 1, when count is a number or any immutable type. So we are actually assigning
to count in the body of averager, and that makes it a local variable. The same problem
affects the total variable.
We did not have this problem in Example 7-9 because we never assigned to the ser
ies name; we only called series.append and invoked sum and len on it. So we took
advantage of the fact that lists are mutable.
The nonlocal Declaration | 195
But with immutable types like numbers, strings, tuples, etc., all you can do is read, but
never update. If you try to rebind them, as in count = count + 1, then you are implicitly
creating a local variable count. It is no longer a free variable, and therefore it is not saved
in the closure.
To work around this, the nonlocal declaration was introduced in Python 3. It lets you
flag a variable as a free variable even when it is assigned a new value within the function.
If a new value is assigned to a nonlocal variable, the binding stored in the closure is
changed. A correct implementation of our newest make_averager looks like
Example 7-14.
Example 7-14. Calculate a running average without keeping all history (fixed with the
use of nonlocal)
def make_averager():
count = 0
total = 0
def averager(new_value):
nonlocal count, total
count += 1
total += new_value
return total / count
return averager
Getting by without nonlocal in Python 2
The lack of nonlocal in Python 2 requires workarounds, one of
which is described in the third code snippet of PEP 3104 � Access
to Names in Outer Scopes, which introduced nonlocal. Essentially
the idea is to store the variables the inner functions need
to change (e.g., count, total) as items or attributes of some mutable
object, like a dict or a simple instance, and bind that object
to a free variable.
Now that we have Python closures covered, we can effectively implement decorators
with nested functions.
Implementing a Simple Decorator
Example 7-15 is a decorator that clocks every invocation of the decorated function and
prints the elapsed time, the arguments passed, and the result of the call.
Example 7-15. A simple decorator to output the running time of functions
import time
196 | Chapter 7: Function Decorators and Closures
def clock(func):
def clocked(*args): #
t0 = time.perf_counter()
result = func(*args) #
elapsed = time.perf_counter() - t0
name = func.__name__
arg_str = ', '.join(repr(arg) for arg in args)
print('[%0.8fs] %s(%s) -> %r' % (elapsed, name, arg_str, result))
return result
return clocked #
Define inner function clocked to accept any number of positional arguments.
This line only works because the closure for clocked encompasses the func free
variable.
Return the inner function to replace the decorated function.
Example 7-16 demonstrates the use of the clock decorator.
Example 7-16. Using the clock decorator
# clockdeco_demo.py
import time
from clockdeco import clock
@clock
def snooze(seconds):
time.sleep(seconds)
@clock
def factorial(n):
return 1 if n < 2 else n*factorial(n-1)
if __name__=='__main__':
print('*' * 40, 'Calling snooze(.123)')
snooze(.123)
print('*' * 40, 'Calling factorial(6)')
print('6! =', factorial(6))
The output of running Example 7-16 looks like this:
$ python3 clockdeco_demo.py
**************************************** Calling snooze(123)
[0.12405610s] snooze(.123) -> None
**************************************** Calling factorial(6)
[0.00000191s] factorial(1) -> 1
[0.00004911s] factorial(2) -> 2
[0.00008488s] factorial(3) -> 6
[0.00013208s] factorial(4) -> 24
[0.00019193s] factorial(5) -> 120
Implementing a Simple Decorator | 197
[0.00026107s] factorial(6) -> 720
6! = 720
How It Works
Remember that this code:
@clock
def factorial(n):
return 1 if n < 2 else n*factorial(n-1)
Actually does this:
def factorial(n):
return 1 if n < 2 else n*factorial(n-1)
factorial = clock(factorial)
So, in both examples, clock gets the factorial function as its func argument (see
Example 7-15). It then creates and returns the clocked function, which the Python
interpreter assigns to factorial behind the scenes. In fact, if you import the clockde
co_demo module and check the __name__ of factorial, this is what you get:
>>> import clockdeco_demo
>>> clockdeco_demo.factorial.__name__
'clocked'
>>>
So factorial now actually holds a reference to the clocked function. From now on,
each time factorial(n) is called, clocked(n) gets executed. In essence, clocked does
the following:
1. Records the initial time t0.
2. Calls the original factorial, saving the result.
3. Computes the elapsed time.
4. Formats and prints the collected data.
5. Returns the result saved in step 2.
This is the typical behavior of a decorator: it replaces the decorated function with a new
function that accepts the same arguments and (usually) returns whatever the decorated
function was supposed to return, while also doing some extra processing.
198 | Chapter 7: Function Decorators and Closures
In Design Patterns by Gamma et al., the short description of the
Decorator pattern starts with: �Attach additional responsibilities to
an object dynamically.� Function decorators fit that description.
But at the implementation level, Python decorators bear little resemblance
to the classic Decorator described in the original Design
Patterns work. �Soapbox� on page 213 has more on this subject.
The clock decorator implemented in Example 7-15 has a few shortcomings: it does not
support keyword arguments, and it masks the __name__ and __doc__ of the decorated
function. Example 7-17 uses the functools.wraps decorator to copy the relevant attributes
from func to clocked. Also, in this new version, keyword arguments are correctly
handled.
Example 7-17. An improved clock decorator
# clockdeco2.py
import time
import functools
def clock(func):
@functools.wraps(func)
def clocked(*args, **kwargs):
t0 = time.time()
result = func(*args, **kwargs)
elapsed = time.time() - t0
name = func.__name__
arg_lst = []
if args:
arg_lst.append(', '.join(repr(arg) for arg in args))
if kwargs:
pairs = ['%s=%r' % (k, w) for k, w in sorted(kwargs.items())]
arg_lst.append(', '.join(pairs))
arg_str = ', '.join(arg_lst)
print('[%0.8fs] %s(%s) -> %r ' % (elapsed, name, arg_str, result))
return result
return clocked
functools.wraps is just one of the ready-to-use decorators in the standard library. In
the next section, we�ll meet two of the most impressive decorators that functools
provides: lru_cache and singledispatch.
Decorators in the Standard Library
Python has three built-in functions that are designed to decorate methods: property,
classmethod, and staticmethod. We will discuss property in �Using a Property for
Decorators in the Standard Library | 199
Attribute Validation� on page 604 and the others in �classmethod Versus staticmethod�
on page 252.
Another frequently seen decorator is functools.wraps, a helper for building wellbehaved
decorators. We used it in Example 7-17. Two of the most interesting decorators
in the standard library are lru_cache and the brand-new singledispatch (added in
Python 3.4). Both are defined in the functools module. We�ll cover them next.
Memoization with functools.lru_cache
A very practical decorator is functools.lru_cache. It implements memoization: an
optimization technique that works by saving the results of previous invocations of an
expensive function, avoiding repeat computations on previously used arguments. The
letters LRU stand for Least Recently Used, meaning that the growth of the cache is
limited by discarding the entries that have not been read for a while.
A good demonstration is to apply lru_cache to the painfully slow recursive function
to generate the nth number in the Fibonacci sequence, as shown in Example 7-18.
Example 7-18. The very costly recursive way to compute the nth number in the Fibonacci
series
from clockdeco import clock
@clock
def fibonacci(n):
if n < 2:
return n
return fibonacci(n-2) + fibonacci(n-1)
if __name__=='__main__':
print(fibonacci(6))
Here is the result of running fibo_demo.py. Except for the last line, all output is generated
by the clock decorator:
$ python3 fibo_demo.py
[0.00000095s] fibonacci(0) -> 0
[0.00000095s] fibonacci(1) -> 1
[0.00007892s] fibonacci(2) -> 1
[0.00000095s] fibonacci(1) -> 1
[0.00000095s] fibonacci(0) -> 0
[0.00000095s] fibonacci(1) -> 1
[0.00003815s] fibonacci(2) -> 1
[0.00007391s] fibonacci(3) -> 2
[0.00018883s] fibonacci(4) -> 3
[0.00000000s] fibonacci(1) -> 1
[0.00000095s] fibonacci(0) -> 0
[0.00000119s] fibonacci(1) -> 1
[0.00004911s] fibonacci(2) -> 1
200 | Chapter 7: Function Decorators and Closures
[0.00009704s] fibonacci(3) -> 2
[0.00000000s] fibonacci(0) -> 0
[0.00000000s] fibonacci(1) -> 1
[0.00002694s] fibonacci(2) -> 1
[0.00000095s] fibonacci(1) -> 1
[0.00000095s] fibonacci(0) -> 0
[0.00000095s] fibonacci(1) -> 1
[0.00005102s] fibonacci(2) -> 1
[0.00008917s] fibonacci(3) -> 2
[0.00015593s] fibonacci(4) -> 3
[0.00029993s] fibonacci(5) -> 5
[0.00052810s] fibonacci(6) -> 8
8
The waste is obvious: fibonacci(1) is called eight times, fibonacci(2) five times, etc.
But if we just add two lines to use lru_cache, performance is much improved. See
Example 7-19.
Example 7-19. Faster implementation using caching
import functools
from clockdeco import clock
@functools.lru_cache() #
@clock #
def fibonacci(n):
if n < 2:
return n
return fibonacci(n-2) + fibonacci(n-1)
if __name__=='__main__':
print(fibonacci(6))
Note that lru_cache must be invoked as a regular function�note the
parentheses in the line: @functools.lru_cache(). The reason is that it accepts
configuration parameters, as we�ll see shortly.
This is an example of stacked decorators: @lru_cache() is applied on the
function returned by @clock.
Execution time is halved, and the function is called only once for each value of n:
$ python3 fibo_demo_lru.py
[0.00000119s] fibonacci(0) -> 0
[0.00000119s] fibonacci(1) -> 1
[0.00010800s] fibonacci(2) -> 1
[0.00000787s] fibonacci(3) -> 2
[0.00016093s] fibonacci(4) -> 3
[0.00001216s] fibonacci(5) -> 5
[0.00025296s] fibonacci(6) -> 8
Decorators in the Standard Library | 201
In another test, to compute fibonacci(30), Example 7-19 made the 31 calls needed in
0.0005s, while the uncached Example 7-18 called fibonacci 2,692,537 times and took
17.7 seconds in an Intel Core i7 notebook.
Besides making silly recursive algorithms viable, lru_cache really shines in applications
that need to fetch information from the Web.
It�s important to note that lru_cache can be tuned by passing two optional arguments.
Its full signature is:
functools.lru_cache(maxsize=128, typed=False)
The maxsize argument determines how many call results are stored. After the cache is
full, older results are discarded to make room. For optimal performance, maxsize should
be a power of 2. The typed argument, if set to True, stores results of different argument
types separately, i.e., distinguishing between float and integer arguments that are normally
considered equal, like 1 and 1.0. By the way, because lru_cache uses a dict to
store the results, and the keys are made from the positional and keyword arguments
used in the calls, all the arguments taken by the decorated function must be hashable.
Now let�s consider the intriguing functools.singledispatch decorator.
Generic Functions with Single Dispatch
Imagine we are creating a tool to debug web applications. We want to be able to generate
HTML displays for different types of Python objects.
We could start with a function like this:
import html
def htmlize(obj):
content = html.escape(repr(obj))
return '<pre>{}</pre>'.format(content)
That will work for any Python type, but now we want to extend it to generate custom
displays for some types:
� str: replace embedded newline characters with '<br>\n' and use <p> tags instead
of <pre>.
� int: show the number in decimal and hexadecimal.
� list: output an HTML list, formatting each item according to its type.
The behavior we want is shown in Example 7-20.
Example 7-20. htmlize generates HTML tailored to different object types
>>> htmlize({1, 2, 3})
'<pre>{1, 2, 3}</pre>'
202 | Chapter 7: Function Decorators and Closures
3. This is what is meant by the term single-dispatch. If more arguments were used to select the specific functions,
we�d have multiple-dispatch.
>>> htmlize(abs)
'<pre>&lt;built-in function abs&gt;</pre>'
>>> htmlize('Heimlich & Co.\n- a game')
'<p>Heimlich &amp; Co.<br>\n- a game</p>'
>>> htmlize(42)
'<pre>42 (0x2a)</pre>'
>>> print(htmlize(['alpha', 66, {3, 2, 1}]))
<ul>
<li><p>alpha</p></li>
<li><pre>66 (0x42)</pre></li>
<li><pre>{1, 2, 3}</pre></li>
</ul>
By default, the HTML-escaped repr of an object is shown enclosed in <pre></
pre>.
str objects are also HTML-escaped but wrapped in <p></p> with <br> line
breaks.
An int is shown in decimal and hexadecimal, inside <pre></pre>.
Each list item is formatted according to its type, and the whole sequence
rendered as an HTML list.
Because we don�t have method or function overloading in Python, we can�t create variations
of htmlize with different signatures for each data type we want to handle differently.
A common solution in Python would be to turn htmlize into a dispatch function,
with a chain of if/elif/elif calling specialized functions like htmlize_str,
htmlize_int, etc. This is not extensible by users of our module, and is unwieldy: over
time, the htmlize dispatcher would become too big, and the coupling between it and
the specialized functions would be very tight.
The new functools.singledispatch decorator in Python 3.4 allows each module to
contribute to the overall solution, and lets you easily provide a specialized function even
for classes that you can�t edit. If you decorate a plain function with @singledispatch,
it becomes a generic function: a group of functions to perform the same operation in
different ways, depending on the type of the first argument.3 Example 7-21 shows how.
functools.singledispatch was added in Python 3.4, but the sin
gledispatch package available on PyPI is a backport compatible
with Python 2.6 to 3.3.
Decorators in the Standard Library | 203
Example 7-21. singledispatch creates a custom htmlize.register to bundle several functions
into a generic function
from functools import singledispatch
from collections import abc
import numbers
import html
@singledispatch
def htmlize(obj):
content = html.escape(repr(obj))
return '<pre>{}</pre>'.format(content)
@htmlize.register(str)
def _(text):
content = html.escape(text).replace('\n', '<br>\n')
return '<p>{0}</p>'.format(content)
@htmlize.register(numbers.Integral)
def _(n):
return '<pre>{0} (0x{0:x})</pre>'.format(n)
@htmlize.register(tuple)
@htmlize.register(abc.MutableSequence)
def _(seq):
inner = '</li>\n<li>'.join(htmlize(item) for item in seq)
return '<ul>\n<li>' + inner + '</li>\n</ul>'
@singledispatch marks the base function that handles the object type.
Each specialized function is decorated with @�base_function�.regis
ter(�type�).
The name of the specialized functions is irrelevant; _ is a good choice to make
this clear.
For each additional type to receive special treatment, register a new function.
numbers.Integral is a virtual superclass of int.
You can stack several register decorators to support different types with the
same function.
When possible, register the specialized functions to handle ABCs (abstract classes) such
as numbers.Integral and abc.MutableSequence instead of concrete implementations
like int and list. This allows your code to support a greater variety of compatible types.
For example, a Python extension can provide alternatives to the int type with fixed bit
lengths as subclasses of numbers.Integral.
204 | Chapter 7: Function Decorators and Closures
Using ABCs for type checking allows your code to support existing
or future classes that are either actual or virtual subclasses of
those ABCs. The use of ABCs and the concept of a virtual subclass
are subjects of Chapter 11.
A notable quality of the singledispatch mechanism is that you can register specialized
functions anywhere in the system, in any module. If you later add a module with a new
user-defined type, you can easily provide a new custom function to handle that type.
And you can write custom functions for classes that you did not write and can�t change.
singledispatch is a well-thought-out addition to the standard library, and it offers
more features than we can describe here. The best documentation for it is PEP 443 �
Single-dispatch generic functions.
@singledispatch is not designed to bring Java-style method
overloading to Python. A single class with many overloaded variations
of a method is better than a single function with a lengthy
stretch of if/elif/elif/elif blocks. But both solutions are
flawed because they concentrate too much responsibility in a single
code unit�the class or the function. The advantage of @sin
gledispath is supporting modular extension: each module can
register a specialized function for each type it supports.
Decorators are functions and therefore they may be composed (i.e., you can apply a
decorator to a function that is already decorated, as shown in Example 7-21). The next
section explains how that works.
Stacked Decorators
Example 7-19 demonstrated the use of stacked decorators: @lru_cache is applied on
the result of @clock over fibonacci. In Example 7-21, the @htmlize.register decorator
was applied twice to the last function in the module.
When two decorators @d1 and @d2 are applied to a function f in that order, the result is
the same as f = d1(d2(f)).
In other words, this:
@d1
@d2
def f():
print('f')
Is the same as:
Stacked Decorators | 205
def f():
print('f')
f = d1(d2(f))
Besides stacked decorators, this chapter has shown some decorators that take arguments,
for example, @lru_cache() and the htmlize.register(�type�) produced by
@singledispatch in Example 7-21. The next section shows how to build decorators
that accept parameters.
Parameterized Decorators
When parsing a decorator in source code, Python takes the decorated function and
passes it as the first argument to the decorator function. So how do you make a decorator
accept other arguments? The answer is: make a decorator factory that takes those arguments
and returns a decorator, which is then applied to the function to be decorated.
Confusing? Sure. Let�s start with an example based on the simplest decorator we�ve seen:
register in Example 7-22.
Example 7-22. Abridged registration.py module from Example 7-2, repeated here for
convenience
registry = []
def register(func):
print('running register(%s)' % func)
registry.append(func)
return func
@register
def f1():
print('running f1()')
print('running main()')
print('registry ->', registry)
f1()
A Parameterized Registration Decorator
In order to make it easy to enable or disable the function registration performed by
register, we�ll make it accept an optional active parameter which, if False, skips
registering the decorated function. Example 7-23 shows how. Conceptually, the new
register function is not a decorator but a decorator factory. When called, it returns
the actual decorator that will be applied to the target function.
206 | Chapter 7: Function Decorators and Closures
Example 7-23. To accept parameters, the new register decorator must be called as a
function
registry = set()
def register(active=True):
def decorate(func):
print('running register(active=%s)->decorate(%s)'
% (active, func))
if active:
registry.add(func)
else:
registry.discard(func)
return func
return decorate
@register(active=False)
def f1():
print('running f1()')
@register()
def f2():
print('running f2()')
def f3():
print('running f3()')
registry is now a set, so adding and removing functions is faster.
register takes an optional keyword argument.
The decorate inner function is the actual decorator; note how it takes a function
as argument.
Register func only if the active argument (retrieved from the closure) is True.
If not active and func in registry, remove it.
Because decorate is a decorator, it must return a function.
register is our decorator factory, so it returns decorate.
The @register factory must be invoked as a function, with the desired
parameters.
If no parameters are passed, register must still be called as a function�@reg
ister()�i.e., to return the actual decorator, decorate.
The main point is that register() returns decorate, which is then applied to the decorated
function.
Parameterized Decorators | 207
The code in Example 7-23 is in a registration_param.py module. If we import it, this is
what we get:
>>> import registration_param
running register(active=False)->decorate(<function f1 at 0x10063c1e0>)
running register(active=True)->decorate(<function f2 at 0x10063c268>)
>>> registration_param.registry
[<function f2 at 0x10063c268>]
Note how only the f2 function appears in the registry; f1 does not appear because
active=False was passed to the register decorator factory, so the decorate that was
applied to f1 did not add it to the registry.
If, instead of using the @ syntax, we used register as a regular function, the syntax
needed to decorate a function f would be register()(f) to add f to the registry, or
register(active=False)(f) to not add it (or remove it). See Example 7-24 for a demo
of adding and removing functions to the registry.
Example 7-24. Using the registration_param module listed in Example 7-23
>>> from registration_param import *
running register(active=False)->decorate(<function f1 at 0x10073c1e0>)
running register(active=True)->decorate(<function f2 at 0x10073c268>)
>>> registry #
{<function f2 at 0x10073c268>}
>>> register()(f3) #
running register(active=True)->decorate(<function f3 at 0x10073c158>)
<function f3 at 0x10073c158>
>>> registry #
{<function f3 at 0x10073c158>, <function f2 at 0x10073c268>}
>>> register(active=False)(f2) #
running register(active=False)->decorate(<function f2 at 0x10073c268>)
<function f2 at 0x10073c268>
>>> registry #
{<function f3 at 0x10073c158>}
When the module is imported, f2 is in the registry.
The register() expression returns decorate, which is then applied to f3.
The previous line added f3 to the registry.
This call removes f2 from the registry.
Confirm that only f3 remains in the registry.
The workings of parameterized decorators are fairly involved, and the one we�ve just
discussed is simpler than most. Parameterized decorators usually replace the decorated
function, and their construction requires yet another level of nesting. Touring such
function pyramids is our next adventure.
208 | Chapter 7: Function Decorators and Closures
The Parameterized Clock Decorator
In this section, we�ll revisit the clock decorator, adding a feature: users may pass a format
string to control the output of the decorated function. See Example 7-25.
For simplicity, Example 7-25 is based on the initial clock implementation
from Example 7-15, and not the improved one from
Example 7-17 that uses @functools.wraps, adding yet another
function layer.
Example 7-25. Module clockdeco_param.py: the parameterized clock decorator
import time
DEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -> {result}'
def clock(fmt=DEFAULT_FMT):
def decorate(func):
def clocked(*_args):
t0 = time.time()
_result = func(*_args)
elapsed = time.time() - t0
name = func.__name__
args = ', '.join(repr(arg) for arg in _args)
result = repr(_result)
print(fmt.format(**locals()))
return _result
return clocked
return decorate
if __name__ == '__main__':
@clock()
def snooze(seconds):
time.sleep(seconds)
for i in range(3):
snooze(.123)
clock is our parameterized decorator factory.
decorate is the actual decorator.
clocked wraps the decorated function.
_result is the actual result of the decorated function.
_args holds the actual arguments of clocked, while args is str used for display.
result is the str representation of _result, for display.
Parameterized Decorators | 209
Using **locals() here allows any local variable of clocked to be referenced in
the fmt.
clocked will replace the decorated function, so it should return whatever that
function returns.
decorate returns clocked.
clock returns decorate.
In this self test, clock() is called without arguments, so the decorator applied
will use the default format str.
If you run Example 7-25 from the shell, this is what you get:
$ python3 clockdeco_param.py
[0.12412500s] snooze(0.123) -> None
[0.12411904s] snooze(0.123) -> None
[0.12410498s] snooze(0.123) -> None
To exercise the new functionality, Examples 7-26 and 7-27 are two other modules using
clockdeco_param, and the outputs they generate.
Example 7-26. clockdeco_param_demo1.py
import time
from clockdeco_param import clock
@clock('{name}: {elapsed}s')
def snooze(seconds):
time.sleep(seconds)
for i in range(3):
snooze(.123)
Output of Example 7-26:
$ python3 clockdeco_param_demo1.py
snooze: 0.12414693832397461s
snooze: 0.1241159439086914s
snooze: 0.12412118911743164s
Example 7-27. clockdeco_param_demo2.py
import time
from clockdeco_param import clock
@clock('{name}({args}) dt={elapsed:0.3f}s')
def snooze(seconds):
time.sleep(seconds)
for i in range(3):
snooze(.123)
210 | Chapter 7: Function Decorators and Closures
Output of Example 7-27:
$ python3 clockdeco_param_demo2.py
snooze(0.123) dt=0.124s
snooze(0.123) dt=0.124s
snooze(0.123) dt=0.124s
This ends our exploration of decorators as far as space permits within the scope of this
book. See �Further Reading� on page 212, in particular Graham Dumpleton�s blog and
wrapt module for industrial-strength techniques when building decorators.
Graham Dumpleton and Lennart Regebro�one of this book�s
technical reviewers�argue that decorators are best coded as
classes implementing __call__, and not as functions like the examples
in this chapter. I agree that approach is better for nontrivial
decorators, but to explain the basic idea of this language
feature, functions are easier to understand.
Chapter Summary
We covered a lot of ground in this chapter, but I tried to make the journey as smooth
as possible even if the terrain is rugged. After all, we did enter the realm of metaprogramming.
We started with a simple @register decorator without an inner function, and finished
with a parameterized @clock() involving two levels of nested functions.
Registration decorators, though simple in essence, have real applications in advanced
Python frameworks. We applied the registration idea to an improvement of our Strategy
design pattern refactoring from Chapter 6.
Parameterized decorators almost aways involve at least two nested functions, maybe
more if you want to use @functools.wraps to produce a decorator that provides better
support for more advanced techniques. One such technique is stacked decorators, which
we briefly covered.
We also visited two awesome function decorators provided in the functools module
of standard library: @lru_cache() and @singledispatch.
Understanding how decorators actually work required covering the difference between
import time and runtime, then diving into variable scoping, closures, and the new
nonlocal declaration. Mastering closures and nonlocal is valuable not only to build
decorators, but also to code event-oriented programs for GUIs or asynchronous I/O
with callbacks.
Chapter Summary | 211
Further Reading
Chapter 9, �Metaprogramming,� of the Python Cookbook, Third Edition by David Beazley
and Brian K. Jones (O�Reilly), has several recipes from elementary decorators to very
sophisticated ones, including one that can be called as a regular decorator or as a decorator
factory, e.g., @clock or @clock(). That�s �Recipe 9.6. Defining a Decorator That
Takes an Optional Argument� in that cookbook.
Graham Dumpleton has a series of in-depth blog posts about techniques for implementing
well-behaved decorators, starting with �How You Implemented Your Python
Decorator is Wrong�. His deep expertise in this matter is also nicely packaged in the
wrapt module he wrote to simplify the implementation of decorators and dynamic
function wrappers, which support introspection and behave correctly when further
decorated, when applied to methods and when used as descriptors. (Descriptors are the
subject of chapter Chapter 20.)
Michele Simionato authored a package aiming to �simplify the usage of decorators for
the average programmer, and to popularize decorators by showing various non-trivial
examples,� according to the docs. It�s available on PyPI as the decorator package.
Created when decorators were still a new feature in Python, the Python Decorator Library
wiki page has dozens of examples. Because that page started years ago, some of
the techniques shown have been superseded, but the page is still an excellent source of
inspiration.
PEP 443 provides the rationale and a detailed description of the single-dispatch generic
functions� facility. An old (March 2005) blog post by Guido van Rossum, �Five-Minute
Multimethods in Python�, walks through an implementation of generic functions (a.k.a.
multimethods) using decorators. His code supports multiple-dispatch (i.e., dispatch
based on more than one positional argument). Guido�s multimethods code is interesting,
but it�s a didactic example. For a modern, production-ready implementation of multipledispatch
generic functions, check out Reg by Martijn Faassen�author of the modeldriven
and REST-savvy Morepath web framework.
�Closures in Python� is a short blog post by Fredrik Lundh that explains the terminology
of closures.
PEP 3104 � Access to Names in Outer Scopes describes the introduction of the
nonlocal declaration to allow rebinding of names that are neither local nor global. It
also includes an excellent overview of how this issue is resolved in other dynamic languages
(Perl, Ruby, JavaScript, etc.) and the pros and cons of the design options available
to Python.
On a more theoretical level, PEP 227 � Statically Nested Scopes documents the introduction
of lexical scoping as an option in Python 2.1 and as a standard in Python 2.2,
212 | Chapter 7: Function Decorators and Closures
explaining the rationale and design choices for the implementation of closures in
Python.
Soapbox
The designer of any language with first-class functions faces this issue: being first-class
objects, functions are defined in a certain scope but may be invoked in other scopes.
The question is: how to evaluate the free variables? The first and simplest answer is
�dynamic scope.� This means that free variables are evaluated by looking into the environment
where the function is invoked.
If Python had dynamic scope and no closures, we could improvise avg�similar to
Example 7-9�like this:
>>> ### this is not a real Python console session! ###
>>> avg = make_averager()
>>> series = [] #
>>> avg(10)
10.0
>>> avg(11) #
10.5
>>> avg(12)
11.0
>>> series = [1] #
>>> avg(5)
3.0
Before using avg, we have to define series = [] ourselves, so we must know
that averager (inside make_averager) refers to a list by that name.
Behind the scenes, series is used to accumulate the values to be averaged.
When series = [1] is executed, the previous list is lost. This could happen by
accident, when handling two independent running averages at the same time.
Functions should be black boxes, with their implementation hidden from users. But
with dynamic scope, if a function uses free variables, the programmer has to know its
internals to set up an environment where it works correctly.
On the other hand, dynamic scope is easier to implement, which is probably why it was
the path taken by John McCarthy when he created Lisp, the first language to have firstclass
functions. Paul Graham�s article �The Roots of Lisp� is an accessible explanation
of John McCarthy�s original paper about the Lisp language: �Recursive Functions of
Symbolic Expressions and Their Computation by Machine, Part I�. McCarthy�s paper
is a masterpiece as great as Beethoven�s 9th Symphony. Paul Graham translated it for the
rest of us, from mathematics to English and running code.
Paul Graham�s commentary also shows how tricky dynamic scoping is. Quoting from
�The Roots of Lisp�:
Further Reading | 213
It�s an eloquent testimony to the dangers of dynamic scope that even the very first
example of higher-order Lisp functions was broken because of it. It may be that Mc?
Carthy was not fully aware of the implications of dynamic scope in 1960. Dynamic scope
remained in Lisp implementations for a surprisingly long time�until Sussman and
Steele developed Scheme in 1975. Lexical scope does not complicate the definition of
eval very much, but it may make compilers harder to write.
Today, lexical scope is the norm: free variables are evaluated considering the environment
where the function is defined. Lexical scope complicates the implementation of
languages with first-class functions, because it requires the support of closures. On the
other hand, lexical scope makes source code easier to read. Most languages invented
since Algol have lexical scope.
For many years, Python lambdas did not provide closures, contributing to the bad name
of this feature among functional-programming geeks in the blogosphere. This was fixed
in Python 2.2 (December 2001), but the blogosphere has a long memory. Since then,
lambda is embarrassing only because of its limited syntax.
Python Decorators and the Decorator Design Pattern
Python function decorators fit the general description of Decorator given by Gamma
et al. in Design Patterns: �Attach additional responsibilities to an object dynamically.
Decorators provide a flexible alternative to subclassing for extending functionality.�
At the implementation level, Python decorators do not resemble the classic Decorator
design pattern, but an analogy can be made.
In the design pattern, Decorator and Component are abstract classes. An instance of a
concrete decorator wraps an instance of a concrete component in order to add behaviors
to it. Quoting from Design Patterns:
The decorator conforms to the interface of the component it decorates so that its presence
is transparent to the component�s clients. The decorator forwards requests to the
component and may perform additional actions (such as drawing a border) before or
after forwarding. Transparency lets you nest decorators recursively, thereby allowing
an unlimited number of added responsibilities.� (p. 175)
In Python, the decorator function plays the role of a concrete Decorator subclass, and
the inner function it returns is a decorator instance. The returned function wraps the
function to be decorated, which is analogous to the component in the design pattern.
The returned function is transparent because it conforms to the interface of the component
by accepting the same arguments. It forwards calls to the component and may
perform additional actions either before or after it. Borrowing from the previous citation,
we can adapt the last sentence to say that �Transparency lets you nest decorators
recursively, thereby allowing an unlimited number of added behaviors.� That is what
enable stacked decorators to work.
Note that I am not suggesting that function decorators should be used to implement the
Decorator pattern in Python programs. Although this can be done in specific situations,
214 | Chapter 7: Function Decorators and Closures
in general the Decorator pattern is best implemented with classes to represent the Decorator
and the components it will wrap.
Further Reading | 215

PART IV
Object-Oriented Idioms

CHAPTER 8
Object References, Mutability,
and Recycling
�You are sad,� the Knight said in an anxious tone: �let me sing you a song to comfort you.
[�] The name of the song is called �HADDOCKS� EYES�.�
�Oh, that�s the name of the song, is it?� Alice said, trying to feel interested.
�No, you don�t understand,� the Knight said, looking a little vexed. �That�s what the name
is CALLED. The name really IS �THE AGED AGED MAN."� (adapted from Chapter VIII.
�It�s my own Invention�).
� Lewis Carroll
Through the Looking-Glass, and What Alice Found There
Alice and the Knight set the tone of what we will see in this chapter. The theme is the
distinction between objects and their names. A name is not the object; a name is a
separate thing.
We start the chapter by presenting a metaphor for variables in Python: variables are
labels, not boxes. If reference variables are old news to you, the analogy may still be
handy if you need to explain aliasing issues to others.
We then discuss the concepts of object identity, value, and aliasing. A surprising trait of
tuples is revealed: they are immutable but their values may change. This leads to a
discussion of shallow and deep copies. References and function parameters are our next
theme: the problem with mutable parameter defaults and the safe handling of mutable
arguments passed by clients of our functions.
The last sections of the chapter cover garbage collection, the del command, and how
to use weak references to �remember� objects without keeping them alive.
This is a rather dry chapter, but its topics lie at the heart of many subtle bugs in real
Python programs.
219
Let�s start by unlearning that a variable is like a box where you store data.
Variables Are Not Boxes
In 1997, I took a summer course on Java at MIT. The professor, Lynn Andrea Stein�
an award-winning computer science educator who currently teaches at Olin College of
Engineering�made the point that the usual �variables as boxes� metaphor actually
hinders the understanding of reference variables in OO languages. Python variables are
like reference variables in Java, so it�s better to think of them as labels attached to objects.
Example 8-1 is a simple interaction that the �variables as boxes� idea cannot explain.
Figure 8-1 illustrates why the box metaphor is wrong for Python, while sticky notes
provide a helpful picture of how variables actually work.
Example 8-1. Variables a and b hold references to the same list, not copies of the list
>>> a = [1, 2, 3]
>>> b = a
>>> a.append(4)
>>> b
[1, 2, 3, 4]
Figure 8-1. If you imagine variables are like boxes, you can�t make sense of assignment
in Python; instead, think of variables as sticky notes�Example 8-1 then becomes easy
to explain
Prof. Stein also spoke about assignment in a very deliberate way. For example, when
talking about a seesaw object in a simulation, she would say: �Variable s is assigned to
the seesaw,� but never �The seesaw is assigned to variable s.� With reference variables,
it makes much more sense to say that the variable is assigned to an object, and not the
other way around. After all, the object is created before the assignment. Example 8-2
proves that the righthand side of an assignment happens first.
220 | Chapter 8: Object References, Mutability, and Recycling
Example 8-2. Variables are assigned to objects only after the objects are created
>>> class Gizmo:
... def __init__(self):
... print('Gizmo id: %d' % id(self))
...
>>> x = Gizmo()
Gizmo id: 4301489152
>>> y = Gizmo() * 10
Gizmo id: 4301489432
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int'
>>>
>>> dir()
['Gizmo', '__builtins__', '__doc__', '__loader__', '__name__',
'__package__', '__spec__', 'x']
The output Gizmo id: ... is a side effect of creating a Gizmo instance.
Multiplying a Gizmo instance will raise an exception.
Here is proof that a second Gizmo was actually instantiated before the
multiplication was attempted.
But variable y was never created, because the exception happened while the righthand
side of the assignment was being evaluated.
To understand an assignment in Python, always read the righthand
side first: that�s where the object is created or retrieved. After
that, the variable on the left is bound to the object, like a label
stuck to it. Just forget about the boxes.
Because variables are mere labels, nothing prevents an object from having several labels
assigned to it. When that happens, you have aliasing, our next topic.
Identity, Equality, and Aliases
Lewis Carroll is the pen name of Prof. Charles Lutwidge Dodgson. Mr. Carroll is not
only equal to Prof. Dodgson: they are one and the same. Example 8-3 expresses this idea
in Python.
Example 8-3. charles and lewis refer to the same object
>>> charles = {'name': 'Charles L. Dodgson', 'born': 1832}
>>> lewis = charles
>>> lewis is charles
True
>>> id(charles), id(lewis)
Identity, Equality, and Aliases | 221
(4300473992, 4300473992)
>>> lewis['balance'] = 950
>>> charles
{'name': 'Charles L. Dodgson', 'balance': 950, 'born': 1832}
lewis is an alias for charles.
The is operator and the id function confirm it.
Adding an item to lewis is the same as adding an item to charles.
However, suppose an impostor�let�s call him Dr. Alexander Pedachenko�claims he is
Charles L. Dodgson, born in 1832. His credentials may be the same, but Dr. Pedachenko
is not Prof. Dodgson. Figure 8-2 illustrates this scenario.
Figure 8-2. charles and lewis are bound to the same object; alex is bound to a separate
object of equal contents
Example 8-4 implements and tests the alex object depicted in Figure 8-2.
Example 8-4. alex and charles compare equal, but alex is not charles
>>> alex = {'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950}
>>> alex == charles
True
>>> alex is not charles
True
alex refers to an object that is a replica of the object assigned to charles.
The objects compare equal, because of the __eq__ implementation in the dict
class.
But they are distinct objects. This is the Pythonic way of writing the negative
identity comparison: a is not b.
Example 8-3 is an example of aliasing. In that code, lewis and charles are aliases: two
variables bound to the same object. On the other hand, alex is not an alias for
222 | Chapter 8: Object References, Mutability, and Recycling
charles: these variables are bound to distinct objects. The objects bound to alex and
charles have the same value�that�s what == compares�but they have different identities.
In The Python Language Reference, �3.1. Objects, values and types� states:
Every object has an identity, a type and a value. An object�s identity never changes once
it has been created; you may think of it as the object�s address in memory. The is operator
compares the identity of two objects; the id() function returns an integer representing
its identity.
The real meaning of an object�s ID is implementation-dependent. In CPython, id()
returns the memory address of the object, but it may be something else in another
Python interpreter. The key point is that the ID is guaranteed to be a unique numeric
label, and it will never change during the life of the object.
In practice, we rarely use the id() function while programming. Identity checks are
most often done with the is operator, and not by comparing IDs. Next, we�ll talk about
is versus ==.
Choosing Between == and is
The == operator compares the values of objects (the data they hold), while is compares
their identities.
We often care about values and not identities, so == appears more frequently than is in
Python code.
However, if you are comparing a variable to a singleton, then it makes sense to use is.
By far, the most common case is checking whether a variable is bound to None. This is
the recommended way to do it:
x is None
And the proper way to write its negation is:
x is not None
The is operator is faster than ==, because it cannot be overloaded, so Python does not
have to find and invoke special methods to evaluate it, and computing is as simple as
comparing two integer IDs. In contrast, a == b is syntactic sugar for a.__eq__(b). The
__eq__ method inherited from object compares object IDs, so it produces the same
result as is. But most built-in types override __eq__ with more meaningful implementations
that actually take into account the values of the object attributes. Equality may
involve a lot of processing�for example, when comparing large collections or deeply
nested structures.
Identity, Equality, and Aliases | 223
1. On the other hand, single-type sequences like str, bytes, and array.array are flat: they don�t contain
references but physically hold their data�characters, bytes, and numbers�in contiguous memory.
To wrap up this discussion of identity versus equality, we�ll see that the famously immutable
tuple is not as rigid as you may expect.
The Relative Immutability of Tuples
Tuples, like most Python collections�lists, dicts, sets, etc.�hold references to objects.
1 If the referenced items are mutable, they may change even if the tuple itself does not.
In other words, the immutability of tuples really refers to the physical contents of the
tuple data structure (i.e., the references it holds), and does not extend to the referenced
objects.
Example 8-5 illustrates the situation in which the value of a tuple changes as result of
changes to a mutable object referenced in it. What can never change in a tuple is the
identity of the items it contains.
Example 8-5. t1 and t2 initially compare equal, but changing a mutable item inside tuple
t1 makes it different
>>> t1 = (1, 2, [30, 40])
>>> t2 = (1, 2, [30, 40])
>>> t1 == t2
True
>>> id(t1[-1])
4302515784
>>> t1[-1].append(99)
>>> t1
(1, 2, [30, 40, 99])
>>> id(t1[-1])
4302515784
>>> t1 == t2
False
t1 is immutable, but t1[-1] is mutable.
Build a tuple t2 whose items are equal to those of t1.
Although distinct objects, t1 and t2 compare equal, as expected.
Inspect the identity of the list at t1[-1].
Modify the t1[-1] list in place.
The identity of t1[-1] has not changed, only its value.
t1 and t2 are now different.
224 | Chapter 8: Object References, Mutability, and Recycling
This relative immutability of tuples is behind the riddle �A += Assignment Puzzler� on
page 40. It�s also the reason why some tuples are unhashable, as we�ve seen in �What Is
Hashable?� on page 65.
The distinction between equality and identity has further implications when you need
to copy an object. A copy is an equal object with a different ID. But if an object contains
other objects, should the copy also duplicate the inner objects, or is it OK to share them?
There�s no single answer. Read on for a discussion.
Copies Are Shallow by Default
The easiest way to copy a list (or most built-in mutable collections) is to use the builtin
constructor for the type itself. For example:
>>> l1 = [3, [55, 44], (7, 8, 9)]
>>> l2 = list(l1)
>>> l2
[3, [55, 44], (7, 8, 9)]
>>> l2 == l1
True
>>> l2 is l1
False
list(l1) creates a copy of l1.
The copies are equal.
But refer to two different objects.
For lists and other mutable sequences, the shortcut l2 = l1[:] also makes a copy.
However, using the constructor or [:] produces a shallow copy (i.e., the outermost
container is duplicated, but the copy is filled with references to the same items held by
the original container). This saves memory and causes no problems if all the items are
immutable. But if there are mutable items, this may lead to unpleasant surprises.
In Example 8-6, we create a shallow copy of a list containing another list and a tuple,
and then make changes to see how they affect the referenced objects.
If you have a connected computer on hand, I highly recommend
watching the interactive animation for Example 8-6 at the Online
Python Tutor. As I write this, direct linking to a prepared example
at pythontutor.com is not working reliably, but the tool is awesome,
so taking the time to copy and paste the code is worthwhile.
Copies Are Shallow by Default | 225
Example 8-6. Making a shallow copy of a list containing another list; copy and paste
this code to see it animated at the Online Python Tutor
l1 = [3, [66, 55, 44], (7, 8, 9)]
l2 = list(l1) #
l1.append(100) #
l1[1].remove(55) #
print('l1:', l1)
print('l2:', l2)
l2[1] += [33, 22] #
l2[2] += (10, 11) #
print('l1:', l1)
print('l2:', l2)
l2 is a shallow copy of l1. This state is depicted in Figure 8-3.
Appending 100 to l1 has no effect on l2.
Here we remove 55 from the inner list l1[1]. This affects l2 because l2[1] is
bound to the same list as l1[1].
For a mutable object like the list referred by l2[1], the operator += changes the
list in place. This change is visible at l1[1], which is an alias for l2[1].
+= on a tuple creates a new tuple and rebinds the variable l2[2] here. This is
the same as doing l2[2] = l2[2] + (10, 11). Now the tuples in the last
position of l1 and l2 are no longer the same object. See Figure 8-4.
The output of Example 8-6 is Example 8-7, and the final state of the objects is depicted
in Figure 8-4.
Example 8-7. Output of Example 8-6
l1: [3, [66, 44], (7, 8, 9), 100]
l2: [3, [66, 44], (7, 8, 9)]
l1: [3, [66, 44, 33, 22], (7, 8, 9), 100]
l2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)]
226 | Chapter 8: Object References, Mutability, and Recycling
Figure 8-3. Program state immediately after the assignment l2 = list(l1) in
Example 8-6. l1 and l2 refer to distinct lists, but the lists share references to the same
inner list object [66, 55, 44] and tuple (7, 8, 9). (Diagram generated by the Online
Python Tutor.)
Figure 8-4. Final state of l1 and l2: they still share references to the same list object, now
containing [66, 44, 33, 22], but the operation l2[2] += (10, 11) created a new tuple with
content (7, 8, 9, 10, 11), unrelated to the tuple (7, 8, 9) referenced by l1[2]. (Diagram
generated by the Online Python Tutor.)
It should be clear now that shallow copies are easy to make, but they may or may not
be what you want. How to make deep copies is our next topic.
Copies Are Shallow by Default | 227
Deep and Shallow Copies of Arbitrary Objects
Working with shallow copies is not always a problem, but sometimes you need to make
deep copies (i.e., duplicates that do not share references of embedded objects). The copy
module provides the deepcopy and copy functions that return deep and shallow copies
of arbitrary objects.
To illustrate the use of copy() and deepcopy(), Example 8-8 defines a simple class,
Bus, representing a school bus that is loaded with passengers and then picks up or drops
off passengers on its route.
Example 8-8. Bus picks up and drops off passengers
class Bus:
def __init__(self, passengers=None):
if passengers is None:
self.passengers = []
else:
self.passengers = list(passengers)
def pick(self, name):
self.passengers.append(name)
def drop(self, name):
self.passengers.remove(name)
Now in the interactive Example 8-9 we will create we will create a bus object (bus1) and
two clones�a shallow copy (bus2) and a deep copy (bus3)�to observe what happens
as bus1 drops off a student.
Example 8-9. Effects of using copy versus deepcopy
>>> import copy
>>> bus1 = Bus(['Alice', 'Bill', 'Claire', 'David'])
>>> bus2 = copy.copy(bus1)
>>> bus3 = copy.deepcopy(bus1)
>>> id(bus1), id(bus2), id(bus3)
(4301498296, 4301499416, 4301499752)
>>> bus1.drop('Bill')
>>> bus2.passengers
['Alice', 'Claire', 'David']
>>> id(bus1.passengers), id(bus2.passengers), id(bus3.passengers)
(4302658568, 4302658568, 4302657800)
>>> bus3.passengers
['Alice', 'Bill', 'Claire', 'David']
Using copy and deepcopy, we create three distinct Bus instances.
After bus1 drops 'Bill', he is also missing from bus2.
228 | Chapter 8: Object References, Mutability, and Recycling
Inspection of the passengers atributes shows that bus1 and bus2 share the same
list object, because bus2 is a shallow copy of bus1.
bus3 is a deep copy of bus1, so its passengers attribute refers to another list.
Note that making deep copies is not a simple matter in the general case. Objects may
have cyclic references that would cause a naive algorithm to enter an infinite loop. The
deepcopy function remembers the objects already copied to handle cyclic references
gracefully. This is demonstrated in Example 8-10.
Example 8-10. Cyclic references: b refers to a, and then is appended to a; deepcopy still
manages to copy a
>>> a = [10, 20]
>>> b = [a, 30]
>>> a.append(b)
>>> a
[10, 20, [[...], 30]]
>>> from copy import deepcopy
>>> c = deepcopy(a)
>>> c
[10, 20, [[...], 30]]
Also, a deep copy may be too deep in some cases. For example, objects may refer to
external resources or singletons that should not be copied. You can control the behavior
of both copy and deepcopy by implementing the __copy__() and __deepcopy__()
special methods as described in the copy module documentation.
The sharing of objects through aliases also explains how parameter passing works in
Python, and the problem of using mutable types as parameter defaults. These issues will
be covered next.
Function Parameters as References
The only mode of parameter passing in Python is call by sharing. That is the same mode
used in most OO languages, including Ruby, SmallTalk, and Java (this applies to Java
reference types; primitive types use call by value). Call by sharing means that each formal
parameter of the function gets a copy of each reference in the arguments. In other words,
the parameters inside the function become aliases of the actual arguments.
The result of this scheme is that a function may change any mutable object passed as a
parameter, but it cannot change the identity of those objects (i.e., it cannot altogether
replace an object with another). Example 8-11 shows a simple function using += on one
of its parameters. As we pass numbers, lists, and tuples to the function, the actual arguments
passed are affected in different ways.
Function Parameters as References | 229
Example 8-11. A function may change any mutable object it receives
>>> def f(a, b):
... a += b
... return a
...
>>> x = 1
>>> y = 2
>>> f(x, y)
3
>>> x, y
(1, 2)
>>> a = [1, 2]
>>> b = [3, 4]
>>> f(a, b)
[1, 2, 3, 4]
>>> a, b
([1, 2, 3, 4], [3, 4])
>>> t = (10, 20)
>>> u = (30, 40)
>>> f(t, u)
(10, 20, 30, 40)
>>> t, u
((10, 20), (30, 40))
The number x is unchanged.
The list a is changed.
The tuple t is unchanged.
Another issue related to function parameters is the use of mutable values for defaults,
as discussed next.
Mutable Types as Parameter Defaults: Bad Idea
Optional parameters with default values are a great feature of Python function definitions,
allowing our APIs to evolve while remaining backward-compatible. However, you
should avoid mutable objects as default values for parameters.
To illustrate this point, in Example 8-12, we take the Bus class from Example 8-8 and
change its __init__ method to create HauntedBus. Here we tried to be clever and instead
of having a default value of passengers=None, we have passengers=[], thus avoiding
the if in the previous __init__. This �cleverness� gets us into trouble.
Example 8-12. A simple class to illustrate the danger of a mutable default
class HauntedBus:
"""A bus model haunted by ghost passengers"""
def __init__(self, passengers=[]):
230 | Chapter 8: Object References, Mutability, and Recycling
self.passengers = passengers
def pick(self, name):
self.passengers.append(name)
def drop(self, name):
self.passengers.remove(name)
When the passengers argument is not passed, this parameter is bound to the
default list object, which is initially empty.
This assignment makes self.passengers an alias for passengers, which is itself
an alias for the default list, when no passengers argument is given.
When the methods .remove() and .append() are used with self.passengers
we are actually mutating the default list, which is an attribute of the function
object.
Example 8-13 shows the eerie behavior of the HauntedBus.
Example 8-13. Buses haunted by ghost passengers
>>> bus1 = HauntedBus(['Alice', 'Bill'])
>>> bus1.passengers
['Alice', 'Bill']
>>> bus1.pick('Charlie')
>>> bus1.drop('Alice')
>>> bus1.passengers
['Bill', 'Charlie']
>>> bus2 = HauntedBus()
>>> bus2.pick('Carrie')
>>> bus2.passengers
['Carrie']
>>> bus3 = HauntedBus()
>>> bus3.passengers
['Carrie']
>>> bus3.pick('Dave')
>>> bus2.passengers
['Carrie', 'Dave']
>>> bus2.passengers is bus3.passengers
True
>>> bus1.passengers
['Bill', 'Charlie']
So far, so good: no surprises with bus1.
bus2 starts empty, so the default empty list is assigned to self.passengers.
bus3 also starts empty, again the default list is assigned.
The default is no longer empty!
Now Dave, picked by bus3, appears in bus2.
Function Parameters as References | 231
The problem: bus2.passengers and bus3.passengers refer to the same list.
But bus1.passengers is a distinct list.
The problem is that Bus instances that don�t get an initial passenger list end up sharing
the same passenger list among themselves.
Such bugs may be subtle. As Example 8-13 demonstrates, when a HauntedBus is instantiated
with passengers, it works as expected. Strange things happen only when a
HauntedBus starts empty, because then self.passengers becomes an alias for the default
value of the passengers parameter. The problem is that each default value is evaluated
when the function is defined�i.e., usually when the module is loaded�and the
default values become attributes of the function object. So if a default value is a mutable
object, and you change it, the change will affect every future call of the function.
After running the lines in Example 8-13, you can inspect the HauntedBus.__init__
object and see the ghost students haunting its __defaults__ attribute:
>>> dir(HauntedBus.__init__) # doctest: +ELLIPSIS
['__annotations__', '__call__', ..., '__defaults__', ...]
>>> HauntedBus.__init__.__defaults__
(['Carrie', 'Dave'],)
Finally, we can verify that bus2.passengers is an alias bound to the first element of the
HauntedBus.__init__.__defaults__ attribute:
>>> HauntedBus.__init__.__defaults__[0] is bus2.passengers
True
The issue with mutable defaults explains why None is often used as the default value for
parameters that may receive mutable values. In Example 8-8, __init__ checks whether
the passengers argument is None, and assigns a new empty list to self.passengers.
As explained in the following section, if passengers is not None, the correct implementation
assigns a copy of it to self.passengers. Let�s now take a closer look.
Defensive Programming with Mutable Parameters
When you are coding a function that receives a mutable parameter, you should carefully
consider whether the caller expects the argument passed to be changed.
For example, if your function receives a dict and needs to modify it while processing
it, should this side effect be visible outside of the function or not? Actually it depends
on the context. It�s really a matter of aligning the expectation of the coder of the function
and that of the caller.
The last bus example in this chapter shows how a TwilightBus breaks expectations by
sharing its passenger list with its clients. Before studying the implementation, see in
232 | Chapter 8: Object References, Mutability, and Recycling
Example 8-14 how the TwilightBus class works from the perspective of a client of the
class.
Example 8-14. Passengers disappear when dropped by a TwilightBus
>>> basketball_team = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat']
>>> bus = TwilightBus(basketball_team)
>>> bus.drop('Tina')
>>> bus.drop('Pat')
>>> basketball_team
['Sue', 'Maya', 'Diana']
basketball_team holds five student names.
A TwilightBus is loaded with the team.
The bus drops one student, then another.
The dropped passengers vanished from the basketball team!
TwilightBus violates the �Principle of least astonishment,� a best practice of interface
design. It surely is astonishing that when the bus drops a student, her name is removed
from the basketball team roster.
Example 8-15 is the implementation TwilightBus and an explanation of the problem.
Example 8-15. A simple class to show the perils of mutating received arguments
class TwilightBus:
"""A bus model that makes passengers vanish"""
def __init__(self, passengers=None):
if passengers is None:
self.passengers = []
else:
self.passengers = passengers
def pick(self, name):
self.passengers.append(name)
def drop(self, name):
self.passengers.remove(name)
Here we are careful to create a new empty list when passengers is None.
However, this assignment makes self.passengers an alias for passengers,
which is itself an alias for the actual argument passed to __init__ (i.e.,basket
ball_team in Example 8-14).
When the methods .remove() and .append() are used with self.passen
gers, we are actually mutating the original list received as argument to the
constructor.
Function Parameters as References | 233
2. If two objects refer to each other, as in Example 8-10, they may be destroyed if the garbage collector determines
that they are otherwise unreachable because their only references are their mutual references.
The problem here is that the bus is aliasing the list that is passed to the constructor.
Instead, it should keep its own passenger list. The fix is simple: in __init__, when the
passengers parameter is provided, self.passengers should be initialized with a copy
of it, as we did correctly in Example 8-8 (�Deep and Shallow Copies of Arbitrary Objects�
on page 228):
def __init__(self, passengers=None):
if passengers is None:
self.passengers = []
else:
self.passengers = list(passengers)
Make a copy of the passengers list, or convert it to a list if it�s not one.
Now our internal handling of the passenger list will not affect the argument used to
initialize the bus. As a bonus, this solution is more flexible: now the argument passed
to the passengers parameter may be a tuple or any other iterable, like a set or even
database results, because the list constructor accepts any iterable. As we create our
own list to manage, we ensure that it supports the necessary .remove() and .ap
pend() operations we use in the .pick() and .drop() methods.
Unless a method is explicitly intended to mutate an object received
as argument, you should think twice before aliasing the
argument object by simply assigning it to an instance variable in
your class. If in doubt, make a copy. Your clients will often be
happier.
del and Garbage Collection
Objects are never explicitly destroyed; however, when they become unreachable they may
be garbage-collected.
� �Data Model� chapter of The Python Language Reference
The del statement deletes names, not objects. An object may be garbage collected as
result of a del command, but only if the variable deleted holds the last reference to the
object, or if the object becomes unreachable.2 Rebinding a variable may also cause the
number of references to an object to reach zero, causing its destruction.
234 | Chapter 8: Object References, Mutability, and Recycling
There is a __del__ special method, but it does not cause the disposal
of the instance, and should not be called by your code.
__del__ is invoked by the Python interpreter when the instance is
about to be destroyed to give it a chance to release external resources.
You will seldom need to implement __del__ in your own
code, yet some Python beginners spend time coding it for no good
reason. The proper use of __del__ is rather tricky. See the __del__
special method documentation in the �Data Model� chapter of The
Python Language Reference.
In CPython, the primary algorithm for garbage collection is reference counting. Essentially,
each object keeps count of how many references point to it. As soon as that
refcount reaches zero, the object is immediately destroyed: CPython calls the __del__
method on the object (if defined) and then frees the memory allocated to the object. In
CPython 2.0, a generational garbage collection algorithm was added to detect groups
of objects involved in reference cycles�which may be unreachable even with outstanding
references to them, when all the mutual references are contained within the group.
Other implementations of Python have more sophisticated garbage collectors that do
not rely on reference counting, which means the __del__ method may not be called
immediately when there are no more references to the object. See �PyPy, Garbage Collection,
and a Deadlock� by A. Jesse Jiryu Davis for discussion of improper and proper
use of __del__.
To demonstrate the end of an object�s life, Example 8-16 uses weakref.finalize to
register a callback function to be called when an object is destroyed.
Example 8-16. Watching the end of an object when no more references point to it
>>> import weakref
>>> s1 = {1, 2, 3}
>>> s2 = s1
>>> def bye():
... print('Gone with the wind...')
...
>>> ender = weakref.finalize(s1, bye)
>>> ender.alive
True
>>> del s1
>>> ender.alive
True
>>> s2 = 'spam'
Gone with the wind...
>>> ender.alive
False
s1 and s2 are aliases referring to the same set, {1, 2, 3}.
del and Garbage Collection | 235
This function must not be a bound method of the object about to be destroyed
or otherwise hold a reference to it.
Register the bye callback on the object referred by s1.
The .alive attribute is True before the finalize object is called.
As discussed, del does not delete an object, just a reference to it.
Rebinding the last reference, s2, makes {1, 2, 3} unreachable. It is destroyed,
the bye callback is invoked, and ender.alive becomes False.
The point of Example 8-16 is to make explicit that del does not delete objects, but objects
may be deleted as a consequence of being unreachable after del is used.
You may be wondering why the {1, 2, 3} object was destroyed in Example 8-16. After
all, the s1 reference was passed to the finalize function, which must have held on to
it in order to monitor the object and invoke the callback. This works because final
ize holds a weak reference to {1, 2, 3}, as explained in the next section.
Weak References
The presence of references is what keeps an object alive in memory. When the reference
count of an object reaches zero, the garbage collector disposes of it. But sometimes it is
useful to have a reference to an object that does not keep it around longer than necessary.
A common use case is a cache.
Weak references to an object do not increase its reference count. The object that is the
target of a reference is called the referent. Therefore, we say that a weak reference does
not prevent the referent from being garbage collected.
Weak references are useful in caching applications because you don�t want the cached
objects to be kept alive just because they are referenced by the cache.
Example 8-17 shows how a weakref.ref instance can be called to reach its referent. If
the object is alive, calling the weak reference returns it, otherwise None is returned.
Example 8-17 is a console session, and the Python console automatically
binds the _ variable to the result of expressions that are
not None. This interfered with my intended demonstration but also
highlights a practical matter: when trying to micro-manage memory
we are often surprised by hidden, implicit assignments that
create new references to our objects. The _ console variable is one
example. Traceback objects are another common source of unexpected
references.
236 | Chapter 8: Object References, Mutability, and Recycling
Example 8-17. A weak reference is a callable that returns the referenced object or None
if the referent is no more
>>> import weakref
>>> a_set = {0, 1}
>>> wref = weakref.ref(a_set)
>>> wref
<weakref at 0x100637598; to 'set' at 0x100636748>
>>> wref()
{0, 1}
>>> a_set = {2, 3, 4}
>>> wref()
{0, 1}
>>> wref() is None
False
>>> wref() is None
True
The wref weak reference object is created and inspected in the next line.
Invoking wref() returns the referenced object, {0, 1}. Because this is a console
session, the result {0, 1} is bound to the _ variable.
a_set no longer refers to the {0, 1} set, so its reference count is decreased. But
the _ variable still refers to it.
Calling wref() still returns {0, 1}.
When this expression is evaluated, {0, 1} lives, therefore wref() is not None.
But _ is then bound to the resulting value, False. Now there are no more strong
references to {0, 1}.
Because the {0, 1} object is now gone, this last call to wref() returns None.
The weakref module documentation makes the point that the weakref.ref class is
actually a low-level interface intended for advanced uses, and that most programs are
better served by the use of the weakref collections and finalize. In other words, consider
using WeakKeyDictionary, WeakValueDictionary, WeakSet, and finalize
(which use weak references internally) instead of creating and handling your own weak
ref.ref instances by hand. We just did that in Example 8-17 in the hope that showing
a single weakref.ref in action could take away some of the mystery around them. But
in practice, most of the time Python programs use the weakref collections.
The next subsection briefly discusses the weakref collections.
The WeakValueDictionary Skit
The class WeakValueDictionary implements a mutable mapping where the values are
weak references to objects. When a referred object is garbage collected elsewhere in the
Weak References | 237
3. cheeseshop.python.org is also an alias for PyPI�the Python Package Index software repository�which
started its life quite empty. At the time of this writing, the Python Cheese Shop has 41,426 packages. Not bad,
but still far from the more than 131,000 modules available in CPAN�the Comprehensive Perl Archive Network�
the envy of all dynamic language communities.
4. Parmesan cheese is aged at least a year at the factory, so it is more durable than fresh cheese, but this is not
the answer we are looking for.
program, the corresponding key is automatically removed from WeakValueDiction
ary. This is commonly used for caching.
Our demonstration of a WeakValueDictionary is inspired by the classic Cheese Shop
skit by Monty Python, in which a customer asks for more than 40 kinds of cheese,
including cheddar and mozzarella, but none are in stock.3
Example 8-18 implements a trivial class to represent each kind of cheese.
Example 8-18. Cheese has a kind attribute and a standard representation
class Cheese:
def __init__(self, kind):
self.kind = kind
def __repr__(self):
return 'Cheese(%r)' % self.kind
In Example 8-19, each cheese is loaded from a catalog to a stock implemented as a
WeakValueDictionary. However, all but one disappear from the stock as soon as the
catalog is deleted. Can you explain why the Parmesan cheese lasts longer than the
others?4 The tip after the code has the answer.
Example 8-19. Customer: �Have you in fact got any cheese here at all?�
>>> import weakref
>>> stock = weakref.WeakValueDictionary()
>>> catalog = [Cheese('Red Leicester'), Cheese('Tilsit'),
... Cheese('Brie'), Cheese('Parmesan')]
...
>>> for cheese in catalog:
... stock[cheese.kind] = cheese
...
>>> sorted(stock.keys())
['Brie', 'Parmesan', 'Red Leicester', 'Tilsit']
>>> del catalog
>>> sorted(stock.keys())
['Parmesan']
>>> del cheese
>>> sorted(stock.keys())
[]
238 | Chapter 8: Object References, Mutability, and Recycling
stock is a WeakValueDictionary.
The stock maps the name of the cheese to a weak reference to the cheese instance
in the catalog.
The stock is complete.
After the catalog is deleted, most cheeses are gone from the stock, as expected
in WeakValueDictionary. Why not all, in this case?
A temporary variable may cause an object to last longer than expected
by holding a reference to it. This is usually not a problem
with local variables: they are destroyed when the function returns.
But in Example 8-19, the for loop variable cheese is a global
variable and will never go away unless explicitly deleted.
A counterpart to the WeakValueDictionary is the WeakKeyDictionary in which the
keys are weak references. The weakref.WeakKeyDictionary documentation hints on
possible uses:
[A WeakKeyDictionary] can be used to associate additional data with an object owned
by other parts of an application without adding attributes to those objects. This can be
especially useful with objects that override attribute accesses.
The weakref module also provides a WeakSet, simply described in the docs as �Set class
that keeps weak references to its elements. An element will be discarded when no strong
reference to it exists any more.� If you need to build a class that is aware of every one of
its instances, a good solution is to create a class attribute with a WeakSet to hold the
references to the instances. Otherwise, if a regular set was used, the instances would
never be garbage collected, because the class itself would have strong references to them,
and classes live as long as the Python process unless you deliberately delete them.
These collections, and weak references in general, are limited in the kinds of objects
they can handle. The next section explains.
Limitations of Weak References
Not every Python object may be the target, or referent, of a weak reference. Basic list
and dict instances may not be referents, but a plain subclass of either can solve this
problem easily:
class MyList(list):
"""list subclass whose instances may be weakly referenced"""
a_list = MyList(range(10))
Weak References | 239
5. This is clearly documented. Type help(tuple) in the Python console to read: �If the argument is a tuple, the
return value is the same object.� I thought I knew everything about tuples before writing this book.
# a_list can be the target of a weak reference
wref_to_a_list = weakref.ref(a_list)
A set instance can be a referent, and that�s why a set was used in Example 8-17. Userdefined
types also pose no problem, which explains why the silly Cheese class was
needed in Example 8-19. But int and tuple instances cannot be targets of weak references,
even if subclasses of those types are created.
Most of these limitations are implementation details of CPython that may not apply to
other Python iterpreters. They are the result of internal optimizations, some of which
are discussed in the following (highly optional) section.
Tricks Python Plays with Immutables
You may safely skip this section. It discusses some Python implementation
details that are not really important for users of Python.
They are shortcuts and optimizations done by the CPython core
developers, which should not bother you when using the language,
and that may not apply to other Python implementations
or even future versions of CPython. Nevertheless, while experimenting
with aliases and copies you may stumble upon these
tricks, so I felt they were worth mentioning.
I was surprised to learn that, for a tuple t, t[:] does not make a copy, but returns a
reference to the same object. You also get a reference to the same tuple if you write
tuple(t).5 Example 8-20 proves it.
Example 8-20. A tuple built from another is actually the same exact tuple
>>> t1 = (1, 2, 3)
>>> t2 = tuple(t1)
>>> t2 is t1
True
>>> t3 = t1[:]
>>> t3 is t1
True
t1 and t2 are bound to the same object.
And so is t3.
The same behavior can be observed with instances of str, bytes, and frozenset. Note
that a frozenset is not a sequence, so fs[:] does not work if fs is a frozenset. But
240 | Chapter 8: Object References, Mutability, and Recycling
6. The white lie of having the copy method not copying anything can be explained by interface compatibility:
it makes frozenset more compatible with set. Anyway, it makes no difference to the end user whether two
identical immutable objects are the same or are copies.
fs.copy() has the same effect: it cheats and returns a reference to the same object, and
not a copy at all, as Example 8-21 shows.6
Example 8-21. String literals may create shared objects
>>> t1 = (1, 2, 3)
>>> t3 = (1, 2, 3) #
>>> t3 is t1 #
False
>>> s1 = 'ABC'
>>> s2 = 'ABC' #
>>> s2 is s1 #
True
Creating a new tuple from scratch.
t1 and t3 are equal, but not the same object.
Creating a second str from scratch.
Surprise: a and b refer to the same str!
The sharing of string literals is an optimization technique called interning. CPython
uses the same technique with small integers to avoid unnecessary duplication of �popular�
numbers like 0, �1, and 42. Note that CPython does not intern all strings or integers,
and the criteria it uses to do so is an undocumented implementation detail.
Never depend on str or int interning! Always use == and not is
to compare them for equality. Interning is a feature for internal use
of the Python interpreter.
The tricks discussed in this section, including the behavior of frozenset.copy(), are
�white lies�; they save memory and make the interpreter faster. Do not worry about
them, they should not give you any trouble because they only apply to immutable types.
Probably the best use of these bits of trivia is to win bets with fellow Pythonistas.
Tricks Python Plays with Immutables | 241
7. Actually the type of an object may be changed by merely assigning a different class to its __class__ attribute,
but that is pure evil and I regret writing this footnote.
Chapter Summary
Every Python object has an identity, a type, and a value. Only the value of an object
changes over time.7
If two variables refer to immutable objects that have equal values (a == b is True), in
practice it rarely matters if they refer to copies or are aliases referring to the same object
because the value of an immutable object does not change, with one exception. The
exception is immutable collections such as tuples and frozensets: if an immutable collection
holds references to mutable items, then its value may actually change when the
value of a mutable item changes. In practice, this scenario is not so common. What
never changes in an immutable collection are the identities of the objects within.
The fact that variables hold references has many practical consequences in Python programming:
� Simple assignment does not create copies.
� Augmented assignment with += or *= creates new objects if the lefthand variable is
bound to an immutable object, but may modify a mutable object in place.
� Assigning a new value to an existing variable does not change the object previously
bound to it. This is called a rebinding: the variable is now bound to a different object.
If that variable was the last reference to the previous object, that object will be
garbage collected.
� Function parameters are passed as aliases, which means the function may change
any mutable object received as an argument. There is no way to prevent this, except
making local copies or using immutable objects (e.g., passing a tuple instead of a
list).
� Using mutable objects as default values for function parameters is dangerous because
if the parameters are changed in place, then the default is changed, affecting
every future call that relies on the default.
In CPython, objects are discarded as soon as the number of references to them reaches
zero. They may also be discarded if they form groups with cyclic references but no
outside references. In some situations, it may be useful to hold a reference to an object
that will not�by itself�keep an object alive. One example is a class that wants to keep
track of all its current instances. This can be done with weak references, a low-level
mechanism underlying the more useful collections WeakValueDictionary, WeakKey
Dictionary, WeakSet, and the finalize function from the weakref module.
242 | Chapter 8: Object References, Mutability, and Recycling
Further Reading
The �Data Model� chapter of The Python Language Reference starts with a clear explanation
of object identities and values.
Wesley Chun, author of the Core Python series of books, made a great presentation about
many of the topics covered in this chapter during OSCON 2013. You can download the
slides from the �Python 103: Memory Model & Best Practices� talk page. There is also
a YouTube video of a longer presentation Wesley gave at EuroPython 2011, covering
not only the theme of this chapter but also the use of special methods.
Doug Hellmann wrote a long series of excellent blog posts titled Python Module of the
Week, which became a book, The Python Standard Library by Example. His posts �copy
� Duplicate Objects� and �weakref � Garbage-Collectable References to Objects� cover
some of the topics we just discussed.
More information on the CPython generational garbage collector can be found in the
gc module documentation, which starts with the sentence �This module provides an
interface to the optional garbage collector.� The �optional� qualifier here may be surprising,
but the �Data Model� chapter also states:
An implementation is allowed to postpone garbage collection or omit it altogether�it is
a matter of implementation quality how garbage collection is implemented, as long as no
objects are collected that are still reachable.
Fredrik Lundh�creator of key libraries like ElementTree, Tkinter, and the PIL image
library�has a short post about the Python garbage collector titled �How Does Python
Manage Memory?� He emphasizes that the garbage collector is an implementation feature
that behaves differently across Python interpreters. For example, Jython uses the
Java garbage collector.
The CPython 3.4 garbage collector improved handling of objects with a __del__ method,
as described in PEP 442 � Safe object finalization.
Wikipedia has an article about string interning, mentioning the use of this technique
in several languages, including Python.
Further Reading | 243
Soapbox
Equal Treatment to All Objects
I learned Java before I discovered Python. The == operator in Java never felt right for
me. It is much more common for programmers to care about equality than identity, but
for objects (not primitive types) the Java == compares references, and not object values.
Even for something as basic as comparing strings, Java forces you to use the .equals
method. Even then, there is another catch: if you write a.equals(b) and a is null, you
get a null pointer exception. The Java designers felt the need to overload + for strings,
so why not go ahead and overload == as well?
Python gets this right. The == operator compares object values and is compares references.
And because Python has operator overloading, == works sensibly with all objects
in the standard library, including None, which is a proper object, unlike Java�s null.
And of course, you can define __eq__ in your own classes to decide what == means for
your instances. If you don�t override __eq__, the method inherited from object compares
object IDs, so the fallback is that every instance of a user-defined class is considered
different.
These are some of the things that made me switch from Java to Python as soon as I
finished reading the Python Tutorial one afternoon in September 1998.
Mutability
This chapter would be redundant if all Python objects were immutable. When you are
dealing with unchanging objects, it makes no difference whether variables hold the
actual objects or references to shared objects. If a == b is true, and neither object can
change, they might as well be the same. That�s why string interning is safe. Object identity
becomes important only when objects are mutable.
In �pure� functional programming, all data is immutable: appending to a collection
actually creates a new collection. Python, however, is not a functional language, much
less a pure one. Instances of user-defined classes are mutable by default in Python�as
in most object-oriented languages. When creating your own objects, you have to be
extra careful to make them immutable, if that is a requirement. Every attribute of the
object must also be immutable, otherwise you end up with something like the tuple:
immutable as far as object IDs go, but the value of a tuple may change if it holds a
mutable object.
Mutable objects are also the main reason why programming with threads is so hard to
get right: threads mutating objects without proper synchronization produce corrupted
data. Excessive synchronization, on the other hand, causes deadlocks.
244 | Chapter 8: Object References, Mutability, and Recycling
Object Destruction and Garbage Collection
There is no mechanism in Python to directly destroy an object, and this omission is
actually a great feature: if you could destroy an object at any time, what would happen
to existing strong references pointing to it?
Garbage collection in CPython is done primarily by reference counting, which is easy
to implement, but is prone to memory leaking when there are reference cycles, so with
version 2.0 (October 2000) a generational garbage collector was implemented, and it is
able to dispose of unreachable objects kept alive by reference cycles.
But the reference counting is still there as a baseline, and it causes the immediate disposal
of objects with zero references. This means that, in CPython�at least for now�it�s safe
to write this:
open('test.txt', 'wt', encoding='utf-8').write('1, 2, 3')
That code is safe because the reference count of the file object will be zero after the write
method returns, and Python will immediately close the file before destroying the object
representing it in memory. However, the same line is not safe in Jython or IronPython
that use the garbage collector of their host runtimes (the Java VM and the .NET CLR),
which are more sophisticated but do not rely on reference counting and may take longer
to destroy the object and close the file. In all cases, including CPython, the best practice
is to explicitly close the file, and the most reliable way of doing it is using the with
statement, which guarantees that the file will be closed even if exceptions are raised while
it is open. Using with, the previous snippet becomes:
with open('test.txt', 'wt', encoding='utf-8') as fp:
fp.write('1, 2, 3')
If you are into the subject of garbage collectors, you may want to read Thomas Perl�s
paper �Python Garbage Collector Implementations: CPython, PyPy and GaS�, from
which I learned the bit about the safety of the open().write() in CPython.
Parameter Passing: Call by Sharing
A popular way of explaining how parameter passing works in Python is the phrase:
�Parameters are passed by value, but the values are references.� This not wrong, but
causes confusion because the most common parameter passing modes in older languages
are call by value (the function gets a copy of the argument) and call by reference
(the function gets a pointer to the argument). In Python, the function gets a copy
of the arguments, but the arguments are always references. So the value of the referenced
objects may be changed, if they are mutable, but their identity cannot. Also, because the
function gets a copy of the reference in an argument, rebinding it has no effect outside
of the function. I adopted the term call by sharing after reading up on the subject in
Programming Language Pragmatics, Third Edition by Michael L. Scott (Morgan Kaufmann),
particularly �8.3.1: Parameter Modes.�
Further Reading | 245
The Full Quote of Alice and the Knights�s Song
I love this passage, but it was too long as a chapter opener. So here is the complete dialog
about the Knight�s song, its name, and how the song and its name are called:
�You are sad,� the Knight said in an anxious tone: �let me sing you a song to comfort you.�
�Is it very long?� Alice asked, for she had heard a good deal of poetry that day.
�It�s long,� said the Knight, �but very, VERY beautiful. Everybody that hears me sing it�
either it brings the TEARS into their eyes, or else��
�Or else what?� said Alice, for the Knight had made a sudden pause.
�Or else it doesn�t, you know. The name of the song is called �HADDOCKS� EYES�.�
�Oh, that�s the name of the song, is it?� Alice said, trying to feel interested.
�No, you don�t understand,� the Knight said, looking a little vexed. �That�s what the name
is CALLED. The name really IS �THE AGED AGED MAN�.�
�Then I ought to have said �That�s what the SONG is called�?� Alice corrected herself.
�No, you oughtn�t: that�s quite another thing! The SONG is called �WAYS AND
MEANS�: but that�s only what it�s CALLED, you know!�
�Well, what IS the song, then?� said Alice, who was by this time completely bewildered.
�I was coming to that,� the Knight said. �The song really IS �A-SITTING ON A GATE�:
and the tune�s my own invention.�
� Lewis Carroll
Chapter VIII, �It�s My Own Invention,� Through the Looking-Glass
246 | Chapter 8: Object References, Mutability, and Recycling
1. From the Paste Style Guide.
CHAPTER 9
A Pythonic Object
Never, ever use two leading underscores. This is annoyingly private.1
� Ian Bicking
Creator of pip, virtualenv, Paste and many other projects
Thanks to the Python data model, your user-defined types can behave as naturally as
the built-in types. And this can be accomplished without inheritance, in the spirit of
duck typing: you just implement the methods needed for your objects to behave as
expected.
In previous chapters, we presented the structure and behavior of many built-in objects.
We will now build user-defined classes that behave as real Python objects.
This chapter starts where Chapter 1 ended, by showing how to implement several special
methods that are commonly seen in Python objects of many different types.
In this chapter, we will see how to:
� Support the built-in functions that produce alternative object representations (e.g.,
repr(), bytes(), etc).
� Implement an alternative constructor as a class method.
� Extend the format mini-language used by the format() built-in and the str.for
mat() method.
� Provide read-only access to attributes.
� Make an object hashable for use in sets and as dict keys.
� Save memory with the use of __slots__.
247
We�ll do all that as we develop a simple two-dimensional Euclidean vector type.
The evolution of the example will be paused to discuss two conceptual topics:
� How and when to use the @classmethod and @staticmethod decorators.
� Private and protected attributes in Python: usage, conventions, and limitations.
Let�s get started with the object representation methods.
Object Representations
Every object-oriented language has at least one standard way of getting a string representation
from any object. Python has two:
repr()
Return a string representing the object as the developer wants to see it.
str()
Return a string representing the object as the user wants to see it.
As you know, we implement the special methods __repr__ and __str__ to support
repr() and str().
There are two additional special methods to support alternative representations of objects:
__bytes__ and __format__. The __bytes__ method is analogous to __str__: it�s
called by bytes() to get the object represented as a byte sequence. Regarding __for
mat__, both the built-in function format() and the str.format() method call it to get
string displays of objects using special formatting codes. We�ll cover __bytes__ in the
next example, and __format__ after that.
If you�re coming from Python 2, remember that in Python 3
__repr__, __str__, and __format__ must always return Unicode
strings (type str). Only __bytes__ is supposed to return a byte
sequence (type bytes).
Vector Class Redux
In order to demonstrate the many methods used to generate object representations,
we�ll use a Vector2d class similar to the one we saw in Chapter 1. We will build on it in
this and future sections. Example 9-1 illustrates the basic behavior we expect from a
Vector2d instance.
Example 9-1. Vector2d instances have several representations
>>> v1 = Vector2d(3, 4)
>>> print(v1.x, v1.y)
248 | Chapter 9: A Pythonic Object
2. I used eval to clone the object here just to make a point about repr; to clone an instance, the copy.copy
function is safer and faster.
3.0 4.0
>>> x, y = v1
>>> x, y
(3.0, 4.0)
>>> v1
Vector2d(3.0, 4.0)
>>> v1_clone = eval(repr(v1))
>>> v1 == v1_clone
True
>>> print(v1)
(3.0, 4.0)
>>> octets = bytes(v1)
>>> octets
b'd\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@'
>>> abs(v1)
5.0
>>> bool(v1), bool(Vector2d(0, 0))
(True, False)
The components of a Vector2d can be accessed directly as attributes (no getter
method calls).
A Vector2d can be unpacked to a tuple of variables.
The repr of a Vector2d emulates the source code for constructing the instance.
Using eval here shows that the repr of a Vector2d is a faithful representation
of its constructor call.2
Vector2d supports comparison with ==; this is useful for testing.
print calls str, which for Vector2d produces an ordered pair display.
bytes uses the __bytes__ method to produce a binary representation.
abs uses the __abs__ method to return the magnitude of the Vector2d.
bool uses the __bool__ method to return False for a Vector2d of zero
magnitude or True otherwise.
Vector2d from Example 9-1 is implemented in vector2d_v0.py (Example 9-2). The code
is based on Example 1-2, but the infix operators will be implemented in Chapter 13�
except for == (which is useful for testing). At this point, Vector2d uses several special
methods to provide operations that a Pythonista expects in a well-designed object.
Example 9-2. vector2d_v0.py: methods so far are all special methods
from array import array
import math
Vector Class Redux | 249
3. This line could also be written as yield self.x; yield.self.y. I have a lot more to say about the
__iter__ special method, generator expressions, and the yield keyword in Chapter 14.
class Vector2d:
typecode = 'd'
def __init__(self, x, y):
self.x = float(x)
self.y = float(y)
def __iter__(self):
return (i for i in (self.x, self.y))
def __repr__(self):
class_name = type(self).__name__
return '{}({!r}, {!r})'.format(class_name, *self)
def __str__(self):
return str(tuple(self))
def __bytes__(self):
return (bytes([ord(self.typecode)]) +
bytes(array(self.typecode, self)))
def __eq__(self, other):
return tuple(self) == tuple(other)
def __abs__(self):
return math.hypot(self.x, self.y)
def __bool__(self):
return bool(abs(self))
typecode is a class attribute we�ll use when converting Vector2d instances to/
from bytes.
Converting x and y to float in __init__ catches errors early, which is helpful
in case Vector2d is called with unsuitable arguments.
__iter__ makes a Vector2d iterable; this is what makes unpacking work (e.g,
x, y = my_vector). We implement it simply by using a generator expression
to yield the components one after the other.3
__repr__ builds a string by interpolating the components with {!r} to get their
repr; because Vector2d is iterable, *self feeds the x and y components to
format.
From an iterable Vector2d, it�s easy to build a tuple for display as an ordered
pair.
250 | Chapter 9: A Pythonic Object
To generate bytes, we convert the typecode to bytes and concatenate�
�bytes converted from an array built by iterating over the instance.
To quickly compare all components, build tuples out of the operands. This works
for operands that are instances of Vector2d, but has issues. See the following
warning.
The magnitude is the length of the hypotenuse of the triangle formed by the x
and y components.
__bool__ uses abs(self) to compute the magnitude, then converts it to bool,
so 0.0 becomes False, nonzero is True.
Method __eq__ in Example 9-2 works for Vector2d operands but
also returns True when comparing Vector2d instances to other
iterables holding the same numeric values (e.g., Vector(3, 4) ==
[3, 4]). This may be considered a feature or a bug. Further discussion
needs to wait until Chapter 13, when we cover operator
overloading.
We have a fairly complete set of basic methods, but one obvious operation is missing:
rebuilding a Vector2d from the binary representation produced by bytes().
An Alternative Constructor
Because we can export a Vector2d as bytes, naturally we need a method that imports a
Vector2d from a binary sequence. Looking at the standard library for inspiration, we
find that array.array has a class method named .frombytes that suits our purpose�
we saw it in �Arrays� on page 48. We adopt its name and use its functionality in a class
method for Vector2d in vector2d_v1.py (Example 9-3).
Example 9-3. Part of vector2d_v1.py: this snippet shows only the frombytes class method,
added to the Vector2d definition in vector2d_v0.py (Example 9-2)
@classmethod
def frombytes(cls, octets):
typecode = chr(octets[0])
memv = memoryview(octets[1:]).cast(typecode)
return cls(*memv)
Class method is modified by the classmethod decorator.
No self argument; instead, the class itself is passed as cls.
Read the typecode from the first byte.
An Alternative Constructor | 251
4. We had a brief introduction to memoryview, explaining its .cast method in �Memory Views� on page 51.
Create a memoryview from the octets binary sequence and use the typecode to
cast it.4
Unpack the memoryview resulting from the cast into the pair of arguments
needed for the constructor.
Because we just used a classmethod decorator, and it is very Python-specific, let�s have
a word about it.
classmethod Versus staticmethod
The classmethod decorator is not mentioned in the Python tutorial, and neither is
staticmethod. Anyone who has learned OO in Java may wonder why Python has both
of these decorators and not just one of them.
Let�s start with classmethod. Example 9-3 shows its use: to define a method that operates
on the class and not on instances. classmethod changes the way the method is called,
so it receives the class itself as the first argument, instead of an instance. Its most common
use is for alternative constructors, like frombytes in Example 9-3. Note how the
last line of frombytes actually uses the cls argument by invoking it to build a new
instance: cls(*memv). By convention, the first parameter of a class method should be
named cls (but Python doesn�t care how it�s named).
In contrast, the staticmethod decorator changes a method so that it receives no special
first argument. In essence, a static method is just like a plain function that happens to
live in a class body, instead of being defined at the module level. Example 9-4 contrasts
the operation of classmethod and staticmethod.
Example 9-4. Comparing behaviors of classmethod and staticmethod
>>> class Demo:
... @classmethod
... def klassmeth(*args):
... return args #
... @staticmethod
... def statmeth(*args):
... return args #
...
>>> Demo.klassmeth() #
(<class '__main__.Demo'>,)
>>> Demo.klassmeth('spam')
(<class '__main__.Demo'>, 'spam')
>>> Demo.statmeth() #
()
252 | Chapter 9: A Pythonic Object
5. Leonardo Rochael, one of the technical reviewers of this book disagrees with my low opinion of staticme
thod, and recommends the blog post �The Definitive Guide on How to Use Static, Class or Abstract Methods
in Python� by Julien Danjou as a counter-argument. Danjou�s post is very good; I do recommend it. But it
wasn�t enough to change my mind about staticmethod. You�ll have to decide for yourself.
>>> Demo.statmeth('spam')
('spam',)
klassmeth just returns all positional arguments.
statmeth does the same.
No matter how you invoke it, Demo.klassmeth receives the Demo class as the first
argument.
Demo.statmeth behaves just like a plain old function.
The classmethod decorator is clearly useful, but I�ve never seen
a compelling use case for staticmethod. If you want to define a
function that does not interact with the class, just define it in the
module. Maybe the function is closely related even if it never
touches the class, so you want to them nearby in the code. Even
so, defining the function right before or after the class in the same
module is close enough for all practical purposes.5
Now that we�ve seen what classmethod is good for (and that staticmethod is not very
useful), let�s go back to the issue of object representation and see how to support formatted
output.
Formatted Displays
The format() built-in function and the str.format() method delegate the actual formatting
to each type by calling their .__format__(format_spec) method. The for
mat_spec is a formatting specifier, which is either:
� The second argument in format(my_obj, format_spec), or
� Whatever appears after the colon in a replacement field delimited with {} inside a
format string used with str.format()
For example:
>>> brl = 1/2.43 # BRL to USD currency conversion rate
>>> brl
0.4115226337448559
>>> format(brl, '0.4f') #
'0.4115'
Formatted Displays | 253
>>> '1 BRL = {rate:0.2f} USD'.format(rate=brl) #
'1 BRL = 0.41 USD'
Formatting specifier is '0.4f'.
Formatting specifier is '0.2f'. The 'rate' substring in the replacement field is
called the field name. It�s unrelated to the formatting specifier, but determines
which argument of .format() goes into that replacement field.
The second callout makes an important point: a format string such as '{0.mass:
5.3e}' actually uses two separate notations. The '0.mass' to the left of the colon is the
field_name part of the replacement field syntax; the '5.3e' after the colon is the formatting
specifier. The notation used in the formatting specifier is called the Format
Specification Mini-Language.
If format() and str.format() are new to you, classroom experience
has shown that it�s best to study the format() function first,
which uses just the Format Specification Mini-Language. After you
get the gist of that, read Format String Syntax to learn about the
{:} replacement field notation, used in the str.format() method
(including the !s, !r, and !a conversion flags).
A few built-in types have their own presentation codes in the Format Specification Mini-
Language. For example�among several other codes�the int type supports b and x for
base 2 and base 16 output, respectively, while float implements f for a fixed-point
display and % for a percentage display:
>>> format(42, 'b')
'101010'
>>> format(2/3, '.1%')
'66.7%'
The Format Specification Mini-Language is extensible because each class gets to interpret
the format_spec argument as it likes. For instance, the classes in the datetime
module use the same format codes in the strftime() functions and in their __for
mat__ methods. Here are a couple examples using the format() built-in and the
str.format() method:
>>> from datetime import datetime
>>> now = datetime.now()
>>> format(now, '%H:%M:%S')
'18:49:05'
>>> "It's now {:%I:%M %p}".format(now)
"It's now 06:49 PM"
If a class has no __format__, the method inherited from object returns str(my_ob
ject). Because Vector2d has a __str__, this works:
254 | Chapter 9: A Pythonic Object
>>> v1 = Vector2d(3, 4)
>>> format(v1)
'(3.0, 4.0)'
However, if you pass a format specifier, object.__format__ raises TypeError:
>>> format(v1, '.3f')
Traceback (most recent call last):
...
TypeError: non-empty format string passed to object.__format__
We will fix that by implementing our own format mini-language. The first step will be
to assume the format specifier provided by the user is intended to format each float
component of the vector. This is the result we want:
>>> v1 = Vector2d(3, 4)
>>> format(v1)
'(3.0, 4.0)'
>>> format(v1, '.2f')
'(3.00, 4.00)'
>>> format(v1, '.3e')
'(3.000e+00, 4.000e+00)'
Example 9-5 implements __format__ to produce the displays just shown.
Example 9-5. Vector2d.format method, take #1
# inside the Vector2d class
def __format__(self, fmt_spec=''):
components = (format(c, fmt_spec) for c in self) #
return '({}, {})'.format(*components) #
Use the format built-in to apply the fmt_spec to each vector component,
building an iterable of formatted strings.
Plug the formatted strings in the formula '(x, y)'.
Now let�s add a custom formatting code to our mini-language: if the format specifier
ends with a 'p', we�ll display the vector in polar coordinates: <r, ?>, where r is the
magnitude and ? (theta) is the angle in radians. The rest of the format specifier (whatever
comes before the 'p') will be used as before.
When choosing the letter for the custom format code I avoided
overlapping with codes used by other types. In Format Specification
Mini-Language we see that integers use the codes 'bcdoxXn',
floats use 'eEfFgGn%', and strings use 's'. So I picked 'p' for polar
coordinates. Because each class interprets these codes independently,
reusing a code letter in a custom format for a new type is not
an error, but may be confusing to users.
Formatted Displays | 255
To generate polar coordinates we already have the __abs__ method for the magnitude,
and we�ll code a simple angle method using the math.atan2() function to get the angle.
This is the code:
# inside the Vector2d class
def angle(self):
return math.atan2(self.y, self.x)
With that, we can enhance our __format__ to produce polar coordinates. See
Example 9-6.
Example 9-6. Vector2d.format method, take #2, now with polar coordinates
def __format__(self, fmt_spec=''):
if fmt_spec.endswith('p'):
fmt_spec = fmt_spec[:-1]
coords = (abs(self), self.angle())
outer_fmt = '<{}, {}>'
else:
coords = self
outer_fmt = '({}, {})'
components = (format(c, fmt_spec) for c in coords)
return outer_fmt.format(*components)
Format ends with 'p': use polar coordinates.
Remove 'p' suffix from fmt_spec.
Build tuple of polar coordinates: (magnitude, angle).
Configure outer format with angle brackets.
Otherwise, use x, y components of self for rectangular coordinates.
Configure outer format with parentheses.
Generate iterable with components as formatted strings.
Plug formatted strings into outer format.
With Example 9-6, we get results similar to these:
>>> format(Vector2d(1, 1), 'p')
'<1.4142135623730951, 0.7853981633974483>'
>>> format(Vector2d(1, 1), '.3ep')
'<1.414e+00, 7.854e-01>'
>>> format(Vector2d(1, 1), '0.5fp')
'<1.41421, 0.78540>'
As this section shows, it�s not hard to extend the format specification mini-language to
support user-defined types.
256 | Chapter 9: A Pythonic Object
Now let�s move to a subject that�s not just about appearances: we will make our Vec
tor2d hashable, so we can build sets of vectors, or use them as dict keys. But before we
can do that, we must make vectors immutable. We�ll do what it takes next.
A Hashable Vector2d
As defined, so far our Vector2d instances are unhashable, so we can�t put them in a set:
>>> v1 = Vector2d(3, 4)
>>> hash(v1)
Traceback (most recent call last):
...
TypeError: unhashable type: 'Vector2d'
>>> set([v1])
Traceback (most recent call last):
...
TypeError: unhashable type: 'Vector2d'
To make a Vector2d hashable, we must implement __hash__ (__eq__ is also required,
and we already have it). We also need to make vector instances immutable, as we�ve seen
in �What Is Hashable?� on page 65.
Right now, anyone can do v1.x = 7 and there is nothing in the code to suggest that
changing a Vector2d is forbidden. This is the behavior we want:
>>> v1.x, v1.y
(3.0, 4.0)
>>> v1.x = 7
Traceback (most recent call last):
...
AttributeError: can't set attribute
We�ll do that by making the x and y components read-only properties in Example 9-7.
Example 9-7. vector2d_v3.py: only the changes needed to make Vector2d immutable
are shown here; see full listing in Example 9-9
class Vector2d:
typecode = 'd'
def __init__(self, x, y):
self.__x = float(x)
self.__y = float(y)
@property
def x(self):
return self.__x
@property
def y(self):
return self.__y
A Hashable Vector2d | 257
6. This is not how Ian Bicking would do it; recall the quote at the start of the chapter. The pros and cons of
private attributes are the subject of the upcoming �Private and �Protected� Attributes in Python� on page 262.
def __iter__(self):
return (i for i in (self.x, self.y))
# remaining methods follow (omitted in book listing)
Use exactly two leading underscores (with zero or one trailing underscore) to
make an attribute private.6
The @property decorator marks the getter method of a property.
The getter method is named after the public property it exposes: x.
Just return self.__x.
Repeat same formula for y property.
Every method that just reads the x, y components can stay as they were, reading
the public properties via self.x and self.y instead of the private attribute, so
this listing omits the rest of the code for the class.
Vector.x and Vector.y are examples of read-only properties.
Read/write properties will be covered in Chapter 19, where we
dive deeper into the @property.
Now that our vectors are reasonably immutable, we can implement the __hash__
method. It should return an int and ideally take into account the hashes of the object
attributes that are also used in the __eq__ method, because objects that compare equal
should have the same hash. The __hash__ special method documentation suggests using
the bitwise XOR operator (^) to mix the hashes of the components, so that�s what we
do. The code for our Vector2d.__hash__ method is really simple, as shown in
Example 9-8.
Example 9-8. vector2d_v3.py: implementation of hash
# inside class Vector2d:
def __hash__(self):
return hash(self.x) ^ hash(self.y)
With the addition of the __hash__ method, we now have hashable vectors:
>>> v1 = Vector2d(3, 4)
>>> v2 = Vector2d(3.1, 4.2)
>>> hash(v1), hash(v2)
258 | Chapter 9: A Pythonic Object
(7, 384307168202284039)
>>> set([v1, v2])
{Vector2d(3.1, 4.2), Vector2d(3.0, 4.0)}
It�s not strictly necessary to implement properties or otherwise
protect the instance attributes to create a hashable type. Implementing
__hash__ and __eq__ correctly is all it takes. But the hash
value of an instance is never supposed to change, so this provides
an excellent opportunity to talk about read-only properties.
If you are creating a type that has a sensible scalar numeric value, you may also implement
the __int__ and __float__ methods, invoked by the int() and float() constructors�
which are used for type coercion in some contexts. There�s also a __com
plex__ method to support the complex() built-in constructor. Perhaps Vector2d
should provide __complex__, but I�ll leave that as an exercise for you.
We have been working on Vector2d for a while, showing just snippets, so Example 9-9
is a consolidated, full listing of vector2d_v3.py, including all the doctests I used when
developing it.
Example 9-9. vector2d_v3.py: the full monty
"""
A two-dimensional vector class
>>> v1 = Vector2d(3, 4)
>>> print(v1.x, v1.y)
3.0 4.0
>>> x, y = v1
>>> x, y
(3.0, 4.0)
>>> v1
Vector2d(3.0, 4.0)
>>> v1_clone = eval(repr(v1))
>>> v1 == v1_clone
True
>>> print(v1)
(3.0, 4.0)
>>> octets = bytes(v1)
>>> octets
b'd\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@'
>>> abs(v1)
5.0
>>> bool(v1), bool(Vector2d(0, 0))
(True, False)
Test of ``.frombytes()`` class method:
A Hashable Vector2d | 259
>>> v1_clone = Vector2d.frombytes(bytes(v1))
>>> v1_clone
Vector2d(3.0, 4.0)
>>> v1 == v1_clone
True
Tests of ``format()`` with Cartesian coordinates:
>>> format(v1)
'(3.0, 4.0)'
>>> format(v1, '.2f')
'(3.00, 4.00)'
>>> format(v1, '.3e')
'(3.000e+00, 4.000e+00)'
Tests of the ``angle`` method::
>>> Vector2d(0, 0).angle()
0.0
>>> Vector2d(1, 0).angle()
0.0
>>> epsilon = 10**-8
>>> abs(Vector2d(0, 1).angle() - math.pi/2) < epsilon
True
>>> abs(Vector2d(1, 1).angle() - math.pi/4) < epsilon
True
Tests of ``format()`` with polar coordinates:
>>> format(Vector2d(1, 1), 'p') # doctest:+ELLIPSIS
'<1.414213..., 0.785398...>'
>>> format(Vector2d(1, 1), '.3ep')
'<1.414e+00, 7.854e-01>'
>>> format(Vector2d(1, 1), '0.5fp')
'<1.41421, 0.78540>'
Tests of `x` and `y` read-only properties:
>>> v1.x, v1.y
(3.0, 4.0)
>>> v1.x = 123
Traceback (most recent call last):
...
AttributeError: can't set attribute
Tests of hashing:
260 | Chapter 9: A Pythonic Object
>>> v1 = Vector2d(3, 4)
>>> v2 = Vector2d(3.1, 4.2)
>>> hash(v1), hash(v2)
(7, 384307168202284039)
>>> len(set([v1, v2]))
2
"""
from array import array
import math
class Vector2d:
typecode = 'd'
def __init__(self, x, y):
self.__x = float(x)
self.__y = float(y)
@property
def x(self):
return self.__x
@property
def y(self):
return self.__y
def __iter__(self):
return (i for i in (self.x, self.y))
def __repr__(self):
class_name = type(self).__name__
return '{}({!r}, {!r})'.format(class_name, *self)
def __str__(self):
return str(tuple(self))
def __bytes__(self):
return (bytes([ord(self.typecode)]) +
bytes(array(self.typecode, self)))
def __eq__(self, other):
return tuple(self) == tuple(other)
def __hash__(self):
return hash(self.x) ^ hash(self.y)
def __abs__(self):
return math.hypot(self.x, self.y)
def __bool__(self):
return bool(abs(self))
A Hashable Vector2d | 261
def angle(self):
return math.atan2(self.y, self.x)
def __format__(self, fmt_spec=''):
if fmt_spec.endswith('p'):
fmt_spec = fmt_spec[:-1]
coords = (abs(self), self.angle())
outer_fmt = '<{}, {}>'
else:
coords = self
outer_fmt = '({}, {})'
components = (format(c, fmt_spec) for c in coords)
return outer_fmt.format(*components)
@classmethod
def frombytes(cls, octets):
typecode = chr(octets[0])
memv = memoryview(octets[1:]).cast(typecode)
return cls(*memv)
To recap, in this and the previous sections, we saw some essential special methods that
you may want to implement to have a full-fledged object. Of course, it is a bad idea to
implement all of these methods if your application has no real use for them. Customers
don�t care if your objects are �Pythonic� or not.
As coded in Example 9-9, Vector2d is a didactic example with a laundry list of special
methods related to object representation, not a template for every user-defined class.
In the next section, we�ll take a break from Vector2d to discuss the design and drawbacks
of the private attribute mechanism in Python�the double-underscore prefix in
self.__x.
Private and �Protected� Attributes in Python
In Python, there is no way to create private variables like there is with the private
modifier in Java. What we have in Python is a simple mechanism to prevent accidental
overwriting of a �private� attribute in a subclass.
Consider this scenario: someone wrote a class named Dog that uses a mood instance
attribute internally, without exposing it. You need to subclass Dog as Beagle. If you create
your own mood instance attribute without being aware of the name clash, you will clobber
the mood attribute used by the methods inherited from Dog. This would be a pain to
debug.
To prevent this, if you name an instance attribute in the form __mood (two leading
underscores and zero or at most one trailing underscore), Python stores the name in
the instance __dict__ prefixed with a leading underscore and the class name, so in the
262 | Chapter 9: A Pythonic Object
Dog class, __mood becomes _Dog__mood, and in Beagle it�s _Beagle__mood. This language
feature goes by the lovely name of name mangling.
Example 9-10 shows the result in the Vector2d class from Example 9-7.
Example 9-10. Private attribute names are �mangled� by prefixing the _ and the class
name
>>> v1 = Vector2d(3, 4)
>>> v1.__dict__
{'_Vector2d__y': 4.0, '_Vector2d__x': 3.0}
>>> v1._Vector2d__x
3.0
Name mangling is about safety, not security: it�s designed to prevent accidental access
and not intentional wrongdoing (Figure 9-1 illustrates another safety device).
Figure 9-1. A cover on a switch is a safety device, not a security one: it prevents accidental
activation, not malicious use
Anyone who knows how private names are mangled can read the private attribute directly,
as the last line of Example 9-10 shows�that�s actually useful for debugging and
serialization. They can also directly assign a value to a private component of a Vector2d
by simply writing v1._Vector__x = 7. But if you are doing that in production code,
you can�t complain if something blows up.
The name mangling functionality is not loved by all Pythonistas, and neither is the
skewed look of names written as self.__x. Some prefer to avoid this syntax and use
just one underscore prefix to �protect� attributes by convention (e.g., self._x). Critics
of the automatic double-underscore mangling suggest that concerns about accidental
attribute clobbering should be addressed by naming conventions. This is the full quote
from the prolific Ian Bicking, cited at the beginning of this chapter:
Private and �Protected� Attributes in Python | 263
7. From the Paste Style Guide.
8. In modules, a single _ in front of a top-level name does have an effect: if you write from mymod import *
the names with a _ prefix are not imported from mymod. However, you can still write from mymod import
_privatefunc. This is explained in the Python Tutorial, section 6.1. More on Modules.
9. One example is in the gettext module docs.
10. If this state of affairs depresses you, and makes you wish Python was more like Java in this regard, don�t read
my discussion of the relative strength of the Java private modifier in �Soapbox� on page 272.
Never, ever use two leading underscores. This is annoyingly private. If name clashes are
a concern, use explicit name mangling instead (e.g., _MyThing_blahblah). This is essentially
the same thing as double-underscore, only it�s transparent where double underscore
obscures.7
The single underscore prefix has no special meaning to the Python interpreter when
used in attribute names, but it�s a very strong convention among Python programmers
that you should not access such attributes from outside the class.8 It�s easy to respect the
privacy of an object that marks its attributes with a single _, just as it�s easy respect the
convention that variables in ALL_CAPS should be treated as constants.
Attributes with a single _ prefix are called �protected� in some corners of the Python
documentation.9 The practice of �protecting� attributes by convention with the form
self._x is widespread, but calling that a �protected� attribute is not so common. Some
even call that a �private� attribute.
To conclude: the Vector2d components are �private� and our Vector2d instances are
�immutable��with scare quotes�because there is no way to make them really private
and immutable.10
We�ll now come back to our Vector2d class. In this final section, we cover a special
attribute (not a method) that affects the internal storage of an object, with potentially
huge impact on the use of memory but little effect on its public interface: __slots__.
Saving Space with the __slots__ Class Attribute
By default, Python stores instance attributes in a per-instance dict named __dict__.
As we saw in �Practical Consequences of How dict Works� on page 90, dictionaries have
a significant memory overhead because of the underlying hash table used to provide
fast access. If you are dealing with millions of instances with few attributes, the
__slots__ class attribute can save a lot of memory, by letting the interpreter store the
instance attributes in a tuple instead of a dict.
264 | Chapter 9: A Pythonic Object
A __slots__ attribute inherited from a superclass has no effect.
Python only takes into account __slots__ attributes defined in
each class individually.
To define __slots__, you create a class attribute with that name and assign it an iterable
of str with identifiers for the instance attributes. I like to use a tuple for that, because
it conveys the message that the __slots__ definition cannot change. See Example 9-11.
Example 9-11. vector2d_v3_slots.py: the slots attribute is the only addition to Vector2d
class Vector2d:
__slots__ = ('__x', '__y')
typecode = 'd'
# methods follow (omitted in book listing)
By defining __slots__ in the class, you are telling the interpreter: �These are all the
instance attributes in this class.� Python then stores them in a tuple-like structure in
each instance, avoiding the memory overhead of the per-instance __dict__. This can
make a huge difference in memory usage if your have millions of instances active at the
same time.
If you are handling millions of objects with numeric data, you
should really be using NumPy arrays (see �NumPy and SciPy� on
page 52), which are not only memory-efficient but have highly
optimized functions for numeric processing, many of which operate
on the entire array at once. I designed the Vector2d class just
to provide context when discussing special methods, because I try
to avoid vague foo and bar examples when I can.
Example 9-12 shows two runs of a script that simply builds a list, using a list comprehension,
with 10,000,000 instances of Vector2d. The mem_test.py script takes the
name of a module with a Vector2d class variant as command-line argument. In the first
run, I am using vector2d_v3.Vector2d (from Example 9-7); in the second run, the
__slots__ version of vector2d_v3_slots.Vector2d is used.
Example 9-12. mem_test.py creates 10 million Vector2d instances using the class defined
in the named module (e.g., vector2d_v3.py)
$ time python3 mem_test.py vector2d_v3.py
Selected Vector2d type: vector2d_v3.Vector2d
Creating 10,000,000 Vector2d instances
Initial RAM usage: 5,623,808
Saving Space with the __slots__ Class Attribute | 265
Final RAM usage: 1,558,482,944
real 0m16.721s
user 0m15.568s
sys 0m1.149s
$ time python3 mem_test.py vector2d_v3_slots.py
Selected Vector2d type: vector2d_v3_slots.Vector2d
Creating 10,000,000 Vector2d instances
Initial RAM usage: 5,718,016
Final RAM usage: 655,466,496
real 0m13.605s
user 0m13.163s
sys 0m0.434s
As Example 9-12 reveals, the RAM footprint of the script grows to 1.5 GB when instance
__dict__ is used in each of the 10 million Vector2d instances, but that is reduced to
655 MB when Vector2d has a __slots__ attribute. The __slots__ version is also faster.
The mem_test.py script in this test basically deals with loading a module, checking
memory usage, and formatting results. The code is not really relevant here so it�s in
Appendix A, Example A-4.
When __slots__ is specified in a class, its instances will not be
allowed to have any other attributes apart from those named in
__slots__. This is really a side effect, and not the reason why
__slots__ exists. It�s considered bad form to use __slots__ just
to prevent users of your class from creating new attributes in the
instances if they want to. __slots__ should used for optimization,
not for programmer restraint.
It may be possible, however, to �save memory and eat it too�: if you add the '__dict__'
name to the __slots__ list, your instances will keep attributes named in __slots__ in
the per-instance tuple, but will also support dynamically created attributes, which will
be stored in the usual __dict__. Of course, having '__dict__' in __slots__ may entirely
defeat its purpose, depending on the number of static and dynamic attributes in
each instance and how they are used. Careless optimization is even worse than premature
optimization.
There is another special per-instance attribute that you may want to keep: the __weak
ref__ attribute is necessary for an object to support weak references (covered in �Weak
References� on page 236). That attribute is present by default in instances of user-defined
classes. However, if the class defines __slots__, and you need the instances to be targets
of weak references, then you need to include '__weakref__' among the attributes
named in __slots__.
266 | Chapter 9: A Pythonic Object
To summarize, __slots__ has some caveats and should not be abused just for the sake
of limiting what attributes can be assigned by users. It is mostly useful when working
with tabular data such as database records where the schema is fixed by definition and
the datasets may be very large. However, if you do this kind of work often, you must
check out not only NumPy, but also the pandas data analysis library, which can handle
nonnumeric data and import/export to many different tabular data formats.
The Problems with __slots__
To summarize, __slots__ may provide significant memory savings if properly used,
but there are a few caveats:
� You must remember to redeclare __slots__ in each subclass, because the inherited
attribute is ignored by the interpreter.
� Instances will only be able to have the attributes listed in __slots__, unless you
include '__dict__' in __slots__ (but doing so may negate the memory savings).
� Instances cannot be targets of weak references unless you remember to include
'__weakref__' in __slots__.
If your program is not handling millions of instances, it�s probably not worth the trouble
of creating a somewhat unusual and tricky class whose instances may not accept dynamic
attributes or may not support weak references. Like any optimization, __slots__
should be used only if justified by a present need and when its benefit is proven by
careful profiling.
The last topic in this chapter has to do with overriding a class attribute in instances and
subclasses.
Overriding Class Attributes
A distinctive feature of Python is how class attributes can be used as default values for
instance attributes. In Vector2d there is the typecode class attribute. It�s used twice in
the __bytes__ method, but we read it as self.typecode by design. Because Vector2d
instances are created without a typecode attribute of their own, self.typecode will get
the Vector2d.typecode class attribute by default.
But if you write to an instance attribute that does not exist, you create a new instance
attribute�e.g., a typecode instance attribute�and the class attribute by the same name
is untouched. However, from then on, whenever the code handling that instance reads
self.typecode, the instance typecode will be retrieved, effectively shadowing the class
attribute by the same name. This opens the possibility of customizing an individual
instance with a different typecode.
Overriding Class Attributes | 267
The default Vector2d.typecode is 'd', meaning each vector component will be represented
as an 8-byte double precision float when exporting to bytes. If we set the type
code of a Vector2d instance to 'f' prior to exporting, each component will be exported
as a 4-byte single precision float. Example 9-13 demonstrates.
We are discussing adding a custom instance attribute, therefore
Example 9-13 uses the Vector2d implementation without
__slots__ as listed in Example 9-9.
Example 9-13. Customizing an instance by setting the typecode attribute that was formerly
inherited from the class
>>> from vector2d_v3 import Vector2d
>>> v1 = Vector2d(1.1, 2.2)
>>> dumpd = bytes(v1)
>>> dumpd
b'd\x9a\x99\x99\x99\x99\x99\xf1?\x9a\x99\x99\x99\x99\x99\x01@'
>>> len(dumpd) #
17
>>> v1.typecode = 'f' #
>>> dumpf = bytes(v1)
>>> dumpf
b'f\xcd\xcc\x8c?\xcd\xcc\x0c@'
>>> len(dumpf) #
9
>>> Vector2d.typecode #
'd'
Default bytes representation is 17 bytes long.
Set typecode to 'f' in the v1 instance.
Now the bytes dump is 9 bytes long.
Vector2d.typecode is unchanged; only the v1 instance uses typecode 'f'.
Now it should be clear why the bytes export of a Vector2d is prefixed by the type
code: we wanted to support different export formats.
If you want to change a class attribute you must set it on the class directly, not through
an instance. You could change the default typecode for all instances (that don�t have
their own typecode) by doing this:
>>> Vector2d.typecode = 'f'
However, there is an idiomatic Python way of achieving a more permanent effect, and
being more explicit about the change. Because class attributes are public, they are inherited
by subclasses, so it�s common practice to subclass just to customize a class data
268 | Chapter 9: A Pythonic Object
attribute. The Django class-based views use this technique extensively. Example 9-14
shows how.
Example 9-14. The ShortVector2d is a subclass of Vector2d, which only overwrites the
default typecode
>>> from vector2d_v3 import Vector2d
>>> class ShortVector2d(Vector2d): #
... typecode = 'f'
...
>>> sv = ShortVector2d(1/11, 1/27) #
>>> sv
ShortVector2d(0.09090909090909091, 0.037037037037037035) #
>>> len(bytes(sv)) #
9
Create ShortVector2d as a Vector2d subclass just to overwrite the typecode
class attribute.
Build ShortVector2d instance sv for demonstration.
Inspect the repr of sv.
Check that the length of the exported bytes is 9, not 17 as before.
This example also explains why I did not hardcode the class_name in Vec
to2d.__repr__, but instead got it from type(self).__name__, like this:
# inside class Vector2d:
def __repr__(self):
class_name = type(self).__name__
return '{}({!r}, {!r})'.format(class_name, *self)
If I had hardcoded the class_name, subclasses of Vector2d like ShortVector2d would
have to overwrite __repr__ just to change the class_name. By reading the name from
the type of the instance, I made __repr__ safer to inherit.
This ends our coverage of implementing a simple class that leverages the data model to
play well with the rest of Python�offering different object representations, implementing
a custom formatting code, exposing read-only attributes, and supporting hash() to
integrate with sets and mappings.
Chapter Summary
The aim of this chapter was to demonstrate the use of special methods and conventions
in the construction of a well-behaved Pythonic class.
Is vector2d_v3.py (Example 9-9) more Pythonic than vector2d_v0.py (Example 9-2)?
The Vector2d class in vector2d_v3.py certainly exhibits more Python features. But
Chapter Summary | 269
whether the first or the last Vector2d implementation is more idiomatic depends on
the context where it would be used. Tim Peter�s Zen of Python says:
Simple is better than complex.
A Pythonic object should be as simple as the requirements allow�and not a parade of
language features.
But my goal in expanding the Vector2d code was to provide context for discussing
Python special methods and coding conventions. If you look back at Table 1-1, the
several listings in this chapter demonstrated:
� All string/bytes representation methods: __repr__, __str__, __format__, and
__bytes__.
� Several methods for converting an object to a number: __abs__, __bool__,
__hash__.
� The __eq__ operator, to test bytes conversion and to enable hashing (along with
__hash__).
While supporting conversion to bytes we also implemented an alternative constructor,
Vector2d.frombytes(), which provided the context for discussing the decorators
@classmethod (very handy) and @staticmethod (not so useful, module-level functions
are simpler). The frombytes method was inspired by it�s namesake in the array.ar
ray class.
We saw that the Format Specification Mini-Language is extensible by implementing a
__format__ method that does some minimal parsing of format_spec provided to the
format(obj, format_spec) built-in or within replacement fields '{:�for
mat_spec�}' in strings used with the str.format method.
In preparation to make Vector2d instances hashable, we made an effort to make them
immutable, at least preventing accidental changes by coding the x and y attributes as
private, and exposing them as read-only properties. We then implemented __hash__
using the recommended technique of xor-ing the hashes of the instance attributes.
We then discussed the memory savings and the caveats of declaring a __slots__ attribute
in Vector2d. Because using __slots__ is somewhat tricky, it really makes sense
only when handling a very large number of instances�think millions of instances, not
just thousands.
The last topic we covered was the overriding of a class attribute accessed via the instances
(e.g., self.typecode). We did that first by creating an instance attribute, and then by
subclassing and overwriting at the class level.
270 | Chapter 9: A Pythonic Object
Throughout the chapter, I mentioned how design choices in the examples were informed
by studying the API of standard Python objects. If this chapter can be summarized
in one sentence, this is it:
To build Pythonic objects, observe how real Python objects behave.
� Ancient Chinese proverb
Further Reading
This chapter covered several special methods of the data model, so naturally the primary
references are the same as the ones provided in Chapter 1, which gave a high-level view
of the same topic. For convenience, I�ll repeat those four earlier recommendations here,
and add a few other ones:
�Data Model� chapter of The Python Language Reference
Most of the methods we used in this chapter are documented in �3.3.1. Basic customization�.
Python in a Nutshell, 2nd Edition, by Alex Martelli
Excellent coverage of the data model, even if only Python 2.5 is covered (in the
second edition). The fundamental concepts are all the same and most of the Data
Model APIs haven�t changed at all since Python 2.2, when built-in types and userdefined
classes became more compatible.
Python Cookbook, 3rd Edition, by David Beazley and Brian K. Jones
Very modern coding practices demonstrated through recipes. Chapter 8, �Classes
and Objects� in particular has several solutions related to discussions in this chapter.
Python Essential Reference, 4th Edition, by David Beazley
Covers the data model in detail in the context of Python 2.6 and Python 3.
In this chapter, we covered every special method related to object representation, except
__index__. It�s used to coerce an object to an integer index in the specific context of
sequence slicing, and was created to solve a need in NumPy. In practice, you and I are
not likely to need to implement __index__ unless we decide to write a new numeric
data type, and we want it to be usable as arguments to __getitem__. If you are curious
about it, A.M. Kuchling�s What�s New in Python 2.5 has a short explanation, and PEP
357 � Allowing Any Object to be Used for Slicing details the need for __index__, from
the perspective of an implementor of a C-extension, Travis Oliphant, the lead author of
NumPy.
An early realization of the need for distinct string representations for objects appeared
in Smalltalk. The 1996 article �How to Display an Object as a String: printString and
displayString� by Bobby Woolf discusses the implementation of the printString and
displayString methods in that language. From that article, I borrowed the pithy de?
Further Reading | 271
scriptions �the way the developer wants to see it� and �the way the user wants to see it�
when defining repr() and str() in �Object Representations� on page 248.
Soapbox
Properties Help Reduce Upfront Costs
In the initial versions of Vector2d, the x and y attributes were public, as are all Python
instance and class attributes by default. Naturally, users of vectors need to be able to
access its components. Although our vectors are iterable and can be unpacked into a
pair of variables, it�s also desirable to be able to write my_vector.x and my_vector.y to
get each component.
When we felt the need to avoid accidental updates to the x and y attributes, we implemented
properties, but nothing changed elsewhere in the code and in the public interface
of Vector2d, as verified by the doctests. We are still able to access my_vector.x and
my_vector.y.
This shows that we can always start our classes in the simplest possible way, with public
attributes, because when (or if) we later need to impose more control with getters and
setters, these can be implemented through properties without changing any of the code
that already interacts with our objects through the names (e.g., x and y) that were initially
simple public attributes.
This approach is the opposite of that encouraged by the Java language: a Java programmer
cannot start with simple public attributes and only later, if needed, implement
properties, because they don�t exist in the language. Therefore, writing getters and setters
is the norm in Java�even when those methods do nothing useful�because the API
cannot evolve from simple public attributes to getters and setters without breaking all
code that uses those attributes.
In addition, as our technical reviewer Alex Martelli points out, typing getter/setter calls
everywhere is goofy. You have to write stuff like:
---
>>> my_object.set_foo(my_object.get_foo() + 1)
---
Just to do this:
---
>>> my_object.foo += 1
---
Ward Cunningham, inventor of the wiki and an Extreme Programming pioneer, recommends
asking �What�s the simplest thing that could possibly work?� The idea is to
272 | Chapter 9: A Pythonic Object
11. See �Simplest Thing that Could Possibly Work: A Conversation with Ward Cunningham, Part V�.
focus on the goal.11 Implementing setters and getters up front is a distraction from the
goal. In Python, we can simply use public attributes knowing we can change them to
properties later, if the need arises.
Safety Versus Security in Private Attributes
Perl doesn�t have an infatuation with enforced privacy. It would prefer that you stayed
out of its living room because you weren�t invited, not because it has a shotgun.
� Larry Wall
Creator of Perl
Python and Perl are polar opposites in many regards, but Larry and Guido seem to agree
on object privacy.
Having taught Python to many Java programmers over the years, I�ve found a lot of them
put too much faith in the privacy guarantees that Java offers. As it turns out, the Java
private and protected modifiers normally provide protection against accidents only
(i.e., safety). They can only guarantee security against malicious intent if the application
is deployed with a security manager, and that seldom happens in practice, even in corporate
settings.
To prove my point, I like to show this Java class (Example 9-15).
Example 9-15. Confidential.java: a Java class with a private field named secret
public class Confidential {
private String secret = "";
public Confidential(String text) {
secret = text.toUpperCase();
}
}
In Example 9-15, I store the text in the secret field after converting it to uppercase,
just to make it obvious that whatever is in that field will be in all caps.
The actual demonstration consists of running expose.py with Jython. That script uses
introspection (�reflection� in Java parlance) to get the value of a private field. The code
is in Example 9-16.
Example 9-16. expose.py: Jython code to read the content of a private field in another
class
import Confidential
message = Confidential('top secret text')
secret_field = Confidential.getDeclaredField('secret')
Further Reading | 273
secret_field.setAccessible(True) # break the lock!
print 'message.secret =', secret_field.get(message)
If you run Example 9-16, this is what you get:
$ jython expose.py
message.secret = TOP SECRET TEXT
The string 'TOP SECRET TEXT' was read from the secret private field of the Confiden
tial class.
There is no black magic here: expose.py uses the Java reflection API to get a reference
to the private field named 'secret', and then calls 'secret_field.setAccessi
ble(True)' to make it readable. The same thing can be done with Java code, of course
(but it takes more than three times as many lines to do it; see the file Expose.java in the
Fluent Python code repository).
The crucial call .setAccessible(True) will fail only if the Jython script or the Java main
program (e.g., Expose.class) is running under the supervision of a SecurityManager.
But in the real world, Java applications are rarely deployed with a SecurityManager�
except for Java applets (remember those?).
My point is: in Java too, access control modifiers are mostly about safety and not security,
at least in practice. So relax and enjoy the power Python gives you. Use it responsibly.
274 | Chapter 9: A Pythonic Object
CHAPTER 10
Sequence Hacking, Hashing, and Slicing
Don�t check whether it is-a duck: check whether it quacks-like-a duck, walks-like-a duck,
etc, etc, depending on exactly what subset of duck-like behavior you need to play your
language-games with. (comp.lang.python, Jul. 26, 2000)
� Alex Martelli
In this chapter, we will create a class to represent a multidimensional Vector class�a
significant step up from the two-dimensional Vector2d of Chapter 9. Vector will behave
like a standard Python immutable flat sequence. Its elements will be floats, and it will
support the following by the end of this chapter:
� Basic sequence protocol: __len__ and __getitem__.
� Safe representation of instances with many items.
� Proper slicing support, producing new Vector instances.
� Aggregate hashing taking into account every contained element value.
� Custom formatting language extension.
We�ll also implement dynamic attribute access with __getattr__ as a way of replacing
the read-only properties we used in Vector2d�although this is not typical of sequence
types.
The code-intensive presentation will be interrupted by a conceptual discussion about
the idea of protocols as an informal interface. We�ll talk about how protocols and duck
typing are related, and its practical implications when you create your own types.
Let�s get started.
275
Vector Applications Beyond Three Dimensions
Who needs a vector with 1,000 dimensions? Hint: not 3D artists! However, ndimensional
vectors (with large values of n) are widely used in information retrieval,
where documents and text queries are represented as vectors, with one dimension per
word. This is called the Vector space model. In this model, a key relevance metric is the
cosine similarity (i.e., the cosine of the angle between a query vector and a document
vector). As the angle decreases, the cosine approaches the maximum value of 1, and so
does the relevance of the document to the query.
Having said that, the Vector class in this chapter is a didactic example and we�ll not do
much math here. Our goal is just to demonstrate some Python special methods in the
context of a sequence type.
NumPy and SciPy are the tools you need for real-world vector math. The PyPI package
gemsim, by Radim Rehurek, implements vector space modeling for natural language
processing and information retrieval, using NumPy and SciPy.
Vector: A User-Defined Sequence Type
Our strategy to implement Vector will be to use composition, not inheritance. We�ll
store the components in an array of floats, and will implement the methods needed for
our Vector to behave like an immutable flat sequence.
But before we implement the sequence methods, let�s make sure we have a baseline
implementation of Vector that is compatible with our earlier Vector2d class�except
where such compatibility would not make sense.
Vector Take #1: Vector2d Compatible
The first version of Vector should be as compatible as possible with our earlier Vec
tor2d class.
However, by design, the Vector constructor is not compatible with the Vector2d constructor.
We could make Vector(3, 4) and Vector(3, 4, 5) work, by taking arbitrary
arguments with *args in __init__, but the best practice for a sequence constructor is
to take the data as an iterable argument in the constructor, like all built-in sequence
types do. Example 10-1 shows some ways of instantiating our new Vector objects.
Example 10-1. Tests of Vector.__init__ and Vector.__repr__
>>> Vector([3.1, 4.2])
Vector([3.1, 4.2])
>>> Vector((3, 4, 5))
Vector([3.0, 4.0, 5.0])
276 | Chapter 10: Sequence Hacking, Hashing, and Slicing
>>> Vector(range(10))
Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])
Apart from new constructor signature, I made sure every test I did with Vector2d (e.g.,
Vector2d(3, 4)) passed and produced the same result with a two-component Vec
tor([3, 4]).
When a Vector has more than six components, the string produced
by repr() is abbreviated with ... as seen in the last line of
Example 10-1. This is crucial in any collection type that may contain
a large number of items, because repr is used for debugging
(and you don�t want a single large object to span thousands of lines
in your console or log). Use the reprlib module to produce
limited-length representations, as in Example 10-2.
The reprlib module is called repr in Python 2. The 2to3 tool
rewrites imports from repr automatically.
Example 10-2 lists the implementation of our first version of Vector (this example builds
on the code shown in Examples 9-2 and 9-3).
Example 10-2. vector_v1.py: derived from vector2d_v1.py
from array import array
import reprlib
import math
class Vector:
typecode = 'd'
def __init__(self, components):
self._components = array(self.typecode, components)
def __iter__(self):
return iter(self._components)
def __repr__(self):
components = reprlib.repr(self._components)
components = components[components.find('['):-1]
return 'Vector({})'.format(components)
def __str__(self):
return str(tuple(self))
def __bytes__(self):
return (bytes([ord(self.typecode)]) +
bytes(self._components))
Vector Take #1: Vector2d Compatible | 277
1. The iter() function is covered in Chapter 14, along with the __iter__ method.
def __eq__(self, other):
return tuple(self) == tuple(other)
def __abs__(self):
return math.sqrt(sum(x * x for x in self))
def __bool__(self):
return bool(abs(self))
@classmethod
def frombytes(cls, octets):
typecode = chr(octets[0])
memv = memoryview(octets[1:]).cast(typecode)
return cls(memv)
The self._components instance �protected� attribute will hold an array with
the Vector components.
To allow iteration, we return an iterator over self._components.1
Use reprlib.repr() to get a limited-length representation of self._compo
nents (e.g., array('d', [0.0, 1.0, 2.0, 3.0, 4.0, ...])).
Remove the array('d', prefix and the trailing ) before plugging the string into
a Vector constructor call.
Build a bytes object directly from self._components.
We can�t use hypot anymore, so we sum the squares of the components and
compute the sqrt of that.
The only change needed from the earlier frombytes is in the last line: we pass
the memoryview directly to the constructor, without unpacking with * as we did
before.
The way I used reprlib.repr deserves some elaboration. That function produces safe
representations of large or recursive structures by limiting the length of the output string
and marking the cut with '...'. I wanted the repr of a Vector to look like Vector([3.0,
4.0, 5.0]) and not Vector(array('d', [3.0, 4.0, 5.0])), because the fact that
there is an array inside a Vector is an implementation detail. Because these constructor
calls build identical Vector objects, I prefer the simpler syntax using a list argument.
When coding __repr__, I could have produced the simplified components display with
this expression: reprlib.repr(list(self._components)). However, this would be
wasteful, as I�d be copying every item from self._components to a list just to use the
list repr. Instead, I decided to apply reprlib.repr to the self._components array
278 | Chapter 10: Sequence Hacking, Hashing, and Slicing
directly, and then chop off the characters outside of the []. That�s what the second line
of __repr__ does in Example 10-2.
Because of its role in debugging, calling repr() on an object should
never raise an exception. If something goes wrong inside your
implementation of __repr__, you must deal with the issue and do
your best to produce some serviceable output that gives the user a
chance of identifying the target object.
Note that the __str__, __eq__, and __bool__ methods are unchanged from Vector2d,
and only one character was changed in frombytes (a * was removed in the last line).
This is one of the benefits of making the original Vector2d iterable.
By the way, we could have subclassed Vector from Vector2d, but I chose not to do it
for two reasons. First, the incompatible constructors really make subclassing not advisable.
I could work around that with some clever parameter handling in __init__,
but the second reason is more important: I want Vector to be a standalone example of
a class implementing the sequence protocol. That�s what we�ll do next, after a discussion
of the term protocol.
Protocols and Duck Typing
As early as Chapter 1, we saw that you don�t need to inherit from any special class to
create a fully functional sequence type in Python; you just need to implement the methods
that fulfill the sequence protocol. But what kind of protocol are we talking about?
In the context of object-oriented programming, a protocol is an informal interface,
defined only in documentation and not in code. For example, the sequence protocol in
Python entails just the __len__ and __getitem__ methods. Any class Spam that implements
those methods with the standard signature and semantics can be used anywhere
a sequence is expected. Whether Spam is a subclass of this or that is irrelevant; all that
matters is that it provides the necessary methods. We saw that in Example 1-1, reproduced
here in Example 10-3.
Example 10-3. Code from Example 1-1, reproduced here for convenience
import collections
Card = collections.namedtuple('Card', ['rank', 'suit'])
class FrenchDeck:
ranks = [str(n) for n in range(2, 11)] + list('JQKA')
suits = 'spades diamonds clubs hearts'.split()
def __init__(self):
Protocols and Duck Typing | 279
self._cards = [Card(rank, suit) for suit in self.suits
for rank in self.ranks]
def __len__(self):
return len(self._cards)
def __getitem__(self, position):
return self._cards[position]
The FrenchDeck class in Example 10-3 takes advantage of many Python facilities because
it implements the sequence protocol, even if that is not declared anywhere in the code.
Any experienced Python coder will look at it and understand that it is a sequence, even
if it subclasses object. We say it is a sequence because it behaves like one, and that is
what matters.
This became known as duck typing, after Alex Martelli�s post quoted at the beginning
of this chapter.
Because protocols are informal and unenforced, you can often get away with implementing
just part of a protocol, if you know the specific context where a class will be
used. For example, to support iteration, only __getitem__ is required; there is no need
to provide __len__.
We�ll now implement the sequence protocol in Vector, initially without proper support
for slicing, but later adding that.
Vector Take #2: A Sliceable Sequence
As we saw with the FrenchDeck example, supporting the sequence protocol is really easy
if you can delegate to a sequence attribute in your object, like our self._components
array. These __len__ and __getitem__ one-liners are a good start:
class Vector:
# many lines omitted
# ...
def __len__(self):
return len(self._components)
def __getitem__(self, index):
return self._components[index]
With these additions, all of these operations now work:
>>> v1 = Vector([3, 4, 5])
>>> len(v1)
3
>>> v1[0], v1[-1]
(3.0, 5.0)
>>> v7 = Vector(range(7))
280 | Chapter 10: Sequence Hacking, Hashing, and Slicing
>>> v7[1:4]
array('d', [1.0, 2.0, 3.0])
As you can see, even slicing is supported�but not very well. It would be better if a slice
of a Vector was also a Vector instance and not a array. The old FrenchDeck class has
a similar problem: when you slice it, you get a list. In the case of Vector, a lot of
functionality is lost when slicing produces plain arrays.
Consider the built-in sequence types: every one of them, when sliced, produces a new
instance of its own type, and not of some other type.
To make Vector produce slices as Vector instances, we can�t just delegate the slicing to
array. We need to analyze the arguments we get in __getitem__ and do the right thing.
Now, let�s see how Python turns the syntax my_seq[1:3] into arguments for
my_seq.__getitem__(...).
How Slicing Works
A demo is worth a thousand words, so take a look at Example 10-4.
Example 10-4. Checking out the behavior of __getitem__ and slices
>>> class MySeq:
... def __getitem__(self, index):
... return index #
...
>>> s = MySeq()
>>> s[1] #
1
>>> s[1:4] #
slice(1, 4, None)
>>> s[1:4:2] #
slice(1, 4, 2)
>>> s[1:4:2, 9] #
(slice(1, 4, 2), 9)
>>> s[1:4:2, 7:9] #
(slice(1, 4, 2), slice(7, 9, None))
For this demonstration, __getitem__ merely returns whatever is passed to it.
A single index, nothing new.
The notation 1:4 becomes slice(1, 4, None).
slice(1, 4, 2) means start at 1, stop at 4, step by 2.
Surprise: the presence of commas inside the [] means __getitem__ receives a
tuple.
The tuple may even hold several slice objects.
Vector Take #2: A Sliceable Sequence | 281
Now let�s take a closer look at slice itself in Example 10-5.
Example 10-5. Inspecting the attributes of the slice class
>>> slice #
<class 'slice'>
>>> dir(slice) #
['__class__', '__delattr__', '__dir__', '__doc__', '__eq__',
'__format__', '__ge__', '__getattribute__', '__gt__',
'__hash__', '__init__', '__le__', '__lt__', '__ne__',
'__new__', '__reduce__', '__reduce_ex__', '__repr__',
'__setattr__', '__sizeof__', '__str__', '__subclasshook__',
'indices', 'start', 'step', 'stop']
slice is a built-in type (we saw it first in �Slice Objects� on page 34).
Inspecting a slice we find the data attributes start, stop, and step, and an
indices method.
In Example 10-5, calling dir(slice) reveals an indices attribute, which turns out to
be a very interesting but little-known method. Here is what help(slice.indices)
reveals:
S.indices(len) -> (start, stop, stride)
Assuming a sequence of length len, calculate the start and stop indices, and the
stride length of the extended slice described by S. Out of bounds indices are clipped
in a manner consistent with the handling of normal slices.
In other words, indices exposes the tricky logic that�s implemented in the built-in
sequences to gracefully handle missing or negative indices and slices that are longer
than the target sequence. This method produces �normalized� tuples of nonnegative
start, stop, and stride integers adjusted to fit within the bounds of a sequence of the
given length.
Here are a couple of examples, considering a sequence of len == 5, e.g., 'ABCDE':
>>> slice(None, 10, 2).indices(5) #
(0, 5, 2)
>>> slice(-3, None, None).indices(5) #
(2, 5, 1)
'ABCDE'[:10:2] is the same as 'ABCDE'[0:5:2]
'ABCDE'[-3:] is the same as 'ABCDE'[2:5:1]
282 | Chapter 10: Sequence Hacking, Hashing, and Slicing
As I write this, the slice.indices method is apparently not documented
in the online Python Library Reference. The Python
Python/C API Reference Manual documents a similar C-level
function, PySlice_GetIndicesEx. I discovered slice.indices
while exploring slice objects in the Python console, using dir()
and help(). Yet another evidence of the value of the interactive
console as a discovery tool.
In our Vector code, we�ll not need the slice.indices() method because when we get
a slice argument we�ll delegate its handling to the _components array. But if you can�t
count on the services of an underlying sequence, this method can be a huge time saver.
Now that we know how to handle slices, let�s take a look at the improved Vector.__ge
titem__ implementation.
A Slice-Aware __getitem__
Example 10-6 lists the two methods needed to make Vector behave as a sequence:
__len__ and __getitem__ (the latter now implemented to handle slicing correctly).
Example 10-6. Part of vector_v2.py: __len__ and __getitem__ methods added to Vector
class from vector_v1.py (see Example 10-2)
def __len__(self):
return len(self._components)
def __getitem__(self, index):
cls = type(self)
if isinstance(index, slice):
return cls(self._components[index])
elif isinstance(index, numbers.Integral):
return self._components[index]
else:
msg = '{cls.__name__} indices must be integers'
raise TypeError(msg.format(cls=cls))
Get the class of the instance (i.e., Vector) for later use.
If the index argument is a slice�
�invoke the class to build another Vector instance from a slice of the _compo
nents array.
If the index is an int or some other kind of integer�
�just return the specific item from _components.
Otherwise, raise an exception.
Vector Take #2: A Sliceable Sequence | 283
Excessive use of isinstance may be a sign of bad OO design, but
handling slices in __getitem__ is a justified use case. Note in
Example 10-6 the test against numbers.Integral�an Abstract
Base Class. Using ABCs in insinstance tests makes an API more
flexible and future-proof. Chapter 11 explains why. Unfortunately,
there is no ABC for slice in the Python 3.4 standard library.
To discover which exception to raise in the else clause of __getitem__, I used the
interactive console to check the result of 'ABC'[1, 2]. I then learned that Python raises
a TypeError, and I also copied the wording from the error message: �indices must be
integers.� To create Pythonic objects, mimic Python�s own objects.
Once the code in Example 10-6 is added to the Vector class, we have proper slicing
behavior, as Example 10-7 demonstrates.
Example 10-7. Tests of enhanced Vector.getitem from Example 10-6
>>> v7 = Vector(range(7))
>>> v7[-1]
6.0
>>> v7[1:4]
Vector([1.0, 2.0, 3.0])
>>> v7[-1:]
Vector([6.0])
>>> v7[1,2]
Traceback (most recent call last):
...
TypeError: Vector indices must be integers
An integer index retrieves just one component value as a float.
A slice index creates a new Vector.
A slice of len == 1 also creates a Vector.
Vector does not support multidimensional indexing, so a tuple of indices or
slices raises an error.
Vector Take #3: Dynamic Attribute Access
In the evolution from Vector2d to Vector, we lost the ability to access vector components
by name (e.g., v.x, v.y). We are now dealing with vectors that may have a large
number of components. Still, it may be convenient to access the first few components
with shortcut letters such as x, y, z instead of v[0], v[1] and v[2].
Here is the alternative syntax we want to provide for reading the first four components
of a vector:
284 | Chapter 10: Sequence Hacking, Hashing, and Slicing
2. Attribute lookup is more complicated than this; we�ll see the gory details in Part VI. For now, this simplified
explanation will do.
>>> v = Vector(range(10))
>>> v.x
0.0
>>> v.y, v.z, v.t
(1.0, 2.0, 3.0)
In Vector2d, we provided read-only access to x and y using the @property decorator
(Example 9-7). We could write four properties in Vector, but it would be tedious. The
__getattr__ special method provides a better way.
�The __getattr__ method is invoked by the interpreter when attribute lookup fails. In
simple terms, given the expression my_obj.x, Python checks if the my_obj instance has
an attribute named x; if not, the search goes to the class (my_obj.__class__), and then
up the inheritance graph.2 If the x attribute is not found, then the __getattr__ method
defined in the class of my_obj is called with self and the name of the attribute as a string
(e.g., 'x').
Example 10-8 lists our __getattr__ method. Essentially it checks whether the attribute
being sought is one of the letters xyzt and if so, returns the corresponding vector component.
Example 10-8. Part of vector_v3.py: __getattr__ method added to Vector class from
vector_v2.py
shortcut_names = 'xyzt'
def __getattr__(self, name):
cls = type(self)
if len(name) == 1:
pos = cls.shortcut_names.find(name)
if 0 <= pos < len(self._components):
return self._components[pos]
msg = '{.__name__!r} object has no attribute {!r}'
raise AttributeError(msg.format(cls, name))
Get the Vector class for later use.
If the name is one character, it may be one of the shortcut_names.
Find position of 1-letter name; str.find would also locate 'yz' and we don�t
want that, this is the reason for the test above.
If the position is within range, return the array element.
If either test failed, raise AttributeError with a standard message text.
Vector Take #3: Dynamic Attribute Access | 285
It�s not hard to implement __getattr__, but in this case it�s not enough. Consider the
bizarre interaction in Example 10-9.
Example 10-9. Inappropriate behavior: assigning to v.x raises no error, but introduces
an inconsistency
>>> v = Vector(range(5))
>>> v
Vector([0.0, 1.0, 2.0, 3.0, 4.0])
>>> v.x #
0.0
>>> v.x = 10 #
>>> v.x #
10
>>> v
Vector([0.0, 1.0, 2.0, 3.0, 4.0]) #
Access element v[0] as v.x.
Assign new value to v.x. This should raise an exception.
Reading v.x shows the new value, 10.
However, the vector components did not change.
Can you explain what is happening? In particular, why the second time v.x returns 10
if that value is not in the vector components array? If you don�t know right off the bat,
study the explanation of __getattr__ given right before Example 10-8. It�s a bit subtle,
but a very important foundation to understand a lot of what comes later in the book.
The inconsistency in Example 10-9 was introduced because of the way __getattr__
works: Python only calls that method as a fall back, when the object does not have the
named attribute. However, after we assign v.x = 10, the v object now has an x attribute,
so __getattr__ will no longer be called to retrieve v.x: the interpreter will just return
the value 10 that is bound to v.x. On the other hand, our implementation of __get
attr__ pays no attention to instance attributes other than self._components, from
where it retrieves the values of the �virtual attributes� listed in shortcut_names.
We need to customize the logic for setting attributes in our Vector class in order to
avoid this inconsistency.
Recall that in the latest Vector2d examples from Chapter 9, trying to assign to the .x
or .y instance attributes raised AttributeError. In Vector we want the same exception
with any attempt at assigning to all single-letter lowercase attribute names, just to avoid
confusion. To do that, we�ll implement __setattr__ as listed in Example 10-10.
Example 10-10. Part of vector_v3.py: __setattr__ method in Vector class
def __setattr__(self, name, value):
cls = type(self)
286 | Chapter 10: Sequence Hacking, Hashing, and Slicing
if len(name) == 1:
if name in cls.shortcut_names:
error = 'readonly attribute {attr_name!r}'
elif name.islower():
error = "can't set attributes 'a' to 'z' in {cls_name!r}"
else:
error = ''
if error:
msg = error.format(cls_name=cls.__name__, attr_name=name)
raise AttributeError(msg)
super().__setattr__(name, value)
Special handling for single-character attribute names.
If name is one of xyzt, set specific error message.
If name is lowercase, set error message about all single-letter names.
Otherwise, set blank error message.
If there is a nonblank error message, raise AttributeError.
Default case: call __setattr__ on superclass for standard behavior.
The super() function provides a way to access methods of superclasses
dynamically, a necessity in a dynamic language supporting
multiple inheritance like Python. It�s used to delegate some task
from a method in a subclass to a suitable method in a superclass,
as seen in Example 10-10. There is more about super in �Multiple
Inheritance and Method Resolution Order� on page 351.
While choosing the error message to display with AttributeError, my first check was
the behavior of the built-in complex type, because they are immutable and have a pair
of data attributes real and imag. Trying to change either of those in a complex instance
raises AttributeError with the message "can't set attribute". On the other hand,
trying to set a read-only attribute protected by a property as we did in �A Hashable
Vector2d� on page 257 produces the message "readonly attribute". I drew inspiration
from both wordings to set the error string in __setitem__, but was more explicit
about the forbidden attributes.
Note that we are not disallowing setting all attributes, only single-letter, lowercase ones,
to avoid confusion with the supported read-only attributes x, y, z, and t.
Vector Take #3: Dynamic Attribute Access | 287
3. The sum, any, and all cover the most common uses of reduce. See the discussion in �Modern Replacements
for map, filter, and reduce� on page 142.
Knowing that declaring __slots__ at the class level prevents setting
new instance attributes, it�s tempting to use that feature instead
of implementing __setattr__ as we did. However, because
of all the caveats discussed in �The Problems with __slots__� on
page 267, using __slots__ just to prevent instance attribute creation
is not recommended. __slots__ should be used only to save
memory, and only if that is a real issue.
Even without supporting writing to the Vector components, here is an important takeaway
from this example: very often when you implement __getattr__ you need to code
__setattr__ as well, to avoid inconsistent behavior in your objects.
If we wanted to allow changing components, we could implement __setitem__ to enable
v[0] = 1.1 and/or __setattr__ to make v.x = 1.1 work. But Vector will remain
immutable because we want to make it hashable in the coming section.
Vector Take #4: Hashing and a Faster ==
Once more we get to implement a __hash__ method. Together with the existing
__eq__, this will make Vector instances hashable.
The __hash__ in Example 9-8 simply computed hash(self.x) ^ hash(self.y). We
now would like to apply the ^ (xor) operator to the hashes of every component, in
succession, like this: v[0] ^ v[1] ^ v[2]�. That is what the functools.reduce
function is for. Previously I said that reduce is not as popular as before,3 but computing
the hash of all vector components is a perfect job for it. Figure 10-1 depicts the general
idea of the reduce function.
Figure 10-1. Reducing functions�reduce, sum, any, all�produce a single aggregate result
from a sequence or from any finite iterable object.
288 | Chapter 10: Sequence Hacking, Hashing, and Slicing
So far we�ve seen that functools.reduce() can be replaced by sum(), but now let�s
properly explain how it works. The key idea is to reduce a series of values to a single
value. The first argument to reduce() is a two-argument function, and the second
argument is an iterable. Let�s say we have a two-argument function fn and a list lst.
When you call reduce(fn, lst), fn will be applied to the first pair of elements�
fn(lst[0], lst[1])�producing a first result, r1. Then fn is applied to r1 and the next
element�fn(r1, lst[2])�producing a second result, r2. Now fn(r2, lst[3]) is
called to produce r3 � and so on until the last element, when a single result, rN, is
returned.
Here is how you could use reduce to compute 5! (the factorial of 5):
>>> 2 * 3 * 4 * 5 # the result we want: 5! == 120
120
>>> import functools
>>> functools.reduce(lambda a,b: a*b, range(1, 6))
120
Back to our hashing problem, Example 10-11 shows the idea of computing the aggregate
xor by doing it in three ways: with a for loop and two reduce calls.
Example 10-11. Three ways of calculating the accumulated xor of integers from 0 to 5
>>> n = 0
>>> for i in range(1, 6): #
... n ^= i
...
>>> n
1
>>> import functools
>>> functools.reduce(lambda a, b: a^b, range(6)) #
1
>>> import operator
>>> functools.reduce(operator.xor, range(6)) #
1
Aggregate xor with a for loop and an accumulator variable.
functools.reduce using an anonymous function.
functools.reduce replacing custom lambda with operator.xor.
From the alternatives in Example 10-11, the last one is my favorite, and the for loop
comes second. What is your preference?
As seen in �The operator Module� on page 156, operator provides the functionality of
all Python infix operators in function form, lessening the need for lambda.
To code Vector.__hash__ in my preferred style, we need to import the functools and
operator modules. Example 10-12 shows the relevant changes.
Vector Take #4: Hashing and a Faster == | 289
Example 10-12. Part of vector_v4.py: two imports and __hash__ method added to Vector
class from vector_v3.py
from array import array
import reprlib
import math
import functools #
import operator #
class Vector:
typecode = 'd'
# many lines omitted in book listing...
def __eq__(self, other): #
return tuple(self) == tuple(other)
def __hash__(self):
hashes = (hash(x) for x in self._components) #
return functools.reduce(operator.xor, hashes, 0) #
# more lines omitted...
Import functools to use reduce.
Import operator to use xor.
No change to __eq__; I listed it here because it�s good practice to keep __eq__
and __hash__ close in source code, because they need to work together.
Create a generator expression to lazily compute the hash of each component.
Feed hashes to reduce with the xor function to compute the aggregate hash
value; the third argument, 0, is the initializer (see next warning).
When using reduce, it�s good practice to provide the third argument,
reduce(function, iterable, initializer), to prevent
this exception: TypeError: reduce() of empty sequence with
no initial value (excellent message: explains the problem and
how to fix it). The initializer is the value returned if the sequence
is empty and is used as the first argument in the reducing
loop, so it should be the identity value of the operation. As examples,
for +, |, ^ the initializer should be 0, but for *, & it should
be 1.
As implemented, the __hash__ method in Example 10-8 is a perfect example of a mapreduce
computation (Figure 10-2).
290 | Chapter 10: Sequence Hacking, Hashing, and Slicing
4. We�ll seriously consider the matter of Vector([1, 2]) == (1, 2) in �Operator Overloading 101� on page
372.
Figure 10-2. Map-reduce: apply function to each item to generate a new series (map),
then compute aggregate (reduce)
The mapping step produces one hash for each component, and the reduce step aggregates
all hashes with the xor operator. Using map instead of a genexp makes the mapping
step even more visible:
def __hash__(self):
hashes = map(hash, self._components)
return functools.reduce(operator.xor, hashes)
The solution with map would be less efficient in Python 2, where
the map function builds a new list with the results. But in Python
3, map is lazy: it creates a generator that yields the results on demand,
thus saving memory�just like the generator expression we
used in the __hash__ method of Example 10-8.
While we are on the topic of reducing functions, we can replace our quick implementation
of __eq__ with another one that will be cheaper in terms of processing and memory,
at least for large vectors. As introduced in Example 9-2, we have this very concise
implementation of __eq__:
def __eq__(self, other):
return tuple(self) == tuple(other)
This works for Vector2d and for Vector�it even considers Vector([1, 2]) equal to
(1, 2), which may be a problem, but we�ll overlook that for now.4 But for Vector
instances that may have thousands of components, it�s very inefficient. It builds two
tuples copying the entire contents of the operands just to use the __eq__ of the tuple
Vector Take #4: Hashing and a Faster == | 291
type. For Vector2d (with only two components), it�s a good shortcut, but not for the
large multidimensional vectors. A better way of comparing one Vector to another
Vector or iterable would be Example 10-13.
Example 10-13. Vector.eq using zip in a for loop for more efficient comparison
def __eq__(self, other):
if len(self) != len(other): #
return False
for a, b in zip(self, other): #
if a != b: #
return False
return True #
If the len of the objects are different, they are not equal.
zip produces a generator of tuples made from the items in each iterable
argument. See �The Awesome zip� on page 293 if zip is new to you. The len
comparison above is needed because zip stops producing values without
warning as soon as one of the inputs is exhausted.
As soon as two components are different, exit returning False.
Otherwise, the objects are equal.
Example 10-13 is efficient, but the all function can produce the same aggregate computation
of the for loop in one line: if all comparisons between corresponding components
in the operands are True, the result is True. As soon as one comparison is False,
all returns False. Example 10-14 shows how __eq__ looks using all.
Example 10-14. Vector.eq using zip and all: same logic as Example 10-13
def __eq__(self, other):
return len(self) == len(other) and all(a == b for a, b in zip(self, other))
Note that we first check that the operands have equal length, because zip will stop at
the shortest operand.
Example 10-14 is the implementation we choose for __eq__ in vector_v4.py.
We wrap up this chapter by bringing back the __format__ method from Vector2d to
Vector.
292 | Chapter 10: Sequence Hacking, Hashing, and Slicing
5. That�s surprising (to me, at least). I think zip should raise ValueError if the sequences are not all of the
same length, which is what happens when unpacking an iterable to a tuple of variables of different length.
The Awesome zip
Having a for loop that iterates over items without fiddling with index variables is great
and prevents lots of bugs, but demands some special utility functions. One of them is
the zip built-in, which makes it easy to iterate in parallel over two or more iterables by
returning tuples that you can unpack into variables, one for each item in the parallel
inputs. See Example 10-15.
The zip function is named after the zipper fastener because
the physical device works by interlocking pairs of teeth taken
from both zipper sides, a good visual analogy for what
zip(left, right) does. No relation with compressed files.
Example 10-15. The zip built-in at work
>>> zip(range(3), 'ABC') #
<zip object at 0x10063ae48>
>>> list(zip(range(3), 'ABC')) #
[(0, 'A'), (1, 'B'), (2, 'C')]
>>> list(zip(range(3), 'ABC', [0.0, 1.1, 2.2, 3.3])) #
[(0, 'A', 0.0), (1, 'B', 1.1), (2, 'C', 2.2)]
>>> from itertools import zip_longest #
>>> list(zip_longest(range(3), 'ABC', [0.0, 1.1, 2.2, 3.3], fillvalue=-1))
[(0, 'A', 0.0), (1, 'B', 1.1), (2, 'C', 2.2), (-1, -1, 3.3)]
zip returns a generator that produces tuples on demand.
Here we build a list from it just for display; usually we iterate over the
generator.
zip has a surprising trait: it stops without warning when one of the iterables is
exhausted.5
The itertools.zip_longest function behaves differently: it uses an optional
fillvalue (None by default) to complete missing values so it can generate tuples
until the last iterable is exhausted.
The enumerate built-in is another generator function often used in for loops to avoid
manual handling of index variables. If you are not familiar with enumerate, you should
definitely check it out in the �Built-in functions� documentation. The zip and enumer
Vector Take #4: Hashing and a Faster == | 293
6. The Wolfram Mathworld site has an article on Hypersphere; on Wikipedia, �hypersphere� redirects to the "nsphere�
entry.
ate built-ins, along with several other generator functions in the standard library, are
covered in �Generator Functions in the Standard Library� on page 424.
Vector Take #5: Formatting
The __format__ method of Vector will resemble that of Vector2d, but instead of providing
a custom display in polar coordinates, Vector will use spherical coordinates�
also known as �hyperspherical� coordinates, because now we support n dimensions,
and spheres are �hyperspheres� in 4D and beyond.6 Accordingly, we�ll change the custom
format suffix from 'p' to 'h'.
As we saw in �Formatted Displays� on page 253, when extending
the Format Specification Mini-Language it�s best to avoid reusing
format codes supported by built-in types. In particular, our extended
mini-language also uses the float formatting codes 'eEfFgGn
%' in their original meaning, so we definitely must avoid these.
Integers use 'bcdoxXn' and strings use 's'. I picked 'p' for Vec
tor2d polar coordinates. Code 'h' for hyperspherical coordinates
is a good choice.
For example, given a Vector object in 4D space (len(v) == 4), the 'h' code will produce
a display like <r, ??, ??, ??> where r is the magnitude (abs(v)) and the remaining
numbers are the angular coordinates ?1, ?2, ?3.
Here are some samples of the spherical coordinate format in 4D, taken from the doctests
of vector_v5.py (see Example 10-16):
>>> format(Vector([-1, -1, -1, -1]), 'h')
'<2.0, 2.0943951023931957, 2.186276035465284, 3.9269908169872414>'
>>> format(Vector([2, 2, 2, 2]), '.3eh')
'<4.000e+00, 1.047e+00, 9.553e-01, 7.854e-01>'
>>> format(Vector([0, 1, 0, 0]), '0.5fh')
'<1.00000, 1.57080, 0.00000, 0.00000>'
Before we can implement the minor changes required in __format__, we need to code
a pair of support methods: angle(n) to compute one of the angular coordinates (e.g.,
?1), and angles() to return an iterable of all angular coordinates. I�ll not describe the
math here; if you�re curious, Wikipedia�s "n-sphere� entry has the formulas I used to
calculate the spherical coordinates from the Cartesian coordinates in the Vector components
array.
294 | Chapter 10: Sequence Hacking, Hashing, and Slicing
Example 10-16 is a full listing of vector_v5.py consolidating all we�ve implemented since
�Vector Take #1: Vector2d Compatible� on page 276 and introducing custom formatting.
Example 10-16. vector_v5.py: doctests and all code for final Vector class; callouts highlight
additions needed to support __format__
"""
A multidimensional ``Vector`` class, take 5
A ``Vector`` is built from an iterable of numbers::
>>> Vector([3.1, 4.2])
Vector([3.1, 4.2])
>>> Vector((3, 4, 5))
Vector([3.0, 4.0, 5.0])
>>> Vector(range(10))
Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])
Tests with two dimensions (same results as ``vector2d_v1.py``)::
>>> v1 = Vector([3, 4])
>>> x, y = v1
>>> x, y
(3.0, 4.0)
>>> v1
Vector([3.0, 4.0])
>>> v1_clone = eval(repr(v1))
>>> v1 == v1_clone
True
>>> print(v1)
(3.0, 4.0)
>>> octets = bytes(v1)
>>> octets
b'd\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@'
>>> abs(v1)
5.0
>>> bool(v1), bool(Vector([0, 0]))
(True, False)
Test of ``.frombytes()`` class method:
>>> v1_clone = Vector.frombytes(bytes(v1))
>>> v1_clone
Vector([3.0, 4.0])
>>> v1 == v1_clone
True
Tests with three dimensions::
Vector Take #5: Formatting | 295
>>> v1 = Vector([3, 4, 5])
>>> x, y, z = v1
>>> x, y, z
(3.0, 4.0, 5.0)
>>> v1
Vector([3.0, 4.0, 5.0])
>>> v1_clone = eval(repr(v1))
>>> v1 == v1_clone
True
>>> print(v1)
(3.0, 4.0, 5.0)
>>> abs(v1) # doctest:+ELLIPSIS
7.071067811...
>>> bool(v1), bool(Vector([0, 0, 0]))
(True, False)
Tests with many dimensions::
>>> v7 = Vector(range(7))
>>> v7
Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])
>>> abs(v7) # doctest:+ELLIPSIS
9.53939201...
Test of ``.__bytes__`` and ``.frombytes()`` methods::
>>> v1 = Vector([3, 4, 5])
>>> v1_clone = Vector.frombytes(bytes(v1))
>>> v1_clone
Vector([3.0, 4.0, 5.0])
>>> v1 == v1_clone
True
Tests of sequence behavior::
>>> v1 = Vector([3, 4, 5])
>>> len(v1)
3
>>> v1[0], v1[len(v1)-1], v1[-1]
(3.0, 5.0, 5.0)
Test of slicing::
>>> v7 = Vector(range(7))
>>> v7[-1]
6.0
>>> v7[1:4]
Vector([1.0, 2.0, 3.0])
296 | Chapter 10: Sequence Hacking, Hashing, and Slicing
>>> v7[-1:]
Vector([6.0])
>>> v7[1,2]
Traceback (most recent call last):
...
TypeError: Vector indices must be integers
Tests of dynamic attribute access::
>>> v7 = Vector(range(10))
>>> v7.x
0.0
>>> v7.y, v7.z, v7.t
(1.0, 2.0, 3.0)
Dynamic attribute lookup failures::
>>> v7.k
Traceback (most recent call last):
...
AttributeError: 'Vector' object has no attribute 'k'
>>> v3 = Vector(range(3))
>>> v3.t
Traceback (most recent call last):
...
AttributeError: 'Vector' object has no attribute 't'
>>> v3.spam
Traceback (most recent call last):
...
AttributeError: 'Vector' object has no attribute 'spam'
Tests of hashing::
>>> v1 = Vector([3, 4])
>>> v2 = Vector([3.1, 4.2])
>>> v3 = Vector([3, 4, 5])
>>> v6 = Vector(range(6))
>>> hash(v1), hash(v3), hash(v6)
(7, 2, 1)
Most hash values of non-integers vary from a 32-bit to 64-bit CPython build::
>>> import sys
>>> hash(v2) == (384307168202284039 if sys.maxsize > 2**32 else 357915986)
True
Tests of ``format()`` with Cartesian coordinates in 2D::
Vector Take #5: Formatting | 297
>>> v1 = Vector([3, 4])
>>> format(v1)
'(3.0, 4.0)'
>>> format(v1, '.2f')
'(3.00, 4.00)'
>>> format(v1, '.3e')
'(3.000e+00, 4.000e+00)'
Tests of ``format()`` with Cartesian coordinates in 3D and 7D::
>>> v3 = Vector([3, 4, 5])
>>> format(v3)
'(3.0, 4.0, 5.0)'
>>> format(Vector(range(7)))
'(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0)'
Tests of ``format()`` with spherical coordinates in 2D, 3D and 4D::
>>> format(Vector([1, 1]), 'h') # doctest:+ELLIPSIS
'<1.414213..., 0.785398...>'
>>> format(Vector([1, 1]), '.3eh')
'<1.414e+00, 7.854e-01>'
>>> format(Vector([1, 1]), '0.5fh')
'<1.41421, 0.78540>'
>>> format(Vector([1, 1, 1]), 'h') # doctest:+ELLIPSIS
'<1.73205..., 0.95531..., 0.78539...>'
>>> format(Vector([2, 2, 2]), '.3eh')
'<3.464e+00, 9.553e-01, 7.854e-01>'
>>> format(Vector([0, 0, 0]), '0.5fh')
'<0.00000, 0.00000, 0.00000>'
>>> format(Vector([-1, -1, -1, -1]), 'h') # doctest:+ELLIPSIS
'<2.0, 2.09439..., 2.18627..., 3.92699...>'
>>> format(Vector([2, 2, 2, 2]), '.3eh')
'<4.000e+00, 1.047e+00, 9.553e-01, 7.854e-01>'
>>> format(Vector([0, 1, 0, 0]), '0.5fh')
'<1.00000, 1.57080, 0.00000, 0.00000>'
"""
from array import array
import reprlib
import math
import numbers
import functools
import operator
import itertools
class Vector:
typecode = 'd'
298 | Chapter 10: Sequence Hacking, Hashing, and Slicing
def __init__(self, components):
self._components = array(self.typecode, components)
def __iter__(self):
return iter(self._components)
def __repr__(self):
components = reprlib.repr(self._components)
components = components[components.find('['):-1]
return 'Vector({})'.format(components)
def __str__(self):
return str(tuple(self))
def __bytes__(self):
return (bytes([ord(self.typecode)]) +
bytes(self._components))
def __eq__(self, other):
return (len(self) == len(other) and
all(a == b for a, b in zip(self, other)))
def __hash__(self):
hashes = (hash(x) for x in self)
return functools.reduce(operator.xor, hashes, 0)
def __abs__(self):
return math.sqrt(sum(x * x for x in self))
def __bool__(self):
return bool(abs(self))
def __len__(self):
return len(self._components)
def __getitem__(self, index):
cls = type(self)
if isinstance(index, slice):
return cls(self._components[index])
elif isinstance(index, numbers.Integral):
return self._components[index]
else:
msg = '{.__name__} indices must be integers'
raise TypeError(msg.format(cls))
shortcut_names = 'xyzt'
def __getattr__(self, name):
cls = type(self)
if len(name) == 1:
pos = cls.shortcut_names.find(name)
if 0 <= pos < len(self._components):
Vector Take #5: Formatting | 299
return self._components[pos]
msg = '{.__name__!r} object has no attribute {!r}'
raise AttributeError(msg.format(cls, name))
def angle(self, n):
r = math.sqrt(sum(x * x for x in self[n:]))
a = math.atan2(r, self[n-1])
if (n == len(self) - 1) and (self[-1] < 0):
return math.pi * 2 - a
else:
return a
def angles(self):
return (self.angle(n) for n in range(1, len(self)))
def __format__(self, fmt_spec=''):
if fmt_spec.endswith('h'): # hyperspherical coordinates
fmt_spec = fmt_spec[:-1]
coords = itertools.chain([abs(self)],
self.angles())
outer_fmt = '<{}>'
else:
coords = self
outer_fmt = '({})'
components = (format(c, fmt_spec) for c in coords)
return outer_fmt.format(', '.join(components))
@classmethod
def frombytes(cls, octets):
typecode = chr(octets[0])
memv = memoryview(octets[1:]).cast(typecode)
return cls(memv)
Import itertools to use chain function in __format__.
Compute one of the angular coordinates, using formulas adapted from the nsphere
article.
Create generator expression to compute all angular coordinates on demand.
Use itertools.chain to produce genexp to iterate seamlessly over the
magnitude and the angular coordinates.
Configure spherical coordinate display with angular brackets.
Configure Cartesian coordinate display with parentheses.
Create generator expression to format each coordinate item on demand.
Plug formatted components separated by commas inside brackets or
parentheses.
300 | Chapter 10: Sequence Hacking, Hashing, and Slicing
We are making heavy use of generator expressions in __for
mat__, angle, and angles but our focus here is in providing
__format__ to bring Vector to the same implementation level as
Vector2d. When we cover generators in Chapter 14 we�ll use some
of the code in Vector as examples, and then the generator tricks
will be explained in detail.
This concludes our mission for this chapter. The Vector class will be enhanced with
infix operators in Chapter 13, but our goal here was to explore techniques for coding
special methods that are useful in a wide variety of collection classes.
Chapter Summary
The Vector example in this chapter was designed to be compatible with Vector2d,
except for the use of a different constructor signature accepting a single iterable argument,
just like the built-in sequence types do. The fact that Vector behaves as a sequence
just by implementing __getitem__ and __len__ prompted a discussion of protocols,
the informal interfaces used in duck-typed languages.
We then looked at how the my_seq[a:b:c] syntax works behind the scenes, by creating
a slice(a, b, c) object and handing it to __getitem__. Armed with this knowledge,
we made Vector respond correctly to slicing, by returning new Vector instances, just
like a Pythonic sequence is expected to do.
The next step was to provide read-only access to the first few Vector components using
notation such as my_vec.x. We did it by implementing __getattr__. Doing that opened
the possibility of tempting the user to assign to those special components by writing
my_vec.x = 7, revealing a potential bug. We fixed it by implementing __setattr__ as
well, to forbid assigning values to single-letter attributes. Very often, when you code a
__getattr__ you need to add __setattr__ too, in order to avoid inconsistent behavior.
Implementing the __hash__ function provided the perfect context for using func
tools.reduce, because we needed to apply the xor operator ^ in succession to the hashes
of all Vector components to produce an aggregate hash value for the whole Vector.
After applying reduce in __hash__, we used the all reducing built-in to create a more
efficient __eq__ method.
The last enhancement to Vector was to reimplement the __format__ method from
Vector2d by supporting spherical coordinates as an alternative to the default Cartesian
coordinates. We used quite a bit of math and several generators to code __format__ and
its auxiliary functions, but these are implementation details�and we�ll come back to
the generators in Chapter 14. The goal of that last section was to support a custom
Chapter Summary | 301
format, thus fulfilling the promise of a Vector that could do everything a Vector2d did,
and more.
As we did in Chapter 9, here we often looked at how standard Python objects behave,
to emulate them and provide a �Pythonic� look-and-feel to Vector.
In Chapter 13, we will implement several infix operators on Vector. The math will be
much simpler than that in the angle() method here, but exploring how infix operators
work in Python is a great lesson in OO design. But before we get to operator overloading,
we�ll step back from working on one class and look at organizing multiple classes with
interfaces and inheritance, the subjects of Chapters 11 and 11.
Further Reading
Most special methods covered in the Vector example also appear in the Vector2d
example from Chapter 9, so the references in �Further Reading� on page 271 are all
relevant here.
The powerful reduce higher-order function is also known as fold, accumulate, aggregate,
compress, and inject. For more information, see Wikipedia�s �Fold (higher-order
function)� article, which presents applications of that higher-order function with emphasis
on functional programming with recursive data structures. The article also includes
a table listing fold-like functions in dozens of programming languages.
Soapbox
Protocols as Informal Interfaces
Protocols are not an invention of Python. The Smalltalk team, who also coined the
expression �object oriented,� used �protocol� as a synonym for what we now call interfaces.
Some Smalltalk programming environments allowed programmers to tag a group
of methods as a protocol, but that was merely a documentation and navigation aid, and
not enforced by the language. That�s why I believe �informal interface� is a reasonable
short explanation for �protocol� when I speak to an audience that is more familiar with
formal (and compiler enforced) interfaces.
Established protocols naturally evolve in any language that uses dynamic typing, that
is, when type-checking done at runtime because there is no static type information in
method signatures and variables. Ruby is another important OO language that has dynamic
typing and uses protocols.
In the Python documentation, you can often tell when a protocol is being discussed
when you see language like �a file-like object.� This is a quick way of saying �something
that behaves sufficiently like a file, by implementing the parts of the file interface that
are relevant in the context.�
302 | Chapter 10: Sequence Hacking, Hashing, and Slicing
You may think that implementing only part of a protocol is sloppy, but it has the advantage
of keeping things simple. Section 3.3 of the �Data Model� chapter suggests:
When implementing a class that emulates any built-in type, it is important that the
emulation only be implemented to the degree that it makes sense for the object being
modeled. For example, some sequences may work well with retrieval of individual elements,
but extracting a slice may not make sense.
� �Data Model� chapter of The Python Language Reference
When we don�t need to code nonsense methods just to fulfill some over-designed interface
contract and keep the compiler happy, it becomes easier to follow the KISS principle.
I�ll have more to say about protocols and interfaces in Chapter 11, where that is actually
the main focus.
Origins of Duck Typing
I believe the Ruby community, more than any other, helped popularize the term �duck
typing,� as they preached to the Java masses. But the expression has been used in Python
discussions before either Ruby or Python were �popular.� According to Wikipedia, an
early example of the duck analogy in object-oriented programming is a message to the
Python-list by Alex Martelli from July 26, 2000: polymorphism (was Re: Type checking
in python?). That�s where the quote at the beginning of this chapter came from. If you
are curious about the literary origins of the �duck typing� term, and the applications of
this OO concept in many languages, check out Wikipedia�s �Duck typing� entry.
A safe format, with Enhanced Usability
While implementing __format__, we did not take any precautions regarding Vector
instances with a very large number of components, as we did in __repr__ using re
prlib. The reasoning is that repr() is for debugging and logging, so it must always
generate some serviceable output, while __format__ is used to display output to end
users who presumably want to see the entire Vector. If you think this is dangerous, then
it would be cool to implement a further extension to the format specifier mini-language.
Here is how I�d do it: by default, any formatted Vector would display a reasonable but
limited number of components, say 30. If there are more elements than that, the default
behavior would be similar to what the reprlib does: chop the excess and put ... in its
place. However, if the format specifier ended with the special * code, meaning �all,� then
the size limitation would be disabled. So a user who�s unaware of the problem of very
long displays will not be bitten by it by accident. But if the default limitation becomes a
nuisance, then the presence of the ... should prompt the user to research the documentation
and discover the * formatting code.
Send a pull request to the Fluent Python repository on GitHub if you implement this!
The Search for a Pythonic Sum
Further Reading | 303
7. I adapted the code for this presentation: in 2003, reduce was a built-in, but in Python 3 we need to import
it; also, I replaced the names x and y with my_list and sub, for sub-list.
There�s no single answer to �What is Pythonic?� just as there�s no single answer to �What
is beautiful?� Saying, as I often do, that it means using �idiomatic Python� is not 100%
satisfactory, because what may be �idiomatic� for you may not be for me. One thing I
know: �idiomatic� does not mean using the most obscure language features.
In the Python-list, there�s a thread from April 2003 titled �Pythonic Way to Sum n-th
List Element?�. It�s relevant to our discussion of reduce in this chapter.
The original poster, Guy Middleton, asked for an improvement on this solution, stating
he did not like to use lambda:7
>>> my_list = [[1, 2, 3], [40, 50, 60], [9, 8, 7]]
>>> import functools
>>> functools.reduce(lambda a, b: a+b, [sub[1] for sub in my_list])
60
That code uses lots of idioms: lambda, reduce, and a list comprehension. It would probably
come last in a popularity contest, because it offends people who hate lambda and
those who despise list comprehensions�pretty much both sides of a divide.
If you�re going to use lambda, there�s probably no reason to use a list comprehension�
except for filtering, which is not the case here.
Here is a solution of my own that will please the lambda lovers:
>>> functools.reduce(lambda a, b: a + b[1], my_list, 0)
60
I did not take part in the original thread, and I wouldn�t use that in real code, because I
don�t like lambda too much myself, but I wanted to show an example without a list
comprehension.
The first answer came from Fernando Perez, creator of IPython, highlighting that Num?
Py supports n-dimensional arrays and n-dimensional slicing:
>>> import numpy as np
>>> my_array = np.array(my_list)
>>> np.sum(my_array[:, 1])
60
I think Perez�s solution is cool, but Guy Middleton praised this next solution, by Paul
Rubin and Skip Montanaro:
>>> import operator
>>> functools.reduce(operator.add, [sub[1] for sub in my_list], 0)
60
Then Evan Simpson asked, �What�s wrong with this?�:
304 | Chapter 10: Sequence Hacking, Hashing, and Slicing
>>> t = 0
>>> for sub in my_list:
... total += sub[1]
>>> t
60
Lots of people agreed that was quite Pythonic. Alex Martelli went as far as saying that�s
probably how Guido would code it.
I like Evan Simpson�s code but I also like David Eppstein�s comment on it:
If you want the sum of a list of items, you should write it in a way that looks like �the
sum of a list of items�, not in a way that looks like �loop over these items, maintain
another variable t, perform a sequence of additions�. Why do we have high level languages
if not to express our intentions at a higher level and let the language worry about
what low-level operations are needed to implement it?
Then Alex Martelli comes back to suggest:
�The sum� is so frequently needed that I wouldn�t mind at all if Python singled it out as
a built-in. But �reduce(operator.add, �� just isn�t a great way to express it, in my opinion
(and yet as an old APL�er, and FP-liker, I should like it�but I don�t).
Alex goes on to suggest a sum() function, which he contributed. It became a built-in in
Python 2.3, released only three months after that conversation took place. So Alex�s
preferred syntax became the norm:
>>> sum([sub[1] for sub in my_list])
60
By the end of the next year (November 2004), Python 2.4 was launched with generator
expressions, providing what is now in my opinion the most Pythonic answer to Guy
Middleton�s original question:
>>> sum(sub[1] for sub in my_list)
60
This is not only more readable than reduce but also avoids the trap of the empty sequence:
sum([]) is 0, simple as that.
In the same conversation, Alex Martelli suggests the reduce built-in in Python 2 was
more trouble than it was worth, because it encouraged coding idioms that were hard to
explain. He was most convincing: the function was demoted to the functools module
in Python 3.
Still, functools.reduce has its place. It solved the problem of our Vector.__hash__ in
a way that I would call Pythonic.
Further Reading | 305

1. Bjarne Stroustrup, The Design and Evolution of C++ (Addison-Wesley, 1994), p. 278.
CHAPTER 11
Interfaces: From Protocols to ABCs
An abstract class represents an interface.1
� Bjarne Stroustrup
Creator of C++
Interfaces are the subject of this chapter: from the dynamic protocols that are the hallmark
of duck typing to abstract base classes (ABCs) that make interfaces explicit and
verify implementations for conformance.
If you have a Java, C#, or similar background, the novelty here is in the informal protocols
of duck typing. But for the long-time Pythonista or Rubyist, that is the �normal�
way of thinking about interfaces, and the news is the formality and type-checking of
ABCs. The language was 15 years old when ABCs were introduced in Python 2.6.
We�ll start the chapter by reviewing how the Python community traditionally understood
interfaces as somewhat loose�in the sense that a partially implemented interface
is often acceptable. We�ll make that clear through a couple examples that highlight the
dynamic nature of duck typing.
Then, a guest essay by Alex Martelli will introduce ABCs and give name to a new trend
in Python programming. The rest of the chapter will be devoted to ABCs, starting with
their common use as superclasses when you need to implement an interface. We�ll then
see when an ABC checks concrete subclasses for conformance to the interface it defines,
and how a registration mechanism lets developers declare that a class implements an
interface without subclassing. Finally, we�ll see how an ABC can be programmed to
automatically �recognize� arbitrary classes that conform to its interface�without subclassing
or explicit registration.
307
We will implement a new ABC to see how that works, but Alex Martelli and I don�t want
to encourage you to start writing your own ABCs left and right. The risk of overengineering
with ABCs is very high.
ABCs, like descriptors and metaclasses, are tools for building
frameworks. Therefore, only a very small minority of Python developers
can create ABCs without imposing unreasonable limitations
and needless work on fellow programmers.
Let�s get started with the Pythonic view of interfaces.
Interfaces and Protocols in Python Culture
Python was already highly successful before ABCs were introduced, and most existing
code does not use them at all. Since Chapter 1, we�ve been talking about duck typing and
protocols. In �Protocols and Duck Typing� on page 279, protocols are defined as the
informal interfaces that make polymorphism work in languages with dynamic typing
like Python.
How do interfaces work in a dynamic-typed language? First, the basics: even without
an interface keyword in the language, and regardless of ABCs, every class has an
interface: the set public attributes (methods or data attributes) implemented or inherited
by the class. This includes special methods, like __getitem__ or __add__.
By definition, protected and private attributes are not part of an interface, even if �protected�
is merely a naming convention (the single leading underscore) and private attributes
are easily accessed (recall �Private and �Protected� Attributes in Python� on
page 262). It is bad form to violate these conventions.
On the other hand, it�s not a sin to have public data attributes as part of the interface of
an object, because�if necessary�a data attribute can always be turned into a property
implementing getter/setter logic without breaking client code that uses the plain
obj.attr syntax. We did that in the Vector2d class: in Example 11-1, we see the first
implementation with public x and y attributes.
Example 11-1. vector2d_v0.py: x and y are public data attributes (same code as
Example 9-2)
class Vector2d:
typecode = 'd'
def __init__(self, x, y):
self.x = float(x)
self.y = float(y)
def __iter__(self):
308 | Chapter 11: Interfaces: From Protocols to ABCs
return (i for i in (self.x, self.y))
# more methods follow (omitted in this listing)
In Example 9-7, we turned x and y into read-only properties (Example 11-2). This is a
significant refactoring, but an essential part of the interface of Vector2d is unchanged:
users can still read my_vector.x and my_vector.y.
Example 11-2. vector2d_v3.py: x and y reimplemented as properties (see full listing in
Example 9-9)
class Vector2d:
typecode = 'd'
def __init__(self, x, y):
self.__x = float(x)
self.__y = float(y)
@property
def x(self):
return self.__x
@property
def y(self):
return self.__y
def __iter__(self):
return (i for i in (self.x, self.y))
# more methods follow (omitted in this listing)
A useful complementary definition of interface is: the subset of an object�s public methods
that enable it to play a specific role in the system. That�s what is implied when the
Python documentation mentions �a file-like object� or �an iterable,� without specifying
a class. An interface seen as a set of methods to fulfill a role is what Smalltalkers called
a procotol, and the term spread to other dynamic language communities. Protocols are
independent of inheritance. A class may implement several protocols, enabling its instances
to fulfill several roles.
Protocols are interfaces, but because they are informal�defined only by documentation
and conventions�protocols cannot be enforced like formal interfaces can (we�ll see
how ABCs enforce interface conformance later in this chapter). A protocol may be
partially implemented in a particular class, and that�s OK. Sometimes all a specific API
requires from �a file-like object� is that it has a .read() method that returns bytes. The
remaining file methods may or may not be relevant in the context.
As I write this, the Python 3 documentation of memoryview says that it works with objects
that �support the buffer protocol, which is only documented at the C API level. The
bytearray constructor accepts an �an object conforming to the buffer interface.� Now
Interfaces and Protocols in Python Culture | 309
2. Issue16518: �add buffer protocol to glossary� was actually resolved by replacing many mentions of �object that
supports the buffer protocol/interface/API� with �bytes-like object�; a follow-up issue is �Other mentions of
the buffer protocol�.
there is a move to adopt �bytes-like object� as a friendlier term.2 I point this out to
emphasize that �X-like object,� �X protocol,� and �X interface� are synonyms in the
minds of Pythonistas.
One of the most fundamental interfaces in Python is the sequence protocol. The interpreter
goes out of its way to handle objects that provide even a minimal implementation
of that protocol, as the next section demonstrates.
Python Digs Sequences
The philosophy of the Python data model is to cooperate with essential protocols as
much as possible. When it comes to sequences, Python tries hard to work with even the
simplest implementations.
Figure 11-1 shows how the formal Sequence interface is defined as an ABC.
Figure 11-1. UML class diagram for the Sequence ABC and related abstract classes
from collections.abc. Inheritance arrows point from subclass to its superclasses. Names
in italic are abstract methods.
Now, take a look at the Foo class in Example 11-3. It does not inherit from abc.Se
quence, and it only implements one method of the sequence protocol: __getitem__
(__len__ is missing).
Example 11-3. Partial sequence protocol implementation with __getitem__: enough for
item access, iteration, and the in operator
>>> class Foo:
... def __getitem__(self, pos):
... return range(0, 30, 10)[pos]
310 | Chapter 11: Interfaces: From Protocols to ABCs
...
>>> f[1]
10
>>> f = Foo()
>>> for i in f: print(i)
...
0
10
20
>>> 20 in f
True
>>> 15 in f
False
There is no method __iter__ yet Foo instances are iterable because�as a fallback�
when Python sees a __getitem__ method, it tries to iterate over the object by calling
that method with integer indexes starting with 0. Because Python is smart enough to
iterate over Foo instances, it can also make the in operator work even if Foo has no
__contains__ method: it does a full scan to check if an item is present.
In summary, given the importance of the sequence protocol, in the absence __iter__
and __contains__ Python still manages to make iteration and the in operator work by
invoking __getitem__.
Our original FrenchDeck from Chapter 1 does not subclass from abc.Sequence either,
but it does implement both methods of the sequence protocol: __getitem__ and
__len__. See Example 11-4.
Example 11-4. A deck as a sequence of cards (same as Example 1-1)
import collections
Card = collections.namedtuple('Card', ['rank', 'suit'])
class FrenchDeck:
ranks = [str(n) for n in range(2, 11)] + list('JQKA')
suits = 'spades diamonds clubs hearts'.split()
def __init__(self):
self._cards = [Card(rank, suit) for suit in self.suits
for rank in self.ranks]
def __len__(self):
return len(self._cards)
def __getitem__(self, position):
return self._cards[position]
A good part of the demos in Chapter 1 work because of the special treatment Python
gives to anything vaguely resembling a sequence. Iteration in Python represents an
Python Digs Sequences | 311
extreme form of duck typing: the interpreter tries two different methods to iterate over
objects.
Now let�s study another example emphasizing the dynamic nature of protocols.
Monkey-Patching to Implement a Protocol at Runtime
The FrenchDeck class from Example 11-4 has a major flaw: it cannot be shuffled. Years
ago when I first wrote the FrenchDeck example I did implement a shuffle method.
Later I had a Pythonic insight: if a FrenchDeck acts like a sequence, then it doesn�t need
its own shuffle method because there is already random.shuffle, documented as
�Shuffle the sequence x in place.�
When you follow established protocols, you improve your chances
of leveraging existing standard library and third-party code,
thanks to duck typing.
The standard random.shuffle function is used like this:
>>> from random import shuffle
>>> l = list(range(10))
>>> shuffle(l)
>>> l
[5, 2, 9, 7, 8, 3, 1, 4, 0, 6]
However, if we try to shuffle a FrenchDeck instance, we get an exception, as in
Example 11-5.
Example 11-5. random.shuffle cannot handle FrenchDeck
>>> from random import shuffle
>>> from frenchdeck import FrenchDeck
>>> deck = FrenchDeck()
>>> shuffle(deck)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File ".../python3.3/random.py", line 265, in shuffle
x[i], x[j] = x[j], x[i]
TypeError: 'FrenchDeck' object does not support item assignment
The error message is quite clear: �'FrenchDeck' object does not support item assignment.�
The problem is that shuffle operates by swapping items inside the collection,
and FrenchDeck only implements the immutable sequence protocol. Mutable sequences
must also provide a __setitem__ method.
312 | Chapter 11: Interfaces: From Protocols to ABCs
Because Python is dynamic, we can fix this at runtime, even at the interactive console.
Example 11-6 shows how to do it.
Example 11-6. Monkey patching FrenchDeck to make it mutable and compatible with
random.shuffle (continuing from Example 11-5)
>>> def set_card(deck, position, card):
... deck._cards[position] = card
...
>>> FrenchDeck.__setitem__ = set_card
>>> shuffle(deck)
>>> deck[:5]
[Card(rank='3', suit='hearts'), Card(rank='4', suit='diamonds'), Card(rank='4',
suit='clubs'), Card(rank='7', suit='hearts'), Card(rank='9', suit='spades')]
Create a function that takes deck, position, and card as arguments.
Assign that function to an attribute named __setitem__ in the FrenchDeck class.
deck can now be sorted because FrenchDeck now implements the necessary
method of the mutable sequence protocol.
The signature of the __setitem__ special method is defined in The Python Language
Reference in �3.3.6. Emulating container types�. Here we named the arguments deck,
position, card�and not self, key, value as in the language reference�to show
that every Python method starts life as a plain function, and naming the first argument
self is merely a convention. This is OK in a console session, but in a Python source file
it�s much better to use self, key, and value as documented.
The trick is that set_card knows that the deck object has an attribute named _cards,
and _cards must be a mutable sequence. The set_card function is then attached to the
FrenchDeck class as the __setitem__ special method. This is an example of monkey
patching: changing a class or module at runtime, without touching the source code.
Monkey patching is powerful, but the code that does the actual patching is very tightly
coupled with the program to be patched, often handling private and undocumented
parts.
Besides being an example of monkey patching, Example 11-6 highlights that protocols
are dynamic: random.shuffle doesn�t care what type of argument it gets, it only needs
the object to implement part of the mutable sequence protocol. It doesn�t even matter
if the object was �born� with the necessary methods or if they were somehow acquired
later.
The theme of this chapter so far has been �duck typing�: operating with objects regardless
of their types, as long as they implement certain protocols.
Monkey-Patching to Implement a Protocol at Runtime | 313
When we did present diagrams with ABCs, the intent was to show how the protocols
are related to the explicit interfaces documented in the abstract classes, but we did not
actually inherit from any ABC so far.
In the following sections, we will leverage ABCs directly, and not just as documentation.
Alex Martelli�s Waterfowl
After reviewing the usual protocol-style interfaces of Python, we move to ABCs. But
before diving into examples and details, Alex Martelli explains in a guest essay why
ABCs were a great addition to Python.
I am very grateful to Alex Martelli. He was already the most cited
person in this book before he became one of the technical
editors. His insights have been invaluable, and then he offered to
write this essay. We are incredibly lucky to have him. Take it away,
Alex!
Waterfowl and ABCs
By Alex Martelli
I�ve been credited on Wikipedia for helping spread the helpful meme and sound-bite
�duck typing� (i.e, ignoring an object�s actual type, focusing instead on ensuring that the
object implements the method names, signatures, and semantics required for its intended
use).
In Python, this mostly boils down to avoiding the use of isinstance to check the object�s
type (not to mention the even worse approach of checking, for example, whether
type(foo) is bar�which is rightly anathema as it inhibits even the simplest forms of
inheritance!).
The overall duck typing approach remains quite useful in many contexts�and yet, in
many others, an often preferable one has evolved over time. And herein lies a tale�
In recent generations, the taxonomy of genus and species (including but not limited to
the family of waterfowl known as Anatidae) has mostly been driven by phenetics�an
approach focused on similarities of morphology and behavior� chiefly, observable
traits. The analogy to �duck typing� was strong.
However, parallel evolution can often produce similar traits, both morphological and
behavioral ones, among species that are actually unrelated, but just happened to evolve
in similar, though separate, ecological niches. Similar �accidental similarities� happen
in programming, too�for example, consider the classic OOP example:
314 | Chapter 11: Interfaces: From Protocols to ABCs
class Artist:
def draw(self): ...
class Gunslinger:
def draw(self): ...
class Lottery:
def draw(self): ...
Clearly, the mere existence of a method called draw, callable without arguments, is far
from sufficient to assure us that two objects x and y such that x.draw() and y.draw()
can be called are in any way exchangeable or abstractly equivalent�nothing about the
similarity of the semantics resulting from such calls can be inferred. Rather, we need a
knowledgeable programmer to somehow positively assert that such an equivalence
holds at some level!
In biology (and other disciplines) this issue has led to the emergence (and, on many
facets, the dominance) of an approach that�s an alternative to phenetics, known as
cladistics�focusing taxonomical choices on characteristics that are inherited from
common ancestors, rather than ones that are independently evolved. (Cheap and rapid
DNA sequencing can make cladistics highly practical in many more cases, in recent
years.)
For example, sheldgeese (once classified as being closer to other geese) and shelducks
(once classified as being closer to other ducks) are now grouped together within the
subfamily Tadornidae (implying they�re closer to each other than to any other Anatidae,
as they share a closer common ancestor). Furthermore, DNA analysis has shown, in
particular, that the white-winged wood duck is not as close to the Muscovy duck (the
latter being a shelduck) as similarity in looks and behavior had long suggested�so the
wood duck was reclassified into its own genus, and entirely out of the subfamily!
Does this matter? It depends on the context! For such purposes as deciding how best to
cook a waterfowl once you�ve bagged it, for example, specific observable traits (not all
of them�plumage, for example, is de minimis in such a context), mostly texture and
flavor (old-fashioned phenetics!), may be far more relevant than cladistics. But for other
issues, such as susceptibility to different pathogens (whether you�re trying to raise waterfowl
in captivity, or preserve them in the wild), DNA closeness can matter much
more�
So, by very loose analogy with these taxonomic revolutions in the world of waterfowls,
I�m recommending supplementing (not entirely replacing�in certain contexts it shall
still serve) good old duck typing with� goose typing!
What goose typing means is: isinstance(obj, cls) is now just fine� as long as cls is
an abstract base class�in other words, cls�s metaclass is abc.ABCMeta.
Alex Martelli�s Waterfowl | 315
3. You can also, of course, define your own ABCs�but I would discourage all but the most advanced Pythonistas
from going that route, just as I would discourage them from defining their own custom metaclasses� and
even for said �most advanced Pythonistas,� those of us sporting deep mastery of every fold and crease in the
language, these are not tools for frequent use: such �deep metaprogramming,� if ever appropriate, is intended
for authors of broad frameworks meant to be independently extended by vast numbers of separate development
teams� less than 1% of �most advanced Pythonistas� may ever need that! � A.M.
You can find many useful existing abstract classes in collections.abc (and additional
ones in the numbers module of The Python Standard Library).3
Among the many conceptual advantages of ABCs over concrete classes (e.g., Scott
Meyer�s �all non-leaf classes should be abstract��see Item 33 in his book, More Effective
C++), Python�s ABCs add one major practical advantage: the register class method,
which lets end-user code �declare� that a certain class becomes a �virtual� subclass of an
ABC (for this purpose the registered class must meet the ABC�s method name and
signature requirements, and more importantly the underlying semantic contract�but
it need not have been developed with any awareness of the ABC, and in particular need
not inherit from it!). This goes a long way toward breaking the rigidity and strong
coupling that make inheritance something to use with much more caution than typically
practiced by most OOP programmers�
Sometimes you don�t even need to register a class for an ABC to recognize it as a subclass!
That�s the case for the ABCs whose essence boils down to a few special methods. For
example:
>>> class Struggle:
... def __len__(self): return 23
...
>>> from collections import abc
>>> isinstance(Struggle(), abc.Sized)
True
As you see, abc.Sized recognizes Struggle as �a subclass,� with no need for registration,
as implementing the special method named __len__ is all it takes (it�s supposed to be
implemented with the proper syntax�callable without arguments�and semantics�
returning a nonnegative integer denoting an object�s �length�; any code that implements
a specially named method, such as __len__, with arbitrary, non-compliant syntax and
semantics has much worse problems anyway).
So, here�s my valediction: whenever you�re implementing a class embodying any of the
concepts represented in the ABCs in numbers, collections.abc, or other framework
you may be using, be sure (if needed) to subclass it from, or register it into, the corresponding
ABC. At the start of your programs using some library or framework defining
classes which have omitted to do that, perform the registrations yourself; then, when
you must check for (most typically) an argument being, e.g, �a sequence,� check whether:
isinstance(the_arg, collections.abc.Sequence)
316 | Chapter 11: Interfaces: From Protocols to ABCs
4. Unfortunately, in Python 3.4, there is no ABC that helps distinguish a str from tuple or other immutable
sequences, so we must test against str. In Python 2, the basestr type exists to help with tests like these.
It�s not an ABC, but it�s a superclass of both str and unicode; however, in Python 3, basestr is gone.
Curiously, there is in Python 3 a collections.abc.ByteString type, but it only helps detecting bytes
and bytearray.
And, don�t define custom ABCs (or metaclasses) in production code� if you feel the
urge to do so, I�d bet it�s likely to be a case of �all problems look like a nail�-syndrome
for somebody who just got a shiny new hammer�you (and future maintainers of your
code) will be much happier sticking with straightforward and simple code, eschewing
such depths. Vale!
Besides coining the �goose typing,� Alex makes the point that inheriting from an ABC
is more than implementing the required methods: it�s also a clear declaration of intent
by the developer. That intent can also be made explicit through registering a virtual
subclass.
In addition, the use of isinstance and issubclass becomes more acceptable to test
against ABCs. In the past, these functions worked against duck typing, but with ABCs
they become more flexible. After all, if a component does not implement an ABC by
subclassing, it can always be registered after the fact so it passes those explicit type
checks.
However, even with ABCs, you should beware that excessive use of isinstance checks
may be a code smell�a symptom of bad OO design. It�s usually not OK to have a chain
of if/elif/elif with insinstance checks performing different actions depending on
the type of an object: you should be using polymorphism for that�i.e., designing your
classes so that the interpreter dispatches calls to the proper methods, instead of you
hardcoding the dispatch logic in if/elif/elif blocks.
There is a common, practical exception to the preceding recommendation:
some Python APIs accept a single str or a sequence
of str items; if it�s just a single str, you want to wrap it in a list,
to ease processing. Because str is a sequence type, the simplest way
to distinguish it from any other immutable sequence is to do an
explicit isinstance(x, str) check.4
On the other hand, it�s usually OK to perform an insinstance check against an ABC if
you must enforce an API contract: �Dude, you have to implement this if you want to
call me,� as technical reviewer Lennart Regebro put it. That�s particularly useful in systems
that have a plug-in architecture. Outside of frameworks, duck typing is often simpler
and more flexible than type checks.
Alex Martelli�s Waterfowl | 317
5. This snippet was extracted from Example 21-2.
For example, in several classes in this book, when I needed to take a sequence of items
and process them as a list, instead of requiring a list argument by type checking, I
simply took the argument and immediately built a list from it: that way I can accept
any iterable, and if the argument is not iterable, the call will fail soon enough with a very
clear message. One example of this code pattern is in the __init__ method in
Example 11-13, later in this chapter. Of course, this approach wouldn�t work if the sequence
argument shouldn�t be copied, either because it�s too large or because my code
needs to change it in place. Then an insinstance(x, abc.MutableSequence) would
be better. If any iterable is acceptable, then calling iter(x) to obtain an iterator would
be the way to go, as we�ll see in �Why Sequences Are Iterable: The iter Function� on page
404.
Another example is how you might imitate the handling of the field_names argument
in collections.namedtuple: field_names accepts a single string with identifiers separated
by spaces or commas, or a sequence of identifiers. It might be tempting to use
isinstance, but Example 11-7 shows how I�d do it using duck typing.5
Example 11-7. Duck typing to handle a string or an iterable of strings
try:
field_names = field_names.replace(',', ' ').split()
except AttributeError:
pass
field_names = tuple(field_names)
Assume it�s a string (EAFP = it�s easier to ask forgiveness than permission).
Convert commas to spaces and split the result into a list of names.
Sorry, field_names doesn�t quack like a str� there�s either no .replace, or it
returns something we can�t .split.
Now we assume it�s already an iterable of names.
To make sure it�s an iterable and to keep our own copy, create a tuple out of what
we have.
Finally, in his essay, Alex reinforces more than once the need for restraint in the creation
of ABCs. An ABC epidemic would be disastrous, imposing excessive ceremony in a
language that became popular because it�s practical and pragmatic. During the Fluent
Python review process, Alex wrote:
ABCs are meant to encapsulate very general concepts, abstractions, introduced by a
framework�things like �a sequence� and �an exact number.� [Readers] most likely don�t
need to write any new ABCs, just use existing ones correctly, to get 99.9% of the benefits
without serious risk of misdesign.
318 | Chapter 11: Interfaces: From Protocols to ABCs
Now let�s see goose typing in practice.
Subclassing an ABC
Following Martelli�s advice, we�ll leverage an existing ABC, collections.MutableSe
quence, before daring to invent our own. In Example 11-8, FrenchDeck2 is explicitly
declared a subclass of collections.MutableSequence.
Example 11-8. frenchdeck2.py: FrenchDeck2, a subclass of collections.MutableSequence
import collections
Card = collections.namedtuple('Card', ['rank', 'suit'])
class FrenchDeck2(collections.MutableSequence):
ranks = [str(n) for n in range(2, 11)] + list('JQKA')
suits = 'spades diamonds clubs hearts'.split()
def __init__(self):
self._cards = [Card(rank, suit) for suit in self.suits
for rank in self.ranks]
def __len__(self):
return len(self._cards)
def __getitem__(self, position):
return self._cards[position]
def __setitem__(self, position, value): #
self._cards[position] = value
def __delitem__(self, position): #
del self._cards[position]
def insert(self, position, value): #
self._cards.insert(position, value)
__setitem__ is all we need to enable shuffling�
But subclassing MutableSequence forces us to implement __delitem__, an
abstract method of that ABC.
We are also required to implement insert, the third abstract method of
MutableSequence.
Python does not check for the implementation of the abstract methods at import time
(when the frenchdeck2.py module is loaded and compiled), but only at runtime when
we actually try to instantiate FrenchDeck2. Then, if we fail to implement any abstract
method, we get a TypeError exception with a message such as "Can't instantiate
Subclassing an ABC | 319
abstract class FrenchDeck2 with abstract methods __delitem__, insert".
That�s why we must implement __delitem__ and insert, even if our FrenchDeck2
examples do not need those behaviors: the MutableSequence ABC demands them.
As Figure 11-2 shows, not all methods of the Sequence and MutableSequence ABCs are
abstract.
Figure 11-2. UML class diagram for the MutableSequence ABC and its superclasses
from collections.abc (inheritance arrows point from subclasses to ancestors; names in
italic are abstract classes and abstract methods)
From Sequence, FrenchDeck2 inherits the following ready-to-use concrete methods:
__contains__, __iter__, __reversed__, index, and count. From MutableSequence, it
gets append, reverse, extend, pop, remove, and __iadd__.
The concrete methods in each collections.abc ABC are implemented in terms of the
public interface of the class, so they work without any knowledge of the internal structure
of instances.
As the coder of a concrete subclass, you may be able to override
methods inherited from ABCs with more efficient implementations.
For example, __contains__ works by doing a full scan of the
sequence, but if your concrete sequence keeps its items sorted, you
can write a faster __contains__ that does a binary search using
bisect function (see �Managing Ordered Sequences with bisect�
on page 44).
To use ABCs well, you need to know what�s available. We�ll review the collections ABCs
next.
320 | Chapter 11: Interfaces: From Protocols to ABCs
6. Multiple inheritance was considered harmful and excluded from Java, except for interfaces: Java interfaces
can extend multiple interfaces, and Java classes can implement multiple interfaces.
ABCs in the Standard Library
Since Python 2.6, ABCs are available in the standard library. Most are defined in the
collections.abc module, but there are others. You can find ABCs in the numbers and
io packages, for example. But the most widely used is collections.abc. Let�s see what
is available there.
ABCs in collections.abc
There are two modules named abc in the standard library. Here
we are talking about collections.abc. To reduce loading time, in
Python 3.4, it�s implemented outside of the collections package,
in Lib/_collections_abc.py), so it�s imported separately from
collections. The other abc module is just abc (i.e., Lib/abc.py)
where the abc.ABC class is defined. Every ABC depends on it, but
we don�t need to import it ourselves except to create a new ABC.
Figure 11-3 is a summary UML class diagram (without attribute names) of all 16 ABCs
defined in collections.abc as of Python 3.4. The official documentation of collec
tions.abc has a nice table summarizing the ABCs, their relationships, and their abstract
and concrete methods (called �mixin methods�). There is plenty of multiple inheritance
going on in Figure 11-3. We�ll devote most of Chapter 12 to multiple inheritance, but
for now it�s enough to say that it is usually not a problem when ABCs are concerned.6
ABCs in the Standard Library | 321
Figure 11-3. UML class diagram for ABCs in collections.abc
Let�s review the clusters in Figure 11-3:
Iterable, Container, and Sized
Every collection should either inherit from these ABCs or at least implement compatible
protocols. Iterable supports iteration with __iter__, Container supports
the in operator with __contains__, and Sized supports len() with __len__.
Sequence, Mapping, and Set
These are the main immutable collection types, and each has a mutable subclass. A
detailed diagram for MutableSequence is in Figure 11-2; for MutableMapping and
MutableSet, there are diagrams in Chapter 3 (Figures 3-1 and 3-2).
MappingView
In Python 3, the objects returned from the mapping methods .items(), .keys(),
and .values() inherit from ItemsView, ValuesView, and ValuesView, respectively.
The first two also inherit the rich interface of Set, with all the operators we saw in
�Set Operations� on page 82.
Callable and Hashable
These ABCs are not so closely related to collections, but collections.abc was the
first package to define ABCs in the standard library, and these two were deemed
important enough to be included. I�ve never seen subclasses of either Callable or
322 | Chapter 11: Interfaces: From Protocols to ABCs
7. For callable detection, there is the callable() built-in function�but there is no equivalent hashable()
function, so isinstance(my_obj, Hashable) is the preferred way to test for a hashable object.
Hashable. Their main use is to support the insinstance built-in as a safe way of
determining whether an object is callable or hashable.7
Iterator
Note that iterator subclasses Iterable. We discuss this further in Chapter 14.
After the collections.abc package, the most useful package of ABCs in the standard
library is numbers, covered next.
The Numbers Tower of ABCs
The numbers package defines the so-called �numerical tower� (i.e., this linear hierarchy
of ABCs), where Number is the topmost superclass, Complex is its immediate subclass,
and so on, down to Integral:
� Number
� Complex
� Real
� Rational
� Integral
So if you need to check for an integer, use isinstance(x, numbers.Integral) to accept
int, bool (which subclasses int) or other integer types that may be provided by external
libraries that register their types with the numbers ABCs. And to satisfy your check, you
or the users of your API may always register any compatible type as a virtual subclass
of numbers.Integral.
If, on the other hand, a value can be a floating-point type, you write isinstance(x,
numbers.Real), and your code will happily take bool, int, float, fractions.Frac
tion, or any other noncomplex numerical type provided by an external library, such as
NumPy, which is suitably registered.
Somewhat surprisingly, decimal.Decimal is not registered as a
virtual subclass of numbers.Real. The reason is that, if you need
the precision of Decimal in your program, then you want to be
protected from accidental mixing of decimals with other less precise
numeric types, particularly floats.
ABCs in the Standard Library | 323
8. Perhaps the client needs to audit the randomizer; or the agency wants to provide a rigged one. You never
know�
9. The Oxford English Dictionary defines tombola as �A kind of lottery resembling lotto.�
After looking at some existing ABCs, let�s practice goose typing by implementing an
ABC from scratch and putting it to use. The goal here is not to encourage everyone to
start coding ABCs left and right, but to learn how to read the source code of the ABCs
you�ll find in the standard library and other packages.
Defining and Using an ABC
To justify creating an ABC, we need to come up with a context for using it as an extension
point in a framework. So here is our context: imagine you need to display advertisements
on a website or a mobile app in random order, but without repeating an ad before the
full inventory of ads is shown. Now let�s assume we are building an ad management
framework called ADAM. One of its requirements is to support user-provided nonrepeating
random-picking classes.8 To make it clear to ADAM users what is expected of a
�nonrepeating random-picking� component, we�ll define an ABC.
Taking a clue from �stack� and �queue� (which describe abstract interfaces in terms of
physical arrangements of objects), I will use a real-world metaphor to name our ABC:
bingo cages and lottery blowers are machines designed to pick items at random from a
finite set, without repeating, until the set is exhausted.
The ABC will be named Tombola, after the Italian name of bingo and the tumbling
container that mixes the numbers.9
The Tombola ABC has four methods. The two abstract methods are:
� .load(�): put items into the container.
� .pick(): remove one item at random from the container, returning it.
The concrete methods are:
� .loaded(): return True if there is at least one item in the container.
� .inspect(): return a sorted tuple built from the items currently in the container,
without changing its contents (its internal ordering is not preserved).
Figure 11-4 shows the Tombola ABC and three concrete implementations.
324 | Chapter 11: Interfaces: From Protocols to ABCs
10. �registered� and �virtual subclass� are not standard UML words. We are using them to represent a class
relationship that is specific to Python.
Figure 11-4. UML diagram for an ABC and three subclasses. The name of the Tombola
ABC and its abstract methods are written in italics, per UML conventions. The dashed
arrow is used for interface implementation, here we are using it to show that Tombo?
List is a virtual subclass of Tombola because it is registered, as we will see later in this
chapter.10
Example 11-9 shows the definition of the Tombola ABC.
Example 11-9. tombola.py: Tombola is an ABC with two abstract methods and two
concrete methods
import abc
class Tombola(abc.ABC):
@abc.abstractmethod
def load(self, iterable):
"""Add items from an iterable."""
@abc.abstractmethod
def pick(self):
"""Remove item at random, returning it.
This method should raise `LookupError` when the instance is empty.
"""
Defining and Using an ABC | 325
11. Before ABCs existed, abstract methods would use the statement raise NotImplementedError to signal
that subclasses were responsible for their implementation.
def loaded(self):
"""Return `True` if there's at least 1 item, `False` otherwise."""
return bool(self.inspect())
def inspect(self):
"""Return a sorted tuple with the items currently inside."""
items = []
while True:
try:
items.append(self.pick())
except LookupError:
break
self.load(items)
return tuple(sorted(items))
To define an ABC, subclass abc.ABC.
An abstract method is marked with the @abstractmethod decorator, and often
its body is empty except for a docstring.11
The docstring instructs implementers to raise LookupError if there are no items
to pick.
An ABC may include concrete methods.
Concrete methods in an ABC must rely only on the interface defined by the
ABC (i.e., other concrete or abstract methods or properties of the ABC).
We can�t know how concrete subclasses will store the items, but we can build
the inspect result by emptying the Tombola with successive calls to .pick()�
�then use .load(�) to put everything back.
An abstract method can actually have an implementation. Even if
it does, subclasses will still be forced to override it, but they will be
able to invoke the abstract method with super(), adding functionality
to it instead of implementing from scratch. See the abc
module documentation for details on @abstractmethod usage.
The .inspect() method in Example 11-9 is perhaps a silly example, but it shows that,
given .pick() and .load(�) we can inspect what�s inside the Tombola by picking all
items and loading them back. The point of this example is to highlight that it�s OK to
provide concrete methods in ABCs, as long as they only depend on other methods in
the interface. Being aware of their internal data structures, concrete subclasses of Tom
326 | Chapter 11: Interfaces: From Protocols to ABCs
bola may always override .inspect() with a smarter implementation, but they don�t
have to.
The .loaded() method in Example 11-9 may not be as silly, but it�s expensive: it
calls .inspect() to build the sorted tuple just to apply bool() on it. This works, but a
concrete subclass can do much better, as we�ll see.
Note that our roundabout implementation of .inspect() requires that we catch a
LookupError thrown by self.pick(). The fact that self.pick() may raise LookupEr
ror is also part of its interface, but there is no way to declare this in Python, except in
the documentation (see the docstring for the abstract pick method in Example 11-9.)
I chose the LookupError exception because of its place in the Python hierarchy of exceptions
in relation to IndexError and KeyError, the most likely exceptions to be raised
by the data structures used to implement a concrete Tombola. Therefore, implementations
can raise LookupError, IndexError, or KeyError to comply. See Example 11-10
(for a complete tree, see �5.4. Exception hierarchy� of The Python Standard Library).
Example 11-10. Part of the Exception class hierarchy
BaseException
+-- SystemExit
+-- KeyboardInterrupt
+-- GeneratorExit
L-- Exception
+-- StopIteration
+-- ArithmeticError
� +-- FloatingPointError
� +-- OverflowError
� L-- ZeroDivisionError
+-- AssertionError
+-- AttributeError
+-- BufferError
+-- EOFError
+-- ImportError
+-- LookupError
� +-- IndexError
� L-- KeyError
+-- MemoryError
... etc.
LookupError is the exception we handle in Tombola.inspect.
IndexError is the LookupError subclass raised when we try to get an item from
a sequence with an index beyond the last position.
KeyError is raised when we use a nonexistent key to get an item from a mapping.
Defining and Using an ABC | 327
We now have our very own Tombola ABC. To witness the interface checking performed
by an ABC, let�s try to fool Tombola with a defective implementation in Example 11-11.
Example 11-11. A fake Tombola doesn�t go undetected
>>> from tombola import Tombola
>>> class Fake(Tombola): #
... def pick(self):
... return 13
...
>>> Fake #
<class '__main__.Fake'>
<class 'abc.ABC'>, <class 'object'>)
>>> f = Fake() #
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: Can't instantiate abstract class Fake with abstract methods load
Declare Fake as a subclass of Tombola.
The class was created, no errors so far.
TypeError is raised when we try to instantiate Fake. The message is very clear:
Fake is considered abstract because it failed to implement load, one of the
abstract methods declared in the Tombola ABC.
So we have our first ABC defined, and we put it to work validating a class. We�ll soon
subclass the Tombola ABC, but first we must cover some ABC coding rules.
ABC Syntax Details
The best way to declare an ABC is to subclass abc.ABC or any other ABC.
However, the abc.ABC class is new in Python 3.4, so if you are using an earlier version
of Python�and it does not make sense to subclass another existing ABC�then you
must use the metaclass= keyword in the class statement, pointing to abc.ABCMeta
(not abc.ABC). In Example 11-9, we would write:
class Tombola(metaclass=abc.ABCMeta):
# ...
The metaclass= keyword argument was introduced in Python 3. In Python 2, you must
use the __metaclass__ class attribute:
class Tombola(object): # this is Python 2!!!
__metaclass__ = abc.ABCMeta
# ...
328 | Chapter 11: Interfaces: From Protocols to ABCs
12. @abc.abstractmethod entry in the abc module documentation.
We�ll explain metaclasses in Chapter 21. For now, let�s accept that a metaclass is a special
kind of class, and agree that an ABC is a special kind of class; for example, �regular�
classes don�t check subclasses, so this is a special behavior of ABCs.
Besides the @abstractmethod, the abc module defines the @abstractclassmethod,
@abstractstaticmethod, and @abstractproperty decorators. However, these last
three are deprecated since Python 3.3, when it became possible to stack decorators on
top of @abstractmethod, making the others redundant. For example, the preferred way
to declare an abstract class method is:
class MyABC(abc.ABC):
@classmethod
@abc.abstractmethod
def an_abstract_classmethod(cls, ...):
pass
The order of stacked function decorators usually matters, and in
the case of @abstractmethod, the documentation is explicit:
When abstractmethod() is applied in combination with
other method descriptors, it should be applied as the
innermost decorator, �12
In other words, no other decorator may appear between @abstract
method and the def statement.
Now that we got these ABC syntax issues covered, let�s put Tombola to use by implementing
some full-fledged concrete descendants of it.
Subclassing the Tombola ABC
Given the Tombola ABC, we�ll now develop two concrete subclasses that satisfy its interface.
These classes were pictured in Figure 11-4, along with the virtual subclass to be
discussed in the next section.
The BingoCage class in Example 11-12 is a variation of Example 5-8 using a better
randomizer. This BingoCage implements the required abstract methods load and
pick, inherits loaded from Tombola, overrides inspect, and adds __call__.
Example 11-12. bingo.py: BingoCage is a concrete subclass of Tombola
import random
from tombola import Tombola
Defining and Using an ABC | 329
class BingoCage(Tombola):
def __init__(self, items):
self._randomizer = random.SystemRandom()
self._items = []
self.load(items)
def load(self, items):
self._items.extend(items)
self._randomizer.shuffle(self._items)
def pick(self):
try:
return self._items.pop()
except IndexError:
raise LookupError('pick from empty BingoCage')
def __call__(self):
self.pick()
This BingoCage class explicitly extends Tombola.
Pretend we�ll use this for online gaming. random.SystemRandom implements the
random API on top of the os.urandom(�) function, which provides random bytes
�suitable for cryptographic use� according to the os module docs.
Delegate initial loading to the .load(�) method.
Instead of the plain random.shuffle() function, we use the .shuffle() method
of our SystemRandom instance.
pick is implemented as in Example 5-8.
__call__ is also from Example 5-8. It�s not needed to satisfy the Tombola
interface, but there�s no harm in adding extra methods.
BingoCage inherits the expensive loaded and the silly inspect methods from Tombo
la. Both could be overridden with much faster one-liners, as in Example 11-13. The
point is: we can be lazy and just inherit the suboptimal concrete methods from an ABC.
The methods inherited from Tombola are not as fast as they could be for BingoCage, but
they do provide correct results for any Tombola subclass that correctly implements pick
and load.
Example 11-13 shows a very different but equally valid implementation of the Tombo
la interface. Instead of shuffling the �balls� and popping the last, LotteryBlower pops
from a random position.
330 | Chapter 11: Interfaces: From Protocols to ABCs
13. I gave this as an example of duck typing after Martelli�s �Waterfowl and ABCs� on page 314.
Example 11-13. lotto.py: LotteryBlower is a concrete subclass that overrides the inspect
and loaded methods from Tombola
import random
from tombola import Tombola
class LotteryBlower(Tombola):
def __init__(self, iterable):
self._balls = list(iterable)
def load(self, iterable):
self._balls.extend(iterable)
def pick(self):
try:
position = random.randrange(len(self._balls))
except ValueError:
raise LookupError('pick from empty BingoCage')
return self._balls.pop(position)
def loaded(self):
return bool(self._balls)
def inspect(self):
return tuple(sorted(self._balls))
The initializer accepts any iterable: the argument is used to build a list.
The random.randrange(�) function raises ValueError if the range is empty, so
we catch that and throw LookupError instead, to be compatible with Tombola.
Otherwise the randomly selected item is popped from self._balls.
Override loaded to avoid calling inspect (as Tombola.loaded does in
Example 11-9). We can make it faster by working with self._balls directly�
no need to build a whole sorted tuple.
Override inspect with one-liner.
Example 11-13 illustrates an idiom worth mentioning: in __init__, self._balls stores
list(iterable) and not just a reference to iterable (i.e., we did not merely assign
iterable to self._balls). As mentioned before,13 this makes our LotteryBlower
flexible because the iterable argument may be any iterable type. At the same time, we
make sure to store its items in a list so we can pop items. And even if we always get
lists as the iterable argument, list(iterable) produces a copy of the argument,
Defining and Using an ABC | 331
14. �Defensive Programming with Mutable Parameters� on page 232 in Chapter 8 was devoted to the aliasing
issue we just avoided here.
which is a good practice considering we will be removing items from it and the client
may not be expecting the list of items she provided to be changed.14
We now come to the crucial dynamic feature of goose typing: declaring virtual subclasses
with the register method.
A Virtual Subclass of Tombola
An essential characteristic of goose typing�and the reason why it deserves a waterfowl
name�is the ability to register a class as a virtual subclass of an ABC, even if it does not
inherit from it. When doing so, we promise that the class faithfully implements the
interface defined in the ABC�and Python will believe us without checking. If we lie,
we�ll be caught by the usual runtime exceptions.
This is done by calling a register method on the ABC. The registered class then becomes
a virtual subclass of the ABC, and will be recognized as such by functions like
issubclass and isinstance, but it will not inherit any methods or attributes from the
ABC.
Virtual subclasses do not inherit from their registered ABCs, and
are not checked for conformance to the ABC interface at any time,
not even when they are instantiated. It�s up to the subclass to actually
implement all the methods needed to avoid runtime errors.
The register method is usually invoked as a plain function (see �Usage of register in
Practice� on page 338), but it can also be used as a decorator. In Example 11-14, we use
the decorator syntax and implement TomboList, a virtual subclass of Tombola depicted
in Figure 11-5.
TomboList works as advertised, and the doctests that prove it are described in �How the
Tombola Subclasses Were Tested� on page 335.
332 | Chapter 11: Interfaces: From Protocols to ABCs
Figure 11-5. UML class diagram for the TomboList, a real subclass of list and a virtual
subclass of Tombola
Example 11-14. tombolist.py: class TomboList is a virtual subclass of Tombola
from random import randrange
from tombola import Tombola
@Tombola.register #
class TomboList(list): #
def pick(self):
if self: #
position = randrange(len(self))
return self.pop(position) #
else:
raise LookupError('pop from empty TomboList')
load = list.extend #
def loaded(self):
return bool(self) #
def inspect(self):
return tuple(sorted(self))
Defining and Using an ABC | 333
15. The same trick I used with load doesn�t work with loaded, because the list type does not implement
__bool__, the method I�d have to bind to loaded. On the other hand, the bool built-in function doesn�t
need __bool__ to work because it can also use __len__. See �4.1. Truth Value Testing� in the �Built-in
Types� chapter.
16. There is a whole section explaining the __mro__ class attribute in �Multiple Inheritance and Method Resolution
Order� on page 351. Right now, this quick explanation will do.
# Tombola.register(TomboList) #
Tombolist is registered as a virtual subclass of Tombola.
Tombolist extends list.
Tombolist inherits __bool__ from list, and that returns True if the list is not
empty.
Our pick calls self.pop, inherited from list, passing a random item index.
Tombolist.load is the same as list.extend.
loaded delegates to bool.15
If you�re using Python 3.3 or earlier, you can�t use .register as a class decorator.
You must use standard call syntax.
Note that because of the registration, the functions issubclass and isinstance act as
if TomboList is a subclass of Tombola:
>>> from tombola import Tombola
>>> from tombolist import TomboList
>>> issubclass(TomboList, Tombola)
True
>>> t = TomboList(range(100))
>>> isinstance(t, Tombola)
True
However, inheritance is guided by a special class attribute named __mro__�the Method
Resolution Order. It basically lists the class and its superclasses in the order Python uses
to search for methods.16 If you inspect the __mro__ of TomboList, you�ll see that it lists
only the �real� superclasses�list and object:
>>> TomboList.__mro__
(<class 'tombolist.TomboList'>, <class 'list'>, <class 'object'>)
Tombola is not in Tombolist.__mro__, so Tombolist does not inherit any methods from
Tombola.
334 | Chapter 11: Interfaces: From Protocols to ABCs
As I coded different classes to implement the same interface, I wanted a way to submit
them all to the same suite of doctests. The next section shows how I leveraged the API
of regular classes and ABCs to do it.
How the Tombola Subclasses Were Tested
The script I used to test the Tombola examples uses two class attributes that allow introspection
of a class hierarchy:
__subclasses__()
Method that returns a list of the immediate subclasses of the class. The list does not
include virtual subclasses.
_abc_registry
Data attribute�available only in ABCs�that is bound to a WeakSet with weak
references to registered virtual subclasses of the abstract class.
To test all Tombola subclasses, I wrote a script to iterate over a list built from Tombo
la.__subclasses__() and Tombola._abc_registry, and bind each class to the name
ConcreteTombola used in the doctests.
A successful run of the test script looks like this:
$ python3 tombola_runner.py
BingoCage 23 tests, 0 failed - OK
LotteryBlower 23 tests, 0 failed - OK
TumblingDrum 23 tests, 0 failed - OK
TomboList 23 tests, 0 failed - OK
The test script is Example 11-15 and the doctests are in Example 11-16.
Example 11-15. tombola_runner.py: test runner for Tombola subclasses
import doctest
from tombola import Tombola
# modules to test
import bingo, lotto, tombolist, drum
TEST_FILE = 'tombola_tests.rst'
TEST_MSG = '{0:16} {1.attempted:2} tests, {1.failed:2} failed - {2}'
def main(argv):
verbose = '-v' in argv
real_subclasses = Tombola.__subclasses__()
virtual_subclasses = list(Tombola._abc_registry)
for cls in real_subclasses + virtual_subclasses:
test(cls, verbose)
How the Tombola Subclasses Were Tested | 335
def test(cls, verbose=False):
res = doctest.testfile(
TEST_FILE,
globs={'ConcreteTombola': cls},
verbose=verbose,
optionflags=doctest.REPORT_ONLY_FIRST_FAILURE)
tag = 'FAIL' if res.failed else 'OK'
print(TEST_MSG.format(cls.__name__, res, tag))
if __name__ == '__main__':
import sys
main(sys.argv)
Import modules containing real or virtual subclasses of Tombola for testing.
__subclasses__() lists the direct descendants that are alive in memory. That�s
why we imported the modules to test, even if there is no further mention of them
in the source code: to load the classes into memory.
Build a list from _abc_registry (which is a WeakSet) so we can concatenate
it with the result of __subclasses__().
Iterate over the subclasses found, passing each to the test function.
The cls argument�the class to be tested�is bound to the name ConcreteTom
bola in the global namespace provided to run the doctest.
The test result is printed with the name of the class, the number of tests
attempted, tests failed, and an 'OK' or 'FAIL' label.
The doctest file is Example 11-16.
Example 11-16. tombola_tests.rst: doctests for Tombola subclasses
==============
Tombola tests
==============
Every concrete subclass of Tombola should pass these tests.
Create and load instance from iterable::
>>> balls = list(range(3))
>>> globe = ConcreteTombola(balls)
>>> globe.loaded()
True
>>> globe.inspect()
(0, 1, 2)
336 | Chapter 11: Interfaces: From Protocols to ABCs
Pick and collect balls::
>>> picks = []
>>> picks.append(globe.pick())
>>> picks.append(globe.pick())
>>> picks.append(globe.pick())
Check state and results::
>>> globe.loaded()
False
>>> sorted(picks) == balls
True
Reload::
>>> globe.load(balls)
>>> globe.loaded()
True
>>> picks = [globe.pick() for i in balls]
>>> globe.loaded()
False
Check that `LookupError` (or a subclass) is the exception
thrown when the device is empty::
>>> globe = ConcreteTombola([])
>>> try:
... globe.pick()
... except LookupError as exc:
... print('OK')
OK
Load and pick 100 balls to verify that they all come out::
>>> balls = list(range(100))
>>> globe = ConcreteTombola(balls)
>>> picks = []
>>> while globe.inspect():
... picks.append(globe.pick())
>>> len(picks) == len(balls)
True
>>> set(picks) == set(balls)
True
How the Tombola Subclasses Were Tested | 337
Check that the order has changed and is not simply reversed::
>>> picks != balls
True
>>> picks[::-1] != balls
True
Note: the previous 2 tests have a *very* small chance of failing
even if the implementation is OK. The probability of the 100
balls coming out, by chance, in the order they were inspect is
1/100!, or approximately 1.07e-158. It's much easier to win the
Lotto or to become a billionaire working as a programmer.
THE END
This concludes our Tombola ABC case study. In the next section, we�ll address how the
register ABC function is used in the wild.
Usage of register in Practice
In Example 11-14, we used Tombola.register as a class decorator. Prior to Python 3.3,
register could not be used like that�it had to be called as a plain function after the
class definition, as suggested by the comment at the end of Example 11-14.
However, even if register can now be used as a decorator, it�s more widely deployed
as a function to register classes defined elsewhere. For example, in the source code for
the collections.abc module, the built-in types tuple, str, range, and memoryview are
registered as virtual subclasses of Sequence like this:
Sequence.register(tuple)
Sequence.register(str)
Sequence.register(range)
Sequence.register(memoryview)
Several other built-in types are registered to ABCs in _collections_abc.py. Those registrations
happen only when that module is imported, which is OK because you�ll have
to import it anyway to get the ABCs: you need access to MutableMapping to be able to
write isinstance(my_dict, MutableMapping).
We�ll wrap up this chapter by explaining a bit of ABC magic that Alex Martelli performed
in �Waterfowl and ABCs� on page 314.
Geese Can Behave as Ducks
In his Waterfowl and ABCs essay, Alex shows that a class can be recognized as a virtual
subclass of an ABC even without registration. Here is his example again, with an added
test using issubclass:
338 | Chapter 11: Interfaces: From Protocols to ABCs
>>> class Struggle:
... def __len__(self): return 23
...
>>> from collections import abc
>>> isinstance(Struggle(), abc.Sized)
True
>>> issubclass(Struggle, abc.Sized)
True
Class Struggle is considered a subclass of abc.Sized by the issubclass function (and,
consequently, by isinstance as well) because abc.Sized implements a special class
method named __subclasshook__. See Example 11-17.
Example 11-17. Sized definition from the source code of Lib/_collections_abc.py
(Python 3.4)
class Sized(metaclass=ABCMeta):
__slots__ = ()
@abstractmethod
def __len__(self):
return 0
@classmethod
def __subclasshook__(cls, C):
if cls is Sized:
if any("__len__" in B.__dict__ for B in C.__mro__): #
return True #
return NotImplemented #
If there is an attribute named __len__ in the __dict__ of any class listed in
C.__mro__ (i.e., C and its superclasses)�
�return True, signaling that C is a virtual subclass of Sized.
Otherwise return NotImplemented to let the subclass check proceed.
If you are interested in the details of the subclass check, see the source code for the
ABCMeta.__subclasscheck__ method in Lib/abc.py. Beware: it has lots of ifs and two
recursive calls.
The __subclasshook__ adds some duck typing DNA to the whole goose typing proposition.
You can have formal interface definitions with ABCs, you can make isin
stance checks everywhere, and still have a completely unrelated class play along just
because it implements a certain method (or because it does whatever it takes to convince
a __subclasshook__ to vouch for it). Of course, this only works for ABCs that do provide
a __subclasshook__.
Geese Can Behave as Ducks | 339
Is it a good idea to implement __subclasshook__ in our own ABCs? Probably not. All
the implementations of __subclasshook__ I�ve seen in the Python source code are in
ABCs like Sized that declare just one special method, and they simply check for that
special method name. Given their �special� status, you can be pretty sure that any method
named __len__ does what you expect. But even in the realm of special methods and
fundamental ABCs, it can be risky to make such assumptions. For example, mappings
implement __len__, __getitem__, and __iter__ but they are rightly not considered a
subtype of Sequence, because you can�t retrieve items using an integer offset and they
make no guarantees about the ordering of items�except of course for OrderedDict,
which preserves the insertion order, but does support item retrieval by offset either.
For ABCs that you and I may write, a __subclasshook__ would be even less dependable.
I am not ready to believe that any class named Spam that implements or inherits load,
pick, inspect, and loaded is guaranteed to behave as a Tombola. It�s better to let the
programmer affirm it by subclassing Spam from Tombola, or at least registering: Tombo
la.register(Spam). Of course, your __subclasshook__ could also check method signatures
and other features, but I just don�t think it�s worthwhile.
Chapter Summary
The goal of this chapter was to travel from the highly dynamic nature of informal interfaces�
called protocols�visit the static interface declarations of ABCs, and conclude
with the dynamic side of ABCs: virtual subclasses and dynamic subclass detection with
__subclasshook__.
We started the journey by reviewing the traditional understanding of interfaces in the
Python community. For most of the history of Python, we�ve been mindful of interfaces,
but they were informal like the protocols from Smalltalk, and the official docs used
language such as �foo protocol,� �foo interface,� and �foo-like object� interchangeably.
Protocol-style interfaces have nothing to do with inheritance; each class stands alone
when implementing a protocol. That�s what interfaces look like when you embrace duck
typing.
With Example 11-3, we observed how deeply Python supports the sequence protocol.
If a class implements __getitem__ and nothing else, Python manages to iterate over it,
and the in operator just works. We then went back to the old FrenchDeck example of
Chapter 1 to support shuffling by dynamically adding a method. This illustrated monkey
patching and emphasized the dynamic nature of protocols. Again we saw how a partially
implemented protocol can be useful: just adding __setitem__ from the mutable sequence
protocol allowed us to leverage a ready-to-use function from the standard library:
random.shuffle. Being aware of existing protocols lets us make the most of the
rich Python standard library.
340 | Chapter 11: Interfaces: From Protocols to ABCs
17. Alex coined the expression �goose typing� and this is the first time ever it appears in a book!
18. PyMOTW, abc module page, section �Why use Abstract Base Classes?�
Alex Martelli then introduced the term �goose typing�17 to describe a new style of Python
programming. With �goose typing,� ABCs are used to make interfaces explicit and
classes may claim to implement an interface by subclassing an ABC or by registering
with it�without requiring the strong and static link of an inheritance relationship.
The FrenchDeck2 example made clear the main drawbacks and advantages of explicit
ABCs. Inheriting from abc.MutableSequence forced us to implement two methods we
did not really need: insert and __delitem__. On the other hand, even a Python newbie
can look at FrenchDeck2 and see that it�s a mutable sequence. And, as bonus, we inherited
11 ready-to-use methods from abc.MutableSequence (five indirectly from abc.Se
quence).
After a panoramic view of existing ABCs from collections.abc in Figure 11-3, we
wrote an ABC from scratch. Doug Hellmann, creator of the cool PyMOTW.com
(Python Module of the Week) explains the motivation:
By defining an abstract base class, a common API can be established for a set of subclasses.
This capability is especially useful in situations where someone less familiar with the
source for an application is going to provide plug-in extensions�18
Putting the Tombola ABC to work, we created three concrete subclasses: two inheriting
from Tombola, the other a virtual subclass registered with it, all passing the same suite
of tests.
In concluding the chapter, we mentioned how several built-in types are registered to
ABCs in the collections.abc module so you can ask isinstance(memoryview,
abc.Sequence) and get True, even if memoryview does not inherit from abc.Se
quence. And finally we went over the __subclasshook__ magic, which lets an ABC
recognize any unregistered class as a subclass, as long as it passes a test that can be as
simple or as complex as you like�the examples in the standard library merely check
for method names.
To sum up, I�d like to restate Alex Martelli�s admonition that we should refrain from
creating our own ABCs, except when we are building user-extensible frameworks�
which most of the time we are not. On a daily basis, our contact with ABCs should be
subclassing or registering classes with existing ABCs. Less often than subclassing or
registering, we might use ABCs for isinstance checks. And even more rarely�if ever
�we find occasion to write a new ABC from scratch.
After 15 years of Python, the first abstract class I ever wrote that is not a didactic example
was the Board class of the Pingo project. The drivers that support different single board
computers and controllers are subclasses of Board, thus sharing the same interface. In
Chapter Summary | 341
19. You�ll find that in the Python standard library too: classes that are in fact abstract but nobody ever made them
explicitly so.
20. Python Cookbook, 3rd Edition (O�Reilly), �Recipe 8.12. Defining an Interface or Abstract Base Class�, p. 276.
reality, although conceived and implemented as an abstract class, the pingo.Board class
does not subclass abc.ABC as I write this.19 I intend to make Board an explicit ABC
eventually�but there are more important things to do in the project.
Here is a fitting quote to end this chapter:
Although ABCs facilitate type checking, it�s not something that you should overuse in a
program. At its heart, Python is a dynamic language that gives you great flexibility. Trying
to enforce type constraints everywhere tends to result in code that is more complicated
than it needs to be. You should embrace Python�s flexibility.20
� David Beazley and Brian Jones
Python Cookbook
Or, as technical reviewer Leonardo Rochael wrote: �If you feel tempted to create a custom
ABC, please first try to solve your problem through regular duck-typing.�
Further Reading
Beazley and Jones�s Python Cookbook, 3rd Edition (O�Reilly) has a section about defining
an ABC (Recipe 8.12). The book was written before Python 3.4, so they don�t use the
now preferred syntax when declaring ABCs by subclassing from abc.ABC instead of
using the metaclass keyword. Apart from this small detail, the recipe covers the major
ABC features very well, and ends with the valuable advice quoted at the end of the
previous section.
The Python Standard Library by Example by Doug Hellmann (Addison-Wesley), has a
chapter about the abc module. It�s also available on the Web in Doug�s excellent Py?
MOTW � Python Module of the Week. Both the book and the site focus on Python 2;
therefore, adjustments must be made if you are using Python 3. And for Python 3.4,
remember that the only recommended ABC method decorator is @abstractmethod�
the others were deprecated. The other quote about ABCs in the chapter summary is
from Doug�s site and book.
When using ABCs, multiple inheritance is not only common but practically inevitable,
because each of the fundamental collection ABCs�Sequence, Mapping, and Set�extends
multiple ABCs (see Figure 11-3). Therefore, Chapter 12 is an important followup
to this one.
PEP 3119 � Introducing Abstract Base Classes gives the rationale for ABCs, and PEP
3141 - A Type Hierarchy for Numbers presents the ABCs of the numbers module.
342 | Chapter 11: Interfaces: From Protocols to ABCs
For a discussion of the pros and cons of dynamic typing, see Guido van Rossum�s interview
to Bill Venners in �Contracts in Python: A Conversation with Guido van Rossum,
Part IV�.
The zope.interface package provides a way of declaring interfaces, checking whether
objects implement them, registering providers, and querying for providers of a given
interface. The package started as a core piece of Zope 3, but it can and has been used
outside of Zope. It is the basis of the flexible component architecture of large-scale
Python projects like Twisted, Pyramid, and Plone. Lennart Regebro has a great introduction
to zope.interface in �A Python Component Architecture�. Baiju M wrote an
entire book about it: A Comprehensive Guide to Zope Component Architecture.
Soapbox
Type Hints
Probably the biggest news in the Python world in 2014 was that Guido van Rossum gave
a green light to the implementation of optional static type checking using function annotations,
similar to what the Mypy checker does. This happened in the Python-ideas
mailing-list on August 15. The message is Optional static typing � the crossroads. The
next month, PEP 484 - Type Hints was published as a draft, authored by Guido.
The idea is to let programmers optionally use annotations to declare parameter and
return types in function definitions. The key word here is optionally. You�d only add
such annotations if you want the benefits and constraints that come with them, and you
could put them in some functions but not in others.
On the surface, this may sound like what Microsoft did with with TypeScript, its Java?
Script superset, except that TypeScript goes much further: it adds new language constructs
(e.g., modules, classes, explicit interfaces, etc.), allows typed variable declarations,
and actually compiles down to plain JavaScript. As of this writing, the goals of
optional static typing in Python are much less ambitious.
To understand the reach of this proposal, there is a key point that Guido makes in the
historic August 15, 2014, email:
I am going to make one additional assumption: the main use cases will be linting, IDEs,
and doc generation. These all have one thing in common: it should be possible to run
a program even though it fails to type check. Also, adding types to a program should
not hinder its performance (nor will it help :-).
So, it seems this is not such a radical move as it seems at first. PEP 482 - Literature
Overview for Type Hints is referenced by PEP 484 - Type Hints, and briefly documents
type hints in third-party Python tools and in other languages.
Radical or not, type hints are upon us: support for PEP 484 in the form of a typing
module is likely to land in Python 3.5 already. The way the proposal is worded and
Further Reading | 343
21. Adapted from Douglas Crockford�s JavaScript: The Good Parts (O�Reilly), Appendix B, p. 109.
implemented makes it clear that no existing code will stop running because of the lack
of type hints�or their addition, for that matter.
Finally, PEP 484 clearly states:
It should also be emphasized that Python will remain a dynamically typed language,
and the authors have no desire to ever make type hints mandatory, even by convention.
Is Python Weakly Typed?
Discussions about language typing disciplines are sometimes confused due to lack of a
uniform terminology. Some writers (like Bill Venners in the interview with Guido mentioned
in �Further Reading� on page 342), say that Python has weak typing, which puts
it into the same category of JavaScript and PHP. A better way of talking about typing
discipline is to consider two different axes:
Strong versus weak typing
If the language rarely performs implicit conversion of types, it�s considered strongly
typed; if it often does it, it�s weakly typed. Java, C++, and Python are strongly typed.
PHP, JavaScript, and Perl are weakly typed.
Static versus dynamic typing
If type-checking is performed at compile time, the language is statically typed; if it
happens at runtime, it�s dynamically typed. Static typing requires type declarations
(some modern languages use type inference to avoid some of that). Fortran and
Lisp are the two oldest programming languages still alive and they use, respectively,
static and dynamic typing.
Strong typing helps catch bugs early.
Here are some examples of why weak typing is bad:21
// this is JavaScript (tested with Node.js v0.10.33)
'' == '0' // false
0 == '' // true
0 == '0' // true
'' < 0 // false
'' < '0' // true
Python does not perform automatic coercion between strings and numbers, so the ==
expressions all result False�preserving the transitivity of ==�and the < comparisons
raise TypeError in Python 3.
Static typing makes it easier for tools (compilers, IDEs) to analyze code to detect errors
and provide other services (optimization, refactoring, etc.). Dynamic typing increases
opportunities for reuse, reducing line count, and allows interfaces to emerge naturally
as protocols, instead of being imposed early on.
344 | Chapter 11: Interfaces: From Protocols to ABCs
To summarize, Python uses dynamic and strong typing. PEP 484 - Type Hints will not
change that, but will allow API authors to add optional type annotations so that tools
can perform some static type checking.
Monkey Patching
Monkey patching has a bad reputation. If abused, it can lead to systems that are hard to
understand and maintain. The patch is usually tightly coupled with its target, making
it brittle. Another problem is that two libraries that apply monkey-patches may step on
each other�s toes, with the second library to run destroying patches of the first.
But monkey patching can also be useful, for example, to make a class implement a
protocol at runtime. The adapter design pattern solves the same problem by implementing
a whole new class.
It�s easy to monkey-patch Python code, but there are limitations. Unlike Ruby and Java?
Script, Python does not let you monkey-patch the built-in types. I actually consider this
an advantage, because you can be certain that a str object will always have those same
methods. This limitation reduces the chance that external libraries try to apply conflicting
patches.
Interfaces in Java, Go, and Ruby
Since C++ 2.0 (1989), abstract classes have been used to specify interfaces in that language.
The designers of Java opted not to have multiple inheritance of classes, which
precluded the use of abstract classes as interface specifications�because often a class
needs to implement more than one interface. But they added the interface as a language
construct, and a class can implement more than one interface�a form of multiple inheritance.
Making interface definitions more explicit than ever was a great contribution
of Java. With Java 8, an interface can provide method implementations, called Default
Methods. With this, Java interfaces became closer to abstract classes in C++ and Python.
The Go language has a completely different approach. First of all, there is no inheritance
in Go. You can define interfaces, but you don�t need (and you actually can�t) explicitly
say that a certain type implements an interface. The compiler determines that automatically.
So what they have in Go could be called �static duck typing,� in the sense that
interfaces are checked at compile time but what matters is what types actually implement.
Compared to Python, it�s as if, in Go, every ABC implemented the __subclasshook__
checking function names and signatures, and you never subclassed or registered an
ABC. If we wanted Python to look more like Go, we would have to perform type checks
on all function arguments. Some of the infrastructure is available (recall �Function Annotations�
on page 154). Guido has already said he thinks it�s OK to use those annotations
for type checking�at least in support tools. See �Soapbox� on page 163 in Chapter
5 for more about this.
Rubyists are firm believers in duck typing, and Ruby has no formal way to declare an
interface or an abstract class, except to do the same we did in Python prior to 2.6: raise
Further Reading | 345
NotImplementedError in the body of methods to make them abstract by forcing the
user to subclass and implement them.
Meanwhile, I read that Yukihiro �Matz� Matsumoto, creator of Ruby, said in a keynote
in September 2014 that static typing may be in the future of the language. That was at
Ruby Kaigi in Japan, one of the most important Ruby conferences every year. As I write
this, I haven�t seen a transcript, but Godfrey Chan posted about it on his blog: �Ruby
Kaigi 2014: Day 2�. From Chan�s report, it seems Matz focused on function annotations.
There is even mention of Python function annotations.
I wonder if function annotations would be really good without ABCs to add structure
to the type system without losing flexibility. So maybe formal interfaces are also in the
future of Ruby.
I believe Python ABCs, with the register function and __subclasshook__, brought
formal interfaces to the language without throwing away the advantages of dynamic
typing.
Perhaps the geese are poised to overtake the ducks.
Metaphors and Idioms in Interfaces
A metaphor fosters understanding by making constraints clear. That�s the value of the
words �stack� and �queue� in describing those fundamental data structures: they make
clear how items can be added or removed. On the other hand, Alan Cooper writes in
About Face, 4E (Wiley):
Strict adherence to metaphors ties interfaces unnecessarily tightly to the workings of
the physical world.
He�s referring to user interfaces, but the admonition applies to APIs as well. But Cooper
does grant that when a �truly appropriate� metaphor �falls on our lap,� we can use it (he
writes �falls on our lap� because it�s so hard to find fitting metaphors that you should
not spend time actively looking for them). I believe the bingo machine imagery I used
in this chapter is appropriate and I stand by it.
About Face is by far the best book about UI design I�ve read�and I�ve read a few. Letting
go of metaphors as a design paradigm, and replacing it with �idiomatic interfaces� was
the most valuable thing I learned from Cooper�s work. As mentioned, Cooper does not
deal with APIs, but the more I think about his ideas, the more I see how they apply to
Python. The fundamental protocols of the language are what Cooper calls �idioms.�
Once we learn what a �sequence� is we can apply that knowledge in different contexts.
This is a main theme of Fluent Python: highlighting the fundamental idioms of the
language, so your code is concise, effective, and readable�for a fluent Pythonista.
346 | Chapter 11: Interfaces: From Protocols to ABCs
1. Alan Kay, �The Early History of Smalltalk,� in SIGPLAN Not. 28, 3 (March 1993), 69�95. Also available
online. Thanks to my friend Christiano Anderson who shared this reference as I was writing this chapter.
CHAPTER 12
Inheritance: For Good or For Worse
[We] started to push on the inheritance idea as a way to let novices build on frameworks
that could only be designed by experts.1.
� Alan Kay
The Early History of Smalltalk
This chapter is about inheritance and subclassing, with emphasis on two particulars
that are very specific to Python:
� The pitfalls of subclassing from built-in types
� Multiple inheritance and the method resolution order
Many consider multiple inheritance more trouble than it�s worth. The lack of it certainly
did not hurt Java; it probably fueled its widespread adoption after many were traumatized
by the excessive use of multiple inheritance in C++.
However, the amazing success and influence of Java means that a lot of programmers
come to Python without having seen multiple inheritance in practice. This is why, instead
of toy examples, our coverage of multiple inheritance will be illustrated by two
important Python projects: the Tkinter GUI toolkit and the Django Web framework.
We�ll start with the issue of subclassing built-ins. The rest of the chapter will cover
multiple inheritance with our case studies and discuss good and bad practices when
building class hierarchies.
347
Subclassing Built-In Types Is Tricky
Before Python 2.2, it was not possible to subclass built-in types such as list or dict.
Since then, it can be done but there is a major caveat: the code of the built-ins (written
in C) does not call special methods overridden by user-defined classes.
A good short description of the problem is in the documentation for PyPy, in �Differences
between PyPy and CPython�, section Subclasses of built-in types:
Officially, CPython has no rule at all for when exactly overridden method of subclasses
of built-in types get implicitly called or not. As an approximation, these methods are
never called by other built-in methods of the same object. For example, an overridden
__getitem__() in a subclass of dict will not be called by e.g. the built-in get()
method.
Example 12-1 illustrates the problem.
Example 12-1. Our __setitem__ override is ignored by the __init__ and __update__
methods of the built-in dict
>>> class DoppelDict(dict):
... def __setitem__(self, key, value):
... super().__setitem__(key, [value] * 2) #
...
>>> dd = DoppelDict(one=1) #
>>> dd
{'one': 1}
>>> dd['two'] = 2 #
>>> dd
{'one': 1, 'two': [2, 2]}
>>> dd.update(three=3) #
>>> dd
{'three': 3, 'one': 1, 'two': [2, 2]}
DoppelDict.__setitem__ duplicates values when storing (for no good reason,
just to have a visible effect). It works by delegating to the superclass.
The __init__ method inherited from dict clearly ignored that __setitem__
was overridden: the value of 'one' is not duplicated.
The [] operator calls our __setitem__ and works as expected: 'two' maps to
the duplicated value [2, 2].
The update method from dict does not use our version of __setitem__ either:
the value of 'three' was not duplicated.
This built-in behavior is a violation of a basic rule of object-oriented programming: the
search for methods should always start from the class of the target instance (self), even
when the call happens inside a method implemented in a superclass. In this sad state of
348 | Chapter 12: Inheritance: For Good or For Worse
affairs, the __missing__ method�which we saw in �The __missing__ Method� on page
72�works as documented only because it�s handled as a special case.
The problem is not limited to calls within an instance�whether self.get() calls
self.__getitem__())�but also happens with overridden methods of other classes that
should be called by the built-in methods. Example 12-2 is an example adapted from the
PyPy documentation.
Example 12-2. The __getitem__ of AnswerDict is bypassed by dict.update
>>> class AnswerDict(dict):
... def __getitem__(self, key): #
... return 42
...
>>> ad = AnswerDict(a='foo') #
>>> ad['a'] #
42
>>> d = {}
>>> d.update(ad) #
>>> d['a'] #
'foo'
>>> d
{'a': 'foo'}
AnswerDict.__getitem__ always returns 42, no matter what the key.
ad is an AnswerDict loaded with the key-value pair ('a', 'foo').
ad['a'] returns 42, as expected.
d is an instance of plain dict, which we update with ad.
The dict.update method ignored our AnswerDict.__getitem__.
Subclassing built-in types like dict or list or str directly is errorprone
because the built-in methods mostly ignore user-defined
overrides. Instead of subclassing the built-ins, derive your classes
from the collections module using UserDict, UserList, and
UserString, which are designed to be easily extended.
If you subclass collections.UserDict instead of dict, the issues exposed in Examples
12-1 and 12-2 are both fixed. See Example 12-3.
Example 12-3. DoppelDict2 and AnswerDict2 work as expected because they extend
UserDict and not dict
>>> import collections
>>>
>>> class DoppelDict2(collections.UserDict):
... def __setitem__(self, key, value):
Subclassing Built-In Types Is Tricky | 349
2. If you are curious, the experiment is in the strkeydict_dictsub.py file in the Fluent Python code repository.
3. By the way, in this regard, PyPy behaves more �correctly� than CPython, at the expense of introducing a minor
incompatibility. See �Differences between PyPy and CPython� for details.
... super().__setitem__(key, [value] * 2)
...
>>> dd = DoppelDict2(one=1)
>>> dd
{'one': [1, 1]}
>>> dd['two'] = 2
>>> dd
{'two': [2, 2], 'one': [1, 1]}
>>> dd.update(three=3)
>>> dd
{'two': [2, 2], 'three': [3, 3], 'one': [1, 1]}
>>>
>>> class AnswerDict2(collections.UserDict):
... def __getitem__(self, key):
... return 42
...
>>> ad = AnswerDict2(a='foo')
>>> ad['a']
42
>>> d = {}
>>> d.update(ad)
>>> d['a']
42
>>> d
{'a': 42}
As an experiment to measure the extra work required to subclass a built-in, I rewrote
the StrKeyDict class from Example 3-8. The original version inherited from collec
tions.UserDict, and implemented just three methods: __missing__, __contains__,
and __setitem__. The experimental StrKeyDict subclassed dict directly, and implemented
the same three methods with minor tweaks due to the way the data was stored.
But in order to make it pass the same suite of tests, I had to implement __init__, get,
and update because the versions inherited from dict refused to cooperate with the
overridden __missing__, __contains__, and __setitem__. The UserDict subclass
from Example 3-8 has 16 lines, while the experimental dict subclass ended up with 37
lines.2
To summarize: the problem described in this section applies only to method delegation
within the C language implementation of the built-in types, and only affects userdefined
classes derived directly from those types. If you subclass from a class coded in
Python, such as UserDict or MutableMapping, you will not be troubled by this.3
350 | Chapter 12: Inheritance: For Good or For Worse
Another matter related to inheritance, particularly of multiple inheritance, is: how does
Python decide which attribute to use if superclasses from parallel branches define attributes
with the same name? The answer is next.
Multiple Inheritance and Method Resolution Order
Any language implementing multiple inheritance needs to deal with potential naming
conflicts when unrelated ancestor classes implement a method by the same name. This
is called the �diamond problem,� and is illustrated in Figure 12-1 and Example 12-4.
Figure 12-1. Left: UML class diagram illustrating the �diamond problem.� Right:
Dashed arrows depict Python MRO (method resolution order) for Example 12-4.
Example 12-4. diamond.py: classes A, B, C, and D form the graph in Figure 12-1
class A:
def ping(self):
print('ping:', self)
class B(A):
def pong(self):
print('pong:', self)
class C(A):
def pong(self):
Multiple Inheritance and Method Resolution Order | 351
print('PONG:', self)
class D(B, C):
def ping(self):
super().ping()
print('post-ping:', self)
def pingpong(self):
self.ping()
super().ping()
self.pong()
super().pong()
C.pong(self)
Note that both classes B and C implement a pong method. The only difference is that
C.pong outputs the word PONG in uppercase.
If you call d.pong() on an instance of D, which pong method actually runs? In C++, the
programmer must qualify method calls with class names to resolve this ambiguity. This
can be done in Python as well. Take a look at Example 12-5.
Example 12-5. Two ways of invoking method pong on an instance of class D
>>> from diamond import *
>>> d = D()
>>> d.pong() #
pong: <diamond.D object at 0x10066c278>
>>> C.pong(d) #
PONG: <diamond.D object at 0x10066c278>
Simply calling d.pong() causes the B version to run.
You can always call a method on a superclass directly, passing the instance as an
explicit argument.
The ambiguity of a call like d.pong() is resolved because Python follows a specific order
when traversing the inheritance graph. That order is called MRO: Method Resolution
Order. Classes have an attribute called __mro__ holding a tuple of references to the
superclasses in MRO order, from the current class all the way to the object class. For
the D class, this is the __mro__ (see Figure 12-1):
>>> D.__mro__
(<class 'diamond.D'>, <class 'diamond.B'>, <class 'diamond.C'>,
<class 'diamond.A'>, <class 'object'>)
The recommended way to delegate method calls to superclasses is the super() built-in
function, which became easier to use in Python 3, as method pingpong of class D in
352 | Chapter 12: Inheritance: For Good or For Worse
4. In Python 2, the first line of D.pingpong would be written as super(D, self).ping() rather than
super().ping()
Example 12-4 illustrates.4. However, it�s also possible, and sometimes convenient, to
bypass the MRO and invoke a method on a superclass directly. For example, the D.ping
method could be written as:
def ping(self):
A.ping(self) # instead of super().ping()
print('post-ping:', self)
Note that when calling an instance method directly on a class, you must pass self
explicitly, because you are accessing an unbound method.
However, it�s safest and more future-proof to use super(), especially when calling
methods on a framework, or any class hierarchies you do not control. Example 12-6
shows that super() follows the MRO when invoking a method.
Example 12-6. Using super() to call ping (source code in Example 12-4)
>>> from diamond import D
>>> d = D()
>>> d.ping() #
ping: <diamond.D object at 0x10cc40630> #
post-ping: <diamond.D object at 0x10cc40630> #
The ping of D makes two calls.
The first call is super().ping(); the super delegates the ping call to class A;
A.ping outputs this line.
The second call is print('post-ping:', self), which outputs this line.
Now let�s see what happens when pingpong is called on an instance of D. See
Example 12-7.
Example 12-7. The five calls made by pingpong (source code in Example 12-4)
>>> from diamond import D
>>> d = D()
>>> d.pingpong()
>>> d.pingpong()
ping: <diamond.D object at 0x10bf235c0> #
post-ping: <diamond.D object at 0x10bf235c0>
ping: <diamond.D object at 0x10bf235c0> #
pong: <diamond.D object at 0x10bf235c0> #
pong: <diamond.D object at 0x10bf235c0> #
PONG: <diamond.D object at 0x10bf235c0> #
Multiple Inheritance and Method Resolution Order | 353
Call #1 is self.ping(), which runs the ping method of D, which outputs this
line and the next one.
Call #2 is super.ping(), which bypasses the ping in D and finds the ping method
in A.
Call #3 is self.pong(), which finds the B implementation of pong, according to
the __mro__.
Call #4 is super.pong(), which finds the same B.pong implementation, also
following the __mro__.
Call #5 is C.pong(self), which finds the C.pong implementation, ignoring the
__mro__.
The MRO takes into account not only the inheritance graph but also the order in which
superclasses are listed in a subclass declaration. In other words, if in diamond.py
(Example 12-4) the D class was declared as class D(C, B):, the __mro__ of class D would
be different: C would be searched before B.
I often check the __mro__ of classes interactively when I am studying them.
Example 12-8 has some examples using familiar classes.
Example 12-8. Inspecting the __mro__ attribute in several classes
>>> bool.__mro__
(<class 'bool'>, <class 'int'>, <class 'object'>)
>>> def print_mro(cls):
... print(', '.join(c.__name__ for c in cls.__mro__))
...
>>> print_mro(bool)
bool, int, object
>>> from frenchdeck2 import FrenchDeck2
>>> print_mro(FrenchDeck2)
FrenchDeck2, MutableSequence, Sequence, Sized, Iterable, Container, object
>>> import numbers
>>> print_mro(numbers.Integral)
Integral, Rational, Real, Complex, Number, object
>>> import io
>>> print_mro(io.BytesIO)
BytesIO, _BufferedIOBase, _IOBase, object
>>> print_mro(io.TextIOWrapper)
TextIOWrapper, _TextIOBase, _IOBase, object
bool inherits methods and attributes from int and object.
print_mro produces more compact displays of the MRO.
The ancestors of FrenchDeck2 include several ABCs from the collec
tions.abc module.
354 | Chapter 12: Inheritance: For Good or For Worse
These are the numeric ABCs provided by the numbers module.
The io module includes ABCs (those with the �Base suffix) and concrete classes
like BytesIO and TextIOWrapper, which are the types of binary and text file
objects returned by open(), depending on the mode argument.
The MRO is computed using an algorithm called C3. The canonical
paper on the Python MRO explaining C3 is Michele Simionato�s
�The Python 2.3 Method Resolution Order�. If you are
interested in the subtleties of the MRO, �Further Reading� on page
367 has other pointers. But don�t fret too much about this, the algorithm
is sensible; as Simionato writes:
[�] unless you make strong use of multiple inheritance
and you have non-trivial hierarchies, you don�t
need to understand the C3 algorithm, and you can easily
skip this paper.
To wrap up this discussion of the MRO, Figure 12-2 illustrates part of the complex
multiple inheritance graph of the Tkinter GUI toolkit from the Python standard library.
To study the picture, start at the Text class at the bottom. The Text class implements a
full featured, multiline editable text widget. It has rich functionality of its own, but also
inherits many methods from other classes. The left side shows a plain UML class diagram.
On the right, it�s decorated with arrows showing the MRO, as listed here with the
help of the print_mro convenience function defined in Example 12-8:
>>> import tkinter
>>> print_mro(tkinter.Text)
Text, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, YView, object
Multiple Inheritance and Method Resolution Order | 355
5. As previously mentioned, Java 8 allows interfaces to provide method implementations as well. The new feature
is called Default Methods in the official Java Tutorial.
Figure 12-2. Left: UML class diagram of the Tkinter Text widget class and its superclasses.
Right: Dashed arrows depict Text.mro.
In the next section, we�ll discuss the pros and cons of multiple inheritance, with examples
from real frameworks that use it.
Multiple Inheritance in the Real World
It is possible to put multiple inheritance to good use. The Adapter pattern in the Design
Patterns book uses multiple inheritance, so it can�t be completely wrong to do it (the
remaining 22 patterns in the book use single inheritance only, so multiple inheritance
is clearly not a cure-all).
In the Python standard library, the most visible use of multiple inheritance is the col
lections.abc package. That is not controversial: after all, even Java supports multiple
inheritance of interfaces, and ABCs are interface declarations that may optionally provide
concrete method implementations.5
An extreme example of multiple inheritance in the standard library is the Tkinter GUI
toolkit (module tkinter: Python interface to Tcl/Tk). I used part of the Tkinter widget
hierarchy to illustrate the MRO in Figure 12-2, but Figure 12-3 shows all the widget
356 | Chapter 12: Inheritance: For Good or For Worse
classes in the tkinter base package (there are more widgets in the tkinter.ttk subpackage).
Figure 12-3. Summary UML diagram for the Tkinter GUI class hierarchy; classes tagged
�mixin� are designed to provide concrete methods to other classes via multiple inheritance
Tkinter is 20 years old as I write this, and is not an example of current best practices.
But it shows how multiple inheritance was used when coders did not appreciate its
drawbacks. And it will serve as a counter-example when we cover some good practices
in the next section.
Consider these classes from Figure 12-3:
? Toplevel: The class of a top-level window in a Tkinter application.
? Widget: The superclass of every visible object that can be placed on a window.
? Button: A plain button widget.
? Entry: A single-line editable text field.
? Text: A multiline editable text field.
Multiple Inheritance in the Real World | 357
Here are the MROs of those classes, displayed by the print_mro function from
Example 12-8:
>>> import tkinter
>>> print_mro(tkinter.Toplevel)
Toplevel, BaseWidget, Misc, Wm, object
>>> print_mro(tkinter.Widget)
Widget, BaseWidget, Misc, Pack, Place, Grid, object
>>> print_mro(tkinter.Button)
Button, Widget, BaseWidget, Misc, Pack, Place, Grid, object
>>> print_mro(tkinter.Entry)
Entry, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, object
>>> print_mro(tkinter.Text)
Text, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, YView, object
Things to note about how these classes relate to others:
� Toplevel is the only graphical class that does not inherit from Widget, because it
is the top-level window and does not behave like a widget�for example, it cannot
be attached to a window or frame. Toplevel inherits from Wm, which provides direct
access functions of the host window manager, like setting the window title and
configuring its borders.
� Widget inherits directly from BaseWidget and from Pack, Place, and Grid. These
last three classes are geometry managers: they are responsible for arranging widgets
inside a window or frame. Each encapsulates a different layout strategy and widget
placement API.
� Button, like most widgets, descends only from Widget, but indirectly from Misc,
which provides dozens of methods to every widget.
� Entry subclasses Widget and XView, the class that implements horizontal scrolling.
� Text subclasses from Widget, XView, and YView, which provides vertical scrolling
functionality.
We�ll now discuss some good practices of multiple inheritance and see whether Tkinter
goes along with them.
Coping with Multiple Inheritance
[�] we needed a better theory about inheritance entirely (and still do). For example,
inheritance and instancing (which is a kind of inheritance) muddles both pragmatics
(such as factoring code to save space) and semantics (used for way too many tasks such
as: specialization, generalization, speciation, etc.).
� Alan Kay
The Early History of Smalltalk
358 | Chapter 12: Inheritance: For Good or For Worse
As Alan Kay wrote, inheritance is used for different reasons, and multiple inheritance
adds alternatives and complexity. It�s easy to create incomprehensible and brittle designs
using multiple inheritance. Because we don�t have a comprehensive theory, here are a
few tips to avoid spaghetti class graphs.
1. Distinguish Interface Inheritance from Implementation
Inheritance
When dealing with multiple inheritance, it�s useful to keep straight the reasons why
subclassing is done in the first place. The main reasons are:
� Inheritance of interface creates a subtype, implying an �is-a� relationship.
� Inheritance of implementation avoids code duplication by reuse.
In practice, both uses are often simultaneous, but whenever you can make the intent
clear, do it. Inheritance for code reuse is an implementation detail, and it can often be
replaced by composition and delegation. On the other hand, interface inheritance is the
backbone of a framework.
2. Make Interfaces Explicit with ABCs
In modern Python, if a class is designed to define an interface, it should be an explicit
ABC. In Python ? 3.4, this means: subclass abc.ABC or another ABC (see �ABC Syntax
Details� on page 328 if you need to support older Python versions).
3. Use Mixins for Code Reuse
If a class is designed to provide method implementations for reuse by multiple unrelated
subclasses, without implying an �is-a� relationship, it should be an explicit mixin class.
Conceptually, a mixin does not define a new type; it merely bundles methods for reuse.
A mixin should never be instantiated, and concrete classes should not inherit only from
a mixin. Each mixin should provide a single specific behavior, implementing few and
very closely related methods.
4. Make Mixins Explicit by Naming
There is no formal way in Python to state that a class is a mixin, so it is highly recommended
that they are named with a �Mixin suffix. Tkinter does not follow this advice,
but if it did, XView would be XViewMixin, Pack would be PackMixin, and so on with all
the classes where I put the �mixin� tag in Figure 12-3.
Coping with Multiple Inheritance | 359
6. In �Waterfowl and ABCs� on page 314, Alex Martelli quotes Scott Meyer�s More Effective C++, which goes
even further: �all non-leaf classes should be abstract� (i.e., concrete classes should not have concrete superclasses
at all).
7. �A class that is constructed primarily by inheriting from mixins and does not add its own structure or behavior
is called an aggregate class.�, Grady Booch et al., Object Oriented Analysis and Design, 3E (Addison-Wesley,
2007), p. 109.
5. An ABC May Also Be a Mixin; The Reverse Is Not True
Because an ABC can implement concrete methods, it works as a mixin as well. An ABC
also defines a type, which a mixin does not. And an ABC can be the sole base class of
any other class, while a mixin should never be subclassed alone except by another, more
specialized mixin�not a common arrangement in real code.
One restriction applies to ABCs and not to mixins: the concrete methods implemented
in an ABC should only collaborate with methods of the same ABC and its superclasses.
This implies that concrete methods in an ABC are always for convenience, because
everything they do, a user of the class can also do by calling other methods of the ABC.
6. Don�t Subclass from More Than One Concrete Class
Concrete classes should have zero or at most one concrete superclass.6 In other words,
all but one of the superclasses of a concrete class should be ABCs or mixins. For example,
in the following code, if Alpha is a concrete class, then Beta and Gamma must be ABCs
or mixins:
class MyConcreteClass(Alpha, Beta, Gamma):
"""This is a concrete class: it can be instantiated."""
# ... more code ...
7. Provide Aggregate Classes to Users
If some combination of ABCs or mixins is particularly useful to client code, provide a
class that brings them together in a sensible way. Grady Booch calls this an aggregate
class.7
For example, here is the complete source code for tkinter.Widget:
class Widget(BaseWidget, Pack, Place, Grid):
"""Internal class.
Base class for a widget which can be positioned with the
geometry managers Pack, Place or Grid."""
pass
The body of Widget is empty, but the class provides a useful service: it brings together
four superclasses so that anyone who needs to create a new widget does not need to
remember all those mixins, or wonder if they need to be declared in a certain order in
360 | Chapter 12: Inheritance: For Good or For Worse
8. Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides, Design Patterns: Elements of Reusable
Object-Oriented Software, Introduction, p. 20.
a class statement. A better example of this is the Django ListView class, which we�ll
discuss shortly, in �A Modern Example: Mixins in Django Generic Views� on page 362.
8. �Favor Object Composition Over Class Inheritance.�
This quote comes straight the Design Patterns book,8 and is the best advice I can offer
here. Once you get comfortable with inheritance, it�s too easy to overuse it. Placing
objects in a neat hierarchy appeals to our sense of order; programmers do it just for fun.
However, favoring composition leads to more flexible designs. For example, in the case
of the tkinter.Widget class, instead of inheriting the methods from all geometry managers,
widget instances could hold a reference to a geometry manager, and invoke its
methods. After all, a Widget should not �be� a geometry manager, but could use the
services of one via delegation. Then you could add a new geometry manager without
touching the widget class hierarchy and without worrying about name clashes. Even
with single inheritance, this principle enhances flexibility, because subclassing is a form
of tight coupling, and tall inheritance trees tend to be brittle.
Composition and delegation can replace the use of mixins to make behaviors available
to different classes, but cannot replace the use of interface inheritance to define a hierarchy
of types.
We will now analyze Tkinter from the point of view of these recommendations.
Tkinter: The Good, the Bad, and the Ugly
Keep in mind that Tkinter has been part of the standard library
since Python 1.1 was released in 1994. Tkinter is a layer on top of
the excellent Tk GUI toolkit of the Tcl language. The Tcl/Tk
combo is not originally object oriented, so the Tk API is basically
a vast catalog of functions. However, the toolkit is very object
oriented in its concepts, if not in its implementation.
Most advice in the previous section is not followed by Tkinter, with #7 being a notable
exception. Even then, it�s not a great example, because composition would probably
work better for integrating the geometry managers into Widget, as discussed in #8.
The docstring of tkinter.Widget starts with the words �Internal class.� This suggests
that Widget should probably be an ABC. Although Widget has no methods of its own,
it does define an interface. Its message is: �You can count on every Tkinter widget providing
basic widget methods (__init__, destroy, and dozens of Tk API functions), in
Coping with Multiple Inheritance | 361
addition to the methods of all three geometry managers.� We can agree that this is not
a great interface definition (it�s just too broad), but it is an interface, and Widget �defines�
it as the union of the interfaces of its superclasses.
The Tk class, which encapsulates the GUI application logic, inherits from Wm and Misc,
neither of which are abstract or mixin (Wm is not proper mixin because TopLevel subclasses
only from it). The name of the Misc class is�by itself�a very strong code
smell. Misc has more than 100 methods, and all widgets inherit from it. Why is it necessary
that every single widget has methods for clipboard handling, text selection, timer
management, and the like? You can�t really paste into a button or select text from a
scrollbar. Misc should be split into several specialized mixin classes, and not all widgets
should inherit from every one of those mixins.
To be fair, as a Tkinter user, you don�t need to know or use multiple inheritance at all.
It�s an implementation detail hidden behind the widget classes that you will instantiate
or subclass in your own code. But you will suffer the consequences of excessive multiple
inheritance when you type dir(tkinter.Button) and try to find the method you need
among the 214 attributes listed.
Despite the problems, Tkinter is stable, flexible, and not necessarily ugly. The legacy
(and default) Tk widgets are not themed to match modern user interfaces, but the
tkinter.ttk package provides pretty, native-looking widgets, making professional GUI
development viable since Python 3.1 (2009). Also, some of the legacy widgets, like
Canvas and Text, are incredibly powerful. With just a little coding, you can turn a Canvas
object into a simple drag-and-drop drawing application. Tkinter and Tcl/Tk are definitely
worth a look if you are interested in GUI programming.
However, our theme here is not GUI programming, but the practice of multiple inheritance.
A more up-to-date example with explicit mixin classes can be found in Django.
A Modern Example: Mixins in Django Generic Views
You don�t need to know Django to follow this section. I am just
using a small part of the framework as a practical example of
multiple inheritance, and I will try to give all the necessary background,
assuming you have some experience with server-side web
development in another language or framework.
In Django, a view is a callable object that takes, as argument, an object representing an
HTTP request and returns an object representing an HTTP response. The different
responses are what interests us in this discussion. They can be as simple as a redirect
response, with no content body, or as complex as a catalog page in an online store,
362 | Chapter 12: Inheritance: For Good or For Worse
9. Django programmers know that the as_view class method is the most visible part of the View interface, but
it�s not relevant to us here.
rendered from an HTML template and listing multiple merchandise with buttons for
buying and links to detail pages.
Originally, Django provided a set of functions, called generic views, that implemented
some common use cases. For example, many sites need to show search results that
include information from numerous items, with the listing spanning multiple pages,
and for each item a link to a page with detailed information about it. In Django, a list
view and a detail view are designed to work together to solve this problem: a list view
renders search results, and a detail view produces pages for individual items.
However, the original generic views were functions, so they were not extensible. If you
needed to do something similar but not exactly like a generic list view, you�d have to
start from scratch.
In Django 1.3, the concept of class-based views was introduced, along with a set of
generic view classes organized as base classes, mixins, and ready-to-use concrete classes.
The base classes and mixins are in the base module of the django.views.generic
package, pictured in Figure 12-4. At the top of the diagram we see two classes that take
care of very distinct responsibilities: View and TemplateResponseMixin.
A great resource to study these classes is the Classy Class-Based
Views website, where you can easily navigate through them, see all
methods in each class (inherited, overridden, and added methods),
view diagrams, browse their documentation, and jump to
their source code on GitHub.
View is the base class of all views (it could be an ABC), and it provides core functionality
like the dispatch method, which delegates to �handler� methods like get, head, post,
etc., implemented by concrete subclasses to handle the different HTTP verbs.9 The
RedirectView class inherits only from View, and you can see that it implements get,
head, post, etc.
Concrete subclasses of View are supposed to implement the handler methods, so why
aren�t they part of the View interface? The reason: subclasses are free to implement just
the handlers they want to support. A TemplateView is used only to display content, so
it only implements get. If an HTTP POST request is sent to a TemplateView, the inherited
A Modern Example: Mixins in Django Generic Views | 363
10. If you are into design patterns, you�ll notice that the Django dispatch mechanism is a dynamic variation of
the Template Method pattern. It�s dynamic because the View class does not force subclasses to implement all
handlers, but dispatch checks at runtime if a concrete handler is available for the specific request.
View.dispatch method checks that there is no post handler, and produces an HTTP
405 Method Not Allowed response.10
Figure 12-4. UML class diagram for the django.views.generic.base module
The TemplateResponseMixin provides functionality that is of interest only to views that
need to use a template. A RedirectView, for example, has no content body, so it has no
need of a template and it does not inherit from this mixin. TemplateResponseMixin
provides behaviors to TemplateView and other template-rendering views, such as List
View, DetailView, etc., defined in other modules of the django.views.generic pack?
364 | Chapter 12: Inheritance: For Good or For Worse
age. Figure 12-5 depicts the django.views.generic.list module and part of the base
module.
Figure 12-5. UML class diagram for the django.views.generic.list module. Here the
three classes of the base module are collapsed (see Figure 12-4). The ListView class has
no methods or attributes: it�s an aggregate class.
For Django users, the most important class in Figure 12-5 is ListView, which is an
aggregate class, with no code at all (its body is just a docstring). When instantiated, a
ListView has an object_list instance attribute through which the template can iterate
to show the page contents, usually the result of a database query returning multiple
objects. All the functionality related to generating this iterable of objects comes from
the MultipleObjectMixin. That mixin also provides the complex pagination logic�to
display part of the results in one page and links to more pages.
Suppose you want to create a view that will not render a template, but will produce a
list of objects in JSON format. Thats� why the BaseListView exists. It provides an easyto-
use extension point that brings together View and MultipleObjectMixin functionality,
without the overhead of the template machinery.
The Django class-based views API is a better example of multiple inheritance than
Tkinter. In particular, it is easy to make sense of its mixin classes: each has a well-defined
purpose, and they are all named with the �Mixin suffix.
Class-based views were not universally embraced by Django users. Many do use them
in a limited way, as black boxes, but when it�s necessary to create something new, a lot
A Modern Example: Mixins in Django Generic Views | 365
of Django coders continue writing monolithic view functions that take care of all those
responsibilities, instead of trying to reuse the base views and mixins.
It does take some time to learn how to leverage class-based views and how to extend
them to fulfill specific application needs, but I found that it was worthwhile to study
them: they eliminate a lot of boilerplate code, make it easier to reuse solutions, and even
improve team communication�for example, by defining standard names to templates,
and to the variables passed to template contexts. Class-based views are Django views
�on rails.�
This concludes our tour of multiple inheritance and mixin classes.
Chapter Summary
We started our coverage of inheritance explaining the problem with subclassing builtin
types: their native methods implemented in C do not call overridden methods in
subclasses, except in very few special cases. That�s why, when we need a custom list,
dict, or str type, it�s easier to subclass UserList, UserDict, or UserString�all defined
in the collections module, which actually wraps the built-in types and delegate operations
to them�three examples of favoring composition over inheritance in the standard
library. If the desired behavior is very different from what the built-ins offer, it may
be easier to subclass the appropriate ABC from collections.abc and write your own
implementation.
The rest of the chapter was devoted to the double-edged sword of multiple inheritance.
First we saw how the method resolution order, encoded in the __mro__ class attribute,
addresses the problem of potential naming conflicts in inherited methods. We also saw
how the super() built-in follows the __mro__ to call a method on a superclass. We then
studied how multiple inheritance is used in the Tkinter GUI toolkit that comes with the
Python standard library. Tkinter is not an example of current best practices, so we
discussed some ways of coping with multiple inheritance, including careful use of mixin
classes and avoiding multiple inheritance altogether by using composition instead. After
considering how multiple inheritance is abused in Tkinter, we wrapped up by studying
the core parts of the Django class-based views hierarchy, which I consider a better example
of mixin usage.
Lennart Regebro�a very experienced Pythonista and one of this book�s technical reviewers�
finds the design of Django�s mixin views hierarchy confusing. But he also
wrote:
The dangers and badness of multiple inheritance are greatly overblown. I�ve actually
never had a real big problem with it.
366 | Chapter 12: Inheritance: For Good or For Worse
In the end, each of us may have different opinions about how to use multiple inheritance,
or whether to use it at all in our own projects. But often we don�t have a choice: the
frameworks we must use impose their own choices.
Further Reading
When using ABCs, multiple inheritance is not only common but practically inevitable,
because each of the most fundamental collection ABCs (Sequence, Mapping, and Set)
extend multiple ABCs. The source code for collections.abc (Lib/_collections_
abc.py) is a good example of multiple inheritance with ABCs�many of which are
also mixin classes.
Raymond Hettinger�s post Python�s super() considered super! explains the workings of
super and multiple inheritance in Python from a positive perspective. It was written in
response to Python�s Super is nifty, but you can�t use it (a.k.a. Python�s Super Considered
Harmful) by James Knight.
Despite the titles of those posts, the problem is not really the super built-in�which in
Python 3 is not as ugly as it was in Python 2. The real issue is multiple inheritance, which
is inherently complicated and tricky. Michele Simionato goes beyond criticizing and
actually offers a solution in his Setting Multiple Inheritance Straight: he implements
traits, a constrained form of mixins that originated in the Self language. Simionato has
a long series of illuminating blog posts about multiple inheritance in Python, including
The wonders of cooperative inheritance, or using super in Python 3; Mixins considered
harmful, part 1 and part 2; and Things to Know About Python Super, part 1, part 2 and
part 3. The oldest posts use the Python 2 super syntax, but are still relevant.
I read the first edition of Grady Booch�s Object Oriented Analysis and Design, 3E
(Addison-Wesley, 2007), and highly recommend it as a general primer on object oriented
thinking, independent of programming language. It is a rare book that covers
multiple inheritance without prejudice.
Soapbox
Think About the Classes You Really Need
The vast majority of programmers write applications, not frameworks. Even those who
do write frameworks are likely to spend a lot (if not most) of their time writing applications.
When we write applications, we normally don�t need to code class hierarchies.
At most, we write classes that subclass from ABCs or other classes provided by the
framework. As application developers, it�s very rare that we need to write a class that
will act as the superclass of another. The classes we code are almost always leaf classes
(i.e., leaves of the inheritance tree).
Further Reading | 367
If, while working as an application developer, you find yourself building multilevel class
hierarchies, it�s likely that one or more of the following applies:
� You are reinventing the wheel. Go look for a framework or library that provides
components you can reuse in your application.
� You are using a badly designed framework. Go look for an alternative.
� You are overengineering. Remember the KISS principle.
� You became bored coding applications and decided to start a new framework.
Congratulations and good luck!
It�s also possible that all of the above apply to your situation: you became bored and
decided to reinvent the wheel by building your own overengineered and badly designed
framework, which is forcing you to code class after class to solve trivial problems.
Hopefully you are having fun, or at least getting paid for it.
Misbehaving Built-ins: Bug or Feature?
The built-in dict, list, and str types are essential building blocks of Python itself, so
they must be fast�any performance issues in them would severely impact pretty much
everything else. That�s why CPython adopted the shortcuts that cause their built-in
methods to misbehave by not cooperating with methods overridden by subclasses. A
possible way out of this dilemma would be to offer two implementations for each of
those types: one �internal,� optimized for use by the interpreter and an external, easily
extensible one.
But wait, this is what we have: UserDict, UserList, and UserString are not as fast as
the built-ins but are easily extensible. The pragmatic approach taken by CPython means
we also get to use, in our own applications, the highly optimized implementations that
are hard to subclass. Which makes sense, considering that it�s not so often that we need
a custom mapping, list, or string, but we use dict, list and str every day. We just need
to be aware of the trade-offs involved.
Inheritance Across Languages
Alan Kay coined the term �object oriented,� and Smalltalk had only single inheritance,
although there are forks with various forms of multiple inheritance support, including
the modern Squeak and Pharo Smalltalk dialects that support traits�a language construct
that fulfills the role of a mixin class, while avoiding some of the issues with multiple
inheritance.
The first popular language to implement multiple inheritance was C++, and the feature
was abused enough that Java�intended as a C++ replacement�was designed without
support for multiple inheritance of implementation (i.e., no mixin classes). That is, until
Java 8 introduced default methods that make interfaces very similar to the abstract
classes used to define interfaces in C++ and in Python. Except that Java interfaces cannot
have state�a key distinction. After Java, probably the most widely deployed JVM language
is Scala, and it implements traits. Other languages supporting traits are the latest
368 | Chapter 12: Inheritance: For Good or For Worse
stable versions of PHP and Groovy, and the under-construction languages Rust and Perl
6�so it�s fair to say that traits are trendy as I write this.
Ruby offers an original take on multiple inheritance: it does not support it, but introduces
mixins as a language feature. A Ruby class can include a module in its body, so
the methods defined in the module become part of the class implementation. This is a
�pure� form of mixin, with no inheritance involved, and it�s clear that a Ruby mixin has
no influence on the type of the class where it�s used. This provides the benefits of mixins,
while avoiding many of its usual problems.
Two recent languages that are getting a lot of traction severely limit inheritance: Go and
Julia. Go has no inheritance at all, but it implements interfaces in a way that resembles
a static form of duck typing (see �Soapbox� on page 343 for more about this). Julia avoids
the terms �classes� and has only �types.� Julia has a type hierarchy but subtypes cannot
inherit structure, only behaviors, and only abstract types can be subtyped. In addition,
Julia methods are implemented using multiple dispatch�a more advanced form of the
mechanism we saw in �Generic Functions with Single Dispatch� on page 202.
Further Reading | 369

1. Source: �The C Family of Languages: Interview with Dennis Ritchie, Bjarne Stroustrup, and James Gosling�.
CHAPTER 13
Operator Overloading: Doing It Right
There are some things that I kind of feel torn about, like operator overloading. I left out
operator overloading as a fairly personal choice because I had seen too many people abuse
it in C++.1
� James Gosling
Creator of Java
Operator overloading allows user-defined objects to interoperate with infix operators
such as + and | or unary operators like - and ~. More generally, function invocation (()),
attribute access (.), and item access/slicing ([]) are also operators in Python, but this
chapter covers unary and infix operators.
In �Emulating Numeric Types� on page 9 (Chapter 1) we saw some trivial implementations
of operators in a bare bones Vector class. The __add__ and __mul__ methods
in Example 1-2 were written to show how special methods support operator overloading,
but there are subtle problems in their implementations that we overlooked. Also,
in Example 9-2, we noted that the Vector2d.__eq__ method considers this to be True:
Vector(3, 4) == [3, 4]�which may or not make sense. We will address those matters
in this chapter.
In the following sections, we will cover:
� How Python supports infix operators with operands of different types
� Using duck typing or explicit type checks to deal with operands of various types
� How an infix operator method should signal it cannot handle an operand
� The special behavior of the rich comparison operators (e.g., ==, >, <=, etc.)
371
� The default handling of augmented assignment operators, like +=, and how to overload
them
Operator Overloading 101
Operator overloading has a bad name in some circles. It is a language feature that can
be (and has been) abused, resulting in programmer confusion, bugs, and unexpected
performance bottlenecks. But if well used, it leads to pleasurable APIs and readable
code. Python strikes a good balance between flexibility, usability, and safety by imposing
some limitations:
� We cannot overload operators for the built-in types.
� We cannot create new operators, only overload existing ones.
� A few operators can�t be overloaded: is, and, or, not (but the bitwise &, |, ~, can).
In Chapter 10, we already had one infix operator in Vector: ==, supported by the __eq__
method. In this chapter, we�ll improve the implementation of __eq__ to better handle
operands of types other than Vector. However, the rich comparison operators (==, !=,
>, <, >=, <=) are special cases in operator overloading, so we�ll start by overloading four
arithmetic operators in Vector: the unary - and +, followed by the infix + and *.
Let�s start with the easiest topic: unary operators.
Unary Operators
In The Python Language Reference, �6.5. Unary arithmetic and bitwise operations� lists
three unary operators, shown here with their associated special methods:
- (__neg__)
Arithmetic unary negation. If x is -2 then -x == 2.
+ (__pos__)
Arithmetic unary plus. Usually x == +x, but there are a few cases when that�s not
true. See �When x and +x Are Not Equal� on page 373 if you�re curious.
~ (__invert__)
Bitwise inverse of an integer, defined as ~x == -(x+1). If x is 2 then ~x == -3.
The Data Model� chapter of The Python Language Reference also lists the abs(�) builtin
function as a unary operator. The associated special method is __abs__, as we�ve seen
before, starting with �Emulating Numeric Types� on page 9.
It�s easy to support the unary operators. Simply implement the appropriate special
method, which will receive just one argument: self. Use whatever logic makes sense in
372 | Chapter 13: Operator Overloading: Doing It Right
your class, but stick to the fundamental rule of operators: always return a new object.
In other words, do not modify self, but create and return a new instance of a suitable
type.
In the case of - and +, the result will probably be an instance of the same class as self;
for +, returning a copy of self is the best approach most of the time. For abs(�), the
result should be a scalar number. As for ~, it�s difficult to say what would be a sensible
result if you�re not dealing with bits in an integer, but in an ORM it could make sense
to return the negation of an SQL WHERE clause, for example.
As promised before, we�ll implement several new operators on the Vector class from
Chapter 10. Example 13-1 shows the __abs__ method we already had in
Example 10-16, and the newly added __neg__ and __pos__ unary operator method.
Example 13-1. vector_v6.py: unary operators - and + added to Example 10-16
def __abs__(self):
return math.sqrt(sum(x * x for x in self))
def __neg__(self):
return Vector(-x for x in self)
def __pos__(self):
return Vector(self)
To compute -v, build a new Vector with every component of self negated.
To compute +v, build a new Vector with every component of self.
Recall that Vector instances are iterable, and the Vector.__init__ takes an iterable
argument, so the implementations of __neg__ and __pos__ are short and sweet.
We�ll not implement __invert__, so if the user tries ~v on a Vector instance, Python
will raise TypeError with a clear message: �bad operand type for unary ~: 'Vector'.�
The following sidebar covers a curiosity that may help you win a bet about unary +
someday. The next important topic is �Overloading + for Vector Addition� on page 375.
When x and +x Are Not Equal
Everybody expects that x == +x, and that is true almost all the time in Python, but I
found two cases in the standard library where x != +x.
The first case involves the decimal.Decimal class. You can have x != +x if x is a Deci
mal instance created in an arithmetic context and +x is then evaluated in a context with
different settings. For example, x is calculated in a context with a certain precision, but
Unary Operators | 373
the precision of the context is changed and then +x is evaluated. See Example 13-2 for
a demonstration.
Example 13-2. A change in the arithmetic context precision may cause x to differ
from +x
>>> import decimal
>>> ctx = decimal.getcontext()
>>> ctx.prec = 40
>>> one_third = decimal.Decimal('1') / decimal.Decimal('3')
>>> one_third
Decimal('0.3333333333333333333333333333333333333333')
>>> one_third == +one_third
True
>>> ctx.prec = 28
>>> one_third == +one_third
False
>>> +one_third
Decimal('0.3333333333333333333333333333')
Get a reference to the current global arithmetic context.
Set the precision of the arithmetic context to 40.
Compute 1/3 using the current precision.
Inspect the result; there are 40 digits after the decimal point.
one_third == +one_third is True.
Lower precision to 28�the default for Decimal arithmetic in Python 3.4.
Now one_third == +one_third is False.
Inspect +one_third; there are 28 digits after the '.' here.
The fact is that each occurrence of the expression +one_third produces a new Deci
mal instance from the value of one_third, but using the precision of the current arithmetic
context.
The second case where x != +x you can find in the collections.Counter documentation.
The Counter class implements several arithmetic operators, including infix + to
add the tallies from two Counter instances. However, for practical reasons, Counter
addition discards from the result any item with a negative or zero count. And the prefix
+ is a shortcut for adding an empty Counter, therefore it produces a new Counter
preserving only the tallies that are greater than zero. See Example 13-3.
Example 13-3. Unary + produces a new Counter without zeroed or negative tallies
>>> ct = Counter('abracadabra')
>>> ct
Counter({'a': 5, 'r': 2, 'b': 2, 'd': 1, 'c': 1})
>>> ct['r'] = -3
>>> ct['d'] = 0
374 | Chapter 13: Operator Overloading: Doing It Right
>>> ct
Counter({'a': 5, 'b': 2, 'c': 1, 'd': 0, 'r': -3})
>>> +ct
Counter({'a': 5, 'b': 2, 'c': 1})
Now, back to our regularly scheduled programming.
Overloading + for Vector Addition
The Vector class is a sequence type, and the section �3.3.6. Emulating
container types� in the �Data Model� chapter says sequences
should support the + operator for concatenation and * for
repetition. However, here we will implement + and * as mathematical
vector operations, which are a bit harder but more meaningful
for a Vector type.
Adding two Euclidean vectors results in a new vector in which the components are the
pairwise additions of the components of the addends. To illustrate:
>>> v1 = Vector([3, 4, 5])
>>> v2 = Vector([6, 7, 8])
>>> v1 + v2
Vector([9.0, 11.0, 13.0])
>>> v1 + v2 == Vector([3+6, 4+7, 5+8])
True
What happens if we try to add two Vector instances of different lengths? We could raise
an error, but considering practical applications (such as information retrieval), it�s better
to fill out the shortest Vector with zeros. This is the result we want:
>>> v1 = Vector([3, 4, 5, 6])
>>> v3 = Vector([1, 2])
>>> v1 + v3
Vector([4.0, 6.0, 5.0, 6.0])
Given these basic requirements, the implementation of __add__ is short and sweet, as
shown in Example 13-4.
Example 13-4. Vector.add method, take #1
# inside the Vector class
def __add__(self, other):
pairs = itertools.zip_longest(self, other, fillvalue=0.0) #
return Vector(a + b for a, b in pairs) #
Overloading + for Vector Addition | 375
pairs is a generator that will produce tuples (a, b) where a is from self, and
b is from other. If self and other have different lengths, fillvalue is used to
supply the missing values for the shortest iterable.
A new Vector is built from a generator expression producing one sum for each
item in pairs.
Note how __add__ returns a new Vector instance, and does not affect self or other.
Special methods implementing unary or infix operators should
never change their operands. Expressions with such operators are
expected to produce results by creating new objects. Only augmented
assignment operators may change the first operand (self),
as discussed in �Augmented Assignment Operators� on page 388.
Example 13-4 allows adding Vector to a Vector2d, and Vector to a tuple or to any
iterable that produces numbers, as Example 13-5 proves.
Example 13-5. Vector.__add__ take #1 supports non-Vector objects, too
>>> v1 = Vector([3, 4, 5])
>>> v1 + (10, 20, 30)
Vector([13.0, 24.0, 35.0])
>>> from vector2d_v3 import Vector2d
>>> v2d = Vector2d(1, 2)
>>> v1 + v2d
Vector([4.0, 6.0, 5.0])
Both additions in Example 13-5 work because __add__ uses zip_longest(�), which
can consume any iterable, and the generator expression to build the new Vector merely
performs a + b with the pairs produced by zip_longest(�), so an iterable producing
any number items will do.
However, if we swap the operands (Example 13-6), the mixed-type additions fail..
Example 13-6. Vector.__add__ take #1 fails with non-Vector left operands
>>> v1 = Vector([3, 4, 5])
>>> (10, 20, 30) + v1
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: can only concatenate tuple (not "Vector") to tuple
>>> from vector2d_v3 import Vector2d
>>> v2d = Vector2d(1, 2)
>>> v2d + v1
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'Vector2d' and 'Vector'
376 | Chapter 13: Operator Overloading: Doing It Right
To support operations involving objects of different types, Python implements a special
dispatching mechanism for the infix operator special methods. Given an expression a
+ b, the interpreter will perform these steps (also see Figure 13-1):
1. If a has __add__, call a.__add__(b) and return result unless it�s NotImplemented.
2. If a doesn�t have __add__, or calling it returns NotImplemented, check if b has
__radd__, then call b.__radd__(a) and return result unless it�s NotImplemented.
3. If b doesn�t have __radd__, or calling it returns NotImplemented, raise TypeError
with an unsupported operand types message.
Figure 13-1. Flowchart for computing a + b with __add__ and __radd__
Overloading + for Vector Addition | 377
2. The Python documentation uses both terms. The �Data Model� chapter uses �reflected,� but �9.1.2.2. Implementing
the arithmetic operations� in the numbers module docs mention �forward� and �reverse� methods,
and I find this terminology better, because �forward� and �reversed� clearly name each of the directions, while
�reflected� doesn�t have an obvious opposite.
The __radd__ method is called the �reflected� or �reversed� version of __add__. I prefer
to call them �reversed� special methods.2 Three of this book�s technical reviewers�Alex,
Anna, and Leo�told me they like to think of them as the �right� special methods,
because they are called on the righthand operand. Whatever �r�-word you prefer, that�s
what the �r� prefix stands for in __radd__, __rsub__, and the like.
Therefore, to make the mixed-type additions in Example 13-6 work, we need to implement
the Vector.__radd__ method, which Python will invoke as a fall back if the left
operand does not implement __add__ or if it does but returns NotImplemented to signal
that it doesn�t know how to handle the right operand.
Do not confuse NotImplemented with NotImplementedError. The
first, NotImplemented, is a special singleton value that an infix
operator special method should return to tell the interpreter it
cannot handle a given operand. In contrast, NotImplementedEr
ror is an exception that stub methods in abstract classes raise to
warn that they must be overwritten by subclasses.
The simplest possible __radd__ that works is shown in Example 13-7.
Example 13-7. Vector.__add__ and __radd__ methods
# inside the Vector class
def __add__(self, other): #
pairs = itertools.zip_longest(self, other, fillvalue=0.0)
return Vector(a + b for a, b in pairs)
def __radd__(self, other): #
return self + other
No changes to __add__ from Example 13-4; listed here because __radd__ uses
it.
__radd__ just delegates to __add__.
Often, __radd__ can be as simple as that: just invoke the proper operator, therefore
delegating to __add__ in this case. This applies to any commutative operator; + is commutative
when dealing with numbers or our vectors, but it�s not commutative when
concatenating sequences in Python.
378 | Chapter 13: Operator Overloading: Doing It Right
The methods in Example 13-4 work with Vector objects, or any iterable with numeric
items, such as a Vector2d, a tuple of integers, or an array of floats. But if provided with
a noniterable object, __add__ fails with a message that is not very helpful, as in
Example 13-8.
Example 13-8. Vector.__add__ method needs an iterable operand
>>> v1 + 1
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "vector_v6.py", line 328, in __add__
pairs = itertools.zip_longest(self, other, fillvalue=0.0)
TypeError: zip_longest argument #2 must support iteration
Another unhelpful message is given if an operand is iterable but its items cannot be
added to the float items in the Vector. See Example 13-9.
Example 13-9. Vector.__add__ method needs an iterable with numeric items
>>> v1 + 'ABC'
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "vector_v6.py", line 329, in __add__
return Vector(a + b for a, b in pairs)
File "vector_v6.py", line 243, in __init__
self._components = array(self.typecode, components)
File "vector_v6.py", line 329, in <genexpr>
return Vector(a + b for a, b in pairs)
TypeError: unsupported operand type(s) for +: 'float' and 'str'
The problems in Examples 13-8 and 13-9 actually go deeper than obscure error messages:
if an operator special method cannot return a valid result because of type incompatibility,
it should return NotImplemented and not raise TypeError. By returning No
tImplemented, you leave the door open for the implementer of the other operand type
to perform the operation when Python tries the reversed method call.
In the spirit of duck typing, we will refrain from testing the type of the other operand,
or the type of its elements. We�ll catch the exceptions and return NotImplemented. If
the interpreter has not yet reversed the operands, it will try that. If the reverse method
call returns NotImplemented, then Python will raise issue TypeError with a standard
error message like �unsupported operand type(s) for +: Vector and str.�
The final implementation of the special methods for Vector addition are in
Example 13-10.
Example 13-10. vector_v6.py: operator + methods added to vector_v5.py
(Example 10-16)
def __add__(self, other):
try:
Overloading + for Vector Addition | 379
3. The @ sign can be used as an infix dot product operator starting with Python 3.5. More about it in �The New
@ Infix Operator in Python 3.5� on page 383.
pairs = itertools.zip_longest(self, other, fillvalue=0.0)
return Vector(a + b for a, b in pairs)
except TypeError:
return NotImplemented
def __radd__(self, other):
return self + other
If an infix operator method raises an exception, it aborts the operator
dispatch algorithm. In the particular case of TypeError, it
is often better to catch it and return NotImplemented. This allows
the interpreter to try calling the reversed operator method,
which may correctly handle the computation with the swapped
operands, if they are of different types.
At this point, we have safely overloaded the + operator by writing __add__ and
__radd__. We will now tackle another infix operator: *.
Overloading * for Scalar Multiplication
What does Vector([1, 2, 3]) * x mean? If x is a number, that would be a scalar
product, and the result would be a new Vector with each component multiplied by x�
also known as an elementwise multiplication:
>>> v1 = Vector([1, 2, 3])
>>> v1 * 10
Vector([10.0, 20.0, 30.0])
>>> 11 * v1
Vector([11.0, 22.0, 33.0])
Another kind of product involving Vector operands would be the dot product of two
vectors�or matrix multiplication, if you take one vector as a 1 ? N matrix and the other
as an N ? 1 matrix. The current practice in NumPy and similar libraries is not to overload
the * with these two meanings, but to use * only for the scalar product. For example, in
NumPy, numpy.dot() computes the dot product.3
Back to our scalar product, again we start with the simplest __mul__ and __rmul__
methods that could possibly work:
# inside the Vector class
def __mul__(self, scalar):
return Vector(n * scalar for n in self)
380 | Chapter 13: Operator Overloading: Doing It Right
def __rmul__(self, scalar):
return self * scalar
Those methods do work, except when provided with incompatible operands. The
scalar argument has to be a number that when multiplied by a float produces another
float (because our Vector class uses an array of floats internally). So a complex number
will not do, but the scalar can be an int, a bool (because bool is a subclass of int), or
even a fractions.Fraction instance.
We could use the same duck typing technique as we did in Example 13-10 and catch a
TypeError in __mul__, but there is another, more explicit way that makes sense in this
situation: goose typing. We use isinstance() to check the type of scalar, but instead
of hardcoding some concrete types, we check against the numbers.Real ABC, which
covers all the types we need, and keeps our implementation open to future numeric
types that declare themselves actual or virtual subclasses of the numbers.Real ABC.
Example 13-11 shows a practical use of goose typing�an explicit check against an abstract
type; see the_ Fluent Python_ code repository for the full listing.
As you may recall from �ABCs in the Standard Library� on page
321, decimal.Decimal is not registered as a virtual subclass of
numbers.Real. Thus, our Vector class will not handle decimal.Dec
imal numbers.
Example 13-11. vector_v7.py: operator * methods added
from array import array
import reprlib
import math
import functools
import operator
import itertools
import numbers #
class Vector:
typecode = 'd'
def __init__(self, components):
self._components = array(self.typecode, components)
# many methods omitted in book listing, see vector_v7.py
# in https://github.com/fluentpython/example-code ...
def __mul__(self, scalar):
if isinstance(scalar, numbers.Real): #
return Vector(n * scalar for n in self)
else: #
return NotImplemented
Overloading * for Scalar Multiplication | 381
def __rmul__(self, scalar):
return self * scalar #
Import the numbers module for type checking.
If scalar is an instance of a numbers.Real subclass, create new Vector with
multiplied component values.
Otherwise, raise TypeError with an explicit message.
In this example, __rmul__ works fine by just performing self * scalar,
delegating to the __mul__ method.
With Example 13-11, we can multiply Vectors by scalar values of the usual and not so
usual numeric types:
>>> v1 = Vector([1.0, 2.0, 3.0])
>>> 14 * v1
Vector([14.0, 28.0, 42.0])
>>> v1 * True
Vector([1.0, 2.0, 3.0])
>>> from fractions import Fraction
>>> v1 * Fraction(1, 3)
Vector([0.3333333333333333, 0.6666666666666666, 1.0])
Implementing + and * we saw the most common patterns for coding infix operators.
The techniques we described for + and * are applicable to all operators listed in Table 13-1
(the in-place operators will be covered in �Augmented Assignment Operators� on page
388).
Table 13-1. Infix operator method names (the in-place operators are used for augmented
assignment; comparison operators are in Table 13-2)
Operator Forward Reverse In-place Description
+ __add__ __radd__ __iadd__ Addition or concatenation
- __sub__ __rsub__ __isub__ Subtraction
* __mul__ __rmul__ __imul__ Multiplication or repetition
/ __truediv__ __rtruediv__ __itruediv__ True division
// __floordiv__ __rfloordiv__ __ifloordiv__ Floor division
% __mod__ __rmod__ __imod__ Modulo
divmod() __divmod__ __rdivmod__ __idivmod__ Returns tuple of floor
division quotient and
modulo
**, pow() __pow__ __rpow__ __ipow__ Exponentiationa
@ __matmul__ __rmatmul__ __imatmul__ Matrix multiplicationb
& __and__ __rand__ __iand__ Bitwise and
| __or__ __ror__ __ior__ Bitwise or
382 | Chapter 13: Operator Overloading: Doing It Right
Operator Forward Reverse In-place Description
^ __xor__ __rxor__ __ixor__ Bitwise xor
<< __lshift__ __rlshift__ __ilshift__ Bitwise shift left
>> __rshift__ __rrshift__ __irshift__ Bitwise shift right
a pow takes an optional third argument, modulo: pow(a, b, modulo), also supported by the special methods when
invoked directly (e.g., a.__pow__(b, modulo)).
b New in Python 3.5.
The rich comparison operators are another category of infix operators, using a slightly
different set of rules. We cover them in the next main section: �Rich Comparison Operators�
on page 384.
The following optional sidebar is about the @ operator introduced in Python 3.5�not
yet released at the time of this writing.
The New @ Infix Operator in Python 3.5
Python 3.4 does not have an infix operator for the dot product. However, as I write this,
Python 3.5 pre-alpha already implements PEP 465 � A dedicated infix operator for
matrix multiplication, making the @ sign available for that purpose (e.g., a @ b is the
dot product of a and b). The @ operator is supported by the special methods __mat
mul__, __rmatmul__, and __imatmul__, named for �matrix multiplication.� These methods
are not used anywhere in the standard library at this time, but are recognized by the
interpreter in Python 3.5 so the NumPy team�and the rest of us�can support the @
operator in user-defined types. The parser was also changed to handle the infix @ (a @
b is a syntax error in Python 3.4).
Just for fun, after compiling Python 3.5 from source, I was able to implement and test
the @ operator for the Vector dot product.
These are the simple tests I did:
>>> va = Vector([1, 2, 3])
>>> vz = Vector([5, 6, 7])
>>> va @ vz == 38.0 # 1*5 + 2*6 + 3*7
True
>>> [10, 20, 30] @ vz
380.0
>>> va @ 3
Traceback (most recent call last):
...
TypeError: unsupported operand type(s) for @: 'Vector' and 'int'
And here is the code of the relevant special methods:
class Vector:
# many methods omitted in book listing
Overloading * for Scalar Multiplication | 383
def __matmul__(self, other):
try:
return sum(a * b for a, b in zip(self, other))
except TypeError:
return NotImplemented
def __rmatmul__(self, other):
return self @ other
The full source is in the vector_py3_5.py file in the Fluent Python code repository.
Remember to try it with Python 3.5, otherwise you�ll get a SyntaxError!
Rich Comparison Operators
The handling of the rich comparison operators ==, !=, >, <, >=, <= by the Python interpreter
is similar to what we just saw, but differs in two important aspects:
� The same set of methods are used in forward and reverse operator calls. The rules
are summarized in Table 13-2. For example, in the case of ==, both the forward and
reverse calls invoke __eq__, only swapping arguments; and a forward call to __gt__
is followed by a reverse call to __lt__ with the swapped arguments.
� In the case of == and !=, if the reverse call fails, Python compares the object IDs
instead of raising TypeError.
Table 13-2. Rich comparison operators: reverse methods invoked when the initial method
call returns NotImplemented
Group Infix operator Forward method call Reverse method call Fall back
Equality a == b a.__eq__(b) b.__eq__(a) Return id(a) == id(b)
a != b a.__ne__(b) b.__ne__(a) Return not (a == b)
Ordering a > b a.__gt__(b) b.__lt__(a) Raise TypeError
a < b a.__lt__(b) b.__gt__(a) Raise TypeError
a >= b a.__ge__(b) b.__le__(a) Raise TypeError
a <= b a.__le__(b) b.__ge__(a) Raise TypeError
384 | Chapter 13: Operator Overloading: Doing It Right
New Behavior in Python 3
The fallback step for all comparison operators changed from
Python 2. For __ne__, Python 3 now returns the negated result of
__eq__. For the ordering comparison operators, Python 3 raises
TypeError with a message like 'unorderable types: int() <
tuple()'. In Python 2, those comparisons produced weird results
taking into account object types and IDs in some arbitrary
way. However, it really makes no sense to compare an int to a
tuple, for example, so raising TypeError in such cases is a real
improvement in the language.
Given these rules, let�s review and improve the behavior of the Vector.__eq__ method,
which was coded as follows in vector_v5.py (Example 10-16):
class Vector:
# many lines omitted
def __eq__(self, other):
return (len(self) == len(other) and
all(a == b for a, b in zip(self, other)))
That method produces the results in Example 13-12.
Example 13-12. Comparing a Vector to a Vector, a Vector2d, and a tuple
>>> va = Vector([1.0, 2.0, 3.0])
>>> vb = Vector(range(1, 4))
>>> va == vb #
True
>>> vc = Vector([1, 2])
>>> from vector2d_v3 import Vector2d
>>> v2d = Vector2d(1, 2)
>>> vc == v2d #
True
>>> t3 = (1, 2, 3)
>>> va == t3 #
True
Two Vector instances with equal numeric components compare equal.
A Vector and a Vector2d are also equal if their components are equal.
A Vector is also considered equal to a tuple or any iterable with numeric items
of equal value.
The last one of the results in Example 13-12 is probably not desirable. I really have no
hard rule about this; it depends on the application context. But the Zen of Python says:
In the face of ambiguity, refuse the temptation to guess.
Rich Comparison Operators | 385
Excessive liberality in the evaluation of operands may lead to surprising results, and
programmers hate surprises.
Taking a clue from Python itself, we can see that [1,2] == (1, 2) is False. Therefore,
let�s be conservative and do some type checking. If the second operand is a Vector
instance (or an instance of a Vector subclass), then use the same logic as the current
__eq__. Otherwise, return NotImplemented and let Python handle that. See
Example 13-13.
Example 13-13. vector_v8.py: improved __eq__ in the Vector class
def __eq__(self, other):
if isinstance(other, Vector):
return (len(self) == len(other) and
all(a == b for a, b in zip(self, other)))
else:
return NotImplemented
If the other operand is an instance of Vector (or of a Vector subclass), perform
the comparison as before.
Otherwise, return NotImplemented.
If you run the tests in Example 13-12 with the new Vector.__eq__ from
Example 13-13, what you get now is shown in Example 13-14.
Example 13-14. Same comparisons as Example 13-12: last result changed
>>> va = Vector([1.0, 2.0, 3.0])
>>> vb = Vector(range(1, 4))
>>> va == vb #
True
>>> vc = Vector([1, 2])
>>> from vector2d_v3 import Vector2d
>>> v2d = Vector2d(1, 2)
>>> vc == v2d #
True
>>> t3 = (1, 2, 3)
>>> va == t3 #
False
Same result as before, as expected.
Same result as before, but why? Explanation coming up.
Different result; this is what we wanted. But why does it work? Read on�
Among the three results in Example 13-14, the first one is no news, but the last two were
caused by __eq__ returning NotImplemented in Example 13-13. Here is what happens
in the example with a Vector and a Vector2d, step by step:
386 | Chapter 13: Operator Overloading: Doing It Right
4. The logic for object.__eq__ and object.__ne__ is in function object_richcompare in Objects/typeobject.
c in the CPython source code.
1. To evaluate vc == v2d, Python calls Vector.__eq__(vc, v2d).
2. Vector.__eq__(vc, v2d) verifies that v2d is not a Vector and returns NotImple
mented.
3. Python gets NotImplemented result, so it tries Vector2d.__eq__(v2d, vc).
4. Vector2d.__eq__(v2d, vc) turns both operands into tuples an compares them:
the result is True (the code for Vector2d.__eq__ is in Example 9-9).
As for the comparison between Vector and tuple in Example 13-14, the actual steps
are:
1. To evaluate va == t3, Python calls Vector.__eq__(va, t3).
2. Vector.__eq__(va, t3) verifies that t3 is not a Vector and returns NotImplemen
ted.
3. Python gets NotImplemented result, so it tries tuple.__eq__(t3, va).
4. tuple.__eq__(t3, va) has no idea what a Vector is, so it returns NotImplemented.
5. In the special case of ==, if the reversed call returns NotImplemented, Python compares
object IDs as a last resort.
How about !=? We don�t need to implement it because the fallback behavior of the
__ne__ inherited from object suits us: when __eq__ is defined and does not return
NotImplemented, __ne__ returns that result negated.
In other words, given the same objects we used in Example 13-14, the results for != are
consistent:
>>> va != vb
False
>>> vc != v2d
False
>>> va != (1, 2, 3)
True
The __ne__ inherited from object works like the following code�except that the original
is written in C:4
def __ne__(self, other):
eq_result = self == other
if eq_result is NotImplemented:
return NotImplemented
Rich Comparison Operators | 387
else:
return not eq_result
Python 3 Documentation Bug
As I write this, the rich comparison method documentation states:
�The truth of x==y does not imply that x!=y is false. Accordingly,
when defining __eq__(), one should also define __ne__() so that
the operators will behave as expected.� That was true for Python
2, but in Python 3 that�s not good advice, because a useful default
__ne__ implementation is inherited from the object class, and it�s
rarely necessary to override it. The new behavior is documented
in Guido�s What�s New in Python 3.0, in the section �Operators
And Special Methods.� The documentation bug is recorded as
issue 4395.
After covering the essentials of infix operator overloading, let�s turn to a different class
of operators: the augmented assignment operators.
Augmented Assignment Operators
Our Vector class already supports the augmented assignment operators += and *=.
Example 13-15 shows them in action.
Example 13-15. Augmented assignment works with immutable targets by creating new
instances and rebinding
>>> v1 = Vector([1, 2, 3])
>>> v1_alias = v1 #
>>> id(v1) #
4302860128
>>> v1 += Vector([4, 5, 6]) #
>>> v1 #
Vector([5.0, 7.0, 9.0])
>>> id(v1) #
4302859904
>>> v1_alias #
Vector([1.0, 2.0, 3.0])
>>> v1 *= 11 #
>>> v1 #
Vector([55.0, 77.0, 99.0])
>>> id(v1)
4302858336
Create alias so we can inspect the Vector([1, 2, 3]) object later.
Remember the ID of the initial Vector bound to v1.
Perform augmented addition.
388 | Chapter 13: Operator Overloading: Doing It Right
The expected result�
�but a new Vector was created.
Inspect v1_alias to confirm the original Vector was not altered.
Perform augmented multiplication.
Again, the expected result, but a new Vector was created.
If a class does not implement the in-place operators listed in Table 13-1, the augmented
assignment operators are just syntactic sugar: a += b is evaluated exactly as a = a +
b. That�s the expected behavior for immutable types, and if you have __add__ then +=
will work with no additional code.
However, if you do implement an in-place operator method such as __iadd__, that
method is called to compute the result of a += b. As the name says, those operators are
expected to change the lefthand operand in place, and not create a new object as the
result.
The in-place special methods should never be implemented for
immutable types like our Vector class. This is fairly obvious, but
worth stating anyway.
To show the code of an in-place operator, we will extend the BingoCage class from
Example 11-12 to implement __add__ and __iadd__.
We�ll call the subclass AddableBingoCage. Example 13-16 is the behavior we want for
the + operator.
Example 13-16. A new AddableBingoCage instance can be created with
>>> vowels = 'AEIOU'
>>> globe = AddableBingoCage(vowels)
>>> globe.inspect()
('A', 'E', 'I', 'O', 'U')
>>> globe.pick() in vowels
True
>>> len(globe.inspect())
4
>>> globe2 = AddableBingoCage('XYZ')
>>> globe3 = globe + globe2
>>> len(globe3.inspect())
7
>>> void = globe + [10, 20]
Traceback (most recent call last):
...
TypeError: unsupported operand type(s) for +: 'AddableBingoCage' and 'list'
Augmented Assignment Operators | 389
Create a globe instance with five items (each of the vowels).
Pop one of the items, and verify it is one the vowels.
Confirm that the globe is down to four items.
Create a second instance, with three items.
Create a third instance by adding the previous two. This instance has seven items.
Attempting to add an AddableBingoCage to a list fails with TypeError. That
error message is produced by the Python interpreter when our __add__ method
returns NotImplemented.
Because an AddableBingoCage is mutable, Example 13-17 shows how it will work when
we implement __iadd__.
Example 13-17. An existing AddableBingoCage can be loaded with += (continuing
from Example 13-16)
>>> globe_orig = globe
>>> len(globe.inspect())
4
>>> globe += globe2
>>> len(globe.inspect())
7
>>> globe += ['M', 'N']
>>> len(globe.inspect())
9
>>> globe is globe_orig
True
>>> globe += 1
Traceback (most recent call last):
...
TypeError: right operand in += must be 'AddableBingoCage' or an iterable
Create an alias so we can check the identity of the object later.
globe has four items here.
An AddableBingoCage instance can receive items from another instance of the
same class.
The righthand operand of += can also be any iterable.
Throughout this example, globe has always referred to the globe_orig object.
Trying to add a noniterable to an AddableBingoCage fails with a proper error
message.
Note that the += operator is more liberal than + with regard to the second operand. With
+, we want both operands to be of the same type (AddableBingoCage, in this case),
because if we accepted different types this might cause confusion as to the type of the
390 | Chapter 13: Operator Overloading: Doing It Right
result. With the +=, the situation is clearer: the lefthand object is updated in place, so
there�s no doubt about the type of the result.
I validated the contrasting behavior of + and += by observing how
the list built-in type works. Writing my_list + x, you can only
concatenate one list to another list, but if you write my_list +=
x, you can extend the lefthand list with items from any iterable
x on the righthand side. This is consistent with how the list.ex
tend() method works: it accepts any iterable argument.
Now that we are clear on the desired behavior for AddableBingoCage, we can look at its
implementation in Example 13-18.
Example 13-18. bingoaddable.py: AddableBingoCage extends BingoCage to support +
and +=
import itertools
from tombola import Tombola
from bingo import BingoCage
class AddableBingoCage(BingoCage):
def __add__(self, other):
if isinstance(other, Tombola):
return AddableBingoCage(self.inspect() + other.inspect())
else:
return NotImplemented
def __iadd__(self, other):
if isinstance(other, Tombola):
other_iterable = other.inspect()
else:
try:
other_iterable = iter(other)
except TypeError:
self_cls = type(self).__name__
msg = "right operand in += must be {!r} or an iterable"
raise TypeError(msg.format(self_cls))
self.load(other_iterable)
return self
PEP 8 � Style Guide for Python Code recommends coding imports from the
standard library above imports of your own modules.
AddableBingoCage extends BingoCage.
Our __add__ will only work with an instance of Tombola as the second operand.
Augmented Assignment Operators | 391
5. The iter built-in function will be covered in the next chapter. Here I could have used tuple(other), and
it would work, but at the cost of building a new tuple when all the .load(�) method needs is to iterate
over its argument.
Retrieve items from other, if it is an instance of Tombola.
Otherwise, try to obtain an iterator over other.5
If that fails, raise an exception explaining what the user should do. When
possible, error messages should explicitly guide the user to the solution.
If we got this far, we can load the other_iterable into self.
Very important: augmented assignment special methods must return self.
We can summarize the whole idea of in-place operators by contrasting the return
statements that produce results in __add__ and __iadd__ in Example 13-18:
__add__
The result is produced by calling the constructor AddableBingoCage to build a new
instance.
__iadd__
The result is produced by returning self, after it has been modified.
To wrap up this example, a final observation on Example 13-18: by design, no __radd__
was coded in AddableBingoCage, because there is no need for it. The forward method
__add__ will only deal with righthand operands of the same type, so if Python is trying
to compute a + b where a is an AddableBingoCage and b is not, we return NotImple
mented�maybe the class of b can make it work. But if the expression is b + a and b is
not an AddableBingoCage, and it returns NotImplemented, then it�s better to let Python
give up and raise TypeError because we cannot handle b.
In general, if a forward infix operator method (e.g., __mul__) is
designed to work only with operands of the same type as self, it�s
useless to implement the corresponding reverse method (e.g.,
__rmul__) because that, by definition, will only be invoked when
dealing with an operand of a different type.
This concludes our exploration of operator overloading in Python.
Chapter Summary
We started this chapter by reviewing some restrictions Python imposes on operator
overloading: no overloading of operators in built-in types, and overloading limited to
existing operators, except for a few ones (is, and, or, not).
392 | Chapter 13: Operator Overloading: Doing It Right
We got down to business with the unary operators, implementing __neg__ and
__pos__. Next came the infix operators, starting with +, supported by the __add__
method. We saw that unary and infix operators are supposed to produce results by
creating new objects, and should never change their operands. To support operations
with other types, we return the NotImplemented special value�not an exception�allowing
the interpreter to try again by swapping the operands and calling the reverse
special method for that operator (e.g., __radd__). The algorithm Python uses to handle
infix operators is summarized in the flowchart in Figure 13-1.
Mixing operand types means we need to detect when we get an operand we can�t handle.
In this chapter, we did this in two ways: in the duck typing way, we just went ahead and
tried the operation, catching a TypeError exception if it happened; later, in __mul__, we
did it with an explicit isinstance test. There are pros and cons to these approaches:
duck typing is more flexible, but explicit type checking is more predictable. When we
did use isinstance, we were careful to avoid testing with a concrete class, but used the
numbers.Real ABC: isinstance(scalar, numbers.Real). This is a good compromise
between flexibility and safety, because existing or future user-defined types can be declared
as actual or virtual subclasses of an ABC, as we saw in Chapter 11.
The next topic we covered was the rich comparison operators. We implemented == with
__eq__ and discovered that Python provides a handy implementation of != in the
__ne__ inherited from the object base class. The way Python evaluates these operators
along with >, <, >=, and <= is slightly different, with a different logic for choosing the
reverse method, and special fallback handling for == and !=, which never generate errors
because Python compares the object IDs as a last resort.
In the last section, we focused on augmented assignment operators. We saw that Python
handles them by default as a combination of plain operator followed by assignment,
that is: a += b is evaluated exactly as a = a + b. That always creates a new object, so it
works for mutable or immutable types. For mutable objects, we can implement in-place
special methods such as __iadd__ for +=, and alter the value of the lefthand operand.
To show this at work, we left behind the immutable Vector class and worked on implementing
a BingoCage subclass to support += for adding items to the random pool,
similar to the way the list built-in supports += as a shortcut for the list.extend()
method. While doing this, we discussed how + tends to be stricter than += regarding the
types it accepts. For sequence types, + usually requires that both operands are of the
same type, while += often accepts any iterable as the righthand operand.
Further Reading
Operator overloading is one area of Python programming where isinstance tests are
common. In general, libraries should leverage dynamic typing�to be more flexible�
by avoiding explicit type tests and just trying operations and then handling the excep?
Further Reading | 393
tions, opening the door for working with objects regardless of their types, as long as
they support the necessary operations. But Python ABCs allow a stricter form of duck
typing, dubbed �goose typing� by Alex Martelli, which is often useful when writing code
that overloads operators. So, if you skipped Chapter 11, make sure to read it.
The main reference for the operator special methods is the �Data Model� chapter. It�s
the canonical source, but at this time it�s plagued by that glaring bug mentioned in
Python 3 Documentation Bug, advising �when defining __eq__(), one should also define
__ne__().� In reality, the __ne__ inherited from the object class in Python 3 covers
the vast majority of needs, so implementing __ne__ is rarely necessary in practice. Another
relevant reading in the Python documentation is �9.1.2.2. Implementing the
arithmetic operations� in the numbers module of The Python Standard Library.
A related technique is generic functions, supported by the @singledispatch decorator
in Python 3 (�Generic Functions with Single Dispatch� on page 202). In Python Cookbook,
3E (O�Reilly), by David Beazley and Brian K. Jones, �Recipe 9.20. Implementing
Multiple Dispatch with Function Annotations� uses some advanced metaprogramming
�involving a metaclass�to implement type-based dispatching with function annotations.
The second edition of the Python Cookbook by Martelli, Ravenscroft, and Ascher
has an interesting recipe (2.13, by Erik Max Francis) showing how to overload the <<
operator to emulate the C++ iostream syntax in Python. Both books have other examples
with operator overloading, I just picked two notable recipes.
The functools.total_ordering function is a class decorator (supported in Python 2.7
and later) that automatically generates methods for all rich comparison operators in any
class that defines at least a couple of them. See the functools module docs.
If you are curious about operator method dispatching in languages with dynamic typing,
two seminal readings are �A Simple Technique for Handling Multiple Polymorphism�
by Dan Ingalls (member of the original Smalltalk team) and �Arithmetic and Double
Dispatching in Smalltalk-80� by Kurt J. Hebel and Ralph Johnson (Johnson became
famous as one of the authors of the original Design Patterns book). Both papers provide
deep insight into the power of polymorphism in languages with dynamic typing, like
Smalltalk, Python, and Ruby. Python does not use double dispatching for handling
operators as described in those articles. The Python algorithm using forward and reverse
operators is easier for user-defined classes to support than double dispatching, but requires
special handling by the interpreter. In contrast, classic double dispatching is a
general technique you can use in Python or any OO language beyond the specific context
of infix operators, and in fact Ingalls, Hebel, and Johnson use very different examples
to describe it.
The article �The C Family of Languages: Interview with Dennis Ritchie, Bjarne Stroustrup,
and James Gosling� from which I quoted the epigraph in this chapter, and two other
snippets in �Soapbox� on page 395, appeared in Java Report, 5(7), July 2000 and C++
394 | Chapter 13: Operator Overloading: Doing It Right
Report, 12(7), July/August 2000. It�s an awesome reading if you are into programming
language design.
Soapbox
Operator Overloading: Pros and Cons
James Gosling, quoted at the start of this chapter, made the conscious decision to leave
operator overloading out when he designed Java. In that same interview (�The C Family
of Languages: Interview with Dennis Ritchie, Bjarne Stroustrup, and James Gosling�)
he says:
Probably about 20 to 30 percent of the population think of operator overloading as the
spawn of the devil; somebody has done something with operator overloading that has
just really ticked them off, because they�ve used like + for list insertion and it makes life
really, really confusing. A lot of that problem stems from the fact that there are only
about half a dozen operators you can sensibly overload, and yet there are thousands or
millions of operators that people would like to define�so you have to pick, and often
the choices conflict with your sense of intuition.
Guido van Rossum picked the middle way in supporting operator overloading: he did
not leave the door open for users creating new arbitrary operators like <=> or :-), which
prevents a Tower of Babel of custom operators, and allows the Python parser to be
simple. Python also does not let you overload the operators of the built-in types, another
limitation that promotes readability and predictable performance.
Gosling goes on to say:
Then there�s a community of about 10 percent that have actually used operator overloading
appropriately and who really care about it, and for whom it�s actually really
important; this is almost exclusively people who do numerical work, where the notation
is very important to appealing to people�s intuition, because they come into it with an
intuition about what the + means, and the ability to say �a + b� where a and b are complex
numbers or matrices or something really does make sense.
The notation side of the issue cannot be underestimated. Here is an illustrative example
from the realm of finances. In Python, you can compute compound interest using a
formula written like this:
interest = principal * ((1 + rate) ** periods - 1)
That same notation works regardless of the numeric types involved. Thus, if you are
doing serious financial work, you can make sure that periods is an int, while rate,
interest, and principal are exact numbers�instances of the Python decimal.Deci
mal class � and that formula will work exactly as written.
But in Java, if you switch from float to BigDecimal to get arbitrary precision, you can�t
use infix operators anymore, because they only work with the primitive types. This is
the same formula coded to work with BigDecimal numbers in Java:
Further Reading | 395
6. My friend Mario Domenech Goulart, a core developer of the CHICKEN Scheme compiler, will probably
disagree with this.
BigDecimal interest = principal.multiply(BigDecimal.ONE.add(rate)
.pow(periods).subtract(BigDecimal.ONE));
It�s clear that infix operators make formulas more readable, at least for most of us.6 And
operator overloading is necessary to support nonprimitive types with infix operator
notation. Having operator overloading in a high-level, easy-to-use language was probably
a key reason for the amazing penetration of Python in scientific computing in recent
years.
Of course, there are benefits to disallowing operator overloading in a language. It is
arguably a sound decision for lower-level systems languages where performance and
safety are paramount. The much newer Go language followed the lead of Java in this
regard and does not support operator overloading.
But overloaded operators, when used sensibly, do make code easier to read and write.
It�s a great feature to have in a modern high-level language.
A Glimpse at Lazy Evaluation
If you look closely at the traceback in Example 13-9, you�ll see evidence of the lazy
evaluation of generator expressions. Example 13-19 is that same traceback, now with
callouts.
Example 13-19. Same as Example 13-9
>>> v1 + 'ABC'
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "vector_v6.py", line 329, in __add__
return Vector(a + b for a, b in pairs) #
File "vector_v6.py", line 243, in __init__
self._components = array(self.typecode, components) #
File "vector_v6.py", line 329, in <genexpr>
return Vector(a + b for a, b in pairs) #
TypeError: unsupported operand type(s) for +: 'float' and 'str'
The Vector call gets a generator expression as its components argument. No
problem at this stage.
The components genexp is passed to the array constructor. Within the array
constructor, Python tries to iterate over the genexp, causing the evaluation of
the first item a + b. That�s when the TypeError occurs.
The exception propagates to the Vector constructor call, where it is reported.
This shows how the generator expression is evaluated at the latest possible moment, and
not where it is defined in the source code.
396 | Chapter 13: Operator Overloading: Doing It Right
In contrast, if the Vector constructor was invoked as Vector([a + b for a, b in
pairs]), then the exception would happen right there, because the list comprehension
tried to build a list to be passed as the argument to the Vector() call. The body of
Vector.__init__ would not be reached at all.
Chapter 14 will cover generator expressions in detail, but I did not want to let this
accidental demonstration of their lazy nature go unnoticed.
Further Reading | 397

PART V
Control Flow

1. From �Revenge of the Nerds�, a blog post.
2. Python 2.2 users could use yield with the directive from __future__ import generators; yield became
available by default in Python 2.3.
CHAPTER 14
Iterables, Iterators, and Generators
When I see patterns in my programs, I consider it a sign of trouble. The shape of a program
should reflect only the problem it needs to solve. Any other regularity in the code is a
sign, to me at least, that I�m using abstractions that aren�t powerful enough�often that
I�m generating by hand the expansions of some macro that I need to write.1
� Paul Graham
Lisp hacker and venture capitalist
Iteration is fundamental to data processing. And when scanning datasets that don�t fit
in memory, we need a way to fetch the items lazily, that is, one at a time and on demand.
This is what the Iterator pattern is about. This chapter shows how the Iterator pattern
is built into the Python language so you never need to implement it by hand.
Python does not have macros like Lisp (Paul Graham�s favorite language), so abstracting
away the Iterator pattern required changing the language: the yield keyword was added
in Python 2.2 (2001).2 The yield keyword allows the construction of generators, which
work as iterators.
Every generator is an iterator: generators fully implement the
iterator interface. But an iterator�as defined in the GoF book�
retrieves items from a collection, while a generator can produce
items �out of thin air.� That�s why the Fibonacci sequence generator
is a common example: an infinite series of numbers cannot
be stored in a collection. However, be aware that the Python
community treats iterator and generator as synonyms most of the
time.
401
Python 3 uses generators in many places. Even the range() built-in now returns a
generator-like object instead of full-blown lists like before. If you must build a list
from range, you have to be explicit (e.g., list(range(100))).
Every collection in Python is iterable, and iterators are used internally to support:
� for loops
� Collection types construction and extension
� Looping over text files line by line
� List, dict, and set comprehensions
� Tuple unpacking
� Unpacking actual parameters with * in function calls
This chapter covers the following topics:
� How the iter(�) built-in function is used internally to handle iterable objects
� How to implement the classic Iterator pattern in Python
� How a generator function works in detail, with line-by-line descriptions
� How the classic Iterator can be replaced by a generator function or generator expression
� Leveraging the general-purpose generator functions in the standard library
� Using the new yield from statement to combine generators
� A case study: using generator functions in a database conversion utility designed
to work with large datasets
� Why generators and coroutines look alike but are actually very different and should
not be mixed
We�ll get started studying how the iter(�) function makes sequences iterable.
Sentence Take #1: A Sequence of Words
We�ll start our exploration of iterables by implementing a Sentence class: you give its
constructor a string with some text, and then you can iterate word by word. The first
version will implement the sequence protocol, and it�s iterable because all sequences are
iterable, as we�ve seen before, but now we�ll see exactly why.
Example 14-1 shows a Sentence class that extracts words from a text by index.
402 | Chapter 14: Iterables, Iterators, and Generators
3. We first used reprlib in �Vector Take #1: Vector2d Compatible� on page 276.
Example 14-1. sentence.py: A Sentence as a sequence of words
import re
import reprlib
RE_WORD = re.compile('\w+')
class Sentence:
def __init__(self, text):
self.text = text
self.words = RE_WORD.findall(text)
def __getitem__(self, index):
return self.words[index]
def __len__(self):
return len(self.words)
def __repr__(self):
return 'Sentence(%s)' % reprlib.repr(self.text)
re.findall returns a list with all nonoverlapping matches of the regular
expression, as a list of strings.
self.words holds the result of .findall, so we simply return the word at the
given index.
To complete the sequence protocol, we implement __len__�but it is not needed
to make an iterable object.
reprlib.repr is a utility function to generate abbreviated string representations
of data structures that can be very large.3
By default, reprlib.repr limits the generated string to 30 characters. See the console
session in Example 14-2 to see how Sentence is used.
Example 14-2. Testing iteration on a Sentence instance
>>> s = Sentence('"The time has come," the Walrus said,') #
>>> s
Sentence('"The time ha... Walrus said,') #
>>> for word in s: #
... print(word)
The
time
has
come
Sentence Take #1: A Sequence of Words | 403
the
Walrus
said
>>> list(s) #
['The', 'time', 'has', 'come', 'the', 'Walrus', 'said']
A sentence is created from a string.
Note the output of __repr__ using ... generated by reprlib.repr.
Sentence instances are iterable; we�ll see why in a moment.
Being iterable, Sentence objects can be used as input to build lists and other
iterable types.
In the following pages, we�ll develop other Sentence classes that pass the tests in
Example 14-2. However, the implementation in Example 14-1 is different from all the
others because it�s also a sequence, so you can get words by index:
>>> s[0]
'The'
>>> s[5]
'Walrus'
>>> s[-1]
'said'
Every Python programmer knows that sequences are iterable. Now we�ll see precisely
why.
Why Sequences Are Iterable: The iter Function
Whenever the interpreter needs to iterate over an object x, it automatically calls iter(x).
The iter built-in function:
1. Checks whether the object implements __iter__, and calls that to obtain an iterator.
2. If __iter__ is not implemented, but __getitem__ is implemented, Python creates
an iterator that attempts to fetch items in order, starting from index 0 (zero).
3. If that fails, Python raises TypeError, usually saying �C object is not iterable,� where
C is the class of the target object.
That is why any Python sequence is iterable: they all implement __getitem__. In fact,
the standard sequences also implement __iter__, and yours should too, because the
special handling of __getitem__ exists for backward compatibility reasons and may be
gone in the future (although it is not deprecated as I write this).
As mentioned in �Python Digs Sequences� on page 310, this is an extreme form of duck
typing: an object is considered iterable not only when it implements the special method
404 | Chapter 14: Iterables, Iterators, and Generators
__iter__, but also when it implements __getitem__, as long as __getitem__ accepts
int keys starting from 0.
In the goose-typing approach, the definition for an iterable is simpler but not as flexible:
an object is considered iterable if it implements the __iter__ method. No subclassing
or registration is required, because abc.Iterable implements the __subclasshook__,
as seen in �Geese Can Behave as Ducks� on page 338. Here is a demonstration:
>>> class Foo:
... def __iter__(self):
... pass
...
>>> from collections import abc
>>> issubclass(Foo, abc.Iterable)
True
>>> f = Foo()
>>> isinstance(f, abc.Iterable)
True
However, note that our initial Sentence class does not pass the issubclass(Sentence,
abc.Iterable) test, even though it is iterable in practice.
As of Python 3.4, the most accurate way to check whether an object
x is iterable is to call iter(x) and handle a TypeError exception
if it isn�t. This is more accurate than using isinstance(x,
abc.Iterable), because iter(x) also considers the legacy
__getitem__ method, while the Iterable ABC does not.
Explicitly checking whether an object is iterable may not be worthwhile if right after the
check you are going to iterate over the object. After all, when the iteration is attempted
on a noniterable, the exception Python raises is clear enough: TypeError: 'C' object
is not iterable . If you can do better than just raising TypeError, then do so in a
try/except block instead of doing an explicit check. The explicit check may make sense
if you are holding on to the object to iterate over it later; in this case, catching the error
early may be useful.
The next section makes explicit the relationship between iterables and iterators.
Iterables Versus Iterators
From the explanation in �Why Sequences Are Iterable: The iter Function� on page 404
we can extrapolate a definition:
iterable
Any object from which the iter built-in function can obtain an iterator. Objects
implementing an __iter__ method returning an iterator are iterable. Sequences
Iterables Versus Iterators | 405
are always iterable; as are objects implementing a __getitem__ method that takes
0-based indexes.
It�s important to be clear about the relationship between iterables and iterators: Python
obtains iterators from iterables.
Here is a simple for loop iterating over a str. The str 'ABC' is the iterable here. You
don�t see it, but there is an iterator behind the curtain:
>>> s = 'ABC'
>>> for char in s:
... print(char)
...
A
B
C
If there was no for statement and we had to emulate the for machinery by hand with
a while loop, this is what we�d have to write:
>>> s = 'ABC'
>>> it = iter(s) #
>>> while True:
... try:
... print(next(it)) #
... except StopIteration: #
... del it #
... break #
...
A
B
C
Build an iterator it from the iterable.
Repeatedly call next on the iterator to obtain the next item.
The iterator raises StopIteration when there are no further items.
Release reference to it�the iterator object is discarded.
Exit the loop.
StopIteration signals that the iterator is exhausted. This exception is handled internally
in for loops and other iteration contexts like list comprehensions, tuple unpacking,
etc.
The standard interface for an iterator has two methods:
__next__
Returns the next available item, raising StopIteration when there are no more
items.
406 | Chapter 14: Iterables, Iterators, and Generators
__iter__
Returns self; this allows iterators to be used where an iterable is expected, for
example, in a for loop.
This is formalized in the collections.abc.Iterator ABC, which defines the __next__
abstract method, and subclasses Iterable�where the abstract __iter__ method is
defined. See Figure 14-1.
Figure 14-1. The Iterable and Iterator ABCs. Methods in italic are abstract. A concrete
Iterable.iter should return a new Iterator instance. A concrete Iterator must implement
next. The Iterator.iter method just returns the instance itself.
The Iterator ABC implements __iter__ by doing return self. This allows an iterator
to be used wherever an iterable is required. The source code for abc.Iterator is in
Example 14-3.
Example 14-3. abc.Iterator class; extracted from Lib/_collections_abc.py
class Iterator(Iterable):
__slots__ = ()
@abstractmethod
def __next__(self):
'Return the next item from the iterator. When exhausted, raise StopIteration'
raise StopIteration
def __iter__(self):
return self
@classmethod
def __subclasshook__(cls, C):
if cls is Iterator:
if (any("__next__" in B.__dict__ for B in C.__mro__) and
any("__iter__" in B.__dict__ for B in C.__mro__)):
Iterables Versus Iterators | 407
return True
return NotImplemented
The Iterator ABC abstract method is it.__next__() in Python
3 and it.next() in Python 2. As usual, you should avoid calling
special methods directly. Just use the next(it): this built-in function
does the right thing in Python 2 and 3.
The Lib/types.py module source code in Python 3.4 has a comment that says:
# Iterators in Python aren't a matter of type but of protocol. A large
# and changing number of builtin types implement *some* flavor of
# iterator. Don't check the type! Use hasattr to check for both
# "__iter__" and "__next__" attributes instead.
In fact, that�s exactly what the __subclasshook__ method of the abc.Iterator ABC
does (see Example 14-3).
Taking into account the advice from Lib/types.py and the logic
implemented in Lib/_collections_abc.py, the best way to check if an
object x is an iterator is to call isinstance(x, abc.Iterator).
Thanks to Iterator.__subclasshook__, this test works even if the
class of x is not a real or virtual subclass of Iterator.
Back to our Sentence class from Example 14-1, you can clearly see how the iterator is
built by iter(�) and consumed by next(�) using the Python console:
>>> s3 = Sentence('Pig and Pepper') #
>>> it = iter(s3) #
>>> it # doctest: +ELLIPSIS
<iterator object at 0x...>
>>> next(it) #
'Pig'
>>> next(it)
'and'
>>> next(it)
'Pepper'
>>> next(it) #
Traceback (most recent call last):
...
StopIteration
>>> list(it) #
[]
>>> list(iter(s3)) #
['Pig', 'and', 'Pepper']
Create a sentence s3 with three words.
408 | Chapter 14: Iterables, Iterators, and Generators
Obtain an iterator from s3.
next(it) fetches the next word.
There are no more words, so the iterator raises a StopIteration exception.
Once exhausted, an iterator becomes useless.
To go over the sentence again, a new iterator must be built.
Because the only methods required of an iterator are __next__ and __iter__, there is
no way to check whether there are remaining items, other than to call next() and catch
StopInteration. Also, it�s not possible to �reset� an iterator. If you need to start over,
you need to call iter(�) on the iterable that built the iterator in the first place. Calling
iter(�) on the iterator itself won�t help, because�as mentioned�Itera
tor.__iter__ is implemented by returning self, so this will not reset a depleted iterator.
To wrap up this section, here is a definition for iterator:
iterator
Any object that implements the __next__ no-argument method that returns the
next item in a series or raises StopIteration when there are no more items. Python
iterators also implement the __iter__ method so they are iterable as well.
This first version of Sentence was iterable thanks to the special treatment the iter(�)
built-in gives to sequences. Now we�ll implement the standard iterable protocol.
Sentence Take #2: A Classic Iterator
The next Sentence class is built according to the classic Iterator design pattern following
the blueprint in the GoF book. Note that this is not idiomatic Python, as the next refactorings
will make very clear. But it serves to make explicit the relationship between
the iterable collection and the iterator object.
Example 14-4 shows an implementation of a Sentence that is iterable because it implements
the __iter__ special method, which builds and returns a SentenceIterator.
This is how the Iterator design pattern is described in the original Design Patterns book.
We are doing it this way here just to make clear the crucial distinction between an iterable
and an iterator and how they are connected.
Example 14-4. sentence_iter.py: Sentence implemented using the Iterator pattern
import re
import reprlib
RE_WORD = re.compile('\w+')
Sentence Take #2: A Classic Iterator | 409
class Sentence:
def __init__(self, text):
self.text = text
self.words = RE_WORD.findall(text)
def __repr__(self):
return 'Sentence(%s)' % reprlib.repr(self.text)
def __iter__(self):
return SentenceIterator(self.words)
class SentenceIterator:
def __init__(self, words):
self.words = words
self.index = 0
def __next__(self):
try:
word = self.words[self.index]
except IndexError:
raise StopIteration()
self.index += 1
return word
def __iter__(self):
return self
The __iter__ method is the only addition to the previous Sentence
implementation. This version has no __getitem__, to make it clear that the class
is iterable because it implements __iter__.
__iter__ fulfills the iterable protocol by instantiating and returning an iterator.
SentenceIterator holds a reference to the list of words.
self.index is used to determine the next word to fetch.
Get the word at self.index.
If there is no word at self.index, raise StopIteration.
Increment self.index.
Return the word.
Implement self.__iter__.
The code in Example 14-4 passes the tests in Example 14-2.
410 | Chapter 14: Iterables, Iterators, and Generators
4. Gamma et. al., Design Patterns: Elements of Reusable Object-Oriented Software, p. 259.
Note that implementing __iter__ in SentenceIterator is not actually needed for this
example to work, but the it�s the right thing to do: iterators are supposed to implement
both __next__ and __iter__, and doing so makes our iterator pass the issubclass(Sen
tenceInterator, abc.Iterator) test. If we had subclassed SentenceIterator from
abc.Iterator, we�d inherit the concrete abc.Iterator.__iter__ method.
That is a lot of work (for us lazy Python programmers, anyway). Note how most code
in SentenceIterator deals with managing the internal state of the iterator. Soon we�ll
see how to make it shorter. But first, a brief detour to address an implementation shortcut
that may be tempting, but is just wrong.
Making Sentence an Iterator: Bad Idea
A common cause of errors in building iterables and iterators is to confuse the two. To
be clear: iterables have an __iter__ method that instantiates a new iterator every time.
Iterators implement a __next__ method that returns individual items, and an __iter__
method that returns self.
Therefore, iterators are also iterable, but iterables are not iterators.
It may be tempting to implement __next__ in addition to __iter__ in the Sentence
class, making each Sentence instance at the same time an iterable and iterator over
itself. But this is a terrible idea. It�s also a common anti-pattern, according to Alex Martelli
who has a lot of experience with Python code reviews.
The �Applicability� section4 of the Iterator design pattern in the GoF book says:
Use the Iterator pattern
� to access an aggregate object�s contents without exposing its internal representation.
� to support multiple traversals of aggregate objects.
� to provide a uniform interface for traversing different aggregate structures (that is,
to support polymorphic iteration).
To �support multiple traversals� it must be possible to obtain multiple independent
iterators from the same iterable instance, and each iterator must keep its own internal
state, so a proper implementation of the pattern requires each call to iter(my_itera
ble) to create a new, independent, iterator. That is why we need the SentenceItera
tor class in this example.
Sentence Take #2: A Classic Iterator | 411
An iterable should never act as an iterator over itself. In other
words, iterables must implement __iter__, but not __next__.
On the other hand, for convenience, iterators should be iterable.
An iterator�s __iter__ should just return self.
Now that the classic Iterator pattern is properly demonstrated, we can get let it go. The
next section presents a more idiomatic implementation of Sentence.
Sentence Take #3: A Generator Function
A Pythonic implementation of the same functionality uses a generator function to replace
the SequenceIterator class. A proper explanation of the generator function
comes right after Example 14-5.
Example 14-5. sentence_gen.py: Sentence implemented using a generator function
import re
import reprlib
RE_WORD = re.compile('\w+')
class Sentence:
def __init__(self, text):
self.text = text
self.words = RE_WORD.findall(text)
def __repr__(self):
return 'Sentence(%s)' % reprlib.repr(self.text)
def __iter__(self):
for word in self.words:
yield word
return
# done!
Iterate over self.word.
Yield the current word.
412 | Chapter 14: Iterables, Iterators, and Generators
5. When reviewing this code, Alex Martelli suggested the body of this method could simply be return
iter(self.words). He is correct, of course: the result of calling __iter__ would also be an iterator, as it
should be. However, I used a for loop with yield here to introduce the syntax of a generator function,
which will be covered in detail in the next section.
6. Sometimes I add a gen prefix or suffix when naming generator functions, but this is not a common practice.
And you can�t do that if you�re implementing an iterable, of course: the necessary special method must
be named __iter__.
7. Thanks to David Kwast for suggesting this example.
This return is not needed; the function can just �fall-through� and return
automatically. Either way, a generator function doesn�t raise StopIteration: it
simply exits when it�s done producing values.5
No need for a separate iterator class!
Here again we have a different implementation of Sentence that passes the tests in
Example 14-2.
Back in the Sentence code in Example 14-4, __iter__ called the SentenceIterator
constructor to build an iterator and return it. Now the iterator in Example 14-5 is in
fact a generator object, built automatically when the __iter__ method is called, because
__iter__ here is a generator function.
A full explanation of generator functions follows.
How a Generator Function Works
Any Python function that has the yield keyword in its body is a generator function: a
function which, when called, returns a generator object. In other words, a generator
function is a generator factory.
The only syntax distinguishing a plain function from a generator
function is the fact that the latter has a yield keyword somewhere
in its body. Some argued that a new keyword like gen should
be used for generator functions instead of def, but Guido did not
agree. His arguments are in PEP 255 � Simple Generators.6
Here is the simplest function useful to demonstrate the behavior of a generator:7
>>> def gen_123(): #
... yield 1 #
... yield 2
... yield 3
...
>>> gen_123 # doctest: +ELLIPSIS
<function gen_123 at 0x...> #
Sentence Take #3: A Generator Function | 413
>>> gen_123() # doctest: +ELLIPSIS
<generator object gen_123 at 0x...> #
>>> for i in gen_123(): #
... print(i)
1
2
3
>>> g = gen_123() #
>>> next(g) #
1
>>> next(g)
2
>>> next(g)
3
>>> next(g) #
Traceback (most recent call last):
...
StopIteration
Any Python function that contains the yield keyword is a generator function.
Usually the body of a generator function has loop, but not necessarily; here I
just repeat yield three times.
Looking closely, we see gen_123 is a function object.
But when invoked, gen_123() returns a generator object.
Generators are iterators that produce the values of the expressions passed to
yield.
For closer inspection, we assign the generator object to g.
Because g is an iterator, calling next(g) fetches the next item produced by yield.
When the body of the function completes, the generator object raises a StopIt
eration.
A generator function builds a generator object that wraps the body of the function.
When we invoke next(�) on the generator object, execution advances to the next yield
in the function body, and the next(�) call evaluates to the value yielded when the function
body is suspended. Finally, when the function body returns, the enclosing generator
object raises StopIteration, in accordance with the Iterator protocol.
414 | Chapter 14: Iterables, Iterators, and Generators
8. Prior to Python 3.3, it was an error to provide a value with the return statement in a generator function.
Now that is legal, but the return still causes a StopIteration exception to be raised. The caller can retrieve
the return value from the exception object. However, this is only relevant when using a generator function
as a coroutine, as we�ll see in �Returning a Value from a Coroutine� on page 475.
I find it helpful to be strict when talking about the results obtained
from a generator: I say that a generator yields or produces
values. But it�s confusing to say a generator �returns� values. Functions
return values. Calling a generator function returns a generator.
A generator yields or produces values. A generator doesn�t
�return� values in the usual way: the return statement in the body
of a generator function causes StopIteration to be raised by the
generator object.8
Example 14-6 makes the interaction between a for loop and the body of the function
more explicit.
Example 14-6. A generator function that prints messages when it runs
>>> def gen_AB(): #
... print('start')
... yield 'A' #
... print('continue')
... yield 'B' #
... print('end.') #
...
>>> for c in gen_AB(): #
... print('-->', c) #
...
start
--> A
continue
--> B
end.
>>>
The generator function is defined like any function, but uses yield.
The first implicit call to next() in the for loop at will print 'start' and stop
at the first yield, producing the value 'A'.
The second implicit call to next() in the for loop will print 'continue' and
stop at the second yield, producing the value 'B'.
The third call to next() will print 'end.' and fall through the end of the function
body, causing the generator object to raise StopIteration.
Sentence Take #3: A Generator Function | 415
To iterate, the for machinery does the equivalent of g = iter(gen_AB()) to get
a generator object, and then next(g) at each iteration.
The loop block prints --> and the value returned by next(g). But this output
will be seen only after the output of the print calls inside the generator function.
The string 'start' appears as a result of print('start') in the generator
function body.
yield 'A' in the generator function body produces the value A consumed by
the for loop, which gets assigned to the c variable and results in the output --
> A.
Iteration continues with a second call next(g), advancing the generator function
body from yield 'A' to yield 'B'. The text continue is output because of the
second print in the generator function body.
yield 'B' produces the value B consumed by the for loop, which gets assigned
to the c loop variable, so the loop prints --> B.
Iteration continues with a third call next(it), advancing to the end of the body
of the function. The text end. appears in the output because of the third print
in the generator function body.
When the generator function body runs to the end, the generator object raises
StopIteration. The for loop machinery catches that exception, and the loop
terminates cleanly.
Now hopefully it�s clear how Sentence.__iter__ in Example 14-5 works: __iter__ is
a generator function which, when called, builds a generator object that implements the
iterator interface, so the SentenceIterator class is no longer needed.
This second version of Sentence is much shorter than the first, but it�s not as lazy as it
could be. Nowadays, laziness is considered a good trait, at least in programming languages
and APIs. A lazy implementation postpones producing values to the last possible
moment. This saves memory and may avoid useless processing as well.
We�ll build a lazy Sentence class next.
Sentence Take #4: A Lazy Implementation
The Iterator interface is designed to be lazy: next(my_iterator) produces one item
at a time. The opposite of lazy is eager: lazy evaluation and eager evaluation are actual
technical terms in programming language theory.
Our Sentence implementations so far have not been lazy because the __init__ eagerly
builds a list of all words in the text, binding it to the self.words attribute. This will
entail processing the entire text, and the list may use as much memory as the text itself
416 | Chapter 14: Iterables, Iterators, and Generators
(probably more; it depends on how many nonword characters are in the text). Most of
this work will be in vain if the user only iterates over the first couple words.
Whenever you are using Python 3 and start wondering �Is there a lazy way of doing
this?�, often the answer is �Yes.�
The re.finditer function is a lazy version of re.findall which, instead of a list, returns
a generator producing re.MatchObject instances on demand. If there are many
matches, re.finditer saves a lot of memory. Using it, our third version of Sentence is
now lazy: it only produces the next word when it is needed. The code is in Example 14-7.
Example 14-7. sentence_gen2.py: Sentence implemented using a generator function
calling the re.finditer generator function
import re
import reprlib
RE_WORD = re.compile('\w+')
class Sentence:
def __init__(self, text):
self.text = text
def __repr__(self):
return 'Sentence(%s)' % reprlib.repr(self.text)
def __iter__(self):
for match in RE_WORD.finditer(self.text):
yield match.group()
No need to have a words list.
finditer builds an iterator over the matches of RE_WORD on self.text, yielding
MatchObject instances.
match.group() extracts the actual matched text from the MatchObject instance.
Generator functions are an awesome shortcut, but the code can be made even shorter
with a generator expression.
Sentence Take #5: A Generator Expression
Simple generator functions like the one in the previous Sentence class (Example 14-7)
can be replaced by a generator expression.
A generator expression can be understood as a lazy version of a list comprehension: it
does not eagerly build a list, but returns a generator that will lazily produce the items
Sentence Take #5: A Generator Expression | 417
on demand. In other words, if a list comprehension is a factory of lists, a generator
expression is a factory of generators.
Example 14-8 is a quick demo of a generator expression, comparing it to a list comprehension.
Example 14-8. The gen_AB generator function is used by a list comprehension, then by
a generator expression
>>> def gen_AB(): #
... print('start')
... yield 'A'
... print('continue')
... yield 'B'
... print('end.')
...
>>> res1 = [x*3 for x in gen_AB()] #
start
continue
end.
>>> for i in res1: #
... print('-->', i)
...
--> AAA
--> BBB
>>> res2 = (x*3 for x in gen_AB()) #
>>> res2 #
<generator object <genexpr> at 0x10063c240>
>>> for i in res2: #
... print('-->', i)
...
start
--> AAA
continue
--> BBB
end.
This is the same gen_AB function from Example 14-6.
The list comprehension eagerly iterates over the items yielded by the generator
object produced by calling gen_AB(): 'A' and 'B'. Note the output in the next
lines: start, continue, end.
This for loop is iterating over the res1 list produced by the list comprehension.
The generator expression returns res2. The call to gen_AB() is made, but that
call returns a generator, which is not consumed here.
res2 is a generator object.
418 | Chapter 14: Iterables, Iterators, and Generators
Only when the for loop iterates over res2, the body of gen_AB actually executes.
Each iteration of the for loop implicitly calls next(res2), advancing gen_AB to
the next yield. Note the output of gen_AB with the output of the print in the
for loop.
So, a generator expression produces a generator, and we can use it to further reduce the
code in the Sentence class. See Example 14-9.
Example 14-9. sentence_genexp.py: Sentence implemented using a generator expression
import re
import reprlib
RE_WORD = re.compile('\w+')
class Sentence:
def __init__(self, text):
self.text = text
def __repr__(self):
return 'Sentence(%s)' % reprlib.repr(self.text)
def __iter__(self):
return (match.group() for match in RE_WORD.finditer(self.text))
The only difference from Example 14-7 is the __iter__ method, which here is not a
generator function (it has no yield) but uses a generator expression to build a generator
and then returns it. The end result is the same: the caller of __iter__ gets a generator
object.
Generator expressions are syntactic sugar: they can always be replaced by generator
functions, but sometimes are more convenient. The next section is about generator
expression usage.
Generator Expressions: When to Use Them
I used several generator expressions when implementing the Vector class in
Example 10-16. Each of the methods __eq__, __hash__, __abs__, angle, angles,
format, __add__, and __mul__ has a generator expression. In all those methods, a list
comprehension would also work, at the cost of using more memory to store the intermediate
list values.
In Example 14-9, we saw that a generator expression is a syntactic shortcut to create a
generator without defining and calling a function. On the other hand, generator func?
Generator Expressions: When to Use Them | 419
tions are much more flexible: you can code complex logic with multiple statements, and
can even use them as coroutines (see Chapter 16).
For the simpler cases, a generator expression will do, and it�s easier to read at a glance,
as the Vector example shows.
My rule of thumb in choosing the syntax to use is simple: if the generator expression
spans more than a couple of lines, I prefer to code a generator function for the sake of
readability. Also, because generator functions have a name, they can be reused. You can
always name a generator expression and use it later by assigning it to a variable, of course,
but that is stretching its intended usage as a one-off generator.
Syntax Tip
When a generator expression is passed as the single argument to
a function or constructor, you don�t need to write a set of parentheses
for the function call and another to enclose the generator
expression. A single pair will do, like in the Vector call from the
__mul__ method in Example 10-16, reproduced here. However, if
there are more function arguments after the generator expression,
you need to enclose it in parentheses to avoid a SyntaxError:
def __mul__(self, scalar):
if isinstance(scalar, numbers.Real):
return Vector(n * scalar for n in self)
else:
return NotImplemented
The Sentence examples we�ve seen exemplify the use of generators playing the role of
classic iterators: retrieving items from a collection. But generators can also be used to
produce values independent of a data source. The next section shows an example of
that.
Another Example: Arithmetic Progression Generator
The classic Iterator pattern is all about traversal: navigating some data structure. But a
standard interface based on a method to fetch the next item in a series is also useful
when the items are produced on the fly, instead of retrieved from a collection. For
example, the range built-in generates a bounded arithmetic progression (AP) of integers,
and the itertools.count function generates a boundless AP.
We�ll cover itertools.count in the next section, but what if you need to generate a
bounded AP of numbers of any type?
Example 14-10 shows a few console tests of an ArithmeticProgression class we will
see in a moment. The signature of the constructor in Example 14-10 is Arithmetic
Progression(begin, step[, end]). The range() function is similar to the Arithme
420 | Chapter 14: Iterables, Iterators, and Generators
ticProgression here, but its full signature is range(start, stop[, step]). I chose
to implement a different signature because for an arithmetic progression the step is
mandatory but end is optional. I also changed the argument names from start/stop
to begin/end to make it very clear that I opted for a different signature. In each test in
Example 14-10 I call list() on the result to inspect the generated values.
Example 14-10. Demonstration of an ArithmeticProgression class
>>> ap = ArithmeticProgression(0, 1, 3)
>>> list(ap)
[0, 1, 2]
>>> ap = ArithmeticProgression(1, .5, 3)
>>> list(ap)
[1.0, 1.5, 2.0, 2.5]
>>> ap = ArithmeticProgression(0, 1/3, 1)
>>> list(ap)
[0.0, 0.3333333333333333, 0.6666666666666666]
>>> from fractions import Fraction
>>> ap = ArithmeticProgression(0, Fraction(1, 3), 1)
>>> list(ap)
[Fraction(0, 1), Fraction(1, 3), Fraction(2, 3)]
>>> from decimal import Decimal
>>> ap = ArithmeticProgression(0, Decimal('.1'), .3)
>>> list(ap)
[Decimal('0.0'), Decimal('0.1'), Decimal('0.2')]
Note that type of the numbers in the resulting arithmetic progression follows the type
of begin or step, according to the numeric coercion rules of Python arithmetic. In
Example 14-10, you see lists of int, float, Fraction, and Decimal numbers.
Example 14-11 lists the implementation of the ArithmeticProgression class.
Example 14-11. The ArithmeticProgression class
class ArithmeticProgression:
def __init__(self, begin, step, end=None):
self.begin = begin
self.step = step
self.end = end # None -> "infinite" series
def __iter__(self):
result = type(self.begin + self.step)(self.begin)
forever = self.end is None
index = 0
while forever or result < self.end:
yield result
index += 1
result = self.begin + self.step * index
Another Example: Arithmetic Progression Generator | 421
9. In Python 2, there was a coerce() built-in function but it�s gone in Python 3, deemed unnecessary because
the numeric coercion rules are implicit in the arithmetic operator methods. So the best way I could think
of to coerce the initial value to be of the same type as the rest of the series was to perform the addition and
use its type to convert the result. I asked about this in the Python-list and got an excellent response from
Steven D�Aprano.
10. The 14-it-generator/ directory in the Fluent Python code repository includes doctests and a script, aritprog_
runner.py, which runs the tests against all variations of the aritprog*.py scripts.
__init__ requires two arguments: begin and step. end is optional, if it�s None,
the series will be unbounded.
This line produces a result value equal to self.begin, but coerced to the type
of the subsequent additions.9
For readability, the forever flag will be True if the self.end attribute is None,
resulting in an unbounded series.
This loop runs forever or until the result matches or exceeds self.end. When
this loop exits, so does the function.
The current result is produced.
The next potential result is calculated. It may never be yielded, because the while
loop may terminate.
In the last line of Example 14-11, instead of simply incrementing the result with
self.step iteratively, I opted to use an index variable and calculate each result by
adding self.begin to self.step multiplied by index to reduce the cumulative effect
of errors when working with with floats.
The ArithmeticProgression class from Example 14-11 works as intended, and is a
clear example of the use of a generator function to implement the __iter__ special
method. However, if the whole point of a class is to build a generator by implementing
__iter__, the class can be reduced to a generator function. A generator function is, after
all, a generator factory.
Example 14-12 shows a generator function called aritprog_gen that does the same job
as ArithmeticProgression but with less code. The tests in Example 14-10 all pass if
you just call aritprog_gen instead of ArithmeticProgression.10
Example 14-12. The aritprog_gen generator function
def aritprog_gen(begin, step, end=None):
result = type(begin + step)(begin)
forever = end is None
index = 0
while forever or result < end:
yield result
422 | Chapter 14: Iterables, Iterators, and Generators
index += 1
result = begin + step * index
Example 14-12 is pretty cool, but always remember: there are plenty of ready-to-use
generators in the standard library, and the next section will show an even cooler implementation
using the itertools module.
Arithmetic Progression with itertools
The itertools module in Python 3.4 has 19 generator functions that can be combined
in a variety of interesting ways.
For example, the itertools.count function returns a generator that produces numbers.
Without arguments, it produces a series of integers starting with 0. But you can provide
optional start and step values to achieve a result very similar to our aritprog_gen
functions:
>>> import itertools
>>> gen = itertools.count(1, .5)
>>> next(gen)
1
>>> next(gen)
1.5
>>> next(gen)
2.0
>>> next(gen)
2.5
However, itertools.count never stops, so if you call list(count()), Python will try
to build a list larger than available memory and your machine will be very grumpy
long before the call fails.
On the other hand, there is the itertools.takewhile function: it produces a generator
that consumes another generator and stops when a given predicate evaluates to False.
So we can combine the two and write this:
>>> gen = itertools.takewhile(lambda n: n < 3, itertools.count(1, .5))
>>> list(gen)
[1, 1.5, 2.0, 2.5]
Leveraging takewhile and count, Example 14-13 is sweet and short.
Example 14-13. aritprog_v3.py: this works like the previous aritprog_gen functions
import itertools
def aritprog_gen(begin, step, end=None):
first = type(begin + step)(begin)
ap_gen = itertools.count(first, step)
if end is not None:
Another Example: Arithmetic Progression Generator | 423
ap_gen = itertools.takewhile(lambda n: n < end, ap_gen)
return ap_gen
Note that aritprog_gen is not a generator function in Example 14-13: it has no yield
in its body. But it returns a generator, so it operates as a generator factory, just as a
generator function does.
The point of Example 14-13 is: when implementing generators, know what is available
in the standard library, otherwise there�s a good chance you�ll reinvent the wheel. That�s
why the next section covers several ready-to-use generator functions.
Generator Functions in the Standard Library
The standard library provides many generators, from plain-text file objects providing
line-by-line iteration, to the awesome os.walk function, which yields filenames while
traversing a directory tree, making recursive filesystem searches as simple as a for loop.
The os.walk generator function is impressive, but in this section I want to focus on
general-purpose functions that take arbitrary iterables as arguments and return generators
that produce selected, computed, or rearranged items. In the following tables, I
summarize two dozen of them, from the built-in, itertools, and functools modules.
For convenience, I grouped them by high-level functionality, regardless of where they
are defined.
Perhaps you know all the functions mentioned in this section, but
some of them are underused, so a quick overview may be good
to recall what�s already available.
The first group are filtering generator functions: they yield a subset of items produced
by the input iterable, without changing the items themselves. We used itertools.take
while previously in this chapter, in �Arithmetic Progression with itertools� on page
423. Like takewhile, most functions listed in Table 14-1 take a predicate, which is a
one-argument Boolean function that will be applied to each item in the input to determine
whether the item is included in the output.
Table 14-1. Filtering generator functions
Module Function Description
iter
tools
compress(it, selec
tor_it)
Consumes two iterables in parallel; yields items from it whenever the
corresponding item in selector_it is truthy
iter
tools
dropwhile(predi
cate, it)
Consumes it skipping items while predicate computes truthy, then yields
every remaining item (no further checks are made)
424 | Chapter 14: Iterables, Iterators, and Generators
11. Here the term �mapping� is unrelated to dictionaries, but has to do with the map built-in.
Module Function Description
(built-in) filter(predicate,
it)
Applies predicate to each item of iterable, yielding the item if predi
cate(item) is truthy; if predicate is None, only truthy items are yielded
iter
tools
filterfalse(predi
cate, it)
Same as filter, with the predicate logic negated: yields items whenever
predicate computes falsy
iter
tools
islice(it, stop)
or islice(it,
start, stop,
step=1)
Yields items from a slice of it, similar to s[:stop] or
s[start:stop:step] except it can be any iterable, and the operation is
lazy
iter
tools
takewhile(predi
cate, it)
Yields items while predicate computes truthy, then stops and no further
checks are made
The console listing in Example 14-14 shows the use of all functions in Table 14-1.
Example 14-14. Filtering generator functions examples
>>> def vowel(c):
... return c.lower() in 'aeiou'
...
>>> list(filter(vowel, 'Aardvark'))
['A', 'a', 'a']
>>> import itertools
>>> list(itertools.filterfalse(vowel, 'Aardvark'))
['r', 'd', 'v', 'r', 'k']
>>> list(itertools.dropwhile(vowel, 'Aardvark'))
['r', 'd', 'v', 'a', 'r', 'k']
>>> list(itertools.takewhile(vowel, 'Aardvark'))
['A', 'a']
>>> list(itertools.compress('Aardvark', (1,0,1,1,0,1)))
['A', 'r', 'd', 'a']
>>> list(itertools.islice('Aardvark', 4))
['A', 'a', 'r', 'd']
>>> list(itertools.islice('Aardvark', 4, 7))
['v', 'a', 'r']
>>> list(itertools.islice('Aardvark', 1, 7, 2))
['a', 'd', 'a']
The next group are the mapping generators: they yield items computed from each individual
item in the input iterable�or iterables, in the case of map and starmap.11 The
generators in Table 14-2 yield one result per item in the input iterables. If the input
comes from more than one iterable, the output stops as soon as the first input iterable
is exhausted.
Generator Functions in the Standard Library | 425
Table 14-2. Mapping generator functions
Module Function Description
itertools accumulate(it,
[func])
Yields accumulated sums; if func is provided, yields the result of applying it to the
first pair of items, then to the first result and next item, etc.
(built-in) enumerate(itera
ble, start=0)
Yields 2-tuples of the form (index, item), where index is counted from
start, and item is taken from the iterable
(built-in) map(func, it1,
[it2, �, itN])
Applies func to each item of it, yielding the result; if N iterables are given, func
must take N arguments and the iterables will be consumed in parallel
itertools starmap(func, it) Applies func to each item of it, yielding the result; the input iterable should yield
iterable items iit, and func is applied as func(*iit)
Example 14-15 demonstrates some uses of itertools.accumulate.
Example 14-15. itertools.accumulate generator function examples
>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]
>>> import itertools
>>> list(itertools.accumulate(sample)) #
[5, 9, 11, 19, 26, 32, 35, 35, 44, 45]
>>> list(itertools.accumulate(sample, min)) #
[5, 4, 2, 2, 2, 2, 2, 0, 0, 0]
>>> list(itertools.accumulate(sample, max)) #
[5, 5, 5, 8, 8, 8, 8, 8, 9, 9]
>>> import operator
>>> list(itertools.accumulate(sample, operator.mul)) #
[5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0]
>>> list(itertools.accumulate(range(1, 11), operator.mul))
[1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800] #
Running sum.
Running minimum.
Running maximum.
Running product.
Factorials from 1! to 10!.
The remaining functions of Table 14-2 are shown in Example 14-16.
Example 14-16. Mapping generator function examples
>>> list(enumerate('albatroz', 1)) #
[(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7, 'o'), (8, 'z')]
>>> import operator
>>> list(map(operator.mul, range(11), range(11))) #
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
>>> list(map(operator.mul, range(11), [2, 4, 8])) #
[0, 4, 16]
>>> list(map(lambda a, b: (a, b), range(11), [2, 4, 8])) #
426 | Chapter 14: Iterables, Iterators, and Generators
[(0, 2), (1, 4), (2, 8)]
>>> import itertools
>>> list(itertools.starmap(operator.mul, enumerate('albatroz', 1))) #
['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo', 'zzzzzzzz']
>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1]
>>> list(itertools.starmap(lambda a, b: b/a,
... enumerate(itertools.accumulate(sample), 1))) #
[5.0, 4.5, 3.6666666666666665, 4.75, 5.2, 5.333333333333333,
5.0, 4.375, 4.888888888888889, 4.5]
Number the letters in the word, starting from 1.
Squares of integers from 0 to 10.
Multiplying numbers from two iterables in parallel: results stop when the
shortest iterable ends.
This is what the zip built-in function does.
Repeat each letter in the word according to its place in it, starting from 1.
Running average.
Next, we have the group of merging generators�all of these yield items from multiple
input iterables. chain and chain.from_iterable consume the input iterables sequentially
(one after the other), while product, zip, and zip_longest consume the input
iterables in parallel. See Table 14-3.
Table 14-3. Generator functions that merge multiple input iterables
Module Function Description
itertools chain(it1, �, itN) Yield all items from it1, then from it2 etc., seamlessly
itertools chain.from_iterable(it) Yield all items from each iterable produced by it, one after the other,
seamlessly; it should yield iterable items, for example, a list of iterables
itertools product(it1, �, itN, re
peat=1)
Cartesian product: yields N-tuples made by combining items from each
input iterable like nested for loops could produce; repeat allows the
input iterables to be consumed more than once
(built-in) zip(it1, �, itN) Yields N-tuples built from items taken from the iterables in parallel, silently
stopping when the first iterable is exhausted
itertools zip_longest(it1, �,
itN, fillvalue=None)
Yields N-tuples built from items taken from the iterables in parallel,
stopping only when the last iterable is exhausted, filling the blanks with
the fillvalue
Example 14-17 shows the use of the itertools.chain and zip generator functions and
their siblings. Recall that the zip function is named after the zip fastener or zipper (no
relation with compression). Both zip and itertools.zip_longest were introduced in
�The Awesome zip� on page 293.
Generator Functions in the Standard Library | 427
Example 14-17. Merging generator function examples
>>> list(itertools.chain('ABC', range(2))) #
['A', 'B', 'C', 0, 1]
>>> list(itertools.chain(enumerate('ABC'))) #
[(0, 'A'), (1, 'B'), (2, 'C')]
>>> list(itertools.chain.from_iterable(enumerate('ABC'))) #
[0, 'A', 1, 'B', 2, 'C']
>>> list(zip('ABC', range(5))) #
[('A', 0), ('B', 1), ('C', 2)]
>>> list(zip('ABC', range(5), [10, 20, 30, 40])) #
[('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]
>>> list(itertools.zip_longest('ABC', range(5))) #
[('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]
>>> list(itertools.zip_longest('ABC', range(5), fillvalue='?')) #
[('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)]
chain is usually called with two or more iterables.
chain does nothing useful when called with a single iterable.
But chain.from_iterable takes each item from the iterable, and chains them
in sequence, as long as each item is itself iterable.
zip is commonly used to merge two iterables into a series of two-tuples.
Any number of iterables can be consumed by zip in parallel, but the generator
stops as soon as the first iterable ends.
itertools.zip_longest works like zip, except it consumes all input iterables
to the end, padding output tuples with None as needed.
The fillvalue keyword argument specifies a custom padding value.
The itertools.product generator is a lazy way of computing Cartesian products,
which we built using list comprehensions with more than one for clause in �Cartesian
Products� on page 23. Generator expressions with multiple for clauses can also be used
to produce Cartesian products lazily. Example 14-18 demonstrates itertools.product.
Example 14-18. itertools.product generator function examples
>>> list(itertools.product('ABC', range(2))) #
[('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]
>>> suits = 'spades hearts diamonds clubs'.split()
>>> list(itertools.product('AK', suits)) #
[('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A', 'clubs'),
('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K', 'clubs')]
>>> list(itertools.product('ABC')) #
[('A',), ('B',), ('C',)]
>>> list(itertools.product('ABC', repeat=2)) #
[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'),
('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]
>>> list(itertools.product(range(2), repeat=3))
428 | Chapter 14: Iterables, Iterators, and Generators
[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),
(1, 0, 1), (1, 1, 0), (1, 1, 1)]
>>> rows = itertools.product('AB', range(2), repeat=2)
>>> for row in rows: print(row)
...
('A', 0, 'A', 0)
('A', 0, 'A', 1)
('A', 0, 'B', 0)
('A', 0, 'B', 1)
('A', 1, 'A', 0)
('A', 1, 'A', 1)
('A', 1, 'B', 0)
('A', 1, 'B', 1)
('B', 0, 'A', 0)
('B', 0, 'A', 1)
('B', 0, 'B', 0)
('B', 0, 'B', 1)
('B', 1, 'A', 0)
('B', 1, 'A', 1)
('B', 1, 'B', 0)
('B', 1, 'B', 1)
The Cartesian product of a str with three characters and a range with two
integers yields six tuples (because 3 * 2 is 6).
The product of two card ranks ('AK'), and four suits is a series of eight tuples.
Given a single iterable, product yields a series of one-tuples, not very useful.
The repeat=N keyword argument tells product to consume each input iterable
N times.
Some generator functions expand the input by yielding more than one value per input
item. They are listed in Table 14-4.
Table 14-4. Generator functions that expand each input item into multiple output
items
Module Function Description
itertools combinations(it,
out_len)
Yield combinations of out_len items from the items yielded by it
itertools combinations_with_re
placement(it, out_len)
Yield combinations of out_len items from the items yielded by it,
including combinations with repeated items
itertools count(start=0, step=1) Yields numbers starting at start, incremented by step, indefinitely
itertools cycle(it) Yields items from it storing a copy of each, then yields the entire
sequence repeatedly, indefinitely
itertools permutations(it,
out_len=None)
Yield permutations of out_len items from the items yielded by it;
by default, out_len is len(list(it))
Generator Functions in the Standard Library | 429
Module Function Description
itertools repeat(item, [times]) Yield the given item repeadedly, indefinetly unless a number of times
is given
The count and repeat functions from itertools return generators that conjure items
out of nothing: neither of them takes an iterable as input. We saw itertools.count in
�Arithmetic Progression with itertools� on page 423. The cycle generator makes a
backup of the input iterable and yields its items repeatedly. Example 14-19 illustrates
the use of count, repeat, and cycle.
Example 14-19. count, cycle, and repeat
>>> ct = itertools.count() #
>>> next(ct) #
0
>>> next(ct), next(ct), next(ct) #
(1, 2, 3)
>>> list(itertools.islice(itertools.count(1, .3), 3)) #
[1, 1.3, 1.6]
>>> cy = itertools.cycle('ABC') #
>>> next(cy)
'A'
>>> list(itertools.islice(cy, 7)) #
['B', 'C', 'A', 'B', 'C', 'A', 'B']
>>> rp = itertools.repeat(7) #
>>> next(rp), next(rp)
(7, 7)
>>> list(itertools.repeat(8, 4)) #
[8, 8, 8, 8]
>>> list(map(operator.mul, range(11), itertools.repeat(5))) #
[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
Build a count generator ct.
Retrieve the first item from ct.
I can�t build a list from ct, because ct never stops, so I fetch the next three
items.
I can build a list from a count generator if it is limited by islice or takewhile.
Build a cycle generator from 'ABC' and fetch its first item, 'A'.
A list can only be built if limited by islice; the next seven items are retrieved
here.
Build a repeat generator that will yield the number 7 forever.
A repeat generator can be limited by passing the times argument: here the
number 8 will be produced 4 times.
430 | Chapter 14: Iterables, Iterators, and Generators
A common use of repeat: providing a fixed argument in map; here it provides
the 5 multiplier.
The combinations, combinations_with_replacement, and permutations generator
functions�together with product�are called the combinatoric generators in the iter
tools documentation page. There is a close relationship between itertools.product
and the remaining combinatoric functions as well, as Example 14-20 shows.
Example 14-20. Combinatoric generator functions yield multiple values per input item
>>> list(itertools.combinations('ABC', 2)) #
[('A', 'B'), ('A', 'C'), ('B', 'C')]
>>> list(itertools.combinations_with_replacement('ABC', 2)) #
[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]
>>> list(itertools.permutations('ABC', 2)) #
[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]
>>> list(itertools.product('ABC', repeat=2)) #
[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'),
('C', 'A'), ('C', 'B'), ('C', 'C')]
All combinations of len()==2 from the items in 'ABC'; item ordering in the
generated tuples is irrelevant (they could be sets).
All combinations of len()==2 from the items in 'ABC', including combinations
with repeated items.
All permutations of len()==2 from the items in 'ABC'; item ordering in the
generated tuples is relevant.
Cartesian product from 'ABC' and 'ABC' (that�s the effect of repeat=2).
The last group of generator functions we�ll cover in this section are designed to yield all
items in the input iterables, but rearranged in some way. Here are two functions that
return multiple generators: itertools.groupby and itertools.tee. The other generator
function in this group, the reversed built-in, is the only one covered in this section
that does not accept any iterable as input, but only sequences. This makes sense: because
reversed will yield the items from last to first, it only works with a sequence with a
known length. But it avoids the cost of making a reversed copy of the sequence by
yielding each item as needed. I put the itertools.product function together with the
merging generators in Table 14-3 because they all consume more than one iterable, while
the generators in Table 14-5 all accept at most one input iterable.
Table 14-5. Rearranging generator functions
Module Function Description
itertools groupby(it,
key=None)
Yields 2-tuples of the form (key, group), where key is the grouping criterion
and group is a generator yielding the items in the group
Generator Functions in the Standard Library | 431
Module Function Description
(built-in) reversed(seq) Yields items from seq in reverse order, from last to first; seq must be a sequence
or implement the __reversed__ special method
itertools tee(it, n=2) Yields a tuple of n generators, each yielding the items of the input iterable
independently
Example 14-21 demonstrates the use of itertools.groupby and the reversed built-in.
Note that itertools.groupby assumes that the input iterable is sorted by the grouping
criterion, or at least that the items are clustered by that criterion�even if not sorted.
Example 14-21. itertools.groupby
>>> list(itertools.groupby('LLLLAAGGG')) #
[('L', <itertools._grouper object at 0x102227cc0>),
('A', <itertools._grouper object at 0x102227b38>),
('G', <itertools._grouper object at 0x102227b70>)]
>>> for char, group in itertools.groupby('LLLLAAAGG'): #
... print(char, '->', list(group))
...
L -> ['L', 'L', 'L', 'L']
A -> ['A', 'A',]
G -> ['G', 'G', 'G']
>>> animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear',
... 'bat', 'dolphin', 'shark', 'lion']
>>> animals.sort(key=len) #
>>> animals
['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark',
'giraffe', 'dolphin']
>>> for length, group in itertools.groupby(animals, len): #
... print(length, '->', list(group))
...
3 -> ['rat', 'bat']
4 -> ['duck', 'bear', 'lion']
5 -> ['eagle', 'shark']
7 -> ['giraffe', 'dolphin']
>>> for length, group in itertools.groupby(reversed(animals), len): #
... print(length, '->', list(group))
...
7 -> ['dolphin', 'giraffe']
5 -> ['shark', 'eagle']
4 -> ['lion', 'bear', 'duck']
3 -> ['bat', 'rat']
>>>
groupby yields tuples of (key, group_generator).
Handling groupby generators involves nested iteration: in this case, the outer
for loop and the inner list constructor.
To use groupby, the input should be sorted; here the words are sorted by length.
432 | Chapter 14: Iterables, Iterators, and Generators
12. The itertools.chain from the standard library is written in C.
Again, loop over the key and group pair, to display the key and expand the group
into a list.
Here the reverse generator is used to iterate over animals from right to left.
The last of the generator functions in this group is iterator.tee, which has a unique
behavior: it yields multiple generators from a single input iterable, each yielding every
item from the input. Those generators can be consumed independently, as shown in
Example 14-22.
Example 14-22. itertools.tee yields multiple generators, each yielding every item of the
input generator
>>> list(itertools.tee('ABC'))
[<itertools._tee object at 0x10222abc8>, <itertools._tee object at 0x10222ac08>]
>>> g1, g2 = itertools.tee('ABC')
>>> next(g1)
'A'
>>> next(g2)
'A'
>>> next(g2)
'B'
>>> list(g1)
['B', 'C']
>>> list(g2)
['C']
>>> list(zip(*itertools.tee('ABC')))
[('A', 'A'), ('B', 'B'), ('C', 'C')]
Note that several examples in this section used combinations of generator functions.
This is a great feature of these functions: because they all take generators as arguments
and return generators, they can be combined in many different ways.
While on the subject of combining generators, the yield from statement, new in Python
3.3, is a tool for doing just that.
New Syntax in Python 3.3: yield from
Nested for loops are the traditional solution when a generator function needs to yield
values produced from another generator.
For example, here is a homemade implementation of a chaining generator:12
>>> def chain(*iterables):
... for it in iterables:
... for i in it:
New Syntax in Python 3.3: yield from | 433
... yield i
...
>>> s = 'ABC'
>>> t = tuple(range(3))
>>> list(chain(s, t))
['A', 'B', 'C', 0, 1, 2]
The chain generator function is delegating to each received iterable in turn. PEP 380
� Syntax for Delegating to a Subgenerator introduced new syntax for doing that, shown
in the next console listing:
>>> def chain(*iterables):
... for i in iterables:
... yield from i
...
>>> list(chain(s, t))
['A', 'B', 'C', 0, 1, 2]
As you can see, yield from i replaces the inner for loop completely. The use of yield
from in this example is correct, and the code reads better, but it seems like mere syntactic
sugar. Besides replacing a loop, yield from creates a channel connecting the inner
generator directly to the client of the outer generator. This channel becomes really important
when generators are used as coroutines and not only produce but also consume
values from the client code. Chapter 16 dives into coroutines, and has several pages
explaining why yield from is much more than syntactic sugar.
After this first encounter with yield from, we�ll go back to our review of iterable-savvy
functions in the standard library.
Iterable Reducing Functions
The functions in Table 14-6 all take an iterable and return a single result. They are known
as �reducing,� �folding,� or �accumulating� functions. Actually, every one of the builtins
listed here can be implemented with functools.reduce, but they exist as built-ins
because they address some common use cases more easily. Also, in the case of all and
any, there is an important optimization that can�t be done with reduce: these functions
short-circuit (i.e., they stop consuming the iterator as soon as the result is determined).
See the last test with any in Example 14-23.
Table 14-6. Built-in functions that read iterables and return single values
Module Function Description
(built-in) all(it) Returns True if all items in it are truthy, otherwise False; all([])
returns True
(built-in) any(it) Returns True if any item in it is truthy, otherwise False; any([])
returns False
434 | Chapter 14: Iterables, Iterators, and Generators
Module Function Description
(built-in) max(it, [key=,] [de
fault=])
Returns the maximum value of the items in it;a key is an ordering
function, as in sorted; default is returned if the iterable is empty
(built-in) min(it, [key=,] [de
fault=])
Returns the minimum value of the items in it.b key is an ordering function,
as in sorted; default is returned if the iterable is empty
functools reduce(func, it, [ini
tial])
Returns the result of applying func to the first pair of items, then to that
result and the third item and so on; if given, initial forms the initial
pair with the first item
(built-in) sum(it, start=0) The sum of all items in it, with the optional start value added (use
math.fsum for better precision when adding floats)
a May also be called as max(arg1, arg2, �, [key=?]), in which case the maximum among the arguments is returned.
b May also be called as min(arg1, arg2, �, [key=?]), in which case the minimum among the arguments is returned.
The operation of all and any is exemplified in Example 14-23.
Example 14-23. Results of all and any for some sequences
>>> all([1, 2, 3])
True
>>> all([1, 0, 3])
False
>>> all([])
True
>>> any([1, 2, 3])
True
>>> any([1, 0, 3])
True
>>> any([0, 0.0])
False
>>> any([])
False
>>> g = (n for n in [0, 0.0, 7, 8])
>>> any(g)
True
>>> next(g)
8
A longer explanation about functools.reduce appeared in �Vector Take #4: Hashing
and a Faster ==� on page 288.
Another built-in that takes an iterable and returns something else is sorted. Unlike
reversed, which is a generator function, sorted builds and returns an actual list. After
all, every single item of the input iterable must be read so they can be sorted, and the
sorting happens in a list, therefore sorted just returns that list after it�s done. I
mention sorted here because it does consume an arbitrary iterable.
Iterable Reducing Functions | 435
Of course, sorted and the reducing functions only work with iterables that eventually
stop. Otherwise, they will keep on collecting items and never return a result.
We�ll now go back to the iter() built-in: it has a little-known feature that we haven�t
covered yet.
A Closer Look at the iter Function
As we�ve seen, Python calls iter(x) when it needs to iterate over an object x.
But iter has another trick: it can be called with two arguments to create an iterator
from a regular function or any callable object. In this usage, the first argument must be
a callable to be invoked repeatedly (with no arguments) to yield values, and the second
argument is a sentinel: a marker value which, when returned by the callable, causes the
iterator to raise StopIteration instead of yielding the sentinel.
The following example shows how to use iter to roll a six-sided die until a 1 is rolled:
>>> def d6():
... return randint(1, 6)
...
>>> d6_iter = iter(d6, 1)
>>> d6_iter
<callable_iterator object at 0x00000000029BE6A0>
>>> for roll in d6_iter:
... print(roll)
...
4
3
6
3
Note that the iter function here returns a callable_iterator. The for loop in the
example may run for a very long time, but it will never display 1, because that is the
sentinel value. As usual with iterators, the d6_iter object in the example becomes useless
once exhausted. To start over, you must rebuild the iterator by invoking iter(�)
again.
A useful example is found in the iter built-in function documentation. This snippet
reads lines from a file until a blank line is found or the end of file is reached:
with open('mydata.txt') as fp:
for line in iter(fp.readline, ''):
process_line(line)
To close this chapter, I present a practical example of using generators to handle a large
volume of data efficiently.
436 | Chapter 14: Iterables, Iterators, and Generators
Case Study: Generators in a Database Conversion Utility
A few years ago I worked at BIREME, a digital library run by PAHO/WHO (Pan-
American Health Organization/World Health Organization) in Sao Paulo, Brazil.
Among the bibliographic datasets created by BIREME are LILACS (Latin American and
Caribbean Health Sciences index) and SciELO (Scientific Electronic Library Online),
two comprehensive databases indexing the scientific and technical literature produced
in the region.
Since the late 1980s, the database system used to manage LILACS is CDS/ISIS, a nonrelational,
document database created by UNESCO and eventually rewritten in C by
BIREME to run on GNU/Linux servers. One of my jobs was to research alternatives for
a possible migration of LILACS�and eventually the much larger SciELO�to a modern,
open source, document database such as CouchDB or MongoDB.
As part of that research, I wrote a Python script, isis2json.py, that reads a CDS/ISIS file
and writes a JSON file suitable for importing to CouchDB or MongoDB. Initially, the
script read files in the ISO-2709 format exported by CDS/ISIS. The reading and writing
had to be done incrementally because the full datasets were much bigger than main
memory. That was easy enough: each iteration of the main for loop read one record
from the .iso file, massaged it, and wrote it to the .json output.
However, for operational reasons, it was deemed necessary that isis2json.py supported
another CDS/ISIS data format: the binary .mst files used in production at BIREME�
to avoid the costly export to ISO-2709.
Now I had a problem: the libraries used to read ISO-2709 and .mst files had very different
APIs. And the JSON writing loop was already complicated because the script accepted
a variety of command-line options to restructure each output record. Reading data using
two different APIs in the same for loop where the JSON was produced would be unwieldy.
The solution was to isolate the reading logic into a pair of generator functions: one for
each supported input format. In the end, the isis2json.py script was split into four functions.
You can see the main Python 2 script in Example A-5, but the full source code
with dependencies is in fluentpython/isis2json on GitHub.
Here is a high-level overview of how the script is structured:
main
The main function uses argparse to read command-line options that configure the
structure of the output records. Based on the input filename extension, a suitable
generator function is selected to read the data and yield the records, one by one.
Case Study: Generators in a Database Conversion Utility | 437
13. The library used to read the complex .mst binary is actually written in Java, so this functionality is only
available when isis2json.py is executed with the Jython interpreter, version 2.5 or newer. For further details,
see the README.rst file in the repository. The dependencies are imported inside the generator functions that
need them, so the script can run even if only one of the external libraries is available.
iter_iso_records
This generator function reads .iso files (assumed to be in the ISO-2709 format). It
takes two arguments: the filename and isis_json_type, one of the options related
to the record structure. Each iteration of its for loop reads one record, creates an
empty dict, populates it with field data, and yields the dict.
iter_mst_records
This other generator functions reads .mst files.13 If you look at the source code for
isis2json.py, you�ll see that it�s not as simple as iter_iso_records, but its interface
and overall structure is the same: it takes a filename and an isis_json_type argument
and enters a for loop, which builds and yields one dict per iteration, representing
a single record.
write_json
This function performs the actual writing of the JSON records, one at a time. It
takes numerous arguments, but the first one�input_gen�is a reference to a generator
function: either iter_iso_records or iter_mst_records. The main for
loop in write_json iterates over the dictionaries yielded by the selected in
put_gen generator, massages it in several ways as determined by the command-line
options, and appends the JSON record to the output file.
By leveraging generator functions, I was able to decouple the reading logic from the
writing logic. Of course, the simplest way to decouple them would be to read all records
to memory, then write them to disk. But that was not a viable option because of the size
of the datasets. Using generators, the reading and writing is interleaved, so the script
can process files of any size.
Now if isis2json.py needs to support an additional input format�say, MARCXML, a
DTD used by the U.S. Library of Congress to represent ISO-2709 data�it will be easy
to add a third generator function to implement the reading logic, without changing
anything in the complicated write_json function.
This is not rocket science, but it�s a real example where generators provided a flexible
solution to processing databases as a stream of records, keeping memory usage low
regardless of the amount of data. Anyone who manages large datasets finds many opportunities
for using generators in practice.
The next section addresses an aspect of generators that we�ll actually skip for now. Read
on to understand why.
438 | Chapter 14: Iterables, Iterators, and Generators
14. Slide 33, �Keeping It Straight,� in �A Curious Course on Coroutines and Concurrency�.
15. According to the Jargon file, to grok is not merely to learn something, but to absorb it so �it becomes part of
you, part of your identity.�
Generators as Coroutines
About five years after generator functions with the yield keyword were introduced in
Python 2.2, PEP 342 � Coroutines via Enhanced Generators was implemented in
Python 2.5. This proposal added extra methods and functionality to generator objects,
most notably the .send() method.
Like .__next__(), .send() causes the generator to advance to the next yield, but it
also allows the client using the generator to send data into it: whatever argument is
passed to .send() becomes the value of the corresponding yield expression inside the
generator function body. In other words, .send() allows two-way data exchange between
the client code and the generator�in contrast with .__next__(), which only lets
the client receive data from the generator.
This is such a major �enhancement� that it actually changes the nature of generators:
when used in this way, they become coroutines. David Beazley�probably the most prolific
writer and speaker about coroutines in the Python community�warned in a famous
PyCon US 2009 tutorial:
� Generators produce data for iteration
� Coroutines are consumers of data
� To keep your brain from exploding, you don�t mix the two concepts together
� Coroutines are not related to iteration
� Note: There is a use of having yield produce a value in a coroutine, but it�s not tied
to iteration.14
� David Beazley
�A Curious Course on Coroutines and Concurrency�
I will follow Dave�s advice and close this chapter�which is really about iteration techniques�
without touching send and the other features that make generators usable as
coroutines. Coroutines will be covered in Chapter 16.
Chapter Summary
Iteration is so deeply embedded in the language that I like to say that Python groks
iterators.15 The integration of the Iterator pattern in the semantics of Python is a prime
example of how design patterns are not equally applicable in all programming lan?
Generators as Coroutines | 439
guages. In Python, a classic iterator implemented �by hand� as in Example 14-4 has no
practical use, except as a didactic example.
In this chapter, we built a few versions of a class to iterate over individual words in text
files that may be very long. Thanks to the use of generators, the successive refactorings
of the Sentence class become shorter and easier to read�when you know how they
work.
We then coded a generator of arithmetic progressions and showed how to leverage the
itertools module to make it simpler. An overview of 24 general-purpose generator
functions in the standard library followed.
Following that, we looked at the iter built-in function: first, to see how it returns an
iterator when called as iter(o), and then to study how it builds an iterator from any
function when called as iter(func, sentinel).
For practical context, I described the implementation of a database conversion utility
using generator functions to decouple the reading to the writing logic, enabling efficient
handling of large datasets and making it easy to support more than one data input
format.
Also mentioned in this chapter were the yield from syntax, new in Python 3.3, and
coroutines. Both topics were just introduced here; they get more coverage later in the
book.
Further Reading
A detailed technical explanation of generators appears in The Python Language Reference
in 6.2.9. Yield expressions. The PEP where generator functions were defined is
PEP 255 � Simple Generators.
The itertools module documentation is excellent because of all the examples included.
Although the functions in that module are implemented in C, the documentation shows
how many of them would be written in Python, often by leveraging other functions in
the module. The usage examples are also great: for instance, there is a snippet showing
how to use the accumulate function to amortize a loan with interest, given a list of
payments over time. There is also an Itertools Recipes section with additional highperformance
functions that use the itertools functions as building blocks.
Chapter 4, �Iterators and Generators,� of Python Cookbook, 3E (O�Reilly), by David
Beazley and Brian K. Jones, has 16 recipes covering this subject from many different
angles, always focusing on practical applications.
The yield from syntax is explained with examples in What�s New in Python 3.3 (see
PEP 380: Syntax for Delegating to a Subgenerator). We�ll also cover it in detail in �Using
yield from� on page 477 and �The Meaning of yield from� on page 483 in Chapter 16.
440 | Chapter 14: Iterables, Iterators, and Generators
16. Joe Gibbs Politz, Alejandro Martinez, Matthew Milano, Sumner Warren, Daniel Patterson, Junsong Li, Anand
Chitipothu, and Shriram Krishnamurthi, �Python: The Full Monty,� SIGPLAN Not. 48, 10 (October 2013),
217-232.
If you are interested in document databases and would like to learn more about the
context of �Case Study: Generators in a Database Conversion Utility� on page 437, the
Code4Lib Journal�which covers the intersection between libraries and technology�
published my paper �From ISIS to CouchDB: Databases and Data Models for Bibliographic
Records�. One section of the paper describes the isis2json.py script. The rest of
it explains why and how the semistructured data model implemented by document
databases like CouchDB and MongoDB are more suitable for cooperative bibliographic
data collection than the relational model.
Soapbox
Generator Function Syntax: More Sugar Would Be Nice
Designers need to ensure that controls and displays for different purposes are significantly
different from one another.
� Donald Norman
The Design of Everyday Things
Source code plays the role of �controls and displays� in programming languages. I think
Python is exceptionally well designed; its source code is often as readable as pseudocode.
But nothing is perfect. Guido van Rossum should have followed Donald Norman�s advice
(previously quoted) and introduced another keyword for defining generator expressions,
instead of reusing def. The �BDFL Pronouncements� section of PEP 255 �
Simple Generators actually argues:
A �yield� statement buried in the body is not enough warning that the semantics are so
different.
But Guido hates introducing new keywords and he did not find that argument convincing,
so we are stuck with def.
Reusing the function syntax for generators has other bad consequences. In the paper
and experimental work �Python, the Full Monty: A Tested Semantics for the Python
Programming Language,� Politz16 et al. show this trivial example of a generator function
(section 4.1 of the paper):
def f(): x=0
while True:
x += 1
yield x
The authors then make the point that we can�t abstract the process of yielding with a
function call (Example 14-24).
Further Reading | 441
17. Slide 31, �A Curious Course on Coroutines and Concurrency�.
Example 14-24. �[This] seems to perform a simple abstraction over the process of
yielding� (Politz et al.)
def f():
def do_yield(n):
yield n
x = 0
while True:
x += 1
do_yield(x)
If we call f() in Example 14-24, we get an infinite loop, and not a generator, because
the yield keyword only makes the immediately enclosing function a generator function.
Although generator functions look like functions, we cannot delegate another generator
function with a simple function call. As a point of comparison, the Lua language does
not impose this limitation. A Lua coroutine can call other functions and any of them
can yield to the original caller.
The new yield from syntax was introduced to allow a Python generator or coroutine
to delegate work to another, without requiring the workaround of an inner for loop.
Example 14-24 can be �fixed� by prefixing the function call with yield from, as in
Example 14-25.
Example 14-25. This actually performs a simple abstraction over the process of
yielding
def f():
def do_yield(n):
yield n
x = 0
while True:
x += 1
yield from do_yield(x)
Reusing def for declaring generators was a usability mistake, and the problem was
compounded in Python 2.5 with coroutines, which are also coded as functions with
yield. In the case of coroutines, the yield just happens to appear�usually�on the
righthand side of an assignment, because it receives the argument of the .send() call
from the client. As David Beazley says:
Despite some similarities, generators and coroutines are basically two different concepts.
17
I believe coroutines also deserved their own keyword. As we�ll see later, coroutines are
often used with special decorators, which do set them apart from other functions. But
generator functions are not decorated as frequently, so we have to scan their bodies for
yield to realize they are not functions at all, but a completely different beast.
442 | Chapter 14: Iterables, Iterators, and Generators
It can be argued that, because those features were made to work with little additional
syntax, extra syntax would be merely �syntactic sugar.� I happen to like syntactic sugar
when it makes features that are different look different. The lack of syntactic sugar is
the main reason why Lisp code is hard to read: every language construct in Lisp looks
like a function call.
Semantics of Generator Versus Iterator
There are at least three ways of thinking about the relationship between iterators and
generators.
The first is the interface viewpoint. The Python iterator protocol defines two methods:
__next__ and __iter__. Generator objects implement both, so from this perspective,
every generator is an iterator. By this definition, objects created by the enumerate()
built-in are iterators:
>>> from collections import abc
>>> e = enumerate('ABC')
>>> isinstance(e, abc.Iterator)
True
The second is the implementation viewpoint. From this angle, a generator is a Python
language construct that can be coded in two ways: as a function with the yield keyword
or as a generator expression. The generator objects resulting from calling a generator
function or evaluating a generator expression are instances of an internal Generator
Type. From this perspective, every generator is also an iterator, because Generator
Type instances implement the iterator interface. But you can write an iterator that is not
a generator�by implementing the classic Iterator pattern, as we saw in Example 14-4,
or by coding an extension in C. The enumerate objects are not generators from this
perspective:
>>> import types
>>> e = enumerate('ABC')
>>> isinstance(e, types.GeneratorType)
False
This happens because types.GeneratorType is defined as �The type of generatoriterator
objects, produced by calling a generator function.�
The third is the conceptual viewpoint. In the classic Iterator design pattern�as defined
in the GoF book�the iterator traverses a collection and yields items from it. The iterator
may be quite complex; for example, it may navigate through a tree-like data structure.
But, however much logic is in a classic iterator, it always reads values from an existing
data source, and when you call next(it), the iterator is not expected to change the item
it gets from the source; it�s supposed to just yield it as is.
In contrast, a generator may produce values without necessarily traversing a collection,
like range does. And even if attached to a collection, generators are not limited to yielding
just the items in it, but may yield some other values derived from them. A clear
example of this is the enumerate function. By the original definition of the design pat?
Further Reading | 443
tern, the generator returned by enumerate is not an iterator because it creates the tuples
it yields.
At this conceptual level, the implementation technique is irrelevant. You can write a
generator without using a Python generator object. Example 14-26 is a Fibonacci generator
I wrote just to make this point.
Example 14-26. fibo_by_hand.py: Fibonacci generator without GeneratorType instances
class Fibonacci:
def __iter__(self):
return FibonacciGenerator()
class FibonacciGenerator:
def __init__(self):
self.a = 0
self.b = 1
def __next__(self):
result = self.a
self.a, self.b = self.b, self.a + self.b
return result
def __iter__(self):
return self
Example 14-26 works but is just a silly example. Here is the Pythonic Fibonacci generator:
def fibonacci():
a, b = 0, 1
while True:
yield a
a, b = b, a + b
And of course, you can always use the generator language construct to perform the basic
duties of an iterator: traversing a collection and yielding items from it.
In reality, Python programmers are not strict about this distinction: generators are also
called iterators, even in the official docs. The canonical definition of an iterator in the
Python Glossary is so general it encompasses both iterators and generators:
Iterator: An object representing a stream of data. [�]
The full definition of iterator in the Python Glossary is worth reading. On the other
hand, the definition of generator there treats iterator and generator as synonyms, and
uses the word �generator� to refer both to the generator function and the generator
444 | Chapter 14: Iterables, Iterators, and Generators
18. Gamma et. al., Design Patterns: Elements of Reusable Object-Oriented Software, p. 261.
object it builds. So, in the Python community lingo, iterator and generator are fairly
close synonyms.
The Minimalistic Iterator Interface in Python
In the �Implementation� section of the Iterator pattern,18 the Gang of Four wrote:
The minimal interface to Iterator consists of the operations First, Next, IsDone, and
CurrentItem.
However, that very sentence has a footnote which reads:
We can make this interface even smaller by merging Next, IsDone, and CurrentItem
into a single operation that advances to the next object and returns it. If the traversal is
finished, then this operation returns a special value (0, for instance) that marks the end
of the iteration.
This is close to what we have in Python: the single method __next__ does the job. But
instead of using a sentinel, which could be overlooked by mistake, the StopIteration
exception signals the end of the iteration. Simple and correct: that�s the Python way.
Further Reading | 445

1. PyCon US 2013 keynote: �What Makes Python Awesome�; the part about with starts at 23:00 and ends at
26:15.
CHAPTER 15
Context Managers and else Blocks
Context managers may end up being almost as important as the subroutine itself. We�ve
only scratched the surface with them. [�] Basic has a with statement, there are with
statements in lots of languages. But they don�t do the same thing, they all do something
very shallow, they save you from repeated dotted [attribute] lookups, they don�t do setup
and tear down. Just because it�s the same name don�t think it�s the same thing. The with
statement is a very big deal.1
� Raymond Hettinger
Eloquent Python evangelist
In this chapter, we will discuss control flow features that are not so common in other
languages, and for this reason tend to be overlooked or underused in Python. They are:
� The with statement and context managers
� The else clause in for, while, and try statements
The with statement sets up a temporary context and reliably tears it down, under the
control of a context manager object. This prevents errors and reduces boilerplate code,
making APIs at the same time safer and easier to use. Python programmers are finding
lots of uses for with blocks beyond automatic file closing.
The else clause is completely unrelated to with. But this is Part V, and I couldn�t find
another place for covering else, and I wouldn�t have a one-page chapter about it, so
here it is.
Let�s review the smaller topic to get to the real substance of this chapter.
447
Do This, Then That: else Blocks Beyond if
This is no secret, but it is an underappreciated language feature: the else clause can be
used not only in if statements but also in for, while, and try statements.
The semantics of for/else, while/else, and try/else are closely related, but very
different from if/else. Initially the word else actually hindered my understanding of
these features, but eventually I got used to it.
Here are the rules:
for
The else block will run only if and when the for loop runs to completion (i.e., not
if the for is aborted with a break).
while
The else block will run only if and when the while loop exits because the condition
became falsy (i.e., not when the while is aborted with a break).
try
The else block will only run if no exception is raised in the try block. The official
docs also state: �Exceptions in the else clause are not handled by the preceding
except clauses.�
In all cases, the else clause is also skipped if an exception or a return, break, or
continue statement causes control to jump out of the main block of the compound
statement.
I think else is a very poor choice for the keyword in all cases
except if. It implies an excluding alternative, like �Run this loop,
otherwise do that,� but the semantics for else in loops is the
opposite: �Run this loop, then do that.� This suggests then as a
better keyword�which would also make sense in the try context:
�Try this, then do that.� However, adding a new keyword is
a breaking change to the language, and Guido avoids it like the
plague.
Using else with these statements often makes the code easier to read and saves the
trouble of setting up control flags or adding extra if statements.
The use of else in loops generally follows the pattern of this snippet:
for item in my_list:
if item.flavor == 'banana':
break
else:
raise ValueError('No banana flavor found!')
448 | Chapter 15: Context Managers and else Blocks
In the case of try/except blocks, else may seem redundant at first. After all, the
after_call() in the following snippet will run only if the dangerous_call() does not
raise an exception, correct?
try:
dangerous_call()
after_call()
except OSError:
log('OSError...')
However, doing so puts the after_call() inside the try block for no good reason. For
clarity and correctness, the body of a try block should only have the statements that
may generate the expected exceptions. This is much better:
try:
dangerous_call()
except OSError:
log('OSError...')
else:
after_call()
Now it�s clear that the try block is guarding against possible errors in dangerous_call()
and not in after_call(). It�s also more obvious that after_call() will only execute if
no exceptions are raised in the try block.
In Python, try/except is commonly used for control flow, and not just for error handling.
There�s even an acronym/slogan for that documented in the official Python glossary:
EAFP
Easier to ask for forgiveness than permission. This common Python coding style
assumes the existence of valid keys or attributes and catches exceptions if the assumption
proves false. This clean and fast style is characterized by the presence of
many try and except statements. The technique contrasts with the LBYL style common
to many other languages such as C.
The glossary then defines LBYL:
LBYL
Look before you leap. This coding style explicitly tests for pre-conditions before
making calls or lookups. This style contrasts with the EAFP approach and is characterized
by the presence of many if statements. In a multi-threaded environment,
the LBYL approach can risk introducing a race condition between �the looking� and
�the leaping�. For example, the code, if key in mapping: return mapping[key] can
fail if another thread removes key from mapping after the test, but before the lookup.
This issue can be solved with locks or by using the EAFP approach.
Given the EAFP style, it makes even more sense to know and use well else blocks in
try/except statements.
Now let�s address the main topic of this chapter: the powerful with statement.
Do This, Then That: else Blocks Beyond if | 449
2. with blocks don�t define a new scope, as functions and modules do.
Context Managers and with Blocks
Context manager objects exist to control a with statement, just like iterators exist to
control a for statement.
The with statement was designed to simplify the try/finally pattern, which guarantees
that some operation is performed after a block of code, even if the block is aborted
because of an exception, a return or sys.exit() call. The code in the finally clause
usually releases a critical resource or restores some previous state that was temporarily
changed.
The context manager protocol consists of the __enter__ and __exit__ methods. At the
start of the with, __enter__ is invoked on the context manager object. The role of the
finally clause is played by a call to __exit__ on the context manager object at the end
of the with block.
The most common example is making sure a file object is closed. See Example 15-1 for
a detailed demonstration of using with to close a file.
Example 15-1. Demonstration of a file object as a context manager
>>> with open('mirror.py') as fp: #
... src = fp.read(60) #
...
>>> len(src)
60
>>> fp #
<_io.TextIOWrapper name='mirror.py' mode='r' encoding='UTF-8'>
>>> fp.closed, fp.encoding #
(True, 'UTF-8')
>>> fp.read(60) #
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ValueError: I/O operation on closed file.
fp is bound to the opened file because the file�s __enter__ method returns self.
Read some data from fp.
The fp variable is still available.2
You can read the attributes of the fp object.
But you can�t perform I/O with fp because at the end of the with block, the
TextIOWrapper.__exit__ method is called and closes the file.
450 | Chapter 15: Context Managers and else Blocks
The first callout in Example 15-1 makes a subtle but crucial point: the context manager
object is the result of evaluating the expression after with, but the value bound to the
target variable (in the as clause) is the result of calling __enter__ on the context manager
object.
It just happens that in Example 15-1, the open() function returns an instance of
TextIOWrapper, and its __enter__ method returns self. But the __enter__ method
may also return some other object instead of the context manager.
When control flow exits the with block in any way, the __exit__ method is invoked on
the context manager object, not on whatever is returned by __enter__.
The as clause of the with statement is optional. In the case of open, you�ll always need
it to get a reference to the file, but some context managers return None because they
have no useful object to give back to the user.
Example 15-2 shows the operation of a perfectly frivolous context manager designed to
highlight the distinction between the context manager and the object returned by its
__enter__ method.
Example 15-2. Test driving the LookingGlass context manager class
>>> from mirror import LookingGlass
>>> with LookingGlass() as what:
... print('Alice, Kitty and Snowdrop')
... print(what)
...
pordwonS dna yttiK ,ecilA
YKCOWREBBAJ
>>> what
'JABBERWOCKY'
>>> print('Back to normal.')
Back to normal.
The context manager is an instance of LookingGlass; Python calls __enter__
on the context manager and the result is bound to what.
Print a str, then the value of the target variable what.
The output of each print comes out backward.
Now the with block is over. We can see that the value returned by __enter__,
held in what, is the string 'JABBERWOCKY'.
Program output is no longer backward.
Example 15-3 shows the implementation of LookingGlass.
Context Managers and with Blocks | 451
Example 15-3. mirror.py: code for the LookingGlass context manager class
class LookingGlass:
def __enter__(self):
import sys
self.original_write = sys.stdout.write
sys.stdout.write = self.reverse_write
return 'JABBERWOCKY'
def reverse_write(self, text):
self.original_write(text[::-1])
def __exit__(self, exc_type, exc_value, traceback):
import sys
sys.stdout.write = self.original_write
if exc_type is ZeroDivisionError:
print('Please DO NOT divide by zero!')
return True
Python invokes __enter__ with no arguments besides self.
Hold the original sys.stdout.write method in an instance attribute for later
use.
Monkey-patch sys.stdout.write, replacing it with our own method.
Return the 'JABBERWOCKY' string just so we have something to put in the target
variable what.
Our replacement to sys.stdout.write reverses the text argument and calls the
original implementation.
Python calls __exit__ with None, None, None if all went well; if an exception
is raised, the three arguments get the exception data, as described next.
It�s cheap to import modules again because Python caches them.
Restore the original method to sys.stdout.write.
If the exception is not None and its type is ZeroDivisionError, print a message�
�and return True to tell the interpreter that the exception was handled.
If __exit__ returns None or anything but True, any exception raised in the with
block will be propagated.
When real applications take over standard output, they often want
to replace sys.stdout with another file-like object for a while, then
switch back to the original. The contextlib.redirect_stdout
context manager does exactly that: just pass it the file-like object
that will stand in for sys.stdout.
452 | Chapter 15: Context Managers and else Blocks
3. The three arguments received by self are exactly what you get if you call sys.exc_info() in the finally
block of a try/finally statement. This makes sense, considering that the with statement is meant to replace
most uses of try/finally, and calling sys.exc_info() was often necessary to determine what clean-up
action would be required.
The interpreter calls the __enter__ method with no arguments�beyond the implicit
self. The three arguments passed to __exit__ are:
exc_type
The exception class (e.g., ZeroDivisionError).
exc_value
The exception instance. Sometimes, parameters passed to the exception constructor�
such as the error message�can be found in exc_value.args.
traceback
A traceback object.3
For a detailed look at how a context manager works, see Example 15-4, where Looking
Glass is used outside of a with block, so we can manually call its __enter__ and
__exit__ methods.
Example 15-4. Exercising LookingGlass without a with block
>>> from mirror import LookingGlass
>>> manager = LookingGlass()
>>> manager
<mirror.LookingGlass object at 0x2a578ac>
>>> monster = manager.__enter__()
>>> monster == 'JABBERWOCKY'
eurT
>>> monster
'YKCOWREBBAJ'
>>> manager
>ca875a2x0 ta tcejbo ssalGgnikooL.rorrim<
>>> manager.__exit__(None, None, None)
>>> monster
'JABBERWOCKY'
Instantiate and inspect the manager instance.
Call the context manager __enter__() method and store result in monster.
Monster is the string 'JABBERWOCKY'. The True identifier appears reversed
because all output via stdout goes through the write method we patched in
__enter__.
Call manager.__exit__ to restore previous stdout.write.
Context Managers and with Blocks | 453
Context managers are a fairly novel feature and slowly but surely the Python community
is finding new, creative uses for them. Some examples from the standard library are:
� Managing transactions in the sqlite3 module; see �12.6.7.3. Using the connection
as a context manager�.
� Holding locks, conditions, and semaphores in threading code; see �17.1.10. Using
locks, conditions, and semaphores in the with statement�.
� Setting up environments for arithmetic operations with Decimal objects; see the
decimal.localcontext documentation.
� Applying temporary patches to objects for testing; see the unittest.mock.patch
function.
The standard library also includes the contextlib utilities, covered next.
The contextlib Utilities
Before rolling your own context manager classes, take a look at �29.6 contextlib �
Utilities for with-statement contexts� in The Python Standard Library. Besides the already
mentioned redirect_stdout, the contextlib module includes classes and other
functions that are more widely applicable:
closing
A function to build context managers out of objects that provide a close() method
but don�t implement the __enter__/__exit__ protocol.
suppress
A context manager to temporarily ignore specified exceptions.
@contextmanager
A decorator that lets you build a context manager from a simple generator function,
instead of creating a class and implementing the protocol.
ContextDecorator
A base class for defining class-based context managers that can also be used as
function decorators, running the entire function within a managed context.
ExitStack
A context manager that lets you enter a variable number of context managers. When
the with block ends, ExitStack calls the stacked context managers� __exit__
methods in LIFO order (last entered, first exited). Use this class when you don�t
know beforehand how many context managers you need to enter in your with
block; for example, when opening all files from an arbitrary list of files at the same
time.
454 | Chapter 15: Context Managers and else Blocks
The most widely used of these utilities is surely the @contextmanager decorator, so it
deserves more attention. That decorator is also intriguing because it shows a use for the
yield statement unrelated to iteration. This paves the way to the concept of a coroutine,
the theme of the next chapter.
Using @contextmanager
The @contextmanager decorator reduces the boilerplate of creating a context manager:
instead of writing a whole class with __enter__/__exit__ methods, you just implement
a generator with a single yield that should produce whatever you want the __en
ter__ method to return.
In a generator decorated with @contextmanager, yield is used to split the body of the
function in two parts: everything before the yield will be executed at the beginning of
the while block when the interpreter calls __enter__; the code after yield will run
when __exit__ is called at the end of the block.
Here is an example. Example 15-5 replaces the LookingGlass class from Example 15-3
with a generator function.
Example 15-5. mirror_gen.py: a context manager implemented with a generator
import contextlib
@contextlib.contextmanager
def looking_glass():
import sys
original_write = sys.stdout.write
def reverse_write(text):
original_write(text[::-1])
sys.stdout.write = reverse_write
yield 'JABBERWOCKY'
sys.stdout.write = original_write
Apply the contextmanager decorator.
Preserve original sys.stdout.write method.
Define custom reverse_write function; original_write will be available in the
closure.
Replace sys.stdout.write with reverse_write.
Yield the value that will be bound to the target variable in the as clause of the
with statement. This function pauses at this point while the body of the with
executes.
Using @contextmanager | 455
4. The actual class is named _GeneratorContextManager. If you want to see exactly how it works, read its
source code in Lib/contextlib.py in the Python 3.4 distribution.
When control exits the with block in any way, execution continues after the
yield; here the original sys.stdout.write is restored.
Example 15-6 shows the looking_glass function in operation.
Example 15-6. Test driving the looking_glass context manager function
>>> from mirror_gen import looking_glass
>>> with looking_glass() as what:
... print('Alice, Kitty and Snowdrop')
... print(what)
...
pordwonS dna yttiK ,ecilA
YKCOWREBBAJ
>>> what
'JABBERWOCKY'
The only difference from Example 15-2 is the name of the context manager:
looking_glass instead of LookingGlass.
Essentially the contextlib.contextmanager decorator wraps the function in a class
that implements the __enter__ and __exit__ methods.4
The __enter__ method of that class:
1. Invokes the generator function and holds on to the generator object�let�s call it gen.
2. Calls next(gen) to make it run to the yield keyword.
3. Returns the value yielded by next(gen), so it can be bound to a target variable in
the with/as form.
When the with block terminates, the __exit__ method:
1. Checks an exception was passed as exc_type; if so, gen.throw(exception) is invoked,
causing the exception to be raised in the yield line inside the generator
function body.
2. Otherwise, next(gen) is called, resuming the execution of the generator function
body after the yield.
Example 15-5 has a serious flaw: if an exception is raised in the body of the with block,
the Python interpreter will catch it and raise it again in the yield expression inside
looking_glass. But there is no error handling there, so the looking_glass function
456 | Chapter 15: Context Managers and else Blocks
5. The exception is sent into the generator using the throw method, covered in �Coroutine Termination and
Exception Handling� on page 471.
will abort without ever restoring the original sys.stdout.write method, leaving the
system in an invalid state.
Example 15-7 adds special handling of the ZeroDivisionError exception, making it
functionally equivalent to the class-based Example 15-3.
Example 15-7. mirror_gen_exc.py: generator-based context manager implementing exception
handling�same external behavior as Example 15-3
import contextlib
@contextlib.contextmanager
def looking_glass():
import sys
original_write = sys.stdout.write
def reverse_write(text):
original_write(text[::-1])
sys.stdout.write = reverse_write
msg = ''
try:
yield 'JABBERWOCKY'
except ZeroDivisionError:
msg = 'Please DO NOT divide by zero!'
finally:
sys.stdout.write = original_write
if msg:
print(msg)
Create a variable for a possible error message; this is the first change in relation
to Example 15-5.
Handle ZeroDivisionError by setting an error message.
Undo monkey-patching of sys.stdout.write.
Display error message, if it was set.
Recall that the __exit__ method tells the interpreter that it has handled the exception
by returning True; in that case, the interpreter suppresses the exception. On the other
hand, if __exit__ does not explicitly return a value, the interpreter gets the usual
None, and propagates the exception. With @contextmanager, the default behavior is
inverted: the __exit__ method provided by the decorator assumes any exception sent
into the generator is handled and should be suppressed.5 You must explicitly re-raise an
Using @contextmanager | 457
6. This convention was adopted because when context managers were created, generators could not return
values, only yield. They now can, as explained in �Returning a Value from a Coroutine� on page 475. As you�ll
see, returning a value from a generator does involve an exception.
7. This tip is quoted literally from a comment by Leonardo Rochael, one of the tech reviewers for this book.
Nicely said, Leo!
exception in the decorated function if you don�t want @contextmanager to suppress it.
6
Having a try/finally (or a with block) around the yield is an
unavoidable price of using @contextmanager, because you never
know what the users of your context manager are going to do inside
their with block.7
An interesting real-life example of @contextmanager outside of the standard library is
Martijn Pieters� in-place file rewriting context manager. Example 15-8 shows how it�s
used.
Example 15-8. A context manager for rewriting files in place
import csv
with inplace(csvfilename, 'r', newline='') as (infh, outfh):
reader = csv.reader(infh)
writer = csv.writer(outfh)
for row in reader:
row += ['new', 'columns']
writer.writerow(row)
The inplace function is a context manager that gives you two handles�infh and outfh
in the example�to the same file, allowing your code to read and write to it at the same
time. It�s easier to use than the standard library�s fileinput.input function (which also
provides a context manager, by the way).
If you want to study Martijn�s inplace source code (listed in the post), find the yield
keyword: everything before it deals with setting up the context, which entails creating
a backup file, then opening and yielding references to the readable and writable file
handles that will be returned by the __enter__ call. The __exit__ processing after the
yield closes the file handles and restores the file from the backup if something went
wrong.
Note that the use of yield in a generator used with the @contextmanager decorator has
nothing to do with iteration. In the examples shown in this section, the generator function
is operating more like a coroutine: a procedure that runs up to a point, then sus?
458 | Chapter 15: Context Managers and else Blocks
pends to let the client code run until the client wants the coroutine to proceed with its
job. Chapter 16 is all about coroutines.
Chapter Summary
This chapter started easily enough with discussion of else blocks in for, while, and
try statements. Once you get used to the peculiar meaning of the else clause in these
statements, I believe else can clarify your intentions.
We then covered context managers and the meaning of the with statement, quickly
moving beyond its common use to automatically close opened files. We implemented
a custom context manager: the LookingGlass class with the __enter__/__exit__
methods, and saw how to handle exceptions in the __exit__ method. A key point that
Raymond Hettinger made in his PyCon US 2013 keynote is that with is not just for
resource management, but it�s a tool for factoring out common setup and teardown
code, or any pair of operations that need to be done before and after another procedure
(slide 21, What Makes Python Awesome?).
Finally, we reviewed functions in the contextlib standard library module. One of them,
the @contextmanager decorator, makes it possible to implement a context manager
using a simple generator with one yield�a leaner solution than coding a class with at
least two methods. We reimplemented the LookingGlass as a looking_glass generator
function, and discussed how to do exception handling when using @contextmanager.
The @contextmanager decorator is an elegant and practical tool that brings together
three distinctive Python features: a function decorator, a generator, and the with statement.
Further Reading
Chapter 8, �Compound Statements,� in The Python Language Reference says pretty much
everything there is to say about else clauses in if, for, while, and try statements.
Regarding Pythonic usage of try/except, with or without else, Raymond Hettinger
has a brilliant answer to the question �Is it a good practice to use try-except-else in
Python?� in StackOverflow. Alex Martelli�s Python in a Nutshell, 2E (O�Reilly), has a
chapter about exceptions with an excellent discussion of the EAFP style, crediting computing
pioneer Grace Hopper for coining the phrase �It�s easier to ask forgiveness than
permission.�
The Python Standard Library, Chapter 4, �Built-in Types,� has a section devoted to
Context Manager Types. The __enter__/__exit__ special methods are also documented
in The Python Language Reference in �3.3.8. With Statement Context Managers�.
Context managers were introduced in PEP 343 � The �with� Statement. This PEP
Chapter Summary | 459
is not easy reading because it spends a lot of time covering corner cases and arguing
against alternative proposals. That�s the nature of PEPs.
Raymond Hettinger highlighted the with statement as a �winning language feature� in
his PyCon US 2013 keynote. He also showed some interesting applications of context
managers in his talk �Transforming Code into Beautiful, Idiomatic Python� at the same
conference.
Jeff Preshing� blog post �The Python with Statement by Example� is interesting for the
examples using context managers with the pycairo graphics library.
Beazley and Jones devised context managers for very different purposes in their Python
Cookbook, 3E (O�Reilly). �Recipe 8.3. Making Objects Support the Context-
Management Protocol� implements a LazyConnection class whose instances are context
managers that open and close network connections automatically in with blocks.
�Recipe 9.22. Defining Context Managers the Easy Way� introduces a context manager
for timing code, and another for making transactional changes to a list object: within
the with block, a working copy of the list instance is made, and all changes are applied
to that working copy. Only when the with block completes without an exception, the
working copy replaces the original list. Simple and ingenious.
Soapbox
Factoring Out the Bread
In his PyCon US 2013 keynote, �What Makes Python Awesome,� Raymond Hettinger
says when he first saw the with statement proposal he thought it was �a little bit arcane.�
Initially, I had a similar reaction. PEPs are often hard to read, and PEP 343 is typical in
that regard.
Then�Hettinger told us�he had an insight: subroutines are the most important invention
in the history of computer languages. If you have sequences of operations like
A;B;C and P;B;Q, you can factor out B in a subroutine. It�s like factoring out the filling
in a sandwich: using tuna with different breads. But what if you want to factor out the
bread, to make sandwiches with wheat bread, using a different filling each time? That�s
what the with statement offers. It�s the complement of the subroutine. Hettinger went
on to say:
The with statement is a very big deal. I encourage you to go out and take this tip of the
iceberg and drill deeper. You can probably do profound things with the with statement.
The best uses of it have not been discovered yet. I expect that if you make good use of
it, it will be copied into other languages and all future languages will have it. You can
be part of discovering something almost as profound as the invention of the subroutine
itself.
460 | Chapter 15: Context Managers and else Blocks
Hettinger admits he is overselling the with statement. Nevertheless, it is a very useful
feature. When he used the sandwich analogy to explain how with is the complement to
the subroutine, many possibilities opened up in my mind.
If you need to convince anyone that Python is awesome, you should watch Hettinger�s
keynote. The bit about context managers is from 23:00 to 26:15. But the entire keynote
is excellent.
Further Reading | 461

CHAPTER 16
Coroutines
If Python books are any guide, [coroutines are] the most poorly documented, obscure,
and apparently useless feature of Python.
� David Beazley
Python author
We find two main senses for the verb �to yield� in dictionaries: to produce or to give
way. Both senses apply in Python when we use the yield keyword in a generator. A line
such as yield item produces a value that is received by the caller of next(�), and it
also gives way, suspending the execution of the generator so that the caller may proceed
until it�s ready to consume another value by invoking next() again. The caller pulls
values from the generator.
A coroutine is syntactically like a generator: just a function with the yield keyword in
its body. However, in a coroutine, yield usually appears on the right side of an expression
(e.g., datum = yield), and it may or may not produce a value�if there is no
expression after the yield keyword, the generator yields None. The coroutine may receive
data from the caller, which uses .send(datum) instead of next(�) to feed the
coroutine. Usually, the caller pushes values into the coroutine.
It is even possible that no data goes in or out through the yield keyword. Regardless of
the flow of data, yield is a control flow device that can be used to implement cooperative
multitasking: each coroutine yields control to a central scheduler so that other coroutines
can be activated.
When you start thinking of yield primarily in terms of control flow, you have the
mindset to understand coroutines.
Python coroutines are the product of a series of enhancements to the humble generator
functions we�ve seen so far in the book. Following the evolution of coroutines in Python
helps understand their features in stages of increasing functionality and complexity.
463
After a brief overview of how generators were enable to act as a coroutine, we jump to
the core of the chapter. Then we�ll see:
� The behavior and states of a generator operating as a coroutine
� Priming a coroutine automatically with a decorator
� How the caller can control a coroutine through the .close() and .throw(�)
methods of the generator object
� How coroutines can return values upon termination
� Usage and semantics of the new yield from syntax
� A use case: coroutines for managing concurrent activities in a simulation
How Coroutines Evolved from Generators
The infrastructure for coroutines appeared in PEP 342 � Coroutines via Enhanced
Generators, implemented in Python 2.5 (2006): since then, the yield keyword can be
used in an expression, and the .send(value) method was added to the generator API.
Using .send(�), the caller of the generator can post data that then becomes the value
of the yield expression inside the generator function. This allows a generator to be used
as a coroutine: a procedure that collaborates with the caller, yielding and receiving values
from the caller.
In addition to .send(�), PEP 342 also added .throw(�) and .close() methods that
respectively allow the caller to throw an exception to be handled inside the generator,
and to terminate it. These features are covered in the next section and in �Coroutine
Termination and Exception Handling� on page 471.
The latest evolutionary step for coroutines came with PEP 380 - Syntax for Delegating
to a Subgenerator, implemented in Python 3.3 (2012). PEP 380 made two syntax changes
to generator functions, to make them more useful as coroutines:
� A generator can now return a value; previously, providing a value to the return
statement inside a generator raised a SyntaxError.
� The yield from syntax enables complex generators to be refactored into smaller,
nested generators while avoiding a lot of boilerplate code previously required for a
generator to delegate to subgenerators.
These latest changes will be addressed in �Returning a Value from a Coroutine� on page
475 and �Using yield from� on page 477.
Let�s follow the established tradition of Fluent Python and start with some very basic
facts and examples, then move into increasingly mind-bending features.
464 | Chapter 16: Coroutines
1. You�ll only see this state in a multithreaded application�or if the generator object calls getgenerator
state on itself, which is not useful.
Basic Behavior of a Generator Used as a Coroutine
Example 16-1 illustrates the behavior of a coroutine.
Example 16-1. Simplest possible demonstration of coroutine in action
>>> def simple_coroutine(): #
... print('-> coroutine started')
... x = yield #
... print('-> coroutine received:', x)
...
>>> my_coro = simple_coroutine()
>>> my_coro #
<generator object simple_coroutine at 0x100c2be10>
>>> next(my_coro) #
-> coroutine started
>>> my_coro.send(42) #
-> coroutine received: 42
Traceback (most recent call last): #
...
StopIteration
A coroutine is defined as a generator function: with yield in its body.
yield is used in an expression; when the coroutine is designed just to receive
data from the client it yields None�this is implicit because there is no expression
to the right of the yield keyword.
As usual with generators, you call the function to get a generator object back.
The first call is next(�) because the generator hasn�t started so it�s not waiting
in a yield and we can�t send it any data initially.
This call makes the yield in the coroutine body evaluate to 42; now the coroutine
resumes and runs until the next yield or termination.
In this case, control flows off the end of the coroutine body, which prompts the
generator machinery to raise StopIteration, as usual.
A coroutine can be in one of four states. You can determine the current state using the
inspect.getgeneratorstate(�) function, which returns one of these strings:
'GEN_CREATED'
Waiting to start execution.
'GEN_RUNNING'
Currently being executed by the interpreter.1
Basic Behavior of a Generator Used as a Coroutine | 465
'GEN_SUSPENDED'
Currently suspended at a yield expression.
'GEN_CLOSED'
Execution has completed.
Because the argument to the send method will become the value of the pending yield
expression, it follows that you can only make a call like my_coro.send(42) if the coroutine
is currently suspended. But that�s not the case if the coroutine has never been
activated�when its state is 'GEN_CREATED'. That�s why the first activation of a coroutine
is always done with next(my_coro)�you can also call my_coro.send(None), and the
effect is the same.
If you create a coroutine object and immediately try to send it a value that is not None,
this is what happens:
>>> my_coro = simple_coroutine()
>>> my_coro.send(1729)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: can't send non-None value to a just-started generator
Note the error message: it�s quite clear.
The initial call next(my_coro) is often described as �priming� the coroutine (i.e., advancing
it to the first yield to make it ready for use as a live coroutine).
To get a better feel for the behavior of a coroutine, an example that yields more than
once is useful. See Example 16-2.
Example 16-2. A coroutine that yields twice
>>> def simple_coro2(a):
... print('-> Started: a =', a)
... b = yield a
... print('-> Received: b =', b)
... c = yield a + b
... print('-> Received: c =', c)
...
>>> my_coro2 = simple_coro2(14)
>>> from inspect import getgeneratorstate
>>> getgeneratorstate(my_coro2)
'GEN_CREATED'
>>> next(my_coro2)
-> Started: a = 14
14
>>> getgeneratorstate(my_coro2)
'GEN_SUSPENDED'
>>> my_coro2.send(28)
-> Received: b = 28
42
466 | Chapter 16: Coroutines
>>> my_coro2.send(99)
-> Received: c = 99
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
StopIteration
>>> getgeneratorstate(my_coro2)
'GEN_CLOSED'
inspect.getgeneratorstate reports GEN_CREATED (i.e., the coroutine has not
started).
Advance coroutine to first yield, printing -> Started: a = 14 message then
yielding value of a and suspending to wait for value to be assigned to b.
getgeneratorstate reports GEN_SUSPENDED (i.e., the coroutine is paused at a
yield expression).
Send number 28 to suspended coroutine; the yield expression evaluates to 28
and that number is bound to b. The -> Received: b = 28 message is displayed,
the value of a + b is yielded (42), and the coroutine is suspended waiting for
the value to be assigned to c.
Send number 99 to suspended coroutine; the yield expression evaluates to 99
the number is bound to c. The -> Received: c = 99 message is displayed, then
the coroutine terminates, causing the generator object to raise StopIteration.
getgeneratorstate reports GEN_CLOSED (i.e., the coroutine execution has
completed).
It�s crucial to understand that the execution of the coroutine is suspended exactly at the
yield keyword. As mentioned before, in an assignment statement, the code to the right
of the = is evaluated before the actual assignment happens. This means that in a line like
b = yield a, the value of b will only be set when the coroutine is activated later by the
client code. It takes some effort to get used to this fact, but understanding it is essential
to make sense of the use of yield in asynchronous programming, as we�ll see later.
Execution of the simple_coro2 coroutine can be split in three phases, as shown in
Figure 16-1:
1. next(my_coro2) prints first message and runs to yield a, yielding number 14.
2. my_coro2.send(28) assigns 28 to b, prints second message, and runs to yield a +
b, yielding number 42.
3. my_coro2.send(99) assigns 99 to c, prints third message, and the coroutine terminates.
Basic Behavior of a Generator Used as a Coroutine | 467
2. This example is inspired by a snippet from Jacob Holm in the Python-ideas list, message titled �Yield-From:
Finalization guarantees.� Some variations appear later in the thread, and Holm further explains his thinking
in message 003912.
Figure 16-1. Three phases in the execution of the simple_coro2 coroutine (note that
each phase ends in a yield expression, and the next phase starts in the very same line,
when the value of the yield expression is assigned to a variable)
Now let�s consider a slightly more involved coroutine example.
Example: Coroutine to Compute a Running Average
While discussing closures in Chapter 7, we studied objects to compute a running average:
Example 7-8 shows a plain class and Example 7-14 presents a higher-order function
producing a closure to keep the total and count variables across invocations.
Example 16-3 shows how to do the same with a coroutine.2
Example 16-3. coroaverager0.py: code for a running average coroutine
def averager():
total = 0.0
count = 0
average = None
while True:
term = yield average
total += term
count += 1
average = total/count
This infinite loop means this coroutine will keep on accepting values and
producing results as long as the caller sends them. This coroutine will only
terminate when the caller calls .close() on it, or when it�s garbage collected
because there are no more references to it.
468 | Chapter 16: Coroutines
The yield statement here is used to suspend the coroutine, produce a result to
the caller, and�later�to get a value sent by the caller to the coroutine, which
resumes its infinite loop.
The advantage of using a coroutine is that total and count can be simple local variables:
no instance attributes or closures are needed to keep the context between calls.
Example 16-4 are doctests to show the averager coroutine in operation.
Example 16-4. coroaverager0.py: doctest for the running average coroutine in
Example 16-3
>>> coro_avg = averager()
>>> next(coro_avg)
>>> coro_avg.send(10)
10.0
>>> coro_avg.send(30)
20.0
>>> coro_avg.send(5)
15.0
Create the coroutine object.
Prime it by calling next.
Now we are in business: each call to .send(�) yields the current average.
In the doctest (Example 16-4), the call next(coro_avg) makes the coroutine advance
to the yield, yielding the initial value for average, which is None, so it does not appear
on the console. At this point, the coroutine is suspended at the yield, waiting for a value
to be sent. The line coro_avg.send(10) provides that value, causing the coroutine to
activate, assigning it to term, updating the total, count, and average variables, and
then starting another iteration in the while loop, which yields the average and waits
for another term.
The attentive reader may be anxious to know how the execution of an averager instance
(e.g., coro_avg) may be terminated, because its body is an infinite loop. We�ll cover that
in �Coroutine Termination and Exception Handling� on page 471.
But before discussing coroutine termination, let�s talk about getting them started. Priming
a coroutine before use is a necessary but easy-to-forget chore. To avoid it, a special
decorator can be applied to the coroutine. One such decorator is presented next.
Decorators for Coroutine Priming
You can�t do much with a coroutine without priming it: we must always remember to
call next(my_coro) before my_coro.send(x). To make coroutine usage more conve?
Decorators for Coroutine Priming | 469
3. There are several similar decorators published on the Web. This one is adapted from the ActiveState recipe
Pipeline made of coroutines by Chaobin Tang, who in turn credits David Beazley.
nient, a priming decorator is sometimes used. The coroutine decorator in Example 16-5
is an example.3
Example 16-5. coroutil.py: decorator for priming coroutine
from functools import wraps
def coroutine(func):
"""Decorator: primes `func` by advancing to first `yield`"""
@wraps(func)
def primer(*args,**kwargs):
gen = func(*args,**kwargs)
next(gen)
return gen
return primer
The decorated generator function is replaced by this primer function which,
when invoked, returns the primed generator.
Call the decorated function to get a generator object.
Prime the generator.
Return it.
Example 16-6 shows the @coroutine decorator in use. Contrast with Example 16-3.
Example 16-6. coroaverager1.py: doctest and code for a running average coroutine using
the @coroutine decorator from Example 16-5
"""
A coroutine to compute a running average
>>> coro_avg = averager()
>>> from inspect import getgeneratorstate
>>> getgeneratorstate(coro_avg)
'GEN_SUSPENDED'
>>> coro_avg.send(10)
10.0
>>> coro_avg.send(30)
20.0
>>> coro_avg.send(5)
15.0
"""
from coroutil import coroutine
@coroutine
470 | Chapter 16: Coroutines
def averager():
total = 0.0
count = 0
average = None
while True:
term = yield average
total += term
count += 1
average = total/count
Call averager(), creating a generator object that is primed inside the primer
function of the coroutine decorator.
getgeneratorstate reports GEN_SUSPENDED, meaning that the coroutine is
ready to receive a value.
You can immediately start sending values to coro_avg: that�s the point of the
decorator.
Import the coroutine decorator.
Apply it to the averager function.
The body of the function is exactly the same as Example 16-3.
Several frameworks provide special decorators designed to work with coroutines. Not
all of them actually prime the coroutine�some provide other services, such as hooking
it to an event loop. One example from the Tornado asynchronous networking library
is the tornado.gen decorator.
The yield from syntax we�ll see in �Using yield from� on page 477 automatically primes
the coroutine called by it, making it incompatible with decorators such as @coroutine
from Example 16-5. The asyncio.coroutine decorator from the Python 3.4 standard
library is designed to work with yield from so it does not prime the coroutine. We�ll
cover it in Chapter 18.
We�ll now focus on essential features of coroutines: the methods used to terminate and
throw exceptions into them.
Coroutine Termination and Exception Handling
An unhandled exception within a coroutine propagates to the caller of the next or send
that triggered it. Example 16-7 is an example using the decorated averager coroutine
from Example 16-6.
Example 16-7. How an unhandled exception kills a coroutine
>>> from coroaverager1 import averager
>>> coro_avg = averager()
>>> coro_avg.send(40) #
Coroutine Termination and Exception Handling | 471
40.0
>>> coro_avg.send(50)
45.0
>>> coro_avg.send('spam') #
Traceback (most recent call last):
...
TypeError: unsupported operand type(s) for +=: 'float' and 'str'
>>> coro_avg.send(60) #
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
StopIteration
Using the @coroutine decorated averager we can immediately start sending
values.
Sending a nonnumeric value causes an exception inside the coroutine.
Because the exception was not handled in the coroutine, it terminated. Any
attempt to reactivate it will raise StopIteration.
The cause of the error was the sending of a value 'spam' that could not be added to the
total variable in the coroutine.
Example 16-7 suggests one way of terminating coroutines: you can use send with some
sentinel value that tells the coroutine to exit. Constant built-in singletons like None and
Ellipsis are convenient sentinel values. Ellipsis has the advantage of being quite
unusual in data streams. Another sentinel value I�ve seen used is StopIteration�the
class itself, not an instance of it (and not raising it). In other words, using it like: my_co
ro.send(StopIteration).
Since Python 2.5, generator objects have two methods that allow the client to explicitly
send exceptions into the coroutine�throw and close:
generator.throw(exc_type[, exc_value[, traceback]])
Causes the yield expression where the generator was paused to raise the exception
given. If the exception is handled by the generator, flow advances to the next
yield, and the value yielded becomes the value of the generator.throw call. If the
exception is not handled by the generator, it propagates to the context of the caller.
generator.close()
Causes the yield expression where the generator was paused to raise a Generator
Exit exception. No error is reported to the caller if the generator does not handle
that exception or raises StopIteration�usually by running to completion. When
receiving a GeneratorExit, the generator must not yield a value, otherwise a Run
timeError is raised. If any other exception is raised by the generator, it propagates
to the caller.
472 | Chapter 16: Coroutines
The official documentation of the generator object methods is
buried deep in The Python Language Reference, (see 6.2.9.1.
Generator-iterator methods).
Let�s see how close and throw control a coroutine. Example 16-8 lists the demo_exc_han
dling function used in the following examples.
Example 16-8. coro_exc_demo.py: test code for studying exception handling in a coroutine
class DemoException(Exception):
"""An exception type for the demonstration."""
def demo_exc_handling():
print('-> coroutine started')
while True:
try:
x = yield
except DemoException:
print('*** DemoException handled. Continuing...')
else:
print('-> coroutine received: {!r}'.format(x))
raise RuntimeError('This line should never run.')
Special handling for DemoException.
If no exception, display received value.
This line will never be executed.
The last line in Example 16-8 is unreachable because the infinite loop can only be aborted
with an unhandled exception, and that terminates the coroutine immediately.
Normal operation of demo_exc_handling is shown in Example 16-9.
Example 16-9. Activating and closing demo_exc_handling without an exception
>>> exc_coro = demo_exc_handling()
>>> next(exc_coro)
-> coroutine started
>>> exc_coro.send(11)
-> coroutine received: 11
>>> exc_coro.send(22)
-> coroutine received: 22
>>> exc_coro.close()
>>> from inspect import getgeneratorstate
>>> getgeneratorstate(exc_coro)
'GEN_CLOSED'
Coroutine Termination and Exception Handling | 473
If the DemoException is thrown into the coroutine, it�s handled and the demo_exc_han
dling coroutine continues, as in Example 16-10.
Example 16-10. Throwing DemoException into demo_exc_handling does not break it
>>> exc_coro = demo_exc_handling()
>>> next(exc_coro)
-> coroutine started
>>> exc_coro.send(11)
-> coroutine received: 11
>>> exc_coro.throw(DemoException)
*** DemoException handled. Continuing...
>>> getgeneratorstate(exc_coro)
'GEN_SUSPENDED'
On the other hand, if an unhandled exception is thrown into the coroutine, it stops�
its state becomes 'GEN_CLOSED'. Example 16-11 demonstrates it.
Example 16-11. Coroutine terminates if it can�t handle an exception thrown into it
>>> exc_coro = demo_exc_handling()
>>> next(exc_coro)
-> coroutine started
>>> exc_coro.send(11)
-> coroutine received: 11
>>> exc_coro.throw(ZeroDivisionError)
Traceback (most recent call last):
...
ZeroDivisionError
>>> getgeneratorstate(exc_coro)
'GEN_CLOSED'
If it�s necessary that some cleanup code is run no matter how the coroutine ends, you
need to wrap the relevant part of the coroutine body in a try/finally block, as in
Example 16-12.
Example 16-12. coro_finally_demo.py: use of try/finally to perform actions on coroutine
termination
class DemoException(Exception):
"""An exception type for the demonstration."""
def demo_finally():
print('-> coroutine started')
try:
while True:
try:
x = yield
except DemoException:
print('*** DemoException handled. Continuing...')
else:
474 | Chapter 16: Coroutines
print('-> coroutine received: {!r}'.format(x))
finally:
print('-> coroutine ending')
One of the main reasons why the yield from construct was added to Python 3.3 has to
do with throwing exceptions into nested coroutines. The other reason was to enable
coroutines to return values more conveniently. Read on to see how.
Returning a Value from a Coroutine
Example 16-13 shows a variation of the averager coroutine that returns a result. For
didactic reasons, it does not yield the running average with each activation. This is to
emphasize that some coroutines do not yield anything interesting, but are designed to
return a value at the end, often the result of some accumulation.
The result returned by averager in Example 16-13 is a namedtuple with the number of
terms averaged (count) and the average. I could have returned just the average value,
but returning a tuple exposes another interesting piece of data that was accumulated:
the count of terms.
Example 16-13. coroaverager2.py: code for an averager coroutine that returns a result
from collections import namedtuple
Result = namedtuple('Result', 'count average')
def averager():
total = 0.0
count = 0
average = None
while True:
term = yield
if term is None:
break
total += term
count += 1
average = total/count
return Result(count, average)
In order to return a value, a coroutine must terminate normally; this is why this
version of averager has a condition to break out of its accumulating loop.
Return a namedtuple with the count and average. Before Python 3.3, it was a
syntax error to return a value in a generator function.
To see how this new averager works, we can drive it from the console, as in
Example 16-14.
Returning a Value from a Coroutine | 475
Example 16-14. coroaverager2.py: doctest showing the behavior of averager
>>> coro_avg = averager()
>>> next(coro_avg)
>>> coro_avg.send(10)
>>> coro_avg.send(30)
>>> coro_avg.send(6.5)
>>> coro_avg.send(None)
Traceback (most recent call last):
...
StopIteration: Result(count=3, average=15.5)
This version does not yield values.
Sending None terminates the loop, causing the coroutine to end by returning the
result. As usual, the generator object raises StopIteration. The value attribute
of the exception carries the value returned.
Note that the value of the return expression is smuggled to the caller as an attribute of
the StopIteration exception. This is a bit of a hack, but it preserves the existing behavior
of generator objects: raising StopIteration when exhausted.
Example 16-15 shows how to retrieve the value returned by the coroutine.
Example 16-15. Catching StopIteration lets us get the value returned by averager
>>> coro_avg = averager()
>>> next(coro_avg)
>>> coro_avg.send(10)
>>> coro_avg.send(30)
>>> coro_avg.send(6.5)
>>> try:
... coro_avg.send(None)
... except StopIteration as exc:
... result = exc.value
...
>>> result
Result(count=3, average=15.5)
This roundabout way of getting the return value from a coroutine makes more sense
when we realize it was defined as part of PEP 380, and the yield from construct handles
it automatically by catching StopIteration internally. This is analogous to the use of
StopIteration in for loops: the exception is handled by the loop machinery in a way
that is transparent to the user. In the case of yield from, the interpreter not only consumes
the StopIteration, but its value attribute becomes the value of the yield
from expression itself. Unfortunately we can�t test this interactively in the console, be?
476 | Chapter 16: Coroutines
4. There is an iPython extension called ipython-yf that enables evaluating yield from directly in the iPython
console. It�s used to test asynchronous code and works with asyncio. It was submitted as a patch to Python
3.5 but was not accepted. See Issue #22412: Towards an asyncio-enabled command line in the Python bug
tracker.
5. As I write this, there is an open PEP proposing the addition of await and async keywords: PEP 492 �
Coroutines with async and await syntax.
cause it�s a syntax error to use yield from�or yield, for that matter�outside of a
function.4
The next section has an example where the averager coroutine is used with yield
from to produce a result, as intended in PEP 380. So let�s tackle yield from.
Using yield from
The first thing to know about yield from is that it is a completely new language construct.
It does so much more than yield that the reuse of that keyword is arguably
misleading. Similar constructs in other languages are called await, and that is a much
better name because it conveys a crucial point: when a generator gen calls yield from
subgen(), the subgen takes over and will yield values to the caller of gen; the caller will
in effect drive subgen directly. Meanwhile gen will be blocked, waiting until subgen
terminates.5
We�ve seen in Chapter 14 that yield from can be used as a shortcut to yield in a for
loop. For example, this:
>>> def gen():
... for c in 'AB':
... yield c
... for i in range(1, 3):
... yield i
...
>>> list(gen())
['A', 'B', 1, 2]
Can be written as:
>>> def gen():
... yield from 'AB'
... yield from range(1, 3)
...
>>> list(gen())
['A', 'B', 1, 2]
Using yield from | 477
6. Example 16-16 is a didactic example only. The itertools module already provides an optimized chain function
written in C.
When we first mentioned yield from in �New Syntax in Python 3.3: yield from� on
page 433, the code from Example 16-16 demonstrates a practical use for it.6
Example 16-16. Chaining iterables with yield from
>>> def chain(*iterables):
... for it in iterables:
... yield from it
...
>>> s = 'ABC'
>>> t = tuple(range(3))
>>> list(chain(s, t))
['A', 'B', 'C', 0, 1, 2]
A slightly more complicated�but more useful�example of yield from is in �Recipe
4.14. Flattening a Nested Sequence� in Beazley and Jones�s Python Cookbook, 3E (source
code available on GitHub).
The first thing the yield from x expression does with the x object is to call iter(x) to
obtain an iterator from it. This means that x can be any iterable.
However, if replacing nested for loops yielding values was the only contribution of
yield from, this language addition wouldn�t have had a good chance of being accepted.
The real nature of yield from cannot be demonstrated with simple iterables; it requires
the mind-expanding use of nested generators. That�s why PEP 380, which introduced
yield from, is titled �Syntax for Delegating to a Subgenerator.�
The main feature of yield from is to open a bidirectional channel from the outermost
caller to the innermost subgenerator, so that values can be sent and yielded back and
forth directly from them, and exceptions can be thrown all the way in without adding
a lot of exception handling boilerplate code in the intermediate coroutines. This is what
enables coroutine delegation in a way that was not possible before.
The use of yield from requires a nontrivial arrangement of code. To talk about the
required moving parts, PEP 380 uses some terms in a very specific way:
delegating generator
The generator function that contains the yield from <iterable> expression.
subgenerator
The generator obtained from the <iterable> part of the yield from expression.
This is the �subgenerator� mentioned in the title of PEP 380: �Syntax for Delegating
to a Subgenerator.�
478 | Chapter 16: Coroutines
7. The picture in Figure 16-2 was inspired by a diagram by Paul Sokolovsky.
caller
PEP 380 uses the term �caller� to refer to the client code that calls the delegating
generator. Depending on context, I use �client� instead of �caller,� to distinguish
from the delegating generator, which is also a �caller� (it calls the subgenerator).
PEP 380 often uses the word �iterator� to refer to the subgenerator.
That�s confusing because the delegating generator is also an
iterator. So I prefer to use the term subgenerator, in line with the
title of the PEP��Syntax for Delegating to a Subgenerator.� However,
the subgenerator can be a simple iterator implementing only
__next__, and yield from can handle that too, although it was
created to support generators implementing __next__, send,
close, and throw.
Example 16-17 provides more context to see yield from at work, and Figure 16-2
identifies the relevant parts of the example.7
Figure 16-2. While the delegating generator is suspended at yield from, the caller sends
data directly to the subgenerator, which yields data back to the caller. The delegating
generator resumes when the subgenerator returns and the interpreter raises StopIteration
with the returned value attached.
The coroaverager3.py script reads a dict with weights and heights from girls and boys
in an imaginary seventh grade class. For example, the key 'boys;m' maps to the heights
of 9 boys, in meters; 'girls;kg' are the weights of 10 girls in kilograms. The script
feeds the data for each group into the averager coroutine we�ve seen before, and produces
a report like this one:
$ python3 coroaverager3.py
9 boys averaging 40.42kg
Using yield from | 479
9 boys averaging 1.39m
10 girls averaging 42.04kg
10 girls averaging 1.43m
The code in Example 16-17 is certainly not the most straightforward solution to the
problem, but it serves to show yield from in action. This example is inspired by the
one given in What�s New in Python 3.3.
Example 16-17. coroaverager3.py: using yield from to drive averager and report
statistics
from collections import namedtuple
Result = namedtuple('Result', 'count average')
# the subgenerator
def averager():
total = 0.0
count = 0
average = None
while True:
term = yield
if term is None:
break
total += term
count += 1
average = total/count
return Result(count, average)
# the delegating generator
def grouper(results, key):
while True:
results[key] = yield from averager()
# the client code, a.k.a. the caller
def main(data):
results = {}
for key, values in data.items():
group = grouper(results, key)
next(group)
for value in values:
group.send(value)
group.send(None) # important!
# print(results) # uncomment to debug
report(results)
# output report
480 | Chapter 16: Coroutines
def report(results):
for key, result in sorted(results.items()):
group, unit = key.split(';')
print('{:2} {:5} averaging {:.2f}{}'.format(
result.count, group, result.average, unit))
data = {
'girls;kg':
[40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5],
'girls;m':
[1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43],
'boys;kg':
[39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3],
'boys;m':
[1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],
}
if __name__ == '__main__':
main(data)
Same averager coroutine from Example 16-13. Here it is the subgenerator.
Each value sent by the client code in main will be bound to term here.
The crucial terminating condition. Without it, a yield from calling this
coroutine will block forever.
The returned Result will be the value of the yield from expression in grouper.
grouper is the delegating generator.
Each iteration in this loop creates a new instance of averager; each is a generator
object operating as a coroutine.
Whenever grouper is sent a value, it�s piped into the averager instance by the
yield from. grouper will be suspended here as long as the averager instance
is consuming values sent by the client. When an averager instance runs to the
end, the value it returns is bound to results[key]. The while loop then
proceeds to create another averager instance to consume more values.
main is the client code, or �caller� in PEP 380 parlance. This is the function that
drives everything.
group is a generator object resulting from calling grouper with the results dict
to collect the results, and a particular key. It will operate as a coroutine.
Prime the coroutine.
Send each value into the grouper. That value ends up in the term = yield line
of averager; grouper never has a chance to see it.
Using yield from | 481
Sending None into grouper causes the current averager instance to terminate,
and allows grouper to run again, which creates another averager for the next
group of values.
The last callout in Example 16-17 with the comment "important!" highlights a crucial
line of code: group.send(None), which terminates one averager and starts the next. If
you comment out that line, the script produces no output. Uncommenting the print(re
sults) line near the end of main reveals that the results dict ends up empty.
If you want to figure out for yourself why no results are collected,
it will be a great way to exercise your understanding of how
yield from works. The code for coroaverager3.py is in the Fluent
Python code repository. The explanation is next.
Here is an overview of how Example 16-17 works, explaining what would happen if we
omitted the call group.send(None) marked �important!� in main:
� Each iteration of the outer for loop creates a new grouper instance named group;
this is the delegating generator.
� The call next(group) primes the grouper delegating generator, which enters its
while True loop and suspends at the yield from, after calling the subgenerator
averager.
� The inner for loop calls group.send(value); this feeds the subgenerator averag
er directly. Meanwhile, the current group instance of grouper is suspended at the
yield from.
� When the inner for loop ends, the group instance is still suspended at the yield
from, so the assignment to results[key] in the body of grouper has not happened
yet.
� Without the last group.send(None) in the outer for loop, the averager subgenerator
never terminates, the delegating generator group is never reactivated, and
the assignment to results[key] never happens.
� When execution loops back to the top of the outer for loop, a new grouper instance
is created and bound to group. The previous grouper instance is garbage collected
(together with its own unfinished averager subgenerator instance).
482 | Chapter 16: Coroutines
8. Message to Python-Dev: �PEP 380 (yield from a subgenerator) comments� (March 21, 2009).
The key takeaway from this experiment is: if a subgenerator never
terminates, the delegating generator will be suspended forever
at the yield from. This will not prevent your program from making
progress because the yield from (like the simple yield) transfers
control to the client code (i.e., the caller of the delegating
generator). But it does mean that some task will be left unfinished.
Example 16-17 demonstrates the simplest arrangement of yield from, with only one
delegating generator and one subgenerator. Because the delegating generator works as
a pipe, you can connect any number of them in a pipeline: one delegating generator uses
yield from to call a subgenerator, which itself is a delegating generator calling another
subgenerator with yield from, and so on. Eventually this chain must end in a simple
generator that uses just yield, but it may also end in any iterable object, as in
Example 16-16.
Every yield from chain must be driven by a client that calls next(�) or .send(�) on
the outermost delegating generator. This call may be implicit, such as a for loop.
Now let�s review the formal description of the yield from construct, as presented in
PEP 380.
The Meaning of yield from
While developing PEP 380, Greg Ewing�the author�was questioned about the complexity
of the proposed semantics. One of his answers was �For humans, almost all the
important information is contained in one paragraph near the top.� He then quoted part
of the draft of PEP 380 which at the time read as follows:
�When the iterator is another generator, the effect is the same as if the body of the subgenerator
were inlined at the point of the yield from expression. Furthermore, the
subgenerator is allowed to execute a return statement with a value, and that value becomes
the value of the yield from expression.�8
Those soothing words are no longer part of the PEP�because they don�t cover all the
corner cases. But they are OK as a first approximation.
The approved version of PEP 380 explains the behavior of yield from in six points in
the Proposal section. I reproduce them almost exactly here, except that I replaced every
occurrence of the ambiguous word �iterator� with �subgenerator� and added a few
clarifications. Example 16-17 illustrates these four points:
The Meaning of yield from | 483
� Any values that the subgenerator yields are passed directly to the caller of the delegating
generator (i.e., the client code).
� Any values sent to the delegating generator using send() are passed directly to the
subgenerator. If the sent value is None, the subgenerator�s __next__() method is
called. If the sent value is not None, the subgenerator�s send() method is called. If
the call raises StopIteration, the delegating generator is resumed. Any other exception
is propagated to the delegating generator.
� return expr in a generator (or subgenerator) causes StopIteration(expr) to be
raised upon exit from the generator.
� The value of the yield from expression is the first argument to the StopItera
tion exception raised by the subgenerator when it terminates.
The other two features of yield from have to do with exceptions and termination:
� Exceptions other than GeneratorExit thrown into the delegating generator are
passed to the throw() method of the subgenerator. If the call raises StopItera
tion, the delegating generator is resumed. Any other exception is propagated to
the delegating generator.
� If a GeneratorExit exception is thrown into the delegating generator, or the
close() method of the delegating generator is called, then the close() method of
the subgenerator is called if it has one. If this call results in an exception, it is
propagated to the delegating generator. Otherwise, GeneratorExit is raised in the
delegating generator.
The detailed semantics of yield from are subtle, especially the points dealing with
exceptions. Greg Ewing did a great job putting them to words in English in PEP 380.
Ewing also documented the behavior of yield from using pseudocode (with Python
syntax). I personally found it useful to spend some time studying the pseudocode in
PEP 380. However, the pseudocode is 40 lines long and not so easy to grasp at first.
A good way to approach that pseudocode is to simplify it to handle only the most basic
and common use case of yield from.
Consider that yield from appears in a delegating generator. The client code drives
delegating generator, which drives the subgenerator. So, to simplify the logic involved,
let�s pretend the client doesn�t ever call .throw(�) or .close() on the delegating generator.
Let�s also pretend the subgenerator never raises an exception until it terminates,
when StopIteration is raised by the interpreter.
484 | Chapter 16: Coroutines
Example 16-17 is a script where those simplifying assumptions hold. In fact, in much
real-life code, the delegating generator is expected to run to completion. So let�s see how
yield from works in this happier, simpler world.
Take a look at Example 16-18, which is an expansion of this single statement, in the
body of the delegating generator:
RESULT = yield from EXPR
Try to follow the logic in Example 16-18.
Example 16-18. Simplified pseudocode equivalent to the statement RESULT = yield
from EXPR in the delegating generator (this covers the simplest case: .throw(�)
and .close() are not supported; the only exception handled is StopIteration)
_i = iter(EXPR)
try:
_y = next(_i)
except StopIteration as _e:
_r = _e.value
else:
while 1:
_s = yield _y
try:
_y = _i.send(_s)
except StopIteration as _e:
_r = _e.value
break
RESULT = _r
The EXPR can be any iterable, because iter() is applied to get an iterator _i (this
is the subgenerator).
The subgenerator is primed; the result is stored to be the first yielded value _y.
If StopIteration was raised, extract the value attribute from the exception and
assign it to _r: this is the RESULT in the simplest case.
While this loop is running, the delegating generator is blocked, operating just
as a channel between the caller and the subgenerator.
Yield the current item yielded from the subgenerator; wait for a value _s sent by
the caller. Note that this is the only yield in this listing.
Try to advance the subgenerator, forwarding the _s sent by the caller.
If the subgenerator raised StopIteration, get the value, assign to _r, and exit
the loop, resuming the delegating generator.
_r is the RESULT: the value of the whole yield from expression.
The Meaning of yield from | 485
In this simplified pseudocode, I preserved the variable names used in the pseudocode
published in PEP 380. The variables are:
_i (iterator)
The subgenerator
_y (yielded)
A value yielded from the subgenerator
_r (result)
The eventual result (i.e., the value of the yield from expression when the subgenerator
ends)
_s (sent)
A value sent by the caller to the delegating generator, which is forwarded to the
subgenerator
_e (exception)
An exception (always an instance of StopIteration in this simplified pseudocode)
Besides not handling .throw(�) and .close(), the simplified pseudocode always
uses .send(�) to forward next() or .send(�) calls by the client to the subgenerator.
Don�t worry about these fine distinctions on a first reading. As mentioned,
Example 16-17 would run perfectly well if the yield from did only what is shown in
the simplified pseudocode in Example 16-18.
But the reality is more complicated, because of the need to handle .throw(�)
and .close() calls from the client, which must be passed into the subgenerator. Also,
the subgenerator may be a plain iterator that does not support .throw(�) or .close(),
so this must be handled by the yield from logic. If the subgenerator does implement
those methods, inside the subgenerator both methods cause exceptions to be raised,
which must be handled by the yield from machinery as well. The subgenerator may
also throw exceptions of its own, unprovoked by the caller, and this must also be dealt
with in the yield from implementation. Finally, as an optimization, if the caller calls
next(�) or .send(None), both are forwarded as a next(�) call on the subgenerator;
only if the caller sends a non-None value, the .send(�) method of the subgenerator is
used.
For your convenience, following is the complete pseudocode of the yield from expansion
from PEP 380, syntax-highlighted and annotated. Example 16-19 was copied verbatim;
only the callout numbers were added by me.
Again, the code shown in Example 16-19 is an expansion of this single statement, in the
body of the delegating generator:
RESULT = yield from EXPR
486 | Chapter 16: Coroutines
Example 16-19. Pseudocode equivalent to the statement RESULT = yield from EXPR in
the delegating generator
_i = iter(EXPR)
try:
_y = next(_i)
except StopIteration as _e:
_r = _e.value
else:
while 1:
try:
_s = yield _y
except GeneratorExit as _e:
try:
_m = _i.close
except AttributeError:
pass
else:
_m()
raise _e
except BaseException as _e:
_x = sys.exc_info()
try:
_m = _i.throw
except AttributeError:
raise _e
else:
try:
_y = _m(*_x)
except StopIteration as _e:
_r = _e.value
break
else:
try:
if _s is None:
_y = next(_i)
else:
_y = _i.send(_s)
except StopIteration as _e:
_r = _e.value
break
RESULT = _r
The EXPR can be any iterable, because iter() is applied to get an iterator _i (this
is the subgenerator).
The subgenerator is primed; the result is stored to be the first yielded value _y.
If StopIteration was raised, extract the value attribute from the exception and
assign it to _r: this is the RESULT in the simplest case.
The Meaning of yield from | 487
9. In a message to Python-ideas on April 5, 2009, Nick Coghlan questioned whether the implicit priming done
by yield from was a good idea.
While this loop is running, the delegating generator is blocked, operating just
as a channel between the caller and the subgenerator.
Yield the current item yielded from the subgenerator; wait for a value _s sent by
the caller. This is the only yield in this listing.
This deals with closing the delegating generator and the subgenerator. Because
the subgenerator can be any iterator, it may not have a close method.
This deals with exceptions thrown in by the caller using .throw(�). Again, the
subgenerator may be an iterator with no throw method to be called�in which
case the exception is raised in the delegating generator.
If the subgenerator has a throw method, call it with the exception passed from
the caller. The subgenerator may handle the exception (and the loop continues);
it may raise StopIteration (the _r result is extracted from it, and the loop ends);
or it may raise the same or another exception, which is not handled here and
propagates to the delegating generator.
If no exception was received when yielding�
Try to advance the subgenerator�
Call next on the subgenerator if the last value received from the caller was None,
otherwise call send.
If the subgenerator raised StopIteration, get the value, assign to _r, and exit
the loop, resuming the delegating generator.
_r is the RESULT: the value of the whole yield from expression.
Most of the logic of the yield from pseudocode is implemented in six try/except
blocks nested up to four levels deep, so it�s a bit hard to read. The only other control
flow keywords used are one while, one if, and one yield. Find the while, the yield,
the next(�), and the .send(�) calls: they will help you get an idea of how the whole
structure works.
Right at the top of Example 16-19, one important detail revealed by the pseudocode is
that the subgenerator is primed (second callout in Example 16-19).9 This means that
auto-priming decorators such as that in �Decorators for Coroutine Priming� on page
469 are incompatible with yield from.
In the same message I quoted in the opening of this section, Greg Ewing has this to say
about the pseudocode expansion of yield from:
488 | Chapter 16: Coroutines
10. Opening sentence of the �Motivation� section in PEP 342.
You�re not meant to learn about it by reading the expansion�that�s only there to pin down
all the details for language lawyers.
Focusing on the details of the pseudocode expansion may not be helpful�depending
on your learning style. Studying real code that uses yield from is certainly more profitable
than poring over the pseudocode of its implementation. However, almost all the
yield from examples I�ve seen are tied to asynchronous programming with the asyn
cio module, so they depend on an active event loop to work. We�ll see yield from
numerous times in Chapter 18. There are a few links in �Further Reading� on page 500 to
interesting code using yield from without an event loop.
We�ll now move on to a classic example of coroutine usage: programming simulations.
This example does not showcase yield from, but it does reveal how coroutines are used
to manage concurrent activities on a single thread.
Use Case: Coroutines for Discrete Event Simulation
Coroutines are a natural way of expressing many algorithms, such as simulations, games,
asynchronous I/O, and other forms of event-driven programming or co-operative multitasking.
10
� Guido van Rossum and Phillip J. Eby
PEP 342�Coroutines via Enhanced Generators
In this section, I will describe a very simple simulation implemented using just coroutines
and standard library objects. Simulation is a classic application of coroutines in
the computer science literature. Simula, the first OO language, introduced the concept
of coroutines precisely to support simulations.
The motivation for the following simulation example is not academic.
Coroutines are the fundamental building block of the
asyncio package. A simulation shows how to implement concurrent
activities using coroutines instead of threads�and this will
greatly help when we tackle asyncio with in Chapter 18.
Before going into the example, a word about simulations.
About Discrete Event Simulations
A discrete event simulation (DES) is a type of simulation where a system is modeled as
a sequence of events. In a DES, the simulation �clock� does not advance by fixed increments,
but advances directly to the simulated time of the next modeled event. For example,
if we are simulating the operation of a taxi cab from a high-level perspective, one
Use Case: Coroutines for Discrete Event Simulation | 489
11. See the official documentation for Simpy�not to be confused with the well-known but unrelated SymPy, a
library for symbolic mathematics.
event is picking up a passenger, the next is dropping the passenger off. It doesn�t matter
if a trip takes 5 or 50 minutes: when the drop off event happens, the clock is updated to
the end time of the trip in a single operation. In a DES, we can simulate a year of cab
trips in less than a second. This is in contrast to a continuous simulation where the clock
advances continuously by a fixed�and usually small�increment.
Intuitively, turn-based games are examples of discrete event simulations: the state of the
game only changes when a player moves, and while a player is deciding the next move,
the simulation clock is frozen. Real-time games, on the other hand, are continuous
simulations where the simulation clock is running all the time, the state of the game is
updated many times per second, and slow players are at a real disadvantage.
Both types of simulations can be written with multiple threads or a single thread using
event-oriented programming techniques such as callbacks or coroutines driven by an
event loop. It�s arguably more natural to implement a continuous simulation using
threads to account for actions happening in parallel in real time. On the other hand,
coroutines offer exactly the right abstraction for writing a DES. SimPy11 is a DES package
for Python that uses one coroutine to represent each process in the simulation.
In the field of simulation, the term process refers to the activities
of an entity in the model, and not to an OS process. A simulation
process may be implemented as an OS process, but usually a thread
or a coroutine is used for that purpose.
If you are interested in simulations, SimPy is well worth studying. However, in this
section, I will describe a very simple DES implemented using only standard library
features. My goal is to help you develop an intuition about programming concurrent
actions with coroutines. Understanding the next section will require careful study, but
the reward will come as insights on how libraries such as asyncio, Twisted, and Tornado
can manage many concurrent activities using a single thread of execution.
The Taxi Fleet Simulation
In our simulation program, taxi_sim.py, a number of taxi cabs are created. Each will
make a fixed number of trips and then go home. A taxi leaves the garage and starts
�prowling��looking for a passenger. This lasts until a passenger is picked up, and a trip
starts. When the passenger is dropped off, the taxi goes back to prowling.
The time elapsed during prowls and trips is generated using an exponential distribution.
For a cleaner display, times are in whole minutes, but the simulation would work as well
490 | Chapter 16: Coroutines
12. I am not an expert in taxi fleet operations, so don�t take my numbers seriously. Exponential distributions are
commonly used in DES. You�ll see some very short trips. Just pretend it�s a rainy day and some passengers
are taking cabs just to go around the block�in an ideal city where there are cabs when it rains.
using float intervals.12 Each change of state in each cab is reported as an event.
Figure 16-3 shows a sample run of the program.
Figure 16-3. Sample run of taxi_sim.py with three taxis. The -s 3 argument sets the
random generator seed so program runs can be reproduced for debugging and demonstration.
Colored arrows highlight taxi trips.
The most important thing to note in Figure 16-3 is the interleaving of the trips by the
three taxis. I manually added the arrows to make it easier to see the taxi trips: each arrow
Use Case: Coroutines for Discrete Event Simulation | 491
13. I was the passenger. I realized I forgot my wallet.
starts when a passenger is picked up and ends when the passenger is dropped off. Intuitively,
this demonstrates how coroutines can be used for managing concurrent activities.
Other things to note about Figure 16-3:
� Each taxi leaves the garage 5 minutes after the other.
� It took 2 minutes for taxi 0 to pick up the first passenger at time=2; 3 minutes for
taxi 1 (time=8), and 5 minutes for taxi 2 (time=15).
� The cabbie in taxi 0 only makes two trips (purple arrows): the first starts at time=2
and ends at time=18; the second starts at time=28 and ends at time=65�the longest
trip in this simulation run.
� Taxi 1 makes four trips (green arrows) then goes home at time=110.
� Taxi 2 makes six trips (red arrows) then goes home at time=109. His last trip lasts
only one minute, starting at time=97.13
� While taxi 1 is making her first trip, starting at time=8, taxi 2 leaves the garage at
time=10 and completes two trips (short red arrows).
� In this sample run, all scheduled events completed in the default simulation time
of 180 minutes; last event was at time=110.
The simulation may also end with pending events. When that happens, the final message
reads like this:
*** end of simulation time: 3 events pending ***
The full listing of taxi_sim.py is at Example A-6. In this chapter, we�ll show only the
parts that are relevant to our study of coroutines. The really important functions are
only two: taxi_process (a coroutine), and the Simulator.run method where the main
loop of the simulation is executed.
Example 16-20 shows the code for taxi_process. This coroutine uses two objects defined
elsewhere: the compute_delay function, which returns a time interval in minutes,
and the Event class, a namedtuple defined like this:
Event = collections.namedtuple('Event', 'time proc action')
In an Event instance, time is the simulation time when the event will occur, proc is the
identifier of the taxi process instance, and action is a string describing the activity.
Let�s review taxi_process play by play in Example 16-20.
492 | Chapter 16: Coroutines
14. The verb �drive� is commonly used to describe the operation of a coroutine: the client code drives the coroutine
by sending it values. In Example 16-21, the client code is what you type in the console.
Example 16-20. taxi_sim.py: taxi_process coroutine that implements the activities of
each taxi
def taxi_process(ident, trips, start_time=0):
"""Yield to simulator issuing event at each state change"""
time = yield Event(start_time, ident, 'leave garage')
for i in range(trips):
time = yield Event(time, ident, 'pick up passenger')
time = yield Event(time, ident, 'drop off passenger')
yield Event(time, ident, 'going home')
# end of taxi process
taxi_process will be called once per taxi, creating a generator object to
represent its operations. ident is the number of the taxi (e.g., 0, 1, 2 in the sample
run); trips is the number of trips this taxi will make before going home;
start_time is when the taxi leaves the garage.
The first Event yielded is 'leave garage'. This suspends the coroutine, and
lets the simulation main loop proceed to the next scheduled event. When it�s
time to reactivate this process, the main loop will send the current simulation
time, which is assigned to time.
This block will be repeated once for each trip.
An Event signaling passenger pick up is yielded. The coroutine pauses here.
When the time comes to reactivate this coroutine, the main loop will again send
the current time.
An Event signaling passenger drop off is yielded. The coroutine is suspended
again, waiting for the main loop to send it the time of when it�s reactivated.
The for loop ends after the given number of trips, and a final 'going home'
event is yielded. The coroutine will suspend for the last time. When reactivated,
it will be sent the time from the simulation main loop, but here I don�t assign it
to any variable because it will not be used.
When the coroutine falls off the end, the generator object raises StopIteration.
You can �drive� a taxi yourself by calling taxi_process in the Python console.14
Example 16-21 shows how.
Example 16-21. Driving the taxi_process coroutine
>>> from taxi_sim import taxi_process
>>> taxi = taxi_process(ident=13, trips=2, start_time=0)
>>> next(taxi)
Use Case: Coroutines for Discrete Event Simulation | 493
Event(time=0, proc=13, action='leave garage')
>>> taxi.send(_.time + 7)
Event(time=7, proc=13, action='pick up passenger')
>>> taxi.send(_.time + 23)
Event(time=30, proc=13, action='drop off passenger')
>>> taxi.send(_.time + 5)
Event(time=35, proc=13, action='pick up passenger')
>>> taxi.send(_.time + 48)
Event(time=83, proc=13, action='drop off passenger')
>>> taxi.send(_.time + 1)
Event(time=84, proc=13, action='going home')
>>> taxi.send(_.time + 10)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
StopIteration
Create a generator object to represent a taxi with ident=13 that will make two
trips and start working at t=0.
Prime the coroutine; it yields the initial event.
We can now send it the current time. In the console, the _ variable is bound to
the last result; here I add 7 to the time, which means the taxi will spend 7
minutes searching for the first passenger.
This is yielded by the for loop at the start of the first trip.
Sending _.time + 23 means the trip with the first passenger will last 23 minutes.
Then the taxi will prowl for 5 minutes.
The last trip will take 48 minutes.
After two complete trips, the loop ends and the 'going home' event is yielded.
The next attempt to send to the coroutine causes it to fall through the end. When
it returns, the interpreter raises StopIteration.
Note that in Example 16-21 I am using the console to emulate the simulation main loop.
I get the .time attribute of an Event yielded by the taxi coroutine, add an arbitrary
number, and use the sum in the next taxi.send call to reactivate it. In the simulation,
the taxi coroutines are driven by the main loop in the Simulator.run method. The
simulation �clock� is held in the sim_time variable, and is updated by the time of each
event yielded.
To instantiate the Simulator class, the main function of taxi_sim.py builds a taxis
dictionary like this:
taxis = {i: taxi_process(i, (i + 1) * 2, i * DEPARTURE_INTERVAL)
for i in range(num_taxis)}
sim = Simulator(taxis)
494 | Chapter 16: Coroutines
DEPARTURE_INTERVAL is 5; if num_taxis is 3 as in the sample run, the preceding lines
will do the same as:
taxis = {0: taxi_process(ident=0, trips=2, start_time=0),
1: taxi_process(ident=1, trips=4, start_time=5),
2: taxi_process(ident=2, trips=6, start_time=10)}
sim = Simulator(taxis)
Therefore, the values of the taxis dictionary will be three distinct generator objects
with different parameters. For instance, taxi 1 will make 4 trips and begin looking for
passengers at start_time=5. This dict is the only argument required to build a Simu
lator instance.
The Simulator.__init__ method is shown in Example 16-22. The main data structures
of Simulator are:
self.events
A PriorityQueue to hold Event instances. A PriorityQueue lets you put items,
then get them ordered by item[0]; i.e., the time attribute in the case of our Event
namedtuple objects.
self.procs
A dict mapping each process number to an active process in the simulation�a
generator object representing one taxi. This will be bound to a copy of taxis dict
shown earlier.
Example 16-22. taxi_sim.py: Simulator class initializer
class Simulator:
def __init__(self, procs_map):
self.events = queue.PriorityQueue()
self.procs = dict(procs_map)
The PriorityQueue to hold the scheduled events, ordered by increasing time.
We get the procs_map argument as a dict (or any mapping), but build a dict
from it, to have a local copy because when the simulation runs, each taxi that
goes home is removed from self.procs, and we don�t want to change the object
passed by the user.
Priority queues are a fundamental building block of discrete event simulations: events
are created in any order, placed in the queue, and later retrieved in order according to
the scheduled time of each one. For example, the first two events placed in the queue
may be:
Event(time=14, proc=0, action='pick up passenger')
Event(time=11, proc=1, action='pick up passenger')
Use Case: Coroutines for Discrete Event Simulation | 495
This means that taxi 0 will take 14 minutes to pick up the first passenger, while taxi 1�
starting at time=10�will take 1 minute and pick up a passenger at time=11. If those
two events are in the queue, the first event the main loop gets from the priority queue
will be Event(time=11, proc=1, action='pick up passenger').
Now let�s study the main algorithm of the simulation, the Simulator.run method. It�s
invoked by the main function right after the Simulator is instantiated, like this:
sim = Simulator(taxis)
sim.run(end_time)
The listing with callouts for the Simulator class is in Example 16-23, but here is a highlevel
view of the algorithm implemented in Simulator.run:
1. Loop over processes representing taxis.
a. Prime the coroutine for each taxi by calling next() on it. This will yield the first
Event for each taxi.
b. Put each event in the self.events queue of the Simulator.
2. Run the main loop of the simulation while sim_time < end_time.
a. Check if self.events is empty; if so, break from the loop.
b. Get the current_event from self.events. This will be the Event object with
the lowest time in the PriorityQueue.
c. Display the Event.
d. Update the simulation time with the time attribute of the current_event.
e. Send the time to the coroutine identified by the proc attribute of the cur
rent_event. The coroutine will yield the next_event.
f. Schedule next_event by adding it to the self.events queue.
The complete Simulator class is Example 16-23.
Example 16-23. taxi_sim.py: Simulator, a bare-bones discrete event simulation class;
focus on the run method
class Simulator:
def __init__(self, procs_map):
self.events = queue.PriorityQueue()
self.procs = dict(procs_map)
def run(self, end_time):
"""Schedule and display events until time is up"""
# schedule the first event for each cab
for _, proc in sorted(self.procs.items()):
first_event = next(proc)
496 | Chapter 16: Coroutines
15. This is typical of a discrete event simulation: the simulation clock is not incremented by a fixed amount on
each loop, but advances according to the duration of each event completed.
self.events.put(first_event)
# main loop of the simulation
sim_time = 0
while sim_time < end_time:
if self.events.empty():
print('*** end of events ***')
break
current_event = self.events.get()
sim_time, proc_id, previous_action = current_event
print('taxi:', proc_id, proc_id * ' ', current_event)
active_proc = self.procs[proc_id]
next_time = sim_time + compute_duration(previous_action)
try:
next_event = active_proc.send(next_time)
except StopIteration:
del self.procs[proc_id]
else:
self.events.put(next_event)
else:
msg = '*** end of simulation time: {} events pending ***'
print(msg.format(self.events.qsize()))
The simulation end_time is the only required argument for run.
Use sorted to retrieve the self.procs items ordered by the key; we don�t care
about the key, so assign it to _.
next(proc) primes each coroutine by advancing it to the first yield, so it�s ready
to be sent data. An Event is yielded.
Add each event to the self.events PriorityQueue. The first event for each taxi
is 'leave garage', as seen in the sample run (Example 16-20).
Zero sim_time, the simulation clock.
Main loop of the simulation: run while sim_time is less than the end_time.
The main loop may also exit if there are no pending events in the queue.
Get Event with the smallest time in the priority queue; this is the current_event.
Unpack the Event data. This line updates the simulation clock, sim_time, to
reflect the time when the event happened.15
Display the Event, identifying the taxi and adding indentation according to the
taxi ID.
Retrieve the coroutine for the active taxi from the self.procs dictionary.
Use Case: Coroutines for Discrete Event Simulation | 497
16. Message to thread �Yield-From: Finalization guarantees� in the Python-ideas mailing list. The David Beazley
tutorial Guido refers to is �A Curious Course on Coroutines and Concurrency�.
Compute the next activation time by adding the sim_time and the result of
calling compute_duration(�) with the previous action (e.g., 'pick up passen
ger', 'drop off passenger', etc.)
Send the time to the taxi coroutine. The coroutine will yield the next_event or
raise StopIteration when it�s finished.
If StopIteration is raised, delete the coroutine from the self.procs dictionary.
Otherwise, put the next_event in the queue.
If the loop exits because the simulation time passed, display the number of events
pending (which may be zero by coincidence, sometimes).
Linking back to Chapter 15, note that the Simulator.run method in Example 16-23
uses else blocks in two places that are not if statements:
� The main while loop has an else statement to report that the simulation ended
because the end_time was reached�and not because there were no more events to
process.
� The try statement at the bottom of the while loop tries to get a next_event by
sending the next_time to the current taxi process, and if that is successful the else
block puts the next_event into the self.events queue.
I believe the code in Simulator.run would be a bit harder to read without those else
blocks.
The point of this example was to show a main loop processing events and driving coroutines
by sending data to them. This is the basic idea behind asyncio, which we�ll
study in Chapter 18.
Chapter Summary
Guido van Rossum wrote there are three different styles of code you can write using
generators:
There�s the traditional �pull� style (iterators), �push� style (like the averaging example),
and then there are �tasks� (Have you read Dave Beazley�s coroutines tutorial yet?�).16
Chapter 14 was devoted to iterators; this chapter introduced coroutines used in �push
style� and also as very simple �tasks��the taxi processes in the simulation example.
Chapter 18 will put them to use as asynchronous tasks in concurrent programming.
498 | Chapter 16: Coroutines
The running average example demonstrated a common use for a coroutine: as an accumulator
processing items sent to it. We saw how a decorator can be applied to prime
a coroutine, making it more convenient to use in some cases. But keep in mind that
priming decorators are not compatible with some uses of coroutines. In particular,
yield from subgenerator() assumes the subgenerator is not primed, and primes it
automatically.
Accumulator coroutines can yield back partial results with each send method call, but
they become more useful when they can return values, a feature that was added in
Python 3.3 with PEP 380. We saw how the statement return the_result in a generator
now raises StopIteration(the_result), allowing the caller to retrieve the_result
from the value attribute of the exception. This is a rather cumbersome way to retrieve
coroutine results, but it�s handled automatically by the yield from syntax introduced
in PEP 380.
The coverage of yield from started with trivial examples using simple iterables, then
moved to an example highlighting the three main components of any significant use of
yield from: the delegating generator (defined by the use of yield from in its body),
the subgenerator activated by yield from, and the client code that actually drives the
whole setup by sending values to the subgenerator through the pass-through channel
established by yield from in the delegating generator. This section was wrapped up
with a look at the formal definition of yield from behavior as described in PEP 380
using English and Python-like pseudocode.
We closed the chapter with the discrete event simulation example, showing how generators
can be used as an alternative to threads and callbacks to support concurrency.
Although simple, the taxi simulation gives a first glimpse at how event-driven frameworks
like Tornado and asyncio use a main loop to drive coroutines executing concurrent
activities with a single thread of execution. In event-oriented programming with
coroutines, each concurrent activity is carried out by a coroutine that repeatedly yields
control back to the main loop, allowing other coroutines to be activated and move
forward. This is a form of cooperative multitasking: coroutines voluntarily and explicitly
yield control to the central scheduler. In contrast, threads implement preemptive multitasking.
The scheduler can suspend threads at any time�even halfway through a
statement�to give way to other threads.
One final note: this chapter adopted a broad, informal definition of a coroutine: a generator
function driven by a client sending it data through .send(�) calls or yield
from. This broad definition is the one used in PEP 342 � Coroutines via Enhanced
Generators and in most existing Python books as I write this. The asyncio library we�ll
see in Chapter 18 is built on coroutines, but a stricter definition of coroutine is adopted
there: asyncio coroutines are (usually) decorated with an @asyncio.coroutine decorator,
and they are always driven by yield from, not by calling .send(�) directly on
Chapter Summary | 499
them. Of course, asyncio coroutines are driven by next(�) and .send(�) under the
covers, but in user code we only use yield from to make them run.
Further Reading
David Beazley is the ultimate authority on Python generators and coroutines. The
Python Cookbook, 3E (O�Reilly) he coauthored with Brian Jones has numerous recipes
with coroutines. Beazley�s PyCon tutorials on the subject are legendary for their depth
and breadth. The first was at PyCon US 2008: �Generator Tricks for Systems Programmers�.
PyCon US 2009 saw the legendary �A Curious Course on Coroutines and Concurrency�
(hard-to-find video links for all three parts: part 1, part 2, part 3). His most
recent tutorial from PyCon 2014 in Montreal was �Generators: The Final Frontier,� in
which he tackles more concurrency examples�so it�s really more about topics in Chapter
18 of Fluent Python. Dave can�t resist making brains explode in his classes, so in the
last part of �The Final Frontier,� coroutines replace the classic Visitor pattern in an
arithmetic expression evaluator.
Coroutines allow new ways of organizing code, and just as recursion or polymorphism
(dynamic dispatch), it takes some time getting used to their possibilities. An interesting
example of classic algorithm rewritten with coroutines is in the post �Greedy algorithm
with coroutines,� by James Powell. You may also want to browse �Popular recipes tagged
coroutine" in the ActiveState Code recipes database.
Paul Sokolovsky implemented yield from in Damien George�s super lean MicroPython
interpreter designed to run on microcontrollers. As he studied the feature, he
created a great, detailed diagram to explain how yield from works, and shared it in the
python-tulip mailing list. Sokolovsky was kind enough to allow me to copy the PDF to
this book�s site, where it has a more permanent URL.
As I write this, the vast majority of uses of yield from to be found are in asyncio itself
or code that uses it. I spent a lot of time looking for examples of yield from that did
not depend on asyncio. Greg Ewing�who penned PEP 380 and implemented yield
from in CPython�published a few examples of its use: a BinaryTree class, a simple
XML parser, and a task scheduler.
Brett Slatkin�s Effective Python (Addison-Wesley) has an excellent short chapter titled
�Consider Coroutines to Run Many Functions Concurrently� (available online as a
sample chapter). That chapter includes the best example of driving generators with
yield from I�ve seen: an implementation of John Conway�s Game of Life in which
coroutines are used to manage the state of each cell as the game runs. The example code
for Effective Python can be found in a GitHub repository. I refactored the code for the
Game of Life example�separating the functions and classes that implement the game
from the testing snippets used in Slatkin�s book (original code). I also rewrote the tests
500 | Chapter 16: Coroutines
17. Nowadays even tenured professors agree that Wikipedia is a good place to start studying pretty much any
subject in computer science. Not true about other subjects, but for computer science, Wikipedia rocks.
as doctests, so you can see the output of the various coroutines and classes without
running the script. The refactored example is posted as a GitHub gist.
Other interesting examples of yield from without asyncio appear in a message to the
Python Tutor list, �Comparing two CSV files using Python� by Peter Otten, and a Rock-
Paper-Scissors game in Ian Ward�s �Iterables, Iterators, and Generators� tutorial published
as an iPython notebook.
Guido van Rossum sent a long message to the python-tulip Google Group titled �The
difference between yield and yield-from" that is worth reading. Nick Coghlan posted
a heavily commented version of the yield from expansion to Python-Dev on March
21, 2009; in the same message, he wrote:
Whether or not different people will find code using yield from difficult to understand
or not will have more to do with their grasp of the concepts of cooperative multitasking
in general more so than the underlying trickery involved in allowing truly nested generators.
PEP 492 � Coroutines with async and await syntax by Yury Selivanov proposes the
addition of two keywords to Python: async and await. The former will be used with
other existing keywords to define new language constructs. For example, async def
will be used to define a coroutine, and async for to loop over asynchronous iterables
with asynchronous iterators (implementing __aiter__ and __anext__, coroutine versions
of __iter__ and __next__). To avoid conflict with the upcoming async keyword,
the essential function asyncio.async() will be renamed asyncio.ensure_future() in
Python 3.4.4. The await keyword will do something similar to yield from, but will
only be allowed inside coroutines defined with async def�where the use of yield and
yield from will be forbidden. With new syntax, the PEP establishes a clear separation
between the legacy generators that evolved into coroutine-like objects and a new breed
of native coroutine objects with better language support thanks to infrastructure like
the async and await keywords and several new special methods. Coroutines are poised
to become really important in the future of Python and the language should be adapted
to better integrate them.
Experimenting with discrete event simulations is a great way to become comfortable
with cooperative multitasking. Wikipedia�s �Discrete event simulation� article is a good
place to start.17 A short tutorial about writing discrete event simulations by hand (no
special libraries) is Ashish Gupta�s �Writing a Discrete Event Simulation: Ten Easy Lessons.�
The code is in Java so it�s class-based and uses no coroutines, but can easily be
ported to Python. Regardless of the code, the tutorial is a good short introduction to
Further Reading | 501
the terminology and components of a discrete event simulation. Converting Gupta�s
examples to Python classes and then to classes leveraging coroutines is a good exercise.
For a ready-to-use library in Python, using coroutines, there is SimPy. Its online documentation
explains:
SimPy is a process-based discrete-event simulation framework based on standard Python.
Its event dispatcher is based on Python�s generators and can also be used for asynchronous
networking or to implement multi-agent systems (with both simulated and real communication).
Coroutines are not so new in Python but they were pretty much tied to niche application
domains before asynchronous programming frameworks started supporting them,
starting with Tornado. The addition of yield from in Python 3.3 and asyncio in Python
3.4 will likely boost the adoption of coroutines�and of Python 3.4 itself. However,
Python 3.4 is less than a year old as I write this�so once you watch David Beazley�s
tutorials and cookbook examples on the subject, there isn�t a whole lot of content out
there that goes deep into Python coroutine programming. For now.
Soapbox
Raise from lambda
In programming languages, keywords establish the basic rules of control flow and expression
evaluation.
A keyword in a language is like a piece in a board game. In the language of Chess, the
keywords are ?, ?, ?, ?, ?, and ?. In the game of Go, it�s ?.
Chess players have six different types of pieces to implement their plans, whereas Go
players seem to have only one type of piece. However, in the semantics of Go, adjacent
pieces form larger, solid pieces of many different shapes, with emerging properties. Some
arrangements of Go pieces are indestructible. Go is more expressive than Chess. In Go
there are 361 possible opening moves, and an estimated 1e+170 legal positions; for
Chess, the numbers are 20 opening moves 1e+50 positions.
Adding a new piece to Chess would be a radical change. Adding a new keyword in a
programming language is also a radical change. So it makes sense for language designers
to be wary of introducing keywords.
Table 16-1. Number of keywords in programming languages
Keywords Language Comment
5 Smalltalk-80 Famous for its minimalist syntax.
25 Go The language, not the game.
32 C That�s ANSI C. C99 has 37 keywords, C11 has 44.
33 Python Python 2.7 has 31 keywords; Python 1.5 had 28.
502 | Chapter 16: Coroutines
18. �The Value Of Syntax?� is an interesting discussion about extensible syntax and programming language
usability. The forum, Lambda the Ultimate, is a watering hole for programming language geeks.
19. A highly recommended post related to this issue in the context of JavaScript, Python, and other languages is
�What Color Is Your Function?� by Bob Nystrom.
Keywords Language Comment
41 Ruby Keywords may be used as identifiers (e.g., class is also a method name).
49 Java As in C, the names of the primitive types (char, float, etc.) are reserved.
60 JavaScript Includes all keywords from Java 1.0, many of which are unused.
65 PHP Since PHP 5.3, seven keywords were introduced, including goto, trait, and yield.
85 C++ According to cppreference.com, C++11 added 10 keywords to the existing 75.
555 COBOL I did not make this up. See this IBM ILE COBOL manual.
? Scheme Anyone can define new keywords.
Python 3 added nonlocal, promoted None, True, and False to keyword status, and
dropped print and exec. It�s very uncommon for a language to drop keywords as it
evolves. Table 16-1 lists some languages, ordered by number of keywords.
Scheme inherited from Lisp a macro facility that allows anyone to create special forms
adding new control structures and evaluation rules to the language. The user-defined
identifiers of those forms are called �syntactic keywords.� The Scheme R5RS standard
states �There are no reserved identifiers� (page 45 of the standard), but a typical implementation
such as MIT/GNU Scheme comes with 34 syntactic keywords predefined,
such as if, lambda, and define-syntax�the keyword that lets you conjure new keywords.
18
Python is like Chess, and Scheme is like Go (the game).
Now, back to Python syntax. I think Guido is too conservative with keywords. It�s nice
to have a small set of them, and adding new keywords potentially breaks a lot of code.
But the use of else in loops reveals a recurring problem: the overloading of existing
keywords when a new one would be a better choice. In the context of for, while, and
try, a new then keyword would be preferable to abusing else.
The most serious manifestation of this problem is the overloading of def: it�s now used
to define functions, generators, and coroutines�objects that are too different to share
the same declaration syntax.19
The introduction of yield from is particularly worrying. Once again, I believe Python
users would be best served by a new keyword. Even worse, this starts a new trend:
chaining existing keywords to create new syntax, instead of adding sensible, descriptive
keywords. I fear one day we may be poring over the meaning of raise from lambda.
Breaking News
Further Reading | 503
As I wrap up this book�s technical review process, it seems Yury Selivanov�s PEP 492 �
Coroutines with async and await syntax is on the way to being accepted for implementation
in Python 3.5 already! The PEP has the support of Guido van Rossum and Victor
Stinner, respectively the author and a leading maintainer of the asyncio library that
would be the main use case for the new syntax. In response to Selivanov�s message to
Python-ideas, Guido even hints at delaying the release of Python 3.5 so the PEP can be
implemented.
Of course, this would put to rest most of the complaints I expressed in the preceding
sections.
504 | Chapter 16: Coroutines
1. From Michele Simionato�s post Threads, processes and concurrency in Python: some thoughts, subtitled
�Removing the hype around the multicore (non) revolution and some (hopefully) sensible comment about
threads and other forms of concurrency.�
CHAPTER 17
Concurrency with Futures
The people bashing threads are typically system programmers which have in mind use
cases that the typical application programmer will never encounter in her life. [�] In
99% of the use cases an application programmer is likely to run into, the simple pattern
of spawning a bunch of independent threads and collecting the results in a queue is
everything one needs to know.1
� Michele Simionato
Python deep thinker
This chapter focuses on the concurrent.futures library introduced in Python 3.2, but
also available for Python 2.5 and newer as the futures package on PyPI. This library
encapsulates the pattern described by Michele Simionato in the preceding quote, making
it almost trivial to use.
Here I also introduce the concept of �futures��objects representing the asynchronous
execution of an operation. This powerful idea is the foundation not only of concur
rent.futures but also of the asyncio package, which we�ll cover in Chapter 18.
We�ll start with a motivating example.
Example: Web Downloads in Three Styles
To handle network I/O efficiently, you need concurrency, as it involves high latency�
so instead of wasting CPU cycles waiting, it�s better to do something else until a response
comes back from the network.
To make this last point with code, I wrote three simple programs to download images
of 20 country flags from the Web. The first one, flags.py, runs sequentially: it only re?
505
quests the next image when the previous one is downloaded and saved to disk. The other
two scripts make concurrent downloads: they request all images practically at the same
time, and save the files as they arrive. The flags_threadpool.py script uses the concur
rent.futures package, while flags_asyncio.py uses asyncio.
Example 17-1 shows the result of running the three scripts, three times each. I also
posted a 73s video on YouTube so you can watch them running while an OS X Finder
window displays the flags as they are saved. The scripts are downloading images from
flupy.org, which is behind a CDN, so you may see slower results in the first runs. The
results in Example 17-1 were obtained after several runs, so the CDN cache was warm.
Example 17-1. Three typical runs of the scripts flags.py, flags_threadpool.py, and
flags_asyncio.py
$ python3 flags.py
BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN
20 flags downloaded in 7.26s
$ python3 flags.py
BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN
20 flags downloaded in 7.20s
$ python3 flags.py
BD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN
20 flags downloaded in 7.09s
$ python3 flags_threadpool.py
DE BD CN JP ID EG NG BR RU CD IR MX US PH FR PK VN IN ET TR
20 flags downloaded in 1.37s
$ python3 flags_threadpool.py
EG BR FR IN BD JP DE RU PK PH CD MX ID US NG TR CN VN ET IR
20 flags downloaded in 1.60s
$ python3 flags_threadpool.py
BD DE EG CN ID RU IN VN ET MX FR CD NG US JP TR PK BR IR PH
20 flags downloaded in 1.22s
$ python3 flags_asyncio.py
BD BR IN ID TR DE CN US IR PK PH FR RU NG VN ET MX EG JP CD
20 flags downloaded in 1.36s
$ python3 flags_asyncio.py
RU CN BR IN FR BD TR EG VN IR PH CD ET ID NG DE JP PK MX US
20 flags downloaded in 1.27s
$ python3 flags_asyncio.py
RU IN ID DE BR VN PK MX US IR ET EG NG BD FR CN JP PH CD TR
20 flags downloaded in 1.42s
The output for each run starts with the country codes of the flags as they are
downloaded, and ends with a message stating the elapsed time.
It took flags.py an average 7.18s to download 20 images.
The average for flags_threadpool.py was 1.40s.
For flags_asyncio.py, 1.35 was the average time.
506 | Chapter 17: Concurrency with Futures
Note the order of the country codes: the downloads happened in a different
order every time with the concurrent scripts.
The difference in performance between the concurrent scripts is not significant, but
they are both more than five times faster than the sequential script�and this is just for
a fairly small task. If you scale the task to hundreds of downloads, the concurrent scripts
can outpace the sequential one by a factor or 20 or more.
While testing concurrent HTTP clients on the public Web you may
inadvertently launch a denial-of-service (DoS) attack, or be suspected
of doing so. In the case of Example 17-1, it�s OK to do it
because those scripts are hardcoded to make only 20 requests. For
testing nontrivial HTTP clients, you should set up your own test
server. The 17-futures/countries/README.rst file in the Fluent
Python code GitHub repository has instructions for setting a local
Nginx server.
Now let�s study the implementations of two of the scripts tested in Example 17-1: flags.py
and flags_threadpool.py. I will leave the third script, flags_asyncio.py, for Chapter 18,
but I wanted to demonstrate all three together to make a point: regardless of the concurrency
strategy you use�threads or asyncio�you�ll see vastly improved throughput
over sequential code in I/O-bound applications, if you code it properly.
On to the code.
A Sequential Download Script
Example 17-2 is not very interesting, but we�ll reuse most of its code and settings to
implement the concurrent scripts, so it deserves some attention.
For clarity, there is no error handling in Example 17-2. We will
deal with exceptions later, but here we want to focus on the basic
structure of the code, to make it easier to contrast this script
with the concurrent ones.
Example 17-2. flags.py: sequential download script; some functions will be reused by
the other scripts
import os
import time
import sys
import requests
Example: Web Downloads in Three Styles | 507
POP20_CC = ('CN IN US ID BR PK NG BD RU JP '
'MX PH VN ET EG DE IR TR CD FR').split()
BASE_URL = 'http://flupy.org/data/flags'
DEST_DIR = 'downloads/'
def save_flag(img, filename):
path = os.path.join(DEST_DIR, filename)
with open(path, 'wb') as fp:
fp.write(img)
def get_flag(cc):
url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())
resp = requests.get(url)
return resp.content
def show(text):
print(text, end=' ')
sys.stdout.flush()
def download_many(cc_list):
for cc in sorted(cc_list):
image = get_flag(cc)
show(cc)
save_flag(image, cc.lower() + '.gif')
return len(cc_list)
def main(download_many):
t0 = time.time()
count = download_many(POP20_CC)
elapsed = time.time() - t0
msg = '\n{} flags downloaded in {:.2f}s'
print(msg.format(count, elapsed))
if __name__ == '__main__':
main(download_many)
Import the requests library; it�s not part of the standard library, so by
convention we import it after the standard library modules os, time, and sys,
and separate it from them with a blank line.
List of the ISO 3166 country codes for the 20 most populous countries in order
of decreasing population.
508 | Chapter 17: Concurrency with Futures
2. The images are originally from the CIA World Factbook, a public-domain, U.S. government publication. I
copied them to my site to avoid the risk of launching a DOS attack on CIA.gov.
The website with the flag images.2
Local directory where the images are saved.
Simply save the img (a byte sequence) to filename in the DEST_DIR.
Given a country code, build the URL and download the image, returning the
binary contents of the response.
Display a string and flush sys.stdout so we can see progress in a one-line
display; this is needed because Python normally waits for a line break to flush
the stdout buffer.
download_many is the key function to compare with the concurrent
implementations.
Loop over the list of country codes in alphabetical order, to make it clear that
the ordering is preserved in the output; return the number of country codes
downloaded.
main records and reports the elapsed time after running download_many.
main must be called with the function that will make the downloads; we pass the
download_many function as an argument so that main can be used as a library
function with other implementations of download_many in the next examples.
The requests library by Kenneth Reitz is available on PyPI and is
more powerful and easier to use than the urllib.request module
from the Python 3 standard library. In fact, requests is considered
a model Pythonic API. It is also compatible with Python
2.6 and up, while the urllib2 from Python 2 was moved and
renamed in Python 3, so it�s more convenient to use requests
regardless of the Python version you�re targeting.
There�s really nothing new to flags.py. It serves as a baseline for comparing the other
scripts and I used it as a library to avoid redundant code when implementing them.
Now let�s see a reimplementation using concurrent.futures.
Downloading with concurrent.futures
The main features of the concurrent.futures package are the ThreadPoolExecutor
and ProcessPoolExecutor classes, which implement an interface that allows you to
submit callables for execution in different threads or processes, respectively. The classes
manage an internal pool of worker threads or processes, and a queue of tasks to be
Example: Web Downloads in Three Styles | 509
executed. But the interface is very high level and we don�t need to know about any of
those details for a simple use case like our flag downloads.
Example 17-3 shows the easiest way to implement the downloads concurrently, using
the ThreadPoolExecutor.map method.
Example 17-3. flags_threadpool.py: threaded download script using futures.Thread?
PoolExecutor
from concurrent import futures
from flags import save_flag, get_flag, show, main
MAX_WORKERS = 20
def download_one(cc):
image = get_flag(cc)
show(cc)
save_flag(image, cc.lower() + '.gif')
return cc
def download_many(cc_list):
workers = min(MAX_WORKERS, len(cc_list))
with futures.ThreadPoolExecutor(workers) as executor:
res = executor.map(download_one, sorted(cc_list))
return len(list(res))
if __name__ == '__main__':
main(download_many)
Reuse some functions from the flags module (Example 17-2).
Maximum number of threads to be used in the ThreadPoolExecutor.
Function to download a single image; this is what each thread will execute.
Set the number of worker threads: use the smaller number between the
maximum we want to allow (MAX_WORKERS) and the actual items to be processed,
so no unnecessary threads are created.
Instantiate the ThreadPoolExecutor with that number of worker threads; the
executor.__exit__ method will call executor.shutdown(wait=True), which
will block until all threads are done.
The map method is similar to the map built-in, except that the download_one
function will be called concurrently from multiple threads; it returns a generator
that can be iterated over to retrieve the value returned by each function.
510 | Chapter 17: Concurrency with Futures
Return the number of results obtained; if any of the threaded calls raised an
exception, that exception would be raised here as the implicit next() call tried
to retrieve the corresponding return value from the iterator.
Call the main function from the flags module, passing the enhanced version of
download_many.
Note that the download_one function from Example 17-3 is essentially the body of the
for loop in the download_many function from Example 17-2. This is a common refactoring
when writing concurrent code: turning the body of a sequential for loop into a
function to be called concurrently.
The library is called concurrency.futures yet there are no futures to be seen in
Example 17-3, so you may be wondering where they are. The next section explains.
Where Are the Futures?
Futures are essential components in the internals of concurrent.futures and of asyn
cio, but as users of these libraries we sometimes don�t see them. Example 17-3 leverages
futures behind the scenes, but the code I wrote does not touch them directly. This section
is an overview of futures, with an example that shows them in action.
As of Python 3.4, there are two classes named Future in the standard library: concur
rent.futures.Future and asyncio.Future. They serve the same purpose: an instance
of either Future class represents a deferred computation that may or may not have
completed. This is similar to the Deferred class in Twisted, the Future class in Tornado,
and Promise objects in various JavaScript libraries.
Futures encapsulate pending operations so that they can be put in queues, their state of
completion can be queried, and their results (or exceptions) can be retrieved when
available.
An important thing to know about futures in general is that you and I should not create
them: they are meant to be instantiated exclusively by the concurrency framework, be
it concurrent.futures or asyncio. It�s easy to understand why: a Future represents
something that will eventually happen, and the only way to be sure that something will
happen is to schedule its execution. Therefore, concurrent.futures.Future instances
are created only as the result of scheduling something for execution with a concur
rent.futures.Executor subclass. For example, the Executor.submit() method takes
a callable, schedules it to run, and returns a future.
Client code is not supposed to change the state of a future: the concurrency framework
changes the state of a future when the computation it represents is done, and we can�t
control when that happens.
Example: Web Downloads in Three Styles | 511
Both types of Future have a .done() method that is nonblocking and returns a Boolean
that tells you whether the callable linked to that future has executed or not. Instead of
asking whether a future is done, client code usually asks to be notified. That�s why both
Future classes have an .add_done_callback() method: you give it a callable, and the
callable will be invoked with the future as the single argument when the future is done.
There is also a .result() method, which works the same in both classes when the future
is done: it returns the result of the callable, or re-raises whatever exception might have
been thrown when the callable was executed. However, when the future is not done, the
behavior of the result method is very different between the two flavors of Future. In
a concurrency.futures.Future instance, invoking f.result() will block the caller�s
thread until the result is ready. An optional timeout argument can be passed, and if the
future is not done in the specified time, a TimeoutError exception is raised. In �asyncio.
Future: Nonblocking by Design� on page 545, we�ll see that the asyncio.Future.re
sult method does not support timeout, and the preferred way to get the result of futures
in that library is to use yield from�which doesn�t work with concurrency.fu
tures.Future instances.
Several functions in both libraries return futures; others use them in their implementation
in a way that is transparent to the user. An example of the latter is the Execu
tor.map we saw in Example 17-3: it returns an iterator in which __next__ calls the
result method of each future, so what we get are the results of the futures, and not the
futures themselves.
To get a practical look at futures, we can rewrite Example 17-3 to use the concur
rent.futures.as_completed function, which takes an iterable of futures and returns
an iterator that yields futures as they are done.
Using futures.as_completed requires changes to the download_many function only.
The higher-level executor.map call is replaced by two for loops: one to create and
schedule the futures, the other to retrieve their results. While we are at it, we�ll add a
few print calls to display each future before and after it�s done. Example 17-4 shows the
code for a new download_many function. The code for download_many grew from 5 to
17 lines, but now we get to inspect the mysterious futures. The remaining functions are
the same as in Example 17-3.
Example 17-4. flags_threadpool_ac.py: replacing executor.map with executor.submit
and futures.as_completed in the download_many function
def download_many(cc_list):
cc_list = cc_list[:5]
with futures.ThreadPoolExecutor(max_workers=3) as executor:
to_do = []
for cc in sorted(cc_list):
future = executor.submit(download_one, cc)
to_do.append(future)
512 | Chapter 17: Concurrency with Futures
msg = 'Scheduled for {}: {}'
print(msg.format(cc, future))
results = []
for future in futures.as_completed(to_do):
res = future.result()
msg = '{} result: {!r}'
print(msg.format(future, res))
results.append(res)
return len(results)
For this demonstration, use only the top five most populous countries.
Hardcode max_workers to 3 so we can observe pending futures in the output.
Iterate over country codes alphabetically, to make it clear that results arrive out
of order.
executor.submit schedules the callable to be executed, and returns a future
representing this pending operation.
Store each future so we can later retrieve them with as_completed.
Display a message with the country code and the respective future.
as_completed yields futures as they are completed.
Get the result of this future.
Display the future and its result.
Note that the future.result() call will never block in this example because the fu
ture is coming out of as_completed. Example 17-5 shows the output of one run of
Example 17-4.
Example 17-5. Output of flags_threadpool_ac.py
$ python3 flags_threadpool_ac.py
Scheduled for BR: <Future at 0x100791518 state=running>
Scheduled for CN: <Future at 0x100791710 state=running>
Scheduled for ID: <Future at 0x100791a90 state=running>
Scheduled for IN: <Future at 0x101807080 state=pending>
Scheduled for US: <Future at 0x101807128 state=pending>
CN <Future at 0x100791710 state=finished returned str> result: 'CN'
BR ID <Future at 0x100791518 state=finished returned str> result: 'BR'
<Future at 0x100791a90 state=finished returned str> result: 'ID'
IN <Future at 0x101807080 state=finished returned str> result: 'IN'
US <Future at 0x101807128 state=finished returned str> result: 'US'
5 flags downloaded in 0.70s
Example: Web Downloads in Three Styles | 513
The futures are scheduled in alphabetical order; the repr() of a future shows
its state: the first three are running, because there are three worker threads.
The last two futures are pending, waiting for worker threads.
The first CN here is the output of download_one in a worker thread; the rest of
the line is the output of download_many.
Here two threads output codes before download_many in the main thread can
display the result of the first thread.
If you run flags_threadpool_ac.py several times, you�ll see the
order of the results varying. Increasing the max_workers argument
to 5 will increase the variation in the order of the results.
Decreasing it to 1 will make this code run sequentially, and the
order of the results will always be the order of the submit calls.
We saw two variants of the download script using concurrent.futures: Example 17-3
with ThreadPoolExecutor.map and Example 17-4 with futures.as_completed. If you
are curious about the code for flags_asyncio.py, you may peek at Example 18-5 in
Chapter 18.
Strictly speaking, none of the concurrent scripts we tested so far can perform downloads
in parallel. The concurrent.futures examples are limited by the GIL, and the
flags_asyncio.py is single-threaded.
At this point, you may have questions about the informal benchmarks we just did:
� How can flags_threadpool.py perform 5? faster than flags.py if Python threads are
limited by a Global Interpreter Lock (GIL) that only lets one thread run at any time?
� How can flags_asyncio.py perform 5? faster than flags.py when both are single
threaded?
I will answer the second question in �Running Circling Around Blocking Calls� on page
552.
Read on to understand why the GIL is nearly harmless with I/O-bound processing.
514 | Chapter 17: Concurrency with Futures
3. This is a limitation of the CPython interpreter, not of the Python language itself. Jython and IronPython are
not limited in this way; but Pypy, the fastest Python interpreter available, also has a GIL.
4. Slide 106 of �Generators: The Final Frontier�.
Blocking I/O and the GIL
The CPython interpreter is not thread-safe internally, so it has a Global Interpreter Lock
(GIL), which allows only one thread at a time to execute Python bytecodes. That�s why
a single Python process usually cannot use multiple CPU cores at the same time.3
When we write Python code, we have no control over the GIL, but a built-in function
or an extension written in C can release the GIL while running time-consuming tasks.
In fact, a Python library coded in C can manage the GIL, launch its own OS threads,
and take advantage of all available CPU cores. This complicates the code of the library
considerably, and most library authors don�t do it.
However, all standard library functions that perform blocking I/O release the GIL when
waiting for a result from the OS. This means Python programs that are I/O bound can
benefit from using threads at the Python level: while one Python thread is waiting for
a response from the network, the blocked I/O function releases the GIL so another
thread can run.
That�s why David Beazley says: �Python threads are great at doing nothing.�4
Every blocking I/O function in the Python standard library releases
the GIL, allowing other threads to run. The time.sleep()
function also releases the GIL. Therefore, Python threads are perfectly
usable in I/O-bound applications, despite the GIL.
Now let�s take a brief look at a simple way to work around the GIL for CPU-bound jobs
using concurrent.futures.
Launching Processes with concurrent.futures
The concurrent.futures documentation page is subtitled �Launching parallel tasks�.
The package does enable truly parallel computations because it supports distributing
work among multiple Python processes using the ProcessPoolExecutor class�thus
bypassing the GIL and leveraging all available CPU cores, if you need to do CPU-bound
processing.
Both ProcessPoolExecutor and ThreadPoolExecutor implement the generic Execu
tor interface, so it�s very easy to switch from a thread-based to a process-based solution
using concurrent.futures.
Blocking I/O and the GIL | 515
There is no advantage in using a ProcessPoolExecutor for the flags download example
or any I/O-bound job. It�s easy to verify this; just change these lines in Example 17-3:
def download_many(cc_list):
workers = min(MAX_WORKERS, len(cc_list))
with futures.ThreadPoolExecutor(workers) as executor:
To this:
def download_many(cc_list):
with futures.ProcessPoolExecutor() as executor:
For simple uses, the only notable difference between the two concrete executor classes
is that ThreadPoolExecutor.__init__ requires a max_workers argument setting the
number of threads in the pool. That is an optional argument in ProcessPoolExecu
tor, and most of the time we don�t use it�the default is the number of CPUs returned
by os.cpu_count(). This makes sense: for CPU-bound processing, it makes no sense
to ask for more workers than CPUs. On the other hand, for I/O-bound processing, you
may use 10, 100, or 1,000 threads in a ThreadPoolExecutor; the best number depends
on what you�re doing and the available memory, and finding the optimal number will
require careful testing.
A few tests revealed that the average time to download the 20 flags increased to 1.8s
with a ProcessPoolExecutor�compared to 1.4s in the original ThreadPoolExecutor
version. The main reason for this is likely to be the limit of four concurrent downloads
on my four-core machine, against 20 workers in the thread pool version.
The value of ProcessPoolExecutor is in CPU-intensive jobs. I did some performance
tests with a couple of CPU-bound scripts:
arcfour_futures.py
Encrypt and decrypt a dozen byte arrays with sizes from 149 KB to 384 KB using a
pure-Python implementation of the RC4 algorithm (listing: Example A-7).
sha_futures.py
Compute the SHA-256 hash of a dozen 1 MB byte arrays with the standard library
hashlib package, which uses the OpenSSL library (listing: Example A-9).
Neither of these scripts do I/O except to display summary results. They build and process
all their data in memory, so I/O does not interfere with their execution time.
Table 17-1 shows the average timings I got after 64 runs of the RC4 example and 48 runs
of the SHA example. The timings include the time to actually spawn the worker processes.
516 | Chapter 17: Concurrency with Futures
Table 17-1. Time and speedup factor for the RC4 and SHA examples with one to four
workers on an Intel Core i7 2.7 GHz quad-core machine, using Python 3.4
Workers RC4 time RC4 factor SHA time SHA factor
1 11.48s 1.00x 22.66s 1.00x
2 8.65s 1.33x 14.90s 1.52x
3 6.04s 1.90x 11.91s 1.90x
4 5.58s 2.06x 10.89s 2.08x
In summary, for cryptographic algorithms, you can expect to double the performance
by spawning four worker processes with a ProcessPoolExecutor, if you have four CPU
cores.
For the pure-Python RC4 example, you can get results 3.8 times faster if you use PyPy
and four workers, compared with CPython and four workers. That�s a speedup of 7.8
times in relation to the baseline of one worker with CPython in Table 17-1.
If you are doing CPU-intensive work in Python, you should try
PyPy. The arcfour_futures.py example ran from 3.8 to 5.1 times
faster using PyPy, depending on the number of workers used. I
tested with PyPy 2.4.0, which is compatible with Python 3.2.5, so
it has concurrent.futures in the standard library.
Now let�s investigate the behavior of a thread pool with a demonstration program that
launches a pool with three workers, running five callables that output timestamped
messages.
Experimenting with Executor.map
The simplest way to run several callables concurrently is with the Executor.map function
we first saw in Example 17-3. Example 17-6 is a script to demonstrate how Execu
tor.map works in some detail. Its output appears in Example 17-7.
Example 17-6. demo_executor_map.py: Simple demonstration of the map method of
ThreadPoolExecutor
from time import sleep, strftime
from concurrent import futures
def display(*args):
print(strftime('[%H:%M:%S]'), end=' ')
print(*args)
Experimenting with Executor.map | 517
def loiter(n):
msg = '{}loiter({}): doing nothing for {}s...'
display(msg.format('\t'*n, n, n))
sleep(n)
msg = '{}loiter({}): done.'
display(msg.format('\t'*n, n))
return n * 10
def main():
display('Script starting.')
executor = futures.ThreadPoolExecutor(max_workers=3)
results = executor.map(loiter, range(5))
display('results:', results) # .
display('Waiting for individual results:')
for i, result in enumerate(results):
display('result {}: {}'.format(i, result))
main()
This function simply prints whatever arguments it gets, preceded by a timestamp
in the format [HH:MM:SS].
loiter does nothing except display a message when it starts, sleep for n seconds,
then display a message when it ends; tabs are used to indent the messages
according to the value of n.
loiter returns n * 10 so we can see how to collect results.
Create a ThreadPoolExecutor with three threads.
Submit five tasks to the executor (because there are only three threads, only
three of those tasks will start immediately: the calls loiter(0), loiter(1), and
loiter(2)); this is a nonblocking call.
Immediately display the results of invoking executor.map: it�s a generator, as
the output in Example 17-7 shows.
The enumerate call in the for loop will implicitly invoke next(results), which
in turn will invoke _f.result() on the (internal) _f future representing the first
call, loiter(0). The result method will block until the future is done, therefore
each iteration in this loop will have to wait for the next result to be ready.
I encourage you to run Example 17-6 and see the display being updated incrementally.
While you�re at it, play with the max_workers argument for the ThreadPoolExecutor
and with the range function that produces the arguments for the executor.map call�
or replace it with lists of handpicked values to create different delays.
Example 17-7 shows a sample run of Example 17-6.
518 | Chapter 17: Concurrency with Futures
5. Your mileage may vary: with threads, you never know the exact sequencing of events that should happen
practically at the same time; it�s possible that, in another machine, you see loiter(1) starting before
loiter(0) finishes, particularly because sleep always releases the GIL so Python may switch to another
thread even if you sleep for 0s.
Example 17-7. Sample run of demo_executor_map.py from Example 17-6
$ python3 demo_executor_map.py
[15:56:50] Script starting.
[15:56:50] loiter(0): doing nothing for 0s...
[15:56:50] loiter(0): done.
[15:56:50] loiter(1): doing nothing for 1s...
[15:56:50] loiter(2): doing nothing for 2s...
[15:56:50] results: <generator object result_iterator at 0x106517168>
[15:56:50] loiter(3): doing nothing for 3s...
[15:56:50] Waiting for individual results:
[15:56:50] result 0: 0
[15:56:51] loiter(1): done.
[15:56:51] loiter(4): doing nothing for 4s...
[15:56:51] result 1: 10
[15:56:52] loiter(2): done.
[15:56:52] result 2: 20
[15:56:53] loiter(3): done.
[15:56:53] result 3: 30
[15:56:55] loiter(4): done.
[15:56:55] result 4: 40
This run started at 15:56:50.
The first thread executes loiter(0), so it will sleep for 0s and return even before
the second thread has a chance to start, but YMMV.5
loiter(1) and loiter(2) start immediately (because the thread pool has three
workers, it can run three functions concurrently).
This shows that the results returned by executor.map is a generator; nothing
so far would block, regardless of the number of tasks and the max_workers
setting.
Because loiter(0) is done, the first worker is now available to start the fourth
thread for loiter(3).
This is where execution may block, depending on the parameters given to the
loiter calls: the __next__ method of the results generator must wait until the
first future is complete. In this case, it won�t block because the call to loi
ter(0) finished before this loop started. Note that everything up to this point
happened within the same second: 15:56:50.
loiter(1) is done one second later, at 15:56:51. The thread is freed to start
loiter(4).
Experimenting with Executor.map | 519
The result of loiter(1) is shown: 10. Now the for loop will block waiting for
the result of loiter(2).
The pattern repeats: loiter(2) is done, its result is shown; same with loiter(3).
There is a 2s delay until loiter(4) is done, because it started at 15:56:51 and
did nothing for 4s.
The Executor.map function is easy to use but it has a feature that may or may not be
helpful, depending on your needs: it returns the results exactly in the same order as the
calls are started: if the first call takes 10s to produce a result, and the others take 1s each,
your code will block for 10s as it tries to retrieve the first result of the generator returned
by map. After that, you�ll get the remaining results without blocking because they will
be done. That�s OK when you must have all the results before proceeding, but often it�s
preferable to get the results as they are ready, regardless of the order they were submitted.
To do that, you need a combination of the Executor.submit method and the fu
tures.as_completed function, as we saw in Example 17-4. We�ll come back to this
technique in �Using futures.as_completed� on page 527.
The combination of executor.submit and futures.as_comple
ted is more flexible than executor.map because you can submit
different callables and arguments, while executor.map is designed
to run the same callable on the different arguments. In
addition, the set of futures you pass to futures.as_completed may
come from more than one executor�perhaps some were created
by a ThreadPoolExecutor instance while others are from a Proc
essPoolExecutor.
In the next section, we will resume the flag download examples with new requirements
that will force us to iterate over the results of futures.as_completed instead of using
executor.map.
Downloads with Progress Display and Error Handling
As mentioned, the scripts in �Example: Web Downloads in Three Styles� on page 505
have no error handling to make them easier to read and to contrast the structure of the
three approaches: sequential, threaded, and asynchronous.
In order to test the handling of a variety of error conditions, I created the flags2
examples:
flags2_common.py
This module contains common functions and settings used by all flags2 examples,
including a main function, which takes care of command-line parsing, timing, and
520 | Chapter 17: Concurrency with Futures
reporting results. This is really support code, not directly relevant to the subject of
this chapter, so the source code is in Appendix A, Example A-10.
flags2_sequential.py
A sequential HTTP client with proper error handling and progress bar display. Its
download_one function is also used by flags2_threadpool.py.
flags2_threadpool.py
Concurrent HTTP client based on futures.ThreadPoolExecutor to demonstrate
error handling and integration of the progress bar.
flags2_asyncio.py
Same functionality as previous example but implemented with asyncio and
aiohttp. This will be covered in �Enhancing the asyncio downloader Script� on
page 554, in Chapter 18.
Be Careful When Testing Concurrent Clients
When testing concurrent HTTP clients on public HTTP servers,
you may generate many requests per second, and that�s how denialof-
service (DoS) attacks are made. We don�t want to attack anyone,
just learn how to build high-performance clients. Carefully
throttle your clients when hitting public servers. For highconcurrency
experiments, set up a local HTTP server for testing.
Instructions for doing it are in the README.rst file in the 17-
futures/countries/ directory of the Fluent Python code repository.
The most visible feature of the flags2 examples is that they have an animated, textmode
progress bar implemented with the TQDM package. I posted a 108s video on
YouTube to show the progress bar and contrast the speed of the three flags2 scripts.
In the video, I start with the sequential download, but I interrupt it after 32s because it
was going to take more than 5 minutes to hit on 676 URLs and get 194 flags; I then run
the threaded and asyncio scripts three times each, and every time they complete the
job in 6s or less (i.e., more than 60 times faster). Figure 17-1 shows two screenshots:
during and after running flags2_threadpool.py.
Downloads with Progress Display and Error Handling | 521
Figure 17-1. Top-left: flags2_threadpool.py running with live progress bar generated by
tqdm; bottom-right: same terminal window after the script is finished.
TQDM is very easy to use, the simplest example appears in an animated .gif in the
project�s README.md. If you type the following code in the Python console after installing
the tqdm package, you�ll see an animated progress bar were the comment is:
>>> import time
>>> from tqdm import tqdm
>>> for i in tqdm(range(1000)):
... time.sleep(.01)
...
>>> # -> progress bar will appear here <-
Besides the neat effect, the tqdm function is also interesting conceptually: it consumes
any iterable and produces an iterator which, while it�s consumed, displays the progress
bar and estimates the remaining time to complete all iterations. To compute that estimate,
tqdm needs to get an iterable that has a len, or receive as a second argument the
expected number of items. Integrating TQDM with our flags2 examples provide an
opportunity to look deeper into how the concurrent scripts actually work, by forcing
us to use the futures.as_completed and the asyncio.as_completed functions so that
tqdm can display progress as each future is completed.
The other feature of the flags2 example is a command-line interface. All three scripts
accept the same options, and you can see them by running any of the scripts with the
-h option. Example 17-8 shows the help text.
Example 17-8. Help screen for the scripts in the flags2 series
$ python3 flags2_threadpool.py -h
usage: flags2_threadpool.py [-h] [-a] [-e] [-l N] [-m CONCURRENT] [-s LABEL]
[-v]
[CC [CC ...]]
522 | Chapter 17: Concurrency with Futures
6. Before configuring Cloudflare, I got HTTP 503 errors�Service Temporarily Unavailable�when testing the
scripts with a few dozen concurrent requests on my inexpensive shared host account. Now those errors are
gone.
Download flags for country codes. Default: top 20 countries by population.
positional arguments:
CC country code or 1st letter (eg. B for BA...BZ)
optional arguments:
-h, --help show this help message and exit
-a, --all get all available flags (AD to ZW)
-e, --every get flags for every possible code (AA...ZZ)
-l N, --limit N limit to N first codes
-m CONCURRENT, --max_req CONCURRENT
maximum concurrent requests (default=30)
-s LABEL, --server LABEL
Server to hit; one of DELAY, ERROR, LOCAL, REMOTE
(default=LOCAL)
-v, --verbose output detailed progress info
All arguments are optional. The most important arguments are discussed next.
One option you can�t ignore is -s/--server: it lets you choose which HTTP server and
base URL will be used in the test. You can pass one of four strings to determine where
the script will look for the flags (the strings are case insensitive):
LOCAL
Use http://localhost:8001/flags; this is the default. You should configure a
local HTTP server to answer at port 8001. I used Nginx for my tests. The README.
rst file for this chapter�s example code explains how to install and configure
it.
REMOTE
Use http://flupy.org/data/flags; that is a public website owned by me, hosted
on a shared server. Please do not pound it with too many concurrent requests. The
flupy.org domain is handled by a free account on the Cloudflare CDN so you may
notice that the first downloads are slower, but they get faster when the CDN cache
warms up.6
DELAY
Use http://localhost:8002/flags; a proxy delaying HTTP responses should be
listening at port 8002. I used a Mozilla Vaurien in front of my local Nginx to introduce
delays. The previously mentioned README.rst file has instructions for running
a Vaurien proxy.
Downloads with Progress Display and Error Handling | 523
ERROR
Use http://localhost:8003/flags; a proxy introducing HTTP errors and delaying
responses should be installed at port 8003. I used a different Vaurien configuration
for this.
The LOCAL option only works if you configure and start a local
HTTP server on port 8001. The DELAY and ERROR options require
proxies listening on ports 8002 and 8003. Configuring Nginx and
Mozilla Vaurien to enable these options is explained in the 17-
futures/countries/README.rst file in the Fluent Python code
repository on GitHub.
By default, each flags2 script will fetch the flags of the 20 most populous countries
from the LOCAL server (http://localhost:8001/flags) using a default number of
concurrent connections, which varies from script to script. Example 17-9 shows a sample
run of the flags2_sequential.py script using all defaults.
Example 17-9. Running flags2_sequential.py with all defaults: LOCAL site, top-20
flags, 1 concurrent connection
$ python3 flags2_sequential.py
LOCAL site: http://localhost:8001/flags
Searching for 20 flags: from BD to VN
1 concurrent connection will be used.
--------------------
20 flags downloaded.
Elapsed time: 0.10s
You can select which flags will be downloaded in several ways. Example 17-10 shows
how to download all flags with country codes starting with the letters A, B, or C.
Example 17-10. Run flags2_threadpool.py to fetch all flags with country codes prefixes
A, B, or C from DELAY server
$ python3 flags2_threadpool.py -s DELAY a b c
DELAY site: http://localhost:8002/flags
Searching for 78 flags: from AA to CZ
30 concurrent connections will be used.
--------------------
43 flags downloaded.
35 not found.
Elapsed time: 1.72s
Regardless of how the country codes are selected, the number of flags to fetch can be
limited with the -l/--limit option. Example 17-11 demonstrates how to run exactly
100 requests, combining the -a option to get all flags with -l 100.
524 | Chapter 17: Concurrency with Futures
Example 17-11. Run flags2_asyncio.py to get 100 flags (-al 100) from the ERROR server,
using 100 concurrent requests (-m 100)
$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100
ERROR site: http://localhost:8003/flags
Searching for 100 flags: from AD to LK
100 concurrent connections will be used.
--------------------
73 flags downloaded.
27 errors.
Elapsed time: 0.64s
That�s the user interface of the flags2 examples. Let�s see how they are implemented.
Error Handling in the flags2 Examples
The common strategy adopted in all three examples to deal with HTTP errors is that
404 errors (Not Found) are handled by the function in charge of downloading a single
file (download_one). Any other exception propagates to be handled by the down
load_many function.
Again, we�ll start by studying the sequential code, which is easier to follow�and mostly
reused by the thread pool script. Example 17-12 shows the functions that perform the
actual downloads in the flags2_sequential.py and flags2_threadpool.py scripts.
Example 17-12. flags2_sequential.py: basic functions in charge of downloading; both
are reused in flags2_threadpool.py
def get_flag(base_url, cc):
url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower())
resp = requests.get(url)
if resp.status_code != 200:
resp.raise_for_status()
return resp.content
def download_one(cc, base_url, verbose=False):
try:
image = get_flag(base_url, cc)
except requests.exceptions.HTTPError as exc:
res = exc.response
if res.status_code == 404:
status = HTTPStatus.not_found
msg = 'not found'
else:
raise
else:
save_flag(image, cc.lower() + '.gif')
status = HTTPStatus.ok
msg = 'OK'
Downloads with Progress Display and Error Handling | 525
if verbose:
print(cc, msg)
return Result(status, cc)
get_flag does no error handling, it uses requests.Response.raise_for_sta
tus to raise an exception for any HTTP code other than 200.
download_one catches requests.exceptions.HTTPError to handle HTTP code
404 specifically�
�by setting its local status to HTTPStatus.not_found; HTTPStatus is an Enum
imported from flags2_common (Example A-10).
Any other HTTPError exception is re-raised; other exceptions will just propagate
to the caller.
If the -v/--verbose command-line option is set, the country code and status
message will be displayed; this how you�ll see progress in the verbose mode.
The Result namedtuple returned by download_one will have a status field with
a value of HTTPStatus.not_found or HTTPStatus.ok.
Example 17-13 lists the sequential version of the download_many function. This code is
straightforward, but its worth studying to contrast with the concurrent versions coming
up. Focus on how it reports progress, handles errors, and tallies downloads.
Example 17-13. flags2_sequential.py: the sequential implementation of download_
many
def download_many(cc_list, base_url, verbose, max_req):
counter = collections.Counter()
cc_iter = sorted(cc_list)
if not verbose:
cc_iter = tqdm.tqdm(cc_iter)
for cc in cc_iter:
try:
res = download_one(cc, base_url, verbose)
except requests.exceptions.HTTPError as exc:
error_msg = 'HTTP error {res.status_code} - {res.reason}'
error_msg = error_msg.format(res=exc.response)
except requests.exceptions.ConnectionError as exc:
error_msg = 'Connection error'
else:
error_msg = ''
status = res.status
if error_msg:
status = HTTPStatus.error
counter[status] += 1
if verbose and error_msg:
526 | Chapter 17: Concurrency with Futures
print('*** Error for {}: {}'.format(cc, error_msg))
return counter
This Counter will tally the different download outcomes: HTTPStatus.ok,
HTTPStatus.not_found, or HTTPStatus.error.
cc_iter holds the list of the country codes received as arguments, ordered
alphabetically.
If not running in verbose mode, cc_iter is passed to the tqdm function, which
will return an iterator that yields the items in cc_iter while also displaying the
animated progress bar.
This for loop iterates over cc_iter and�
�performs the download by successive calls to download_one.
HTTP-related exceptions raised by get_flag and not handled by down
load_one are handled here.
Other network-related exceptions are handled here. Any other exception will
abort the script, because the flags2_common.main function that calls down
load_many has no try/except.
If no exception escaped download_one, then the status is retrieved from the
HTTPStatus namedtuple returned by download_one.
If there was an error, set the local status accordingly.
Increment the counter by using the value of the HTTPStatus Enum as key.
If running in verbose mode, display the error message for the current country
code, if any.
Return the counter so that the main function can display the numbers in its final
report.
We�ll now study the refactored thread pool example, flags2_threadpool.py.
Using futures.as_completed
In order to integrate the TQDM progress bar and handle errors on each request, the
flags2_threadpool.py script uses futures.ThreadPoolExecutor with the
futures.as_completed function we�ve already seen. Example 17-14 is the full listing of
flags2_threadpool.py. Only the download_many function is implemented; the other
functions are reused from the flags2_common and flags2_sequential modules.
Example 17-14. flags2_threadpool.py: full listing
import collections
from concurrent import futures
Downloads with Progress Display and Error Handling | 527
import requests
import tqdm
from flags2_common import main, HTTPStatus
from flags2_sequential import download_one
DEFAULT_CONCUR_REQ = 30
MAX_CONCUR_REQ = 1000
def download_many(cc_list, base_url, verbose, concur_req):
counter = collections.Counter()
with futures.ThreadPoolExecutor(max_workers=concur_req) as executor:
to_do_map = {}
for cc in sorted(cc_list):
future = executor.submit(download_one,
cc, base_url, verbose)
to_do_map[future] = cc
done_iter = futures.as_completed(to_do_map)
if not verbose:
done_iter = tqdm.tqdm(done_iter, total=len(cc_list))
for future in done_iter:
try:
res = future.result()
except requests.exceptions.HTTPError as exc:
error_msg = 'HTTP {res.status_code} - {res.reason}'
error_msg = error_msg.format(res=exc.response)
except requests.exceptions.ConnectionError as exc:
error_msg = 'Connection error'
else:
error_msg = ''
status = res.status
if error_msg:
status = HTTPStatus.error
counter[status] += 1
if verbose and error_msg:
cc = to_do_map[future]
print('*** Error for {}: {}'.format(cc, error_msg))
return counter
if __name__ == '__main__':
main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)
Import the progress-bar display library.
Import one function and one Enum from the flags2_common module.
Reuse the donwload_one from flags2_sequential (Example 17-12).
528 | Chapter 17: Concurrency with Futures
If the -m/--max_req command-line option is not given, this will be the
maximum number of concurrent requests, implemented as the size of the thread
pool; the actual number may be smaller, if the number of flags to download is
smaller.
MAX_CONCUR_REQ caps the maximum number of concurrent requests regardless
of the number of flags to download or the -m/--max_req command-line option;
it�s a safety precaution.
Create the executor with max_workers set to concur_req, computed by the main
function as the smaller of: MAX_CONCUR_REQ, the length of cc_list, and the value
of the -m/--max_req command-line option. This avoids creating more threads
than necessary.
This dict will map each Future instance�representing one download�with
the respective country code for error reporting.
Iterate over the list of country codes in alphabetical order. The order of the results
will depend on the timing of the HTTP responses more than anything, but if
the size of the thread pool (given by concur_req) is much smaller than
len(cc_list), you may notice the downloads batched alphabetically.
Each call to executor.submit schedules the execution of one callable and
returns a Future instance. The first argument is the callable, the rest are the
arguments it will receive.
Store the future and the country code in the dict.
futures.as_completed returns an iterator that yields futures as they are done.
If not in verbose mode, wrap the result of as_completed with the tqdm function
to display the progress bar; because done_iter has no len, we must tell tqdm
what is the expected number of items as the total= argument, so tqdm can
estimate the work remaining.
Iterate over the futures as they are completed.
Calling the result method on a future either returns the value returned by the
callable, or raises whatever exception was caught when the callable was executed.
This method may block waiting for a resolution, but not in this example because
as_completed only returns futures that are done.
Handle the potential exceptions; the rest of this function is identical to the
sequential version of download_many (Example 17-13), except for the next
callout.
To provide context for the error message, retrieve the country code from the
to_do_map using the current future as key. This was not necessary in the
sequential version because we were iterating over the list of country codes, so
we had the current cc; here we are iterating over the futures.
Downloads with Progress Display and Error Handling | 529
7. The threading module has been available since Python 1.5.1 (1998), yet some insist on using the old thread
module. In Python 3, it was renamed to _thread to highlight the fact that it�s just a low-level implementation
detail, and shouldn�t be used in application code.
Example 17-14 uses an idiom that�s very useful with futures.as_completed: building
a dict to map each future to other data that may be useful when the future is completed.
Here the to_do_map maps each future to the country code assigned to it. This makes it
easy to do follow-up processing with the result of the futures, despite the fact that they
are produced out of order.
Python threads are well suited for I/O-intensive applications, and the concurrent.fu
tures package makes them trivially simple to use for certain use cases. This concludes
our basic introduction to concurrent.futures. Let�s now discuss alternatives for when
ThreadPoolExecutor or ProcessPoolExecutor are not suitable.
Threading and Multiprocessing Alternatives
Python has supported threads since its release 0.9.8 (1993); concurrent.futures is just
the latest way of using them. In Python 3, the original thread module was deprecated
in favor of the higher-level threading module.7 If futures.ThreadPoolExecutor is not
flexible enough for a certain job, you may need to build your own solution out of basic
threading components such as Thread, Lock, Semaphore, etc.�possibly using the
thread-safe queues of the queue module for passing data between threads. Those moving
parts are encapsulated by futures.ThreadPoolExecutor.
For CPU-bound work, you need to sidestep the GIL by launching multiple processes.
The futures.ProcessPoolExecutor is the easiest way to do it. But again, if your use
case is complex, you�ll need more advanced tools. The multiprocessing package emulates
the threading API but delegates jobs to multiple processes. For simple programs,
multiprocessing can replace threading with few changes. But multiprocessing also
offers facilities to solve the biggest challenge faced by collaborating processes: how to
pass around data.
Chapter Summary
We started the chapter by comparing two concurrent HTTP clients with a sequential
one, demonstrating significant performance gains over the sequential script.
After studying the first example based on concurrent.futures, we took a closer look
at future objects, either instances of concurrent.futures.Future, or asyncio.Fu
ture, emphasizing what these classes have in common (their differences will be emphasized
in Chapter 18). We saw how to create futures by calling Executor.sub
530 | Chapter 17: Concurrency with Futures
mit(�), and iterate over completed futures with concurrent.futures.as_comple
ted(�).
Next, we saw why Python threads are well suited for I/O-bound applications, despite
the GIL: every standard library I/O function written in C releases the GIL, so while a
given thread is waiting for I/O, the Python scheduler can switch to another thread. We
then discussed the use of multiple processes with the concurrent.futures.Proces
sPoolExecutor class, to go around the GIL and use multiple CPU cores to run cryptographic
algorithms, achieving speedups of more than 100% when using four workers.
In the following section, we took a close look at how the concurrent.futures.Thread
PoolExecutor works, with a didactic example launching tasks that did nothing for a
few seconds, except displaying their status with a timestamp.
Next we went back to the flag downloading examples. Enhancing them with a progress
bar and proper error handling prompted further exploration of the future.as_comple
ted generator function showing a common pattern: storing futures in a dict to link
further information to them when submitting, so that we can use that information when
the future comes out of the as_completed iterator.
We concluded the coverage of concurrency with threads and processes with a brief
reminder of the lower-level, but more flexible threading and multiprocessing modules,
which represent the traditional way of leveraging threads and processes in Python.
Further Reading
The concurrent.futures package was contributed by Brian Quinlan, who presented
it in a great talk titled �The Future Is Soon!� at PyCon Australia 2010. Quinlan�s talk has
no slides; he shows what the library does by typing code directly in the Python console.
As a motivating example, the presentation features a short video with XKCD cartoonist/
programmer Randall Munroe making an unintended DOS attack on Google Maps to
build a colored map of driving times around his city. The formal introduction to the
library is PEP 3148 - futures - execute computations asynchronously. In the PEP,
Quinlan wrote that the concurrent.futures library was �heavily influenced by the Java
java.util.concurrent package.�
Parallel Programming with Python (Packt), by Jan Palach, covers several tools for concurrent
programming, including the concurrent.futures, threading, and multiproc
essing modules. It goes beyond the standard library to discuss Celery, a task queue
used to distribute work across threads and processes, even on different machines. In
the Django community, Celery is probably the most widely used system to offload heavy
tasks such as PDF generation to other processes, thus avoiding delays in producing an
HTTP response.
Further Reading | 531
8. Thanks to Lucas Brunialti for sending me a link to this talk.
In the Beazley and Jones Python Cookbook, 3E (O�Reilly) there are recipes using con
current.futures starting with �Recipe 11.12. Understanding Event-Driven I/O.�
�Recipe 12.7. Creating a Thread Pool� shows a simple TCP echo server, and �Recipe
12.8. Performing Simple Parallel Programming� offers a very practical example: analyzing
a whole directory of gzip compressed Apache logfiles with the help of a Proces
sPoolExecutor. For more about threads, the entire Chapter 12 of Beazley and Jones is
great, with special mention to �Recipe 12.10. Defining an Actor Task,� which demonstrates
the Actor model: a proven way of coordinating threads through message passing.
Brett Slatkin�s Effective Python (Addison-Wesley) has a multitopic chapter about concurrency,
including coverage of coroutines, concurrent.futures with threads and
processes, and the use of locks and queues for thread programming without the Thread
PoolExecutor.
High Performance Python (O�Reilly) by Micha Gorelick and Ian Ozsvald and The Python
Standard Library by Example (Addison-Wesley), by Doug Hellmann, also cover threads
and processes.
For a modern take on concurrency without threads or callbacks, Seven Concurrency
Models in Seven Weeks, by Paul Butcher (Pragmatic Bookshelf) is an excellent read. I
love its subtitle: �When Threads Unravel.� In that book, threads and locks are covered
in Chapter 1, and the remaining six chapters are devoted to modern alternatives to
concurrent programming, as supported by different languages. Python, Ruby, and Java?
Script are not among them.
If you are intrigued about the GIL, start with the Python Library and Extension FAQ
(�Can�t we get rid of the Global Interpreter Lock?�). Also worth reading are posts by
Guido van Rossum and Jesse Noller (contributor of the multiprocessing package): �It
isn�t Easy to Remove the GIL� and �Python Threads and the Global Interpreter Lock.�
Finally, David Beazley has a detailed exploration on the inner workings of the GIL:
�Understanding the Python GIL.�8 In slide #54 of the presentation, Beazley reports some
alarming results, including a 20? increase in processing time for a particular benchmark
with the new GIL algorithm introduced in Python 3.2. However, Beazley apparently
used an empty while True: pass to simulate CPU-bound work, and that is not realistic.
The issue is not significant with real workloads, according to a comment by Antoine
Pitrou�who implemented the new GIL algorithm�in the bug report submitted by
Beazley.
While the GIL is real problem and is not likely to go away soon, Jesse Noller and Richard
Oudkerk contributed a library to make it easier to work around it in CPU-bound applications:
the multiprocessing package, which emulates the threading API across
processes, along with supporting infrastructure of locks, queues, pipes, shared memory,
532 | Chapter 17: Concurrency with Futures
9. Slide #9 from �A Curious Course on Coroutines and Concurrency,� tutorial presented at PyCon
2009.
etc. The package was introduced in PEP 371 � Addition of the multiprocessing package
to the standard library. The official documentation for the package is a 93 KB .rst file�
that�s about 63 pages�making it one of the longest chapters in the Python standard
library. Multiprocessing is the basis for the concurrent.futures.ProcessPoolExecu
tor.
For CPU- and data-intensive parallel processing, a new option with a lot of momentum
in the big data community is the Apache Spark distributed computing engine, offering
a friendly Python API and support for Python objects as data, as shown in their examples
page.
Two elegant and super easy libraries for parallelizing tasks over processes are lelo by
Joao S. O. Bueno and python-parallelize by Nat Pryce. The lelo package defines a
@parallel decorator that you can apply to any function to magically make it unblocking:
when you call the decorated function, its execution is started in another process. Nat
Pryce�s python-parallelize package provides a parallelize generator that you can
use to distribute the execution of a for loop over multiple CPUs. Both packages use the
multiprocessing module under the covers.
Soapbox
Thread Avoidance
Concurrency: one of the most difficult topics in computer science (usually best avoided).
9
� David Beazley
Python coach and mad scientist
I agree with the apparently contradictory quotes by David Beazley, above, and Michele
Simionato at the start of this chapter. After attending a concurrency course at the university�
in which �concurrent programming� was equated to managing threads and
locks�I came to the conclusion that I don�t want to manage threads and locks myself,
any more than I want to manage memory allocation and deallocation. Those jobs are
best carried out by the systems programmers who have the know-how, the inclination,
and the time to get them right�hopefully.
That�s why I think the concurrent.futures package is exciting: it treats threads, processes,
and queues as infrastructure at your service, not something you have to deal with
directly. Of course, it�s designed with simple jobs in mind, the so-called �embarrassingly
parallel� problems. But that�s a large slice of the concurrency problems we face when
writing applications�as opposed to operating systems or database servers, as Simionato
points out in that quote.
Further Reading | 533
For �nonembarrassing� concurrency problems, threads and locks are not the answer
either. Threads will never disappear at the OS level, but every programming language
I�ve found exciting in the last several years provides better, higher-level, concurrency
abstractions, as the Seven Concurrency Models book demonstrates. Go, Elixir, and Clojure
are among them. Erlang�the implementation language of Elixir�is a prime example
of a language designed from the ground up with concurrency in mind. It doesn�t
excite me for a simple reason: I find its syntax ugly. Python spoiled me that way.
Jose Valim, well-known as a Ruby on Rails core contributor, designed Elixir with a
pleasant, modern syntax. Like Lisp and Clojure, Elixir implements syntactic macros.
That�s a double-edged sword. Syntactic macros enable powerful DSLs, but the proliferation
of sublanguages can lead to incompatible codebases and community fragmentation.
Lisp drowned in a flood of macros, with each Lisp shop using its own arcane dialect.
Standardizing around Common Lisp resulted in a bloated language. I hope Jose Valim
can inspire the Elixir community to avoid a similar outcome.
Like Elixir, Go is a modern language with fresh ideas. But, in some regards, it�s a conservative
language, compared to Elixir. Go doesn�t have macros, and its syntax is simpler
than Python�s. Go doesn�t support inheritance or operator overloading, and it offers
fewer opportunities for metaprogramming than Python. These limitations are considered
features. They lead to more predictable behavior and performance. That�s a big plus
in the highly concurrent, mission-critical settings where Go aims to replace C++, Java,
and Python.
While Elixir and Go are direct competitors in the high-concurrency space, their design
philosophies appeal to different crowds. Both are likely to thrive. But in the history of
programming languages, the conservative ones tend to attract more coders. I�d like to
become fluent in Go and Elixir.
About the GIL
The GIL simplifies the implementation of the CPython interpreter and of extensions
written in C, so we can thank the GIL for the vast number of extensions in C available
for Python�and that is certainly one of the key reasons why Python is so popular today.
For many years, I was under the impression that the GIL made Python threads nearly
useless beyond toy applications. It was not until I discovered that every blocking I/O call
in the standard library releases the GIL that I realized Python threads are excellent for
I/O-bound systems�the kind of applications customers usually pay me to develop,
given my professional experience.
534 | Chapter 17: Concurrency with Futures
Concurrency in the Competition
MRI�the reference implementation of Ruby�also has a GIL, so its threads are under
the same limitations as Python�s. Meanwhile, JavaScript interpreters don�t support userlevel
threads at all; asynchronous programming with callbacks is their only path to
concurrency. I mention this because Ruby and JavaScript are the closest direct competitors
to Python as general-purpose, dynamic programming languages.
Looking at the concurrency-savvy new crop of languages, Go and Elixir are probably
the ones best positioned to eat Python�s lunch. But now we have asyncio. If hordes of
people believe Node.js with raw callbacks is a viable platform for concurrent programming,
how hard can it be to win them over to Python when the asyncio ecosystem
matures? But that�s a topic for the next �Soapbox� on page 580.
Further Reading | 535

1. Slide 5 of the talk �Concurrency Is Not Parallelism (It�s Better)�.
2. Imre Simon (1943�2009) was a pioneer of computer science in Brazil who made seminal contributions to
Automata Theory and started the field of Tropical Mathematics. He was also an advocate of free software and
free culture. I was fortunate to study, work, and hang out with him.
CHAPTER 18
Concurrency with asyncio
Concurrency is about dealing with lots of things at once.
Parallelism is about doing lots of things at once.
Not the same, but related.
One is about structure, one is about execution.
Concurrency provides a way to structure a solution to solve a problem that may (but not
necessarily) be parallelizable.1
� Rob Pike
Co-inventor of the Go language
Professor Imre Simon2 liked to say there are two major sins in science: using different
words to mean the same thing and using one word to mean different things. If you do
any research on concurrent or parallel programming you will find different definitions
for �concurrency� and �parallelism.� I will adopt the informal definitions by Rob Pike,
quoted above.
For real parallelism, you must have multiple cores. A modern laptop has four CPU cores
but is routinely running more than 100 processes at any given time under normal, casual
use. So, in practice, most processing happens concurrently and not in parallel. The
computer is constantly dealing with 100+ processes, making sure each has an opportunity
to make progress, even if the CPU itself can�t do more than four things at once.
Ten years ago we used machines that were also able to handle 100 processes concurrently,
but on a single core. That�s why Rob Pike titled that talk �Concurrency Is Not Parallelism
(It�s Better).�
537
This chapter introduces asyncio, a package that implements concurrency with coroutines
driven by an event loop. It�s one of the largest and most ambitious libraries ever
added to Python. Guido van Rossum developed asyncio outside of the Python repository
and gave the project a code name of �Tulip��so you�ll see references to that flower
when researching this topic online. For example, the main discussion group is still called
python-tulip.
Tulip was renamed to asyncio when it was added to the standard library in Python 3.4.
It�s also compatible with Python 3.3�you can find it on PyPI under the new official
name. Because it uses yield from expressions extensively, asyncio is incompatible with
older versions of Python.
The Trollius project�also named after a flower�is a backport of
asyncio to Python 2.6 and newer, replacing yield from with
yield and clever callables named From and Return. A yield from
� expression becomes yield From(�); and when a coroutine needs
to return a result, you write raise Return(result) instead of
return result. Trollius is led by Victor Stinner, who is also an
asyncio core developer, and who kindly agreed to review this
chapter as this book was going into production.
In this chapter we�ll see:
� A comparison between a simple threaded program and the asyncio equivalent,
showing the relationship between threads and asynchronous tasks
� How the asyncio.Future class differs from concurrent.futures.Future
� Asynchronous versions of the flag download examples from Chapter 17
� How asynchronous programming manages high concurrency in network applications,
without using threads or processes
� How coroutines are a major improvement over callbacks for asynchronous programming
� How to avoid blocking the event loop by offloading blocking operations to a thread
pool
� Writing asyncio servers, and how to rethink web applications for high concurrency
� Why asyncio is poised to have a big impact in the Python ecosystem
Let�s get started with the simple example contrasting threading and asyncio.
538 | Chapter 18: Concurrency with asyncio
Thread Versus Coroutine: A Comparison
During a discussion about threads and the GIL, Michele Simionato posted a simple but
fun example using multiprocessing to display an animated spinner made with the
ASCII characters "|/-\" on the console while some long computation is running.
I adapted Simionato�s example to use a thread with the Threading module and then a
coroutine with asyncio, so you can see the two examples side by side and understand
how to code concurrent behavior without threads.
The output shown in Examples 18-1 and 18-2 is animated, so you really should run the
scripts to see what happens. If you�re in the subway (or somewhere else without a WiFi
connection), take a look at Figure 18-1 and imagine the \ bar before the word �thinking�
is spinning.
Figure 18-1. The scripts spinner_thread.py and spinner_asyncio.py produce similar
output: the repr of a spinner object and the text Answer: 42. In the screenshot, spinner_
asyncio.py is still running, and the spinner message \ thinking! is shown; when the
script ends, that line will be replaced by the Answer: 42.
Let�s review the spinner_thread.py script first (Example 18-1).
Example 18-1. spinner_thread.py: animating a text spinner with a thread
import threading
import itertools
import time
import sys
class Signal:
go = True
def spin(msg, signal):
Thread Versus Coroutine: A Comparison | 539
write, flush = sys.stdout.write, sys.stdout.flush
for char in itertools.cycle('|/-\\'):
status = char + ' ' + msg
write(status)
flush()
write('\x08' * len(status))
time.sleep(.1)
if not signal.go:
break
write(' ' * len(status) + '\x08' * len(status))
def slow_function():
# pretend waiting a long time for I/O
time.sleep(3)
return 42
def supervisor():
signal = Signal()
spinner = threading.Thread(target=spin,
args=('thinking!', signal))
print('spinner object:', spinner)
spinner.start()
result = slow_function()
signal.go = False
spinner.join()
return result
def main():
result = supervisor()
print('Answer:', result)
if __name__ == '__main__':
main()
This class defines a simple mutable object with a go attribute we�ll use to control
the thread from outside.
This function will run in a separate thread. The signal argument is an instance
of the Signal class just defined.
This is actually an infinite loop because itertools.cycle produces items
cycling from the given sequence forever.
The trick to do text-mode animation: move the cursor back with backspace
characters (\x08).
If the go attribute is no longer True, exit the loop.
540 | Chapter 18: Concurrency with asyncio
Clear the status line by overwriting with spaces and moving the cursor back to
the beginning.
Imagine this is some costly computation.
Calling sleep will block the main thread, but crucially, the GIL will be released
so the secondary thread will proceed.
This function sets up the secondary thread, displays the thread object, runs the
slow computation, and kills the thread.
Display the secondary thread object. The output looks like <Thread(Thread-1,
initial)>.
Start the secondary thread.
Run slow_function; this blocks the main thread. Meanwhile, the spinner is
animated by the secondary thread.
Change the state of the signal; this will terminate the for loop inside the spin
function.
Wait until the spinner thread finishes.
Run the supervisor function.
Note that, by design, there is no API for terminating a thread in Python. You must send
it a message to shut down. Here I used the signal.go attribute: when the main thread
sets it to false, the spinner thread will eventually notice and exit cleanly.
Now let�s see how the same behavior can be achieved with an @asyncio.coroutine
instead of a thread.
As noted in the �Chapter Summary� on page 498 (Chapter 16),
asyncio uses a stricter definition of �coroutine.� A coroutine
suitable for use with the asyncio API must use yield from and
not yield in its body. Also, an asyncio coroutine should be driven
by a caller invoking it through yield from or by passing the
coroutine to one of the asyncio functions such as asyn
cio.async(�) and others covered in this chapter. Finally, the
@asyncio.coroutine decorator should be applied to coroutines,
as shown in the examples.
Take a look at Example 18-2.
Example 18-2. spinner_asyncio.py: animating a text spinner with a coroutine
import asyncio
import itertools
import sys
Thread Versus Coroutine: A Comparison | 541
@asyncio.coroutine
def spin(msg):
write, flush = sys.stdout.write, sys.stdout.flush
for char in itertools.cycle('|/-\\'):
status = char + ' ' + msg
write(status)
flush()
write('\x08' * len(status))
try:
yield from asyncio.sleep(.1)
except asyncio.CancelledError:
break
write(' ' * len(status) + '\x08' * len(status))
@asyncio.coroutine
def slow_function():
# pretend waiting a long time for I/O
yield from asyncio.sleep(3)
return 42
@asyncio.coroutine
def supervisor():
spinner = asyncio.async(spin('thinking!'))
print('spinner object:', spinner)
result = yield from slow_function()
spinner.cancel()
return result
def main():
loop = asyncio.get_event_loop()
result = loop.run_until_complete(supervisor())
loop.close()
print('Answer:', result)
if __name__ == '__main__':
main()
Coroutines intended for use with asyncio should be decorated with @asyn
cio.coroutine. This not mandatory, but is highly advisable. See explanation
following this listing.
Here we don�t need the signal argument that was used to shut down the thread
in the spin function of Example 18-1.
542 | Chapter 18: Concurrency with asyncio
Use yield from asyncio.sleep(.1) instead of just time.sleep(.1), to sleep
without blocking the event loop.
If asyncio.CancelledError is raised after spin wakes up, it�s because
cancellation was requested, so exit the loop.
slow_function is now a coroutine, and uses yield from to let the event loop
proceed while this coroutine pretends to do I/O by sleeping.
The yield from asyncio.sleep(3) expression handles the control flow to the
main loop, which will resume this coroutine after the sleep delay.
supervisor is now a coroutine as well, so it can drive slow_function with yield
from.
asyncio.async(�) schedules the spin coroutine to run, wrapping it in a Task
object, which is returned immediately.
Display the Task object. The output looks like <Task pending coro=<spin()
running at spinner_asyncio.py:12>>.
Drive the slow_function(). When that is done, get the returned value.
Meanwhile, the event loop will continue running because slow_function
ultimately uses yield from asyncio.sleep(3) to hand control back to the main
loop.
A Task object can be cancelled; this raises asyncio.CancelledError at the yield
line where the coroutine is currently suspended. The coroutine may catch the
exception and delay or even refuse to cancel.
Get a reference to the event loop.
Drive the supervisor coroutine to completion; the return value of the coroutine
is the return value of this call.
Never use time.sleep(�) in asyncio coroutines unless you want
to block the main thread, therefore freezing the event loop and
probably the whole application as well. If a coroutine needs to
spend some time doing nothing, it should yield from asyn
cio.sleep(DELAY).
The use of the @asyncio.coroutine decorator is not mandatory, but highly recommended:
it makes the coroutines stand out among regular functions, and helps with
debugging by issuing a warning when a coroutine is garbage collected without being
yielded from�which means some operation was left unfinished and is likely a bug. This
is not a priming decorator.
Thread Versus Coroutine: A Comparison | 543
Note that the line count of spinner_thread.py and spinner_asyncio.py is nearly the same.
The supervisor functions are the heart of these examples. Let�s compare them in detail.
Example 18-3 lists only the supervisor from the Threading example.
Example 18-3. spinner_thread.py: the threaded supervisor function
def supervisor():
signal = Signal()
spinner = threading.Thread(target=spin,
args=('thinking!', signal))
print('spinner object:', spinner)
spinner.start()
result = slow_function()
signal.go = False
spinner.join()
return result
For comparison, Example 18-4 shows the supervisor coroutine.
Example 18-4. spinner_asyncio.py: the asynchronous supervisor coroutine
@asyncio.coroutine
def supervisor():
spinner = asyncio.async(spin('thinking!'))
print('spinner object:', spinner)
result = yield from slow_function()
spinner.cancel()
return result
Here is a summary of the main differences to note between the two supervisor implementations:
� An asyncio.Task is roughly the equivalent of a threading.Thread. Victor Stinner,
special technical reviewer for this chapter, points out that �a Task is like a green
thread in libraries that implement cooperative multitasking, such as gevent.�
� A Task drives a coroutine, and a Thread invokes a callable.
� You don�t instantiate Task objects yourself, you get them by passing a coroutine to
asyncio.async(�) or loop.create_task(�).
� When you get a Task object, it is already scheduled to run (e.g., by asyn
cio.async); a Thread instance must be explicitly told to run by calling its start
method.
� In the threaded supervisor, the slow_function is a plain function and is directly
invoked by the thread. In the asyncio supervisor, slow_function is a coroutine
driven by yield from.
� There�s no API to terminate a thread from the outside, because a thread could be
interrupted at any point, leaving the system in an invalid state. For tasks, there is
544 | Chapter 18: Concurrency with asyncio
the Task.cancel() instance method, which raises CancelledError inside the coroutine.
The coroutine can deal with this by catching the exception in the yield
where it�s suspended.
� The supervisor coroutine must be executed with loop.run_until_complete in
the main function.
This comparison should help you understand how concurrent jobs are orchestrated
with asyncio, in contrast to how it�s done with the more familiar Threading module.
One final point related to threads versus coroutines: if you�ve done any nontrivial programming
with threads, you know how challenging it is to reason about the program
because the scheduler can interrupt a thread at any time. You must remember to hold
locks to protect the critical sections of your program, to avoid getting interrupted in the
middle of a multistep operation�which could leave data in an invalid state.
With coroutines, everything is protected against interruption by default. You must explicitly
yield to let the rest of the program run. Instead of holding locks to synchronize
the operations of multiple threads, you have coroutines that are �synchronized� by definition:
only one of them is running at any time. And when you want to give up control,
you use yield or yield from to give control back to the scheduler. That�s why it is
possible to safely cancel a coroutine: by definition, a coroutine can only be cancelled
when it�s suspended at a yield point, so you can perform cleanup by handling the
CancelledError exception.
We�ll now see how the asyncio.Future class differs from the concurrent.futures.Fu
ture class we saw in Chapter 17.
asyncio.Future: Nonblocking by Design
The asyncio.Future and the concurrent.futures.Future classes have mostly the
same interface, but are implemented differently and are not interchangeable. PEP-3156
� Asynchronous IO Support Rebooted: the �asyncio� Module has this to say about this
unfortunate situation:
In the future (pun intended) we may unify asyncio.Future and concurrent.fu
tures.Future (e.g., by adding an __iter__ method to the latter that works with yield
from).
As mentioned in �Where Are the Futures?� on page 511, futures are created only as the
result of scheduling something for execution. In asyncio, BaseEventLoop.cre
ate_task(�) takes a coroutine, schedules it to run, and returns an asyncio.Task instance�
which is also an instance of asyncio.Future because Task is a subclass of
Future designed to wrap a coroutine. This is analogous to how we create concur
rent.futures.Future instances by invoking Executor.submit(�).
Thread Versus Coroutine: A Comparison | 545
Like its concurrent.futures.Future counterpart, the asyncio.Future class provides
.done(), .add_done_callback(�), and .results() methods, among others. The
first two methods work as described in �Where Are the Futures?� on page 511,
but .result() is very different.
In asyncio.Future, the .result() method takes no arguments, so you can�t specify a
timeout. Also, if you call .result() and the future is not done, it does not block waiting
for the result. Instead, an asyncio.InvalidStateError is raised.
However, the usual way to get the result of an asyncio.Future is to yield from it, as
we�ll see in Example 18-8.
Using yield from with a future automatically takes care of waiting for it to finish,
without blocking the event loop�because in asyncio, yield from is used to give control
back to the event loop.
Note that using yield from with a future is the coroutine equivalent of the functionality
offered by add_done_callback: instead of triggering a callback, when the delayed operation
is done, the event loop sets the result of the future, and the yield from expression
produces a return value inside our suspended coroutine, allowing it to resume.
In summary, because asyncio.Future is designed to work with yield from, these
methods are often not needed:
� You don�t need my_future.add_done_callback(�) because you can simply put
whatever processing you would do after the future is done in the lines that follow
yield from my_future in your coroutine. That�s the big advantage of having coroutines:
functions that can be suspended and resumed.
� You don�t need my_future.result() because the value of a yield from expression
on a future is the result (e.g., result = yield from my_future).
Of course, there are situations in which .done(), .add_done_callback(�), and .re
sults() are useful. But in normal usage, asyncio futures are driven by yield from,
not by calling those methods.
We�ll now consider how yield from and the asyncio API brings together futures, tasks,
and coroutines.
Yielding from Futures, Tasks, and Coroutines
In asyncio, there is a close relationship between futures and coroutines because you
can get the result of an asyncio.Future by yielding from it. This means that res =
yield from foo() works if foo is a coroutine function (therefore it returns a coroutine
object when called) or if foo is a plain function that returns a Future or Task instance.
546 | Chapter 18: Concurrency with asyncio
3. Suggested by Petr Viktorin in a September 11, 2014, message to the Python-ideas list.
This is one of the reasons why coroutines and futures are interchangeable in many parts
of the asyncio API.
In order to execute, a coroutine must be scheduled, and then it�s wrapped in an asyn
cio.Task. Given a coroutine, there are two main ways of obtaining a Task:
asyncio.async(coro_or_future, *, loop=None)
This function unifies coroutines and futures: the first argument can be either one.
If it�s a Future or Task, it�s returned unchanged. If it�s a coroutine, async calls
loop.create_task(�) on it to create a Task. An optional event loop may be passed
as the loop= keyword argument; if omitted, async gets the loop object by calling
asyncio.get_event_loop().
BaseEventLoop.create_task(coro)
This method schedules the coroutine for execution and returns an asyncio.Task
object. If called on a custom subclass of BaseEventLoop, the object returned may
be an instance of some other Task-compatible class provided by an external library
(e.g., Tornado).
BaseEventLoop.create_task(�) is only available in Python 3.4.2
or later. If you�re using an older version of Python 3.3 or 3.4, you
need to use asyncio.async(�), or install a more recent version of
asyncio from PyPI.
Several asyncio functions accept coroutines and wrap them in asyncio.Task objects
automatically, using asyncio.async internally. One example is BaseEventLoop.run_un
til_complete(�).
If you want to experiment with futures and coroutines on the Python console or in small
tests, you can use the following snippet:3
>>> import asyncio
>>> def run_sync(coro_or_future):
... loop = asyncio.get_event_loop()
... return loop.run_until_complete(coro_or_future)
...
>>> a = run_sync(some_coroutine())
The relationship between coroutines, futures, and tasks is documented in section 18.5.3.
Tasks and coroutines of the asyncio documentation, where you�ll find this note:
In this documentation, some methods are documented as coroutines, even if they are
plain Python functions returning a Future. This is intentional to have a freedom of
tweaking the implementation of these functions in the future.
Thread Versus Coroutine: A Comparison | 547
Having covered these fundamentals, we�ll now study the code for the asynchronous flag
download script flags_asyncio.py demonstrated along with the sequential and thread
pool scripts in Example 17-1 (Chapter 17).
Downloading with asyncio and aiohttp
As of Python 3.4, asyncio only supports TCP and UDP directly. For HTTP or any other
protocol, we need third-party packages; aiohttp is the one everyone seems to be using
for asyncio HTTP clients and servers at this time.
Example 18-5 is the full listing for the flag downloading script flags_asyncio.py. Here is
a high-level view of how it works:
1. We start the process in download_many by feeding the event loop with several coroutine
objects produced by calling download_one.
2. The asyncio event loop activates each coroutine in turn.
3. When a client coroutine such as get_flag uses yield from to delegate to a library
coroutine�such as aiohttp.request�control goes back to the event loop, which
can execute another previously scheduled coroutine.
4. The event loop uses low-level APIs based on callbacks to get notified when a blocking
operation is completed.
5. When that happens, the main loop sends a result to the suspended coroutine.
6. The coroutine then advances to the next yield, for example, yield from
resp.read() in get_flag. The event loop takes charge again. Steps 4, 5, and 6 repeat
until the event loop is terminated.
This is similar to the example we looked at in �The Taxi Fleet Simulation� on page
490, where a main loop started several taxi processes in turn. As each taxi process yielded,
the main loop scheduled the next event for that taxi (to happen in the future), and
proceeded to activate the next taxi in the queue. The taxi simulation is much simpler,
and you can easily understand its main loop. But the general flow is the same as in
asyncio: a single-threaded program where a main loop activates queued coroutines one
by one. Each coroutine advances a few steps, then yields control back to the main loop,
which then activates the next coroutine in the queue.
Now let�s review Example 18-5 play by play.
Example 18-5. flags_asyncio.py: asynchronous download script with asyncio and
aiohttp
import asyncio
import aiohttp
548 | Chapter 18: Concurrency with asyncio
from flags import BASE_URL, save_flag, show, main
@asyncio.coroutine
def get_flag(cc):
url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())
resp = yield from aiohttp.request('GET', url)
image = yield from resp.read()
return image
@asyncio.coroutine
def download_one(cc):
image = yield from get_flag(cc)
show(cc)
save_flag(image, cc.lower() + '.gif')
return cc
def download_many(cc_list):
loop = asyncio.get_event_loop()
to_do = [download_one(cc) for cc in sorted(cc_list)]
wait_coro = asyncio.wait(to_do)
res, _ = loop.run_until_complete(wait_coro)
loop.close()
return len(res)
if __name__ == '__main__':
main(download_many)
aiohttp must be installed�it�s not in the standard library.
Reuse some functions from the flags module (Example 17-2).
Coroutines should be decorated with @asyncio.coroutine.
Blocking operations are implemented as coroutines, and your code delegates to
them via yield from so they run asynchronously.
Reading the response contents is a separate asynchronous operation.
download_one must also be a coroutine, because it uses yield from.
The only difference from the sequential implementation of download_one are
the words yield from in this line; the rest of the function body is exactly as
before.
Get a reference to the underlying event-loop implementation.
Build a list of generator objects by calling the download_one function once for
each flag to be retrieved.
Downloading with asyncio and aiohttp | 549
Despite its name, wait is not a blocking function. It�s a coroutine that completes
when all the coroutines passed to it are done (that�s the default behavior of wait;
see explanation after this example).
Execute the event loop until wait_coro is done; this is where the script will block
while the event loop runs. We ignore the second item returned by run_un
til_complete. The reason is explained next.
Shut down the event loop.
It would be nice if event loop instances were context managers,
so we could use a with block to make sure the loop is closed.
However, the situation is complicated by the fact that client code
never creates the event loop directly, but gets a reference to it by
calling asyncio.get_event_loop(). Sometimes our code does
not �own� the event loop, so it would be wrong to close it. For
example, when using an external GUI event loop with a package
like Quamash, the Qt library is responsible for shutting down the
loop when the application quits.
The asyncio.wait(�) coroutine accepts an iterable of futures or coroutines; wait wraps
each coroutine in a Task. The end result is that all objects managed by wait become
instances of Future, one way or another. Because it is a coroutine function, calling
wait(�) returns a coroutine/generator object; this is what the wait_coro variable holds.
To drive the coroutine, we pass it to loop.run_until_complete(�).
The loop.run_until_complete function accepts a future or a coroutine. If it gets a
coroutine, run_until_complete wraps it into a Task, similar to what wait does. Coroutines,
futures, and tasks can all be driven by yield from, and this is what run_un
til_complete does with the wait_coro object returned by the wait call. When wait_co
ro runs to completion, it returns a 2-tuple where the first item is the set of completed
futures, and the second is the set of those not completed. In Example 18-5, the second
set will always be empty�that�s why we explicitly ignore it by assigning to _. But wait
accepts two keyword-only arguments that may cause it to return even if some of the
futures are not complete: timeout and return_when. See the asyncio.wait documentation
for details.
Note that in Example 18-5 I could not reuse the get_flag function from flags.py
(Example 17-2) because that uses the requests library, which performs blocking I/O.
To leverage asyncio, we must replace every function that hits the network with an
asynchronous version that is invoked with yield from, so that control is given back to
the event loop. Using yield from in get_flag means that it must be driven as a coroutine.
550 | Chapter 18: Concurrency with asyncio
That�s why I could not reuse the download_one function from flags_threadpool.py
(Example 17-3) either. The code in Example 18-5 drives get_flag with yield_from, so
download_one is itself also a coroutine. For each request, a download_one coroutine
object is created in download_many, and they are all driven by the loop.run_until_com
plete function, after being wrapped by the asyncio.wait coroutine.
There are a lot of new concepts to grasp in asyncio but the overall logic of Example 18-5
is easy to follow if you employ a trick suggested by Guido van Rossum himself: squint
and pretend the yield from keywords are not there. If you do that, you�ll notice that
the code is as easy to read as plain old sequential code.
For example, imagine that the body of this coroutine�
@asyncio.coroutine
def get_flag(cc):
url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())
resp = yield from aiohttp.request('GET', url)
image = yield from resp.read()
return image
�works like the following function, except that it never blocks:
def get_flag(cc):
url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())
resp = aiohttp.request('GET', url)
image = resp.read()
return image
Using the yield from foo syntax avoids blocking because the current coroutine is
suspended (i.e., the delegating generator where the yield from code is), but the control
flow goes back to the event loop, which can drive other coroutines. When the foo future
or coroutine is done, it returns a result to the suspended coroutine, resuming it.
At the end of the section �Using yield from� on page 477, I stated two facts about every
usage of yield from. Here they are, summarized:
� Every arrangement of coroutines chained with yield from must be ultimately
driven by a caller that is not a coroutine, which invokes next(�) or .send(�) on
the outermost delegating generator, explicitly or implicitly (e.g., in a for loop).
� The innermost subgenerator in the chain must be a simple generator that uses just
yield�or an iterable object.
When using yield from with the asyncio API, both facts remain true, with the following
specifics:
� The coroutine chains we write are always driven by passing our outermost delegating
generator to an asyncio API call, such as loop.run_until_complete(�).
Downloading with asyncio and aiohttp | 551
4. Video: Introduction to Node.js at 4:55.
In other words, when using asyncio our code doesn�t drive a coroutine chain by
calling next(�) or .send(�) on it�the asyncio event loop does that.
� The coroutine chains we write always end by delegating with yield from to some
asyncio coroutine function or coroutine method (e.g., yield from asyn
cio.sleep(�) in Example 18-2) or coroutines from libraries that implement
higher-level protocols (e.g., resp = yield from aiohttp.request('GET', url)
in the get_flag coroutine of Example 18-5).
In other words, the innermost subgenerator will be a library function that does the
actual I/O, not something we write.
To summarize: as we use asyncio, our asynchronous code consists of coroutines that
are delegating generators driven by asyncio itself and that ultimately delegate to asyn
cio library coroutines�possibly by way of some third-party library such as aiohttp.
This arrangement creates pipelines where the asyncio event loop drives�through our
coroutines�the library functions that perform the low-level asynchronous I/O.
We are now ready to answer one question raised in Chapter 17:
� How can flags_asyncio.py perform 5? faster than flags.py when both are single
threaded?
Running Circling Around Blocking Calls
Ryan Dahl, the inventor of Node.js, introduces the philosophy of his project by saying
�We�re doing I/O completely wrong.4" He defines a blocking function as one that does
disk or network I/O, and argues that we can�t treat them as we treat nonblocking functions.
To explain why, he presents the numbers in the first two columns of Table 18-1.
Table 18-1. Modern computer latency for reading data from different devices; third column
shows proportional times in a scale easier to understand for us slow humans
Device CPU cycles Proportional �human� scale
L1 cache 3 3 seconds
L2 cache 14 14 seconds
RAM 250 250 seconds
disk 41,000,000 1.3 years
network 240,000,000 7.6 years
552 | Chapter 18: Concurrency with asyncio
5. In fact, although Node.js does not support user-level threads written in JavaScript, behind the scenes it implements
a thread pool in C with the libeio library, to provide its callback-based file APIs�because as of
2014 there are no stable and portable asynchronous file handling APIs for most OSes.
To make sense of Table 18-1, bear in mind that modern CPUs with GHz clocks run
billions of cycles per second. Let�s say that a CPU runs exactly 1 billion cycles per second.
That CPU can make 333,333,333 L1 cache reads in one second, or 4 (four!) network
reads in the same time. The third column of Table 18-1 puts those numbers in perspective
by multiplying the second column by a constant factor. So, in an alternate universe,
if one read from L1 cache took 3 seconds, then a network read would take 7.6 years!
There are two ways to prevent blocking calls to halt the progress of the entire application:
� Run each blocking operation in a separate thread.
� Turn every blocking operation into a nonblocking asynchronous call.
Threads work fine, but the memory overhead for each OS thread�the kind that Python
uses�is on the order of megabytes, depending on the OS. We can�t afford one thread
per connection if we are handling thousands of connections.
Callbacks are the traditional way to implement asynchronous calls with low memory
overhead. They are a low-level concept, similar to the oldest and most primitive concurrency
mechanism of all: hardware interrupts. Instead of waiting for a response, we
register a function to be called when something happens. In this way, every call we make
can be nonblocking. Ryan Dahl advocates callbacks for their simplicity and low overhead.
Of course, we can only make callbacks work because the event loop underlying our
asynchronous applications can rely on infrastructure that uses interrupts, threads, polling,
background processes, etc. to ensure that multiple concurrent requests make progress
and they eventually get done.5 When the event loop gets a response, it calls back
our code. But the single main thread shared by the event loop and our application code
is never blocked�if we don�t make mistakes.
When used as coroutines, generators provide an alternative way to do asynchronous
programming. From the perspective of the event loop, invoking a callback or calling
.send() on a suspended coroutine is pretty much the same. There is a memory
overhead for each suspended coroutine, but it�s orders of magnitude smaller than the
overhead for each thread. And they avoid the dreaded �callback hell,� which we�ll discuss
in �From Callbacks to Futures and Coroutines� on page 562.
Now the five-fold performance advantage of flags_asyncio.py over flags.py should make
sense: flags.py spends billions of CPU cycles waiting for each download, one after the
other. The CPU is actually doing a lot meanwhile, just not running your program. In
contrast, when loop_until_complete is called in the download_many function of
Running Circling Around Blocking Calls | 553
flags_asyncio.py, the event loop drives each download_one coroutine to the first yield
from, and this in turn drives each get_flag coroutine to the first yield from, calling
aiohttp.request(�). None of these calls are blocking, so all requests are started in a
fraction of a second.
As the asyncio infrastructure gets the first response back, the event loop sends it to the
waiting get_flag coroutine. As get_flag gets a response, it advances to the next yield
from, which calls resp.read() and yields control back to the main loop. Other responses
arrive in close succession (because they were made almost at the same time). As each
get_flag returns, the delegating generator download_flag resumes and saves the image
file.
For maximum performance, the save_flag operation should be
asynchronous, but asyncio does not provide an asynchronous
filesystem API at this time�as Node does. If that becomes a bottleneck
in your application, you can use the loop.run_in_execu
tor function to run save_flag in a thread pool. Example 18-9
will show how.
Because the asynchronous operations are interleaved, the total time needed to download
many images concurrently is much less than doing it sequentially. When making 600
HTTP requests with asyncio I got all results back more than 70 times faster than with
a sequential script.
Now let�s go back to the HTTP client example to see how we can display an animated
progress bar and perform proper error handling.
Enhancing the asyncio downloader Script
Recall from �Downloads with Progress Display and Error Handling� on page 520 that
the flags2 set of examples share the same command-line interface. This includes the
flags2_asyncio.py we will analyze in this section. For instance, Example 18-6 shows how
to get 100 flags (-al 100) from the ERROR server, using 100 concurrent requests (-m 100).
Example 18-6. Running flags2_asyncio.py
$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100
ERROR site: http://localhost:8003/flags
Searching for 100 flags: from AD to LK
100 concurrent connections will be used.
--------------------
73 flags downloaded.
27 errors.
Elapsed time: 0.64s
554 | Chapter 18: Concurrency with asyncio
Act Responsibly When Testing Concurrent Clients
Even if the overall download time is not different between the
threaded and asyncio HTTP clients, asyncio can send requests
faster, so it�s even more likely that the server will suspect a DOS
attack. To really exercise these concurrent clients at full speed, set
up a local HTTP server for testing, as explained in the README.
rst inside the 17-futures/countries/ directory of the Fluent
Python code repository.
Now let�s see how flags2_asyncio.py is implemented.
Using asyncio.as_completed
In Example 18-5, I passed a list of coroutines to asyncio.wait, which�when driven
by loop.run_until.complete�would return the results of the downloads when all
were done. But to update a progress bar we need to get results as they are done. Fortunately,
there is an asyncio equivalent of the as_completed generator function we used
in the thread pool example with the progress bar (Example 17-14).
Writing a flags2 example to leverage asyncio entails rewriting several functions that
the concurrent.future version could reuse. That�s because there�s only one main thread
in an asyncio program and we can�t afford to have blocking calls in that thread, as it�s
the same thread that runs the event loop. So I had to rewrite get_flag to use yield
from for all network access. Now get_flag is a coroutine, so download_one must drive
it with yield from, therefore download_one itself becomes a coroutine. Previously, in
Example 18-5, download_one was driven by download_many: the calls to down
load_one were wrapped in an asyncio.wait call and passed to loop.run_until_com
plete. Now we need finer control for progress reporting and error handling, so I moved
most of the logic from download_many into a new downloader_coro coroutine, and use
download_many just to set up the event loop and schedule downloader_coro.
Example 18-7 shows the top of the flags2_asyncio.py script where the get_flag and
download_one coroutines are defined. Example 18-8 lists the rest of the source, with
downloader_coro and download_many.
Example 18-7. flags2_asyncio.py: Top portion of the script; remaining code is in
Example 18-8
import asyncio
import collections
import aiohttp
from aiohttp import web
import tqdm
Enhancing the asyncio downloader Script | 555
from flags2_common import main, HTTPStatus, Result, save_flag
# default set low to avoid errors from remote site, such as
# 503 - Service Temporarily Unavailable
DEFAULT_CONCUR_REQ = 5
MAX_CONCUR_REQ = 1000
class FetchError(Exception):
def __init__(self, country_code):
self.country_code = country_code
@asyncio.coroutine
def get_flag(base_url, cc):
url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower())
resp = yield from aiohttp.request('GET', url)
if resp.status == 200:
image = yield from resp.read()
return image
elif resp.status == 404:
raise web.HTTPNotFound()
else:
raise aiohttp.HttpProcessingError(
code=resp.status, message=resp.reason,
headers=resp.headers)
@asyncio.coroutine
def download_one(cc, base_url, semaphore, verbose):
try:
with (yield from semaphore):
image = yield from get_flag(base_url, cc)
except web.HTTPNotFound:
status = HTTPStatus.not_found
msg = 'not found'
except Exception as exc:
raise FetchError(cc) from exc
else:
save_flag(image, cc.lower() + '.gif')
status = HTTPStatus.ok
msg = 'OK'
if verbose and msg:
print(cc, msg)
return Result(status, cc)
This custom exception will be used to wrap other HTTP or network exceptions
and carry the country_code for error reporting.
556 | Chapter 18: Concurrency with asyncio
6. Thanks to Guto Maia who noted that Semaphore was not explained in the book draft.
get_flag will either return the bytes of the image downloaded, raise
web.HTTPNotFound if the HTTP response status is 404, or raise an
aiohttp.HttpProcessingError for other HTTP status codes.
The semaphore argument is an instance of asyncio.Semaphore, a
synchronization device that limits the number of concurrent requests.
A semaphore is used as a context manager in a yield from expression so that
the system as whole is not blocked: only this coroutine is blocked while the
semaphore counter is at the maximum allowed number.
When this with statement exits, the semaphore counter is decremented,
unblocking some other coroutine instance that may be waiting for the same
semaphore object.
If the flag was not found, just set the status for the Result accordingly.
Any other exception will be reported as a FetchError with the country code and
the original exception chained using the raise X from Y syntax introduced in
PEP 3134 � Exception Chaining and Embedded Tracebacks.
This function call actually saves the flag image to disk.
In Example 18-7, you can see that the code for get_flag and download_one changed
significantly from the sequential version because these functions are now coroutines
using yield from to make asynchronous calls.
Network client code of the sort we are studying should always use some throttling
mechanism to avoid pounding the server with too many concurrent requests�the
overall performance of the system may degrade if the server is overloaded. In
flags2_threadpool.py (Example 17-14), the throttling was done by instantiating the
ThreadPoolExecutor with the required max_workers argument set to concur_req in
the download_many function, so only concur_req threads are started in the pool. In
flags2_asyncio.py, I used an asyncio.Semaphore, which is created by the download
er_coro function (shown next, in Example 18-8) and is passed as the semaphore argument
to download_one in Example 18-7.6
A Semaphore is an object that holds an internal counter that is decremented whenever
we call the .acquire() coroutine method on it, and incremented when we call
the .release() coroutine method. The initial value of the counter is set when the
Semaphore is instantiated, as in this line of downloader_coro:
semaphore = asyncio.Semaphore(concur_req)
Enhancing the asyncio downloader Script | 557
Calling .acquire() does not block when the counter is greater than zero, but if the
counter is zero, .acquire() will block the calling coroutine until some other coroutine
calls .release() on the same Semaphore, thus incrementing the counter. In
Example 18-7, I don�t call .acquire() or .release(), but use the semaphore as a context
manager in this block of code inside download_one:
with (yield from semaphore):
image = yield from get_flag(base_url, cc)
That snippet guarantees that no more than concur_req instances of get_flags coroutines
will be started at any time.
Now let�s take a look at the rest of the script in Example 18-8. Note that most functionality
of the old download_many function is now in a coroutine, downloader_coro. This was
necessary because we must use yield from to retrieve the results of the futures yielded
by asyncio.as_completed, therefore as_completed must be invoked in a coroutine.
However, I couldn�t simply turn download_many into a coroutine, because I must pass
it to the main function from flags2_common in the last line of the script, and that main
function is not expecting a coroutine, just a plain function. Therefore I created down
loader_coro to run the as_completed loop, and now download_many simply sets up
the event loop and schedules downloader_coro by passing it to loop.run_until_com
plete.
Example 18-8. flags2_asyncio.py: Script continued from Example 18-7
@asyncio.coroutine
def downloader_coro(cc_list, base_url, verbose, concur_req):
counter = collections.Counter()
semaphore = asyncio.Semaphore(concur_req)
to_do = [download_one(cc, base_url, semaphore, verbose)
for cc in sorted(cc_list)]
to_do_iter = asyncio.as_completed(to_do)
if not verbose:
to_do_iter = tqdm.tqdm(to_do_iter, total=len(cc_list))
for future in to_do_iter:
try:
res = yield from future
except FetchError as exc:
country_code = exc.country_code
try:
error_msg = exc.__cause__.args[0]
except IndexError:
error_msg = exc.__cause__.__class__.__name__
if verbose and error_msg:
msg = '*** Error for {}: {}'
print(msg.format(country_code, error_msg))
status = HTTPStatus.error
else:
558 | Chapter 18: Concurrency with asyncio
status = res.status
counter[status] += 1
return counter
def download_many(cc_list, base_url, verbose, concur_req):
loop = asyncio.get_event_loop()
coro = downloader_coro(cc_list, base_url, verbose, concur_req)
counts = loop.run_until_complete(coro)
loop.close()
return counts
if __name__ == '__main__':
main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)
The coroutine receives the same arguments as download_many, but it cannot be
invoked directly from main precisely because it�s a coroutine function and not a
plain function like download_many.
Create an asyncio.Semaphore that will allow up to concur_req active coroutines
among those using this semaphore.
Create a list of coroutine objects, one per call to the download_one coroutine.
Get an iterator that will return futures as they are done.
Wrap the iterator in the tqdm function to display progress.
Iterate over the completed futures; this loop is very similar to the one in down
load_many in Example 17-14; most changes have to do with exception handling
because of differences in the HTTP libraries (requests versus aiohttp).
The easiest way to retrieve the result of an asyncio.Future is using yield from
instead of calling future.result().
Every exception in download_one is wrapped in a FetchError with the original
exception chained.
Get the country code where the error occurred from the FetchError exception.
Try to retrieve the error message from the original exception (__cause__).
If the error message cannot be found in the original exception, use the name of
the chained exception class as the error message.
Tally outcomes.
Return the counter, as done in the other scripts.
Enhancing the asyncio downloader Script | 559
7. A detailed discussion about this can be found in a thread I started in the python-tulip group, titled �Which
other futures my come out of asyncio.as_completed?�. Guido responds, and gives insight on the implementation
of as_completed as well as the close relationship between futures and coroutines in asyncio.
download_many simply instantiates the coroutine and passes it to the event loop
with run_until_complete.
When all work is done, shut down the event loop and return counts.
In Example 18-8, we could not use the mapping of futures to country codes we saw in
Example 17-14 because the futures returned by asyncio.as_completed are not necessarily
the same futures we pass into the as_completed call. Internally, the asyncio
machinery replaces the future objects we provide with others that will, in the end, produce
the same results.7
Because I could not use the futures as keys to retrieve the country code from a dict in
case of failure, I implemented the custom FetchError exception (shown in
Example 18-7). FetchError wraps a network exception and holds the country code
associated with it, so the country code can be reported with the error in verbose mode.
If there is no error, the country code is available as the result of the yield from fu
ture expression at the top of the for loop.
This wraps up the discussion of an asyncio example functionally equivalent to the
flags2_threadpool.py we saw earlier. Next, we�ll implement enhancements to
flags2_asyncio.py that will let us explore asyncio further.
While discussing Example 18-7, I noted that save_flag performs disk I/O and should
be executed asynchronously. The following section shows how.
Using an Executor to Avoid Blocking the Event Loop
In the Python community, we tend to overlook the fact that local filesystem access is
blocking, rationalizing that it doesn�t suffer from the higher latency of network access
(which is also dangerously unpredictable). In contrast, Node.js programmers are constantly
reminded that all filesystem functions are blocking because their signatures require
a callback. Recall from Table 18-1 that blocking for disk I/O wastes millions of
CPU cycles, and this may have a significant impact on the performance of the application.
In Example 18-7, the blocking function is save_flag. In the threaded version of the
script (Example 17-14), save_flag blocks the thread that�s running the download_one
function, but that�s only one of several worker threads. Behind the scenes, the blocking
I/O call releases the GIL, so another thread can proceed. But in flags2_asyncio.py,
save_flag blocks the single thread our code shares with the asyncio event loop, there?
560 | Chapter 18: Concurrency with asyncio
fore the whole application freezes while the file is being saved. The solution to this
problem is the run_in_executor method of the event loop object.
Behind the scenes, the asyncio event loop has a thread pool executor, and you can send
callables to be executed by it with run_in_executor. To use this feature in our example,
only a few lines need to change in the download_one coroutine, as shown in
Example 18-9.
Example 18-9. flags2_asyncio_executor.py: Using the default thread pool executor to
run save_flag
@asyncio.coroutine
def download_one(cc, base_url, semaphore, verbose):
try:
with (yield from semaphore):
image = yield from get_flag(base_url, cc)
except web.HTTPNotFound:
status = HTTPStatus.not_found
msg = 'not found'
except Exception as exc:
raise FetchError(cc) from exc
else:
loop = asyncio.get_event_loop()
loop.run_in_executor(None,
save_flag, image, cc.lower() + '.gif')
status = HTTPStatus.ok
msg = 'OK'
if verbose and msg:
print(cc, msg)
return Result(status, cc)
Get a reference to the event loop object.
The first argument to run_in_executor is an executor instance; if None, the
default thread pool executor of the event loop is used.
The remaining arguments are the callable and its positional arguments.
When I tested Example 18-9, there was no noticeable change in
performance for using run_in_executor to save the image files
because they are not large (13 KB each, on average). But you�ll see
an effect if you edit the save_flag function in flags2_common.
py to save 10 times as many bytes on each file�just by coding
fp.write(img*10) instead of fp.write(img). With an average
download size of 130 KB, the advantage of using run_in_ex
ecutor becomes clear. If you�re downloading megapixel images,
the speedup will be significant.
Enhancing the asyncio downloader Script | 561
The advantage of coroutines over callbacks becomes evident when we need to coordinate
asynchronous requests, and not just make completely independent requests. The
next section explains the problem and the solution.
From Callbacks to Futures and Coroutines
Event-oriented programming with coroutines requires some effort to master, so it�s good
to be clear on how it improves on the classic callback style. This is the theme of this
section.
Anyone with some experience in callback-style event-oriented programming knows the
term �callback hell�: the nesting of callbacks when one operation depends on the result
of the previous operation. If you have three asynchronous calls that must happen in
succession, you need to code callbacks nested three levels deep. Example 18-10 is an
example in JavaScript.
Example 18-10. Callback hell in JavaScript: nested anonymous functions, a.k.a. Pyramid
of Doom
api_call1(request1, function (response1) {
// stage 1
var request2 = step1(response1);
api_call2(request2, function (response2) {
// stage 2
var request3 = step2(response2);
api_call3(request3, function (response3) {
// stage 3
step3(response3);
});
});
});
In Example 18-10, api_call1, api_call2, and api_call3 are library functions your
code uses to retrieve results asynchronously�perhaps api_call1 goes to a database
and api_call2 gets data from a web service, for example. Each of these take a callback
function, which in JavaScript are often anonymous functions (they are named stage1,
stage2, and stage3 in the following Python example). The step1, step2, and step3
here represent regular functions of your application that process the responses received
by the callbacks.
Example 18-11 shows what callback hell looks like in Python.
Example 18-11. Callback hell in Python: chained callbacks
def stage1(response1):
request2 = step1(response1)
api_call2(request2, stage2)
562 | Chapter 18: Concurrency with asyncio
def stage2(response2):
request3 = step2(response2)
api_call3(request3, stage3)
def stage3(response3):
step3(response3)
api_call1(request1, stage1)
Although the code in Example 18-11 is arranged very differently from Example 18-10,
they do exactly the same thing, and the JavaScript example could be written using the
same arrangement (but the Python code can�t be written in the JavaScript style because
of the syntactic limitations of lambda).
Code organized as Example 18-10 or Example 18-11 is hard to read, but it�s even harder
to write: each function does part of the job, sets up the next callback, and returns, to let
the event loop proceed. At this point, all local context is lost. When the next callback
(e.g., stage2) is executed, you don�t have the value of request2 any more. If you need
it, you must rely on closures or external data structures to store it between the different
stages of the processing.
That�s where coroutines really help. Within a coroutine, to perform three asynchronous
actions in succession, you yield three times to let the event loop continue running.
When a result is ready, the coroutine is activated with a .send() call. From the perspective
of the event loop, that�s similar to invoking a callback. But for the users of a
coroutine-style asynchronous API, the situation is vastly improved: the entire sequence
of three operations is in one function body, like plain old sequential code with local
variables to retain the context of the overall task under way. See Example 18-12.
Example 18-12. Coroutines and yield from enable asynchronous programming without
callbacks
@asyncio.coroutine
def three_stages(request1):
response1 = yield from api_call1(request1)
# stage 1
request2 = step1(response1)
response2 = yield from api_call2(request2)
# stage 2
request3 = step2(response2)
response3 = yield from api_call3(request3)
# stage 3
step3(response3)
From Callbacks to Futures and Coroutines | 563
loop.create_task(three_stages(request1)) # must explicitly schedule execution
Example 18-12 is much easier to follow the previous JavaScript and Python examples:
the three stages of the operation appear one after the other inside the same function.
This makes it trivial to use previous results in follow-up processing. It also provides a
context for error reporting through exceptions.
Suppose in Example 18-11 the processing of the call api_call2(request2, stage2)
raises an I/O exception (that�s the last line of the stage1 function). The exception cannot
be caught in stage1 because api_call2 is an asynchronous call: it returns immediately,
before any I/O is performed. In callback-based APIs, this is solved by registering two
callbacks for each asynchronous call: one for handling the result of successful operations,
another for handling errors. Work conditions in callback hell quickly deteriorate
when error handling is involved.
In contrast, in Example 18-12, all the asynchronous calls for this three-stage operation
are inside the same function, three_stages, and if the asynchronous calls api_call1,
api_call2, and api_call3 raise exceptions we can handle them by putting the respective
yield from lines inside try/except blocks.
This is a much better place than callback hell, but I wouldn�t call it coroutine heaven
because there is a price to pay. Instead of regular functions, you must use coroutines
and get used to yield from, so that�s the first obstacle. Once you write yield from in
a function, it�s now a coroutine and you can�t simply call it, like we called api_call1(re
quest1, stage1) in Example 18-11 to start the callback chain. You must explicitly
schedule the execution of the coroutine with the event loop, or activate it using yield
from in another coroutine that is scheduled for execution. Without the call loop.cre
ate_task(three_stages(request1)) in the last line, nothing would happen in
Example 18-12.
The next example puts this theory into practice.
Doing Multiple Requests for Each Download
Suppose you want to save each country flag with the name of the country and the country
code, instead of just the country code. Now you need to make two HTTP requests per
flag: one to get the flag image itself, the other to get the metadata.json file in the same
directory as the image: that�s where the name of the country is recorded.
Articulating multiple requests in the same task is easy in the threaded script: just make
one request then the other, blocking the thread twice, and keeping both pieces of data
(country code and name) in local variables, ready to use when saving the files. If you
need to do the same in an asynchronous script with callbacks, you start to smell the
sulfur of callback hell: the country code and name will need to be passed around in a
564 | Chapter 18: Concurrency with asyncio
closure or held somewhere until you can save the file because each callback runs in a
different local context. Coroutines and yield from provide relief from that. The solution
is not as simple as with threads, but more manageable than chained or nested
callbacks.
Example 18-13 shows code from the third variation of the asyncio flag downloading
script, using the country name to save each flag. The download_many and download
er_coro are unchanged from flags2_asyncio.py (Examples 18-7 and 18-8). The changes
are:
download_one
This coroutine now uses yield from to delegate to get_flag and the new get_coun
try coroutine.
get_flag
Most code from this coroutine was moved to a new http_get coroutine so it can
also be used by get_country.
get_country
This coroutine fetches the metadata.json file for the country code, and gets the
name of the country from it.
http_get
Common code for getting a file from the Web.
Example 18-13. flags3_asyncio.py: more coroutine delegation to perform two requests
per flag
@asyncio.coroutine
def http_get(url):
res = yield from aiohttp.request('GET', url)
if res.status == 200:
ctype = res.headers.get('Content-type', '').lower()
if 'json' in ctype or url.endswith('json'):
data = yield from res.json()
else:
data = yield from res.read()
return data
elif res.status == 404:
raise web.HTTPNotFound()
else:
raise aiohttp.errors.HttpProcessingError(
code=res.status, message=res.reason,
headers=res.headers)
@asyncio.coroutine
def get_country(base_url, cc):
url = '{}/{cc}/metadata.json'.format(base_url, cc=cc.lower())
From Callbacks to Futures and Coroutines | 565
metadata = yield from http_get(url)
return metadata['country']
@asyncio.coroutine
def get_flag(base_url, cc):
url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower())
return (yield from http_get(url))
@asyncio.coroutine
def download_one(cc, base_url, semaphore, verbose):
try:
with (yield from semaphore):
image = yield from get_flag(base_url, cc)
with (yield from semaphore):
country = yield from get_country(base_url, cc)
except web.HTTPNotFound:
status = HTTPStatus.not_found
msg = 'not found'
except Exception as exc:
raise FetchError(cc) from exc
else:
country = country.replace(' ', '_')
filename = '{}-{}.gif'.format(country, cc)
loop = asyncio.get_event_loop()
loop.run_in_executor(None, save_flag, image, filename)
status = HTTPStatus.ok
msg = 'OK'
if verbose and msg:
print(cc, msg)
return Result(status, cc)
If the content type has 'json' in it or the url ends with .json, use the
response .json() method to parse it and return a Python data structure�in this
case, a dict.
Otherwise, use .read() to fetch the bytes as they are.
metadata will receive a Python dict built from the JSON contents.
The outer parentheses here are required because the Python parser gets confused
and produces a syntax error when it sees the keywords return yield from lined
up like that.
I put the calls to get_flag and get_country in separate with blocks controlled
by the semaphore because I want to keep it acquired for the shortest possible
time.
566 | Chapter 18: Concurrency with asyncio
The yield from syntax appears nine times in Example 18-13. By now you should be
getting the hang of how this construct is used to delegate from one coroutine to another
without blocking the event loop.
The challenge is to know when you have to use yield from and when you can�t use it.
The answer in principle is easy, you yield from coroutines and asyncio.Future instances�
including tasks. But some APIs are tricky, mixing coroutines and plain functions
in seemingly arbitrary ways, like the StreamWriter class we�ll use in one of the
servers in the next section.
Example 18-13 wraps up the flags2 set of examples. I encourage you to play with them
to develop an intuition of how concurrent HTTP clients perform. Use the -a, -e, and
-l command-line options to control the number of downloads, and the -m option to
set the number of concurrent downloads. Run tests against the LOCAL, REMOTE, DELAY,
and ERROR servers. Discover the optimum number of concurrent downloads to maximize
throughput against each server. Tweak the settings of the vaurien_error_delay.sh
script to add or remove errors and delays.
We�ll now go from client scripts to writing servers with asyncio.
Writing asyncio Servers
The classic toy example of a TCP server is an echo server. We�ll build slightly more
interesting toys: Unicode character finders, first using plain TCP, then using HTTP.
These servers will allow clients to query for Unicode characters based on words in their
canonical names, using the unicodedata module we discussed in �The Unicode Database�
on page 127. A Telnet session with the TCP character finder server, searching for
chess pieces and characters with the word �sun� is shown in Figure 18-2.
Writing asyncio Servers | 567
Figure 18-2. A Telnet session with the tcp_charfinder.py server: querying for �chess
black� and �sun�.
Now, on to the implementations.
An asyncio TCP Server
Most of the logic in these examples is in the charfinder.py module, which has nothing
concurrent about it. You can use charfinder.py as a command-line character finder, but
more importantly, it was designed to provide content for our asyncio servers. The code
for charfinder.py is in the Fluent Python code repository.
The charfinder module indexes each word that appears in character names in the
Unicode database bundled with Python, and creates an inverted index stored in a
dict. For example, the inverted index entry for the key 'SUN' contains a set with the
10 Unicode characters that have that word in their names. The inverted index is saved
in a local charfinder_index.pickle file. If multiple words appear in the query, charfind
er computes the intersection of the sets retrieved from the index.
568 | Chapter 18: Concurrency with asyncio
We�ll now focus on the tcp_charfinder.py script that is answering the queries in
Figure 18-2. Because I have a lot to say about this code, I�ve split it into two parts:
Example 18-14 and Example 18-15.
Example 18-14. tcp_charfinder.py: a simple TCP server using asyncio.start_server; code
for this module continues in Example 18-15
import sys
import asyncio
from charfinder import UnicodeNameIndex
CRLF = b'\r\n'
PROMPT = b'?> '
index = UnicodeNameIndex()
@asyncio.coroutine
def handle_queries(reader, writer):
while True:
writer.write(PROMPT) # can't yield from!
yield from writer.drain() # must yield from!
data = yield from reader.readline()
try:
query = data.decode().strip()
except UnicodeDecodeError:
query = '\x00'
client = writer.get_extra_info('peername')
print('Received from {}: {!r}'.format(client, query))
if query:
if ord(query[:1]) < 32:
break
lines = list(index.find_description_strs(query))
if lines:
writer.writelines(line.encode() + CRLF for line in lines)
writer.write(index.status(query, len(lines)).encode() + CRLF)
yield from writer.drain()
print('Sent {} results'.format(len(lines)))
print('Close the client socket')
writer.close()
UnicodeNameIndex is the class that builds the index of names and provides
querying methods.
Writing asyncio Servers | 569
8. Leonardo Rochael pointed out that building the UnicodeNameIndex could be delegated to another thread
using loop.run_with_executor() in the main function of Example 18-15, so the server would be ready
to take requests immediately while the index is built. That is true, but querying the index is the only thing
this app does, so that would not be a big win. It�s an interesting exercise to do as Leo suggests, though. Go
ahead and do it, if you like.
When instantiated, UnicodeNameIndex uses charfinder_index.pickle, if
available, or builds it, so the first run may take a few seconds longer to start.8
This is the coroutine we need to pass to asyncio_startserver; the arguments
received are an asyncio.StreamReader and an asyncio.StreamWriter.
This loop handles a session that lasts until any control character is received from
the client.
The StreamWriter.write method is not a coroutine, just a plain function; this
line sends the ?> prompt.
StreamWriter.drain flushes the writer buffer; it is a coroutine, so it must be
called with yield from.
StreamWriter.readline is a coroutine; it returns bytes.
A UnicodeDecodeError may happen when the Telnet client sends control
characters; if that happens, we pretend a null character was sent, for simplicity.
This returns the remote address to which the socket is connected.
Log the query to the server console.
Exit the loop if a control or null character was received.
This returns a generator that yields strings with the Unicode codepoint, the
actual character and its name (e.g., U+0039\t9\tDIGIT NINE); for simplicity, I
build a list from it.
Send the lines converted to bytes using the default UTF-8 encoding, appending
a carriage return and a line feed to each; note that the argument is a generator
expression.
Write a status line such as 627 matches for 'digit'.
Flush the output buffer.
Log the response to the server console.
Log the end of the session to the server console.
Close the StreamWriter.
The handle_queries coroutine has a plural name because it starts an interactive session
and handles multiple queries from each client.
570 | Chapter 18: Concurrency with asyncio
Note that all I/O in Example 18-14 is in bytes. We need to decode the strings received
from the network, and encode strings sent out. In Python 3, the default encoding is
UTF-8, and that�s what we are using implicitly.
One caveat is that some of the I/O methods are coroutines and must be driven with
yield from, while others are simple functions. For example, StreamWriter.write is a
plain function, on the assumption that most of the time it does not block because it
writes to a buffer. On the other hand, StreamWriter.drain, which flushes the buffer
and performs the actual I/O is a coroutine, as is Streamreader.readline. While I was
writing this book, a major improvement to the asyncio API docs was the clear labeling
of coroutines as such.
Example 18-15 lists the main function for the module started in Example 18-14.
Example 18-15. tcp_charfinder.py (continued from Example 18-14): the main function
sets up and tears down the event loop and the socket server
def main(address='127.0.0.1', port=2323):
port = int(port)
loop = asyncio.get_event_loop()
server_coro = asyncio.start_server(handle_queries, address, port,
loop=loop)
server = loop.run_until_complete(server_coro)
host = server.sockets[0].getsockname()
print('Serving on {}. Hit CTRL-C to stop.'.format(host))
try:
loop.run_forever()
except KeyboardInterrupt: # CTRL+C pressed
pass
print('Server shutting down.')
server.close()
loop.run_until_complete(server.wait_closed())
loop.close()
if __name__ == '__main__':
main(*sys.argv[1:])
The main function can be called with no arguments.
When completed, the coroutine object returned by asyncio.start_server
returns an instance of asyncio.Server, a TCP socket server.
Drive server_coro to bring up the server.
Get address and port of the first socket of the server and�
�display it on the server console. This is the first output generated by this script
on the server console.
Writing asyncio Servers | 571
Run the event loop; this is where main will block until killed when CTRL-C is
pressed on the server console.
Close the server.
server.wait_closed() returns a future; use loop.run_until_complete to let
the future do its job.
Terminate the event loop.
This is a shortcut for handling optional command-line arguments: explode
sys.argv[1:] and pass it to a main function with suitable default arguments.
Note how run_until_complete accepts either a coroutine (the result of start_serv
er) or a Future (the result of server.wait_closed). If run_until_complete gets a
coroutine as argument, it wraps the coroutine in a Task.
You may find it easier to understand how control flows in tcp_charfinder.py if you take
a close look at the output it generates on the server console, listed in Example 18-16.
Example 18-16. tcp_charfinder.py: this is the server side of the session depicted in
Figure 18-2
$ python3 tcp_charfinder.py
Serving on ('127.0.0.1', 2323). Hit CTRL-C to stop.
Received from ('127.0.0.1', 62910): 'chess black'
Sent 6 results
Received from ('127.0.0.1', 62910): 'sun'
Sent 10 results
Received from ('127.0.0.1', 62910): '\x00'
Close the client socket
This is output by main.
First iteration of the while loop in handle_queries.
Second iteration of the while loop.
The user hit CTRL-C; the server receives a control character and closes the session.
The client socket is closed but the server is still running, ready to service another
client.
Note how main almost immediately displays the Serving on... message and blocks in
the loop.run_forever() call. At that point, control flows into the event loop and stays
there, occasionally coming back to the handle_queries coroutine, which yields control
back to the event loop whenever it needs to wait for the network as it sends or receives
data. While the event loop is alive, a new instance of the handle_queries coroutine will
be started for each client that connects to the server. In this way, multiple clients can be
572 | Chapter 18: Concurrency with asyncio
handled concurrently by this simple server. This continues until a KeyboardInter
rupt occurs or the process is killed by the OS.
The tcp_charfinder.py code leverages the high-level asyncio Streams API that provides
a ready-to-use server so you only need to implement a handler function, which can be
a plain callback or a coroutine. There is also a lower-level Transports and Protocols
API, inspired by the transport and protocols abstractions in the Twisted framework.
Refer to the asyncio Transports and Protocols documentation for more information,
including a TCP echo server implemented with that lower-level API.
The next section presents an HTTP character finder server.
An aiohttp Web Server
The aiohttp library we used for the asyncio flags examples also supports server-side
HTTP, so that�s what I used to implement the http_charfinder.py script. Figure 18-3
shows the simple web interface of the server, displaying the result of a search for a �cat
face� emoji.
Figure 18-3. Browser window displaying search results for �cat face� on the http_charfinder.
py server
Writing asyncio Servers | 573
Some browsers are better than others at displaying Unicode. The
screenshot in Figure 18-3 was captured with Firefox on OS X, and
I got the same result with Safari. But up-to-date Chrome and Opera
browsers on the same machine did not display emoji characters
like the cat faces. Other search results (e.g., �chess�) looked
fine, so it�s likely a font issue on Chrome and Opera on OSX.
We�ll start by analyzing the most interesting part of http_charfinder.py: the bottom half
where the event loop and the HTTP server is set up and torn down. See Example 18-17.
Example 18-17. http_charfinder.py: the main and init functions
@asyncio.coroutine
def init(loop, address, port):
app = web.Application(loop=loop)
app.router.add_route('GET', '/', home)
handler = app.make_handler()
server = yield from loop.create_server(handler,
address, port)
return server.sockets[0].getsockname()
def main(address="127.0.0.1", port=8888):
port = int(port)
loop = asyncio.get_event_loop()
host = loop.run_until_complete(init(loop, address, port))
print('Serving on {}. Hit CTRL-C to stop.'.format(host))
try:
loop.run_forever()
except KeyboardInterrupt: # CTRL+C pressed
pass
print('Server shutting down.')
loop.close()
if __name__ == '__main__':
main(*sys.argv[1:])
The init coroutine yields a server for the event loop to drive.
The aiohttp.web.Application class represents a web application�
�with routes mapping URL patterns to handler functions; here GET / is routed
to the home function (see Example 18-18).
The app.make_handler method returns an aiohttp.web.RequestHandler
instance to handle HTTP requests according to the routes set up in the app
object.
create_server brings up the server, using handler as the protocol handler and
binding it to address and port.
574 | Chapter 18: Concurrency with asyncio
Return the address and port of the first server socket.
Run init to start the server and get its address and port.
Run the event loop; main will block here while the event loop is in control.
Close the event loop.
As you get acquainted with the asyncio API, it�s interesting to contrast how the servers
are set up in Example 18-17 and in the TCP example (Example 18-15) shown earlier.
In the earlier TCP example, the server was created and scheduled to run in the main
function with these two lines:
server_coro = asyncio.start_server(handle_queries, address, port,
loop=loop)
server = loop.run_until_complete(server_coro)
In the HTTP example, the init function creates the server like this:
server = yield from loop.create_server(handler,
address, port)
But init itself is a coroutine, and what makes it run is the main function, with this line:
host = loop.run_until_complete(init(loop, address, port))
Both asyncio.start_server and loop.create_server are coroutines that return
asyncio.Server objects. In order to start up a server and return a reference to it, each
of these coroutines must be driven to completion. In the TCP example, that was done
by calling loop.run_until_complete(server_coro), where server_coro was the result
of asyncio.start_server. In the HTTP example, create_server is invoked on a
yield_from expression inside the init coroutine, which is in turn driven by the main
function when it calls loop.run_until_complete(init(...)).
I mention this to emphasize this essential fact we�ve discussed before: a coroutine only
does anything when driven, and to drive an asyncio.coroutine you either use yield
from or pass it to one of several asyncio functions that take coroutine or future arguments,
such as run_until_complete.
Example 18-18 shows the home function, which is configured to handle the / (root) URL
in our HTTP server.
Example 18-18. http_charfinder.py: the home function
def home(request):
query = request.GET.get('query', '').strip()
print('Query: {!r}'.format(query))
if query:
descriptions = list(index.find_descriptions(query))
res = '\n'.join(ROW_TPL.format(**vars(descr))
for descr in descriptions)
Writing asyncio Servers | 575
msg = index.status(query, len(descriptions))
else:
descriptions = []
res = ''
msg = 'Enter words describing characters.'
html = template.format(query=query, result=res,
message=msg)
print('Sending {} results'.format(len(descriptions)))
return web.Response(content_type=CONTENT_TYPE, text=html)
A route handler receives an aiohttp.web.Request instance.
Get the query string stripped of leading and trailing blanks.
Log query to server console.
If there was a query, bind res to HTML table rows rendered from result of the
query to the index, and msg to a status message.
Render the HTML page.
Log response to server console.
Build Response and return it.
Note that home is not a coroutine, and does not need to be if there are no yield from
expressions in it. The aiohttp documentation for the add_route method states that the
handler �is converted to coroutine internally when it is a regular function.�
There is a downside to the simplicity of the home function in Example 18-18. The fact
that it�s a plain function and not a coroutine is a symptom of a larger issue: the need to
rethink how we code web applications to achieve high concurrency. Let�s consider this
matter.
Smarter Clients for Better Concurrency
The home function in Example 18-18 looks very much like a view function in Django
or Flask. There is nothing asynchronous about its implementation: it gets a request,
fetches data from a database, and builds a response by rendering a full HTML page. In
this example, the �database� is the UnicodeNameIndex object, which is in memory. But
accessing a real database should be done asynchronously, otherwise you�re blocking the
event loop while waiting for database results. For example, the aiopg package provides
an asynchronous PostgreSQL driver compatible with asyncio; it lets you use yield
from to send queries and fetch results, so your view function can behave as a proper
coroutine.
Besides avoiding blocking calls, highly concurrent systems must split large chunks of
work into smaller pieces to stay responsive. The http_charfinder.py server illustrates this
576 | Chapter 18: Concurrency with asyncio
9. That�s what CJK stands for: the ever-expanding set of Chinese, Japanese, and Korean characters. Future
versions of Python may support more CJK ideographs than Python 3.4 does.
10. I have more to say about this trend in �Soapbox� on page 580.
point: if you search for �cjk� you�ll get back 75,821 Chinese, Japanese, and Korean
ideographs.9 In this case, the home function will return a 5.3 MB HTML document,
featuring a table with 75,821 rows.
On my machine, it takes 2s to fetch the response to the �cjk� query, using the curl
command-line HTTP client from a local http_charfinder.py server. A browser takes
even longer to actually layout the page with such a huge table. Of course, most queries
return much smaller responses: a query for �braille� returns 256 rows in a 19 KB page
and takes 0.017s on my machine. But if the server spends 2s serving a single �cjk� query,
all the other clients will be waiting for at least 2s, and that is not acceptable.
The way to avoid the long response problem is to implement pagination: return results
with at most, say, 200 rows, and have the user click or scroll the page to fetch more. If
you look up the charfinder.py module in the Fluent Python code repository, you�ll see
that the UnicodeNameIndex.find_descriptions method takes optional start and
stop arguments: they are offsets to support pagination. So you could return the first
200 results, then use AJAX or even WebSockets to send the next batch when�and if�
the user wants to see it.
Most of the necessary coding for sending results in batches would be on the browser.
This explains why Google and all large-scale Internet properties rely on lots of clientside
coding to build their services: smart asynchronous clients make better use of server
resources.
Although smart clients can help even old-style Django applications, to really serve them
well we need frameworks that support asynchronous programming all the way: from
the handling of HTTP requests and responses, to the database access. This is especially
true if you want to implement real-time services such as games and media streaming
with WebSockets.10
Enhancing http_charfinder.py to support progressive download is left as an exercise to
the reader. Bonus points if you implement �infinite scroll,� like Twitter does. With this
challenge, I wrap up our coverage of concurrent programming with asyncio.
Chapter Summary
This chapter introduced a whole new way of coding concurrency in Python, leveraging
yield from, coroutines, futures, and the asyncio event loop. The first simple examples,
the spinner scripts, were designed to demonstrate a side-by-side comparison of the
threading and the asyncio approaches to concurrency.
Chapter Summary | 577
We then discussed the specifics of asyncio.Future, focusing on its support for yield
from, and its relationship with coroutines and asyncio.Task. Next, we analyzed the
asyncio-based flag download script.
We then reflected on Ryan Dahl�s numbers for I/O latency and the effect of blocking
calls. To keep a program alive despite the inevitable blocking functions, there are two
solutions: using threads or asynchronous calls�the latter being implemented as callbacks
or coroutines.
In practice, asynchronous libraries depend on lower-level threads to work�down to
kernel-level threads�but the user of the library doesn�t create threads and doesn�t need
to be aware of their use in the infrastructure. At the application level, we just make sure
none of our code is blocking, and the event loop takes care of the concurrency under
the hood. Avoiding the overhead of user-level threads is the main reason why asynchronous
systems can manage more concurrent connections than multithreaded systems.
Resuming the flag downloading examples, adding a progress bar and proper error handling
required significant refactoring, particularly with the switch from asyn
cio.wait to asyncio.as_completed, which forced us to move most of the functionality
of download_many to a new downloader_coro coroutine, so we could use yield from
to get the results from the futures produced by asyncio.as_completed, one by one.
We then saw how to delegate blocking jobs�such as saving a file�to a thread pool using
the loop.run_in_executor method.
This was followed by a discussion of how coroutines solve the main problems of callbacks:
loss of context when carrying out multistep asynchronous tasks, and lack of a
proper context for error handling.
The next example�fetching the country names along with the flag images�demonstrated
how the combination of coroutines and yield from avoids the so-called callback
hell. A multistep procedure making asynchronous calls with yield from looks like
simple sequential code, if you pay no attention to the yield from keywords.
The final examples in the chapter were asyncio TCP and HTTP servers that allow
searching for Unicode characters by name. Analysis of the HTTP server ended with a
discussion on the importance of client-side JavaScript to support higher concurrency
on the server side, by enabling the client to make smaller requests on demand, instead
of downloading large HTML pages.
578 | Chapter 18: Concurrency with asyncio
11. Comment on PEP-3156 in a Jan. 20, 2013 message to the python-ideas list.
Further Reading
Nick Coghlan, a Python core developer, made the following comment on the draft of
PEP-3156 � Asynchronous IO Support Rebooted: the �asyncio� Module in January
2013:
Somewhere early in the PEP, there may need to be a concise description of the two APIs
for waiting for an asynchronous Future:
1. f.add_done_callback(�)
2. yield from f in a coroutine (resumes the coroutine when the future completes,
with either the result or exception as appropriate)
At the moment, these are buried in amongst much larger APIs, yet they�re key to understanding
the way everything above the core event loop layer interacts.11
Guido van Rossum, the author of PEP-3156, did not heed Coghlan�s advice. Starting
with PEP-3156, the asyncio documentation is very detailed but not user friendly. The
nine .rst files that make up the asyncio package docs total 128 KB�that�s roughly 71
pages. In the standard library, only the �Built-in Types� chapter is bigger, and it covers
the API for the numeric types, sequence types, generators, mappings, sets, bool, context
managers, etc.
Most pages in the asyncio manual focus on concepts and the API. There are useful
diagrams and examples scattered all over it, but one section that is very practical is
�18.5.11. Develop with asyncio,� which presents essential usage patterns. The asyncio
docs need more content explaining how asyncio should be used.
Because it�s very new, asyncio lacks coverage in print. Jan Palach�s Parallel Programming
with Python (Packt, 2014) is the only book I found that has a chapter about asyncio,
but it�s a short chapter.
There are, however, excellent presentations about asyncio. The best I found is Brett
Slatkin�s �Fan-In and Fan-Out: The Crucial Components of Concurrency,� subtitled
�Why do we need Tulip? (a.k.a., PEP 3156�asyncio),� which he presented at PyCon
2014 in Montreal (video). In 30 minutes, Slatkin shows a simple web crawler example,
highlighting how asyncio is intended to be used. Guido van Rossum is in the audience
and mentions that he also wrote a web crawler as a motivating example for asyncio;
Guido�s code does not depend on aiohttp�it uses only the standard library. Slatkin
also wrote the insightful post �Python�s asyncio Is for Composition, Not Raw Performance.�
Further Reading | 579
Other must-see asyncio talks are by Guido van Rossum himself: the PyCon US 2013
keynote, and talks he gave at LinkedIn and Twitter University. Also recommended are
Saul Ibarra Corretge�s �A Deep Dive into PEP-3156 and the New asyncio Module�
(slides, video).
Dino Viehland showed how asyncio can be integrated with the Tkinter event loop in
his �Using futures for async GUI programming in Python 3.3� talk at PyCon US 2013.
Viehland shows how easy it is to implement the essential parts of the asyncio.Abstrac
tEventLoop interface on top of another event loop. His code was written with Tulip,
prior to the addition of asyncio to the standard library; I adapted it to work with the
Python 3.4 release of asyncio. My updated refactoring is on GitHub.
Victor Stinner�an asyncio core contributor and author of the Trollius backport�
regularly updates a list of relevant links: The new Python asyncio module aka �tulip�.
Other collections of asyncio resources are Asyncio.org and aio-libs on Github, where
you�ll find asynchronous drivers for PostgreSQL, MySQL, and several NoSQL databases.
I haven�t tested these drivers, but the projects seem very active as I write this.
Web services are going to be an important use case for asyncio. Your code will likely
depend on the aiohttp library led by Andrew Svetlov. You�ll also want to set up an
environment to test your error handling code, and the Vaurien �chaos TCP proxy�
designed by Alexis Metaireau and Tarek Ziade is invaluable for that. Vaurien was created
for the Mozilla Services project and lets you introduce delays and random errors into
the TCP traffic between your program and backend servers such as databases and web
services providers.
Soapbox
The One Loop
For a long time, asynchronous programming has been the approach favored by most
Pythonistas for network applications, but there was always the dilemma of picking one
of the mutually incompatible libraries. Ryan Dahl cites Twisted as a source of inspiration
for Node.js, and Tornado championed the use of coroutines for event-oriented programming
in Python.
In the JavaScript world, there is some debate between advocates of simple callbacks and
proponents of various competing higher-level abstractions. Early versions the Node.js
API used Promises�similar to our Futures�but Ryan Dahl decided to standardize on
callbacks only. James Coglan argues this was Node�s biggest missed opportunity.
In Python, the debate is over: the addition of asyncio to the standard library establishes
coroutines and futures as the Pythonic way of writing asynchronous code. Furthermore,
the asyncio package defines standard interfaces for asynchronous futures and the event
loop, providing reference implementations for them.
580 | Chapter 18: Concurrency with asyncio
The Zen of Python applies perfectly:
There should be one�and preferably only one�obvious way to do it.
Although that way may not be obvious at first unless you�re Dutch.
Maybe it takes a Dutch passport to find yield from obvious. It was not obvious at first
for this Brazilian, but after a while I got the hang of it.
More importantly, asyncio was designed so that its event loop can be replaced by an
external package. That�s why the asyncio.get_event_loop and set_event_loop functions
exist; they are part of an abstract Event Loop Policy API.
Tornado already has an AsyncIOMainLoop class that implements the asyncio.Ab
stractEventLoop interface, so you can run asynchronous code using both libraries on
the same event loop. There is also the intriguing Quamash project that integrates asyn
cio to the Qt event loop for developing GUI applications with PyQt or PySide. These
are just two of a growing number of interoperable event-oriented packages made possible
by asyncio.
Smarter HTTP clients such as single-page web applications (like Gmail) or smartphone
apps demand quick, lightweight responses and push updates. These needs are better
served by asynchronous frameworks instead of traditional web frameworks like Django,
which are designed to serve fully rendered HTML pages and lack support for asynchronous
database access.
The WebSockets protocol was designed to enable real-time updates for clients that are
always connected, from games to streaming applications. This requires highly concurrent
asynchronous servers able to keep ongoing interactions with hundreds or thousands
of clients. WebSockets is very well supported by the asyncio architecture and at
least two libraries already implement it on top of asyncio: Autobahn|Python and Web?
Sockets.
This overall trend�dubbed �the real-time Web��is a key factor in the demand for
Node.js, and the reason why rallying around asyncio is so important for the Python
ecosystem. There�s still a lot of work to do. For starters, we need an asynchronous HTTP
server and client API in the standard library, an asynchronous DBAPI 3.0, and new
database drivers built on asyncio.
The biggest advantage Python 3.4 with asyncio has over Node.js is Python itself: a better
designed language, with coroutines and yield from to make asynchronous code more
maintainable than the primitive callbacks of JavaScript. Our biggest disadvantage is the
libraries: Python comes with �batteries included,� but our batteries are not designed for
asynchronous programming. The rich ecosystem of libraries for Node.js is entirely built
around async calls. But Python and Node.js both have a problem that Go and Erlang
have solved from the start: we have no transparent way to write code that leverages all
available CPU cores.
Further Reading | 581
12. See Guido�s January 29, 2015, message, immediately followed by an answer from Glyph.
Standardizing the event loop interface and an asynchronous library was a major coup,
and only our BDFL could have pulled it off, given that there were well-entrenched, highquality
alternatives available. He did it in consultation with the authors of the major
Python asynchronous frameworks. The influence of Glyph Lefkowitz, the leader of
Twisted, is most evident. Guido�s �Deconstructing Deferred� post to the Python-tulip
group is a must-read if you want to understand why asyncio.Future is not like the
Twisted Deferred class. Making clear his respect for the oldest and largest Python asynchronous
framework, Guido also started the meme WWTD�What Would Twisted Do?
�when discussing design options in the python-twisted group.12
Fortunately, Guido van Rossum led the charge so Python is better positioned to face the
concurrency challenges of the present. Mastering asyncio takes effort. But if you plan
to write concurrent network applications in Python, seek the One Loop:
One Loop to rule them all, One Loop to find them,
One Loop to bring them all and in liveness bind them.
582 | Chapter 18: Concurrency with asyncio
PART VI
Metaprogramming

1. Alex Martelli, Python in a Nutshell, 2E (O�Reilly), p. 101.
2. Bertrand Meyer, Object-Oriented Software Construction, 2E, p. 57.
CHAPTER 19
Dynamic Attributes and Properties
The crucial importance of properties is that their existence makes it perfectly safe and
indeed advisable for you to expose public data attributes as part of your class�s public
interface.1
� Alex Martelli
Python contributor and book author
Data attributes and methods are collectively known as attributes in Python: a method
is just an attribute that is callable. Besides data attributes and methods, we can also create
properties, which can be used to replace a public data attribute with accessor methods
(i.e., getter/setter), without changing the class interface. This agrees with the Uniform
access principle:
All services offered by a module should be available through a uniform notation, which
does not betray whether they are implemented through storage or through computation.2
Besides properties, Python provides a rich API for controlling attribute access and implementing
dynamic attributes. The interpreter calls special methods such as __get
attr__ and __setattr__ to evaluate attribute access using dot notation (e.g.,
obj.attr). A user-defined class implementing __getattr__ can implement �virtual
attributes� by computing values on the fly whenever somebody tries to read a nonexistent
attribute like obj.no_such_attribute.
Coding dynamic attributes is the kind of metaprogramming that framework authors
do. However, in Python, the basic techniques are so straightforward that anyone can
put them to work, even for everyday data wrangling tasks. That�s how we�ll start this
chapter.
585
3. You can read about this feed and rules for using it at �DIY: OSCON schedule�. The original 744KB JSON file
is still online as I write this. A copy named osconfeed.json can be found in the oscon-schedule/data/ directory
in the Fluent Python code repository.
Data Wrangling with Dynamic Attributes
In the next few examples, we�ll leverage dynamic attributes to work with a JSON data
feed published by O�Reilly for the OSCON 2014 conference. Example 19-1 shows four
records from that data feed.3
Example 19-1. Sample records from osconfeed.json; some field contents abbreviated
{ "Schedule":
{ "conferences": [{"serial": 115 }],
"events": [
{ "serial": 34505,
"name": "Why Schools Don?t Use Open Source to Teach Programming",
"event_type": "40-minute conference session",
"time_start": "2014-07-23 11:30:00",
"time_stop": "2014-07-23 12:10:00",
"venue_serial": 1462,
"description": "Aside from the fact that high school programming...",
"website_url": "http://oscon.com/oscon2014/public/schedule/detail/34505",
"speakers": [157509],
"categories": ["Education"] }
],
"speakers": [
{ "serial": 157509,
"name": "Robert Lefkowitz",
"photo": null,
"url": "http://sharewave.com/",
"position": "CTO",
"affiliation": "Sharewave",
"twitter": "sharewaveteam",
"bio": "Robert ?r0ml? Lefkowitz is the CTO at Sharewave, a startup..." }
],
"venues": [
{ "serial": 1462,
"name": "F151",
"category": "Conference Venues" }
]
}
}
Example 19-1 shows 4 out of the 895 records in the JSON feed. As you can see, the entire
dataset is a single JSON object with the key "Schedule", and its value is another mapping
with four keys: "conferences", "events", "speakers", and "venues". Each of those
four keys is paired with a list of records. In Example 19-1, each list has one record, but
in the full dataset, those lists have dozens or hundreds of records�with the exception
586 | Chapter 19: Dynamic Attributes and Properties
of "conferences", which holds just the single record shown. Every item in those four
lists has a "serial" field, which is a unique identifier within the list.
The first script I wrote to deal with the OSCON feed simply downloads the feed, avoiding
unnecessary traffic by checking if there is a local copy. This makes sense because OSCON
2014 is history now, so that feed will not be updated.
There is no metaprogramming in Example 19-2; pretty much everything boils down to
this expression: json.load(fp), but that�s enough to let us explore the dataset. The
osconfeed.load function will be used in the next several examples.
Example 19-2. osconfeed.py: downloading osconfeed.json (doctests are in
Example 19-3)
from urllib.request import urlopen
import warnings
import os
import json
URL = 'http://www.oreilly.com/pub/sc/osconfeed'
JSON = 'data/osconfeed.json'
def load():
if not os.path.exists(JSON):
msg = 'downloading {} to {}'.format(URL, JSON)
warnings.warn(msg)
with urlopen(URL) as remote, open(JSON, 'wb') as local:
local.write(remote.read())
with open(JSON) as fp:
return json.load(fp)
Issue a warning if a new download will be made.
with using two context managers (allowed since Python 2.7 and 3.1) to read the
remote file and save it.
The json.load function parses a JSON file and returns native Python objects.
In this feed, we have the types: dict, list, str, and int.
With the code in Example 19-2, we can inspect any field in the data. See Example 19-3.
Example 19-3. osconfeed.py: doctests for Example 19-2
>>> feed = load()
>>> sorted(feed['Schedule'].keys())
['conferences', 'events', 'speakers', 'venues']
>>> for key, value in sorted(feed['Schedule'].items()):
... print('{:3} {}'.format(len(value), key))
...
Data Wrangling with Dynamic Attributes | 587
4. An often mentioned one is AttrDict; another, allowing quick creation of nested mappings is addict.
1 conferences
484 events
357 speakers
53 venues
>>> feed['Schedule']['speakers'][-1]['name']
'Carina C. Zona'
>>> feed['Schedule']['speakers'][-1]['serial']
141590
>>> feed['Schedule']['events'][40]['name']
'There *Will* Be Bugs'
>>> feed['Schedule']['events'][40]['speakers']
[3471, 5199]
feed is a dict holding nested dicts and lists, with string and integer values.
List the four record collections inside "Schedule".
Display record counts for each collection.
Navigate through the nested dicts and lists to get the name of the last speaker.
Get serial number of that same speaker.
Each event has a 'speakers' list with 0 or more speaker serial numbers.
Exploring JSON-Like Data with Dynamic Attributes
Example 19-2 is simple enough, but the syntax feed['Schedule']['events'][40]
['name'] is cumbersome. In JavaScript, you can get the same value by writing
feed.Schedule.events[40].name. It�s easy to implement a dict-like class that does the
same in Python�there are plenty of implementations on the Web.4 I implemented my
own FrozenJSON, which is simpler than most recipes because it supports reading only:
it�s just for exploring the data. However, it�s also recursive, dealing automatically with
nested mappings and lists.
Example 19-4 is a demonstration of FrozenJSON and the source code is in Example 19-5.
Example 19-4. FrozenJSON from Example 19-5 allows reading attributes like name
and calling methods like .keys() and .items()
>>> from osconfeed import load
>>> raw_feed = load()
>>> feed = FrozenJSON(raw_feed)
>>> len(feed.Schedule.speakers)
357
>>> sorted(feed.Schedule.keys())
['conferences', 'events', 'speakers', 'venues']
>>> for key, value in sorted(feed.Schedule.items()):
... print('{:3} {}'.format(len(value), key))
588 | Chapter 19: Dynamic Attributes and Properties
...
1 conferences
484 events
357 speakers
53 venues
>>> feed.Schedule.speakers[-1].name
'Carina C. Zona'
>>> talk = feed.Schedule.events[40]
>>> type(talk)
<class 'explore0.FrozenJSON'>
>>> talk.name
'There *Will* Be Bugs'
>>> talk.speakers
[3471, 5199]
>>> talk.flavor
Traceback (most recent call last):
...
KeyError: 'flavor'
Build a FrozenJSON instance from the raw_feed made of nested dicts and lists.
FrozenJSON allows traversing nested dicts by using attribute notation; here we
show the length of the list of speakers.
Methods of the underlying dicts can also be accessed, like .keys(), to retrieve
the record collection names.
Using items(), we can retrieve the record collection names and their contents,
to display the len() of each of them.
A list, such as feed.Schedule.speakers, remains a list, but the items inside
are converted to FrozenJSON if they are mappings.
Item 40 in the events list was a JSON object; now it�s a FrozenJSON instance.
Event records have a speakers list with speaker serial numbers.
Trying to read a missing attribute raises KeyError, instead of the usual Attrib
uteError.
The keystone of the FrozenJSON class is the __getattr__ method, which we already
used in the Vector example in �Vector Take #3: Dynamic Attribute Access� on page
284, to retrieve Vector components by letter�v.x, v.y, v.z, etc. It�s essential to recall
that the __getattr__ special method is only invoked by the interpreter when the usual
process fails to retrieve an attribute (i.e., when the named attribute cannot be found in
the instance, nor in the class or in its superclasses).
The last line of Example 19-4 exposes a minor issue with the implementation: ideally,
trying to read a missing attribute should raise AttributeError. I actually did implement
the error handling, but it doubled the size of the __getattr__ method and distracted
from the most important logic I wanted to show, so I left it out for didactic reasons.
Data Wrangling with Dynamic Attributes | 589
As shown in Example 19-5, the FrozenJSON class has only two methods (__init__,
__getattr__) and a __data instance attribute, so attempts to retrieve an attribute by
any other name will trigger __getattr__. This method will first look if the self.__da
ta dict has an attribute (not a key!) by that name; this allows FrozenJSON instances to
handle any dict method such as items, by delegating to self.__data.items(). If
self.___data doesn�t have an attribute with the given name, __getattr__ uses name as
a key to retrieve an item from self.__dict, and passes that item to FrozenJ
SON.build. This allows navigating through nested structures in the JSON data, as each
nested mapping is converted to another FrozenJSON instance by the build class method.
Example 19-5. explore0.py: turn a JSON dataset into a FrozenJSON holding nested
FrozenJSON objects, lists, and simple types
from collections import abc
class FrozenJSON:
"""A read-only facade for navigating a JSON-like object
using attribute notation
"""
def __init__(self, mapping):
self.__data = dict(mapping)
def __getattr__(self, name):
if hasattr(self.__data, name):
return getattr(self.__data, name)
else:
return FrozenJSON.build(self.__data[name])
@classmethod
def build(cls, obj):
if isinstance(obj, abc.Mapping):
return cls(obj)
elif isinstance(obj, abc.MutableSequence):
return [cls.build(item) for item in obj]
else:
return obj
Build a dict from the mapping argument. This serves two purposes: ensures we
got a dict (or something that can be converted to one) and makes a copy for
safety.
__getattr__ is called only when there�s no attribute with that name.
If name matches an attribute of the instance __data, return that. This is how calls
to methods like keys are handled.
590 | Chapter 19: Dynamic Attributes and Properties
5. This line is where a KeyError exception may occur, in the expression self.__data[name]. It should be
handled and an AttributeError raised instead, because that�s what is expected from __getattr__. The
diligent reader is invited to code the error handling as an exercise.
6. The source of the data is JSON, and the only collection types in JSON data are dict and list.
Otherwise, fetch the item with the key name from self.__data, and return the
result of calling FrozenJSON.build() on that.5
This is an alternate constructor, a common use for the @classmethod decorator.
If obj is a mapping, build a FrozenJSON with it.
If it is a MutableSequence, it must be a list,6 so we build a list by passing every
item in obj recursively to .build().
If it�s not a dict or a list, return the item as it is.
Note that no caching or transformation of the original feed is done. As the feed is
traversed, the nested data structures are converted again and again into FrozenJSON.
But that�s OK for a dataset of this size, and for a script that will only be used to explore
or convert the data.
Any script that generates or emulates dynamic attribute names from arbitrary sources
must deal with one issue: the keys in the original data may not be suitable attribute
names. The next section addresses this.
The Invalid Attribute Name Problem
The FrozenJSON class has a limitation: there is no special handling for attribute names
that are Python keywords. For example, if you build an object like this:
>>> grad = FrozenJSON({'name': 'Jim Bo', 'class': 1982})
You won�t be able to read grad.class because class is a reserved word in Python:
>>> grad.class
File "<stdin>", line 1
grad.class
^
SyntaxError: invalid syntax
You can always do this, of course:
>>> getattr(grad, 'class')
1982
But the idea of FrozenJSON is to provide convenient access to the data, so a better solution
is checking whether a key in the mapping given to FrozenJSON.__init__ is a
keyword, and if so, append an _ to it, so the attribute can be read like this:
Data Wrangling with Dynamic Attributes | 591
>>> grad.class_
1982
This can be achieved by replacing the one-liner __init__ from Example 19-5 with the
version in Example 19-6.
Example 19-6. explore1.py: append a _ to attribute names that are Python keywords
def __init__(self, mapping):
self.__data = {}
for key, value in mapping.items():
if keyword.iskeyword(key):
key += '_'
self.__data[key] = value
The keyword.iskeyword(�) function is exactly what we need; to use it, the
keyword module must be imported, which is not shown in this snippet.
A similar problem may arise if a key in the JSON is not a valid Python identifier:
>>> x = FrozenJSON({'2be':'or not'})
>>> x.2be
File "<stdin>", line 1
x.2be
^
SyntaxError: invalid syntax
Such problematic keys are easy to detect in Python 3 because the str class provides the
s.isidentifier() method, which tells you whether s is a valid Python identifier according
to the language grammar. But turning a key that is not a valid identifier into
valid attribute name is not trivial. Two simple solutions would be raising an exception
or replacing the invalid keys with generic names like attr_0, attr_1, and so on. For the
sake of simplicity, I will not worry about this issue.
After giving some thought to the dynamic attribute names, let�s turn to another essential
feature of FrozenJSON: the logic of the build class method, which is used by __get
attr__ to return a different type of object depending on the value of the attribute being
accessed, so that nested structures are converted to FrozenJSON instances or lists of
FrozenJSON instances.
Instead of a class method, the same logic could be implemented as the __new__ special
method, as we�ll see next.
Flexible Object Creation with __new__
We often refer to __init__ as the constructor method, but that�s because we adopted
jargon from other languages. The special method that actually constructs an instance
is __new__: it�s a class method (but gets special treatment, so the @classmethod decorator
592 | Chapter 19: Dynamic Attributes and Properties
is not used), and it must return an instance. That instance will in turn be passed as the
first argument self of __init__. Because __init__ gets an instance when called, and
it�s actually forbidden from returning anything, __init__ is really an �initializer.� The
real constructor is __new__�which we rarely need to code because the implementation
inherited from object suffices.
The path just described, from __new__ to __init__, is the most common, but not the
only one. The __new__ method can also return an instance of a different class, and when
that happens, the interpreter does not call __init__.
In other words, the process of building an object in Python can be summarized with
this pseudocode:
# pseudo-code for object construction
def object_maker(the_class, some_arg):
new_object = the_class.__new__(some_arg)
if isinstance(new_object, the_class):
the_class.__init__(new_object, some_arg)
return new_object
# the following statements are roughly equivalent
x = Foo('bar')
x = object_maker(Foo, 'bar')
Example 19-7 shows a variation of FrozenJSON where the logic of the former build class
method was moved to __new__.
Example 19-7. explore2.py: using new instead of build to construct new objects that
may or may not be instances of FrozenJSON
from collections import abc
class FrozenJSON:
"""A read-only facade for navigating a JSON-like object
using attribute notation
"""
def __new__(cls, arg):
if isinstance(arg, abc.Mapping):
return super().__new__(cls)
elif isinstance(arg, abc.MutableSequence):
return [cls(item) for item in arg]
else:
return arg
def __init__(self, mapping):
self.__data = {}
for key, value in mapping.items():
if iskeyword(key):
key += '_'
Data Wrangling with Dynamic Attributes | 593
self.__data[key] = value
def __getattr__(self, name):
if hasattr(self.__data, name):
return getattr(self.__data, name)
else:
return FrozenJSON(self.__data[name])
As a class method, the first argument __new__ gets is the class itself, and the
remaining arguments are the same that __init__ gets, except for self.
The default behavior is to delegate to the __new__ of a super class. In this case,
we are calling __new__ from the object base class, passing FrozenJSON as the
only argument.
The remaining lines of __new__ are exactly as in the old build method.
This was where FrozenJSON.build was called before; now we just call the Fro
zenJSON constructor.
The __new__ method gets the class as the first argument because, usually, the created
object will be an instance of that class. So, in FrozenJSON.__new__, when the expression
super().__new__(cls) effectively calls object.__new__(FrozenJSON), the instance
built by the object class is actually an instance of FrozenJSON�i.e., the __class__
attribute of the new instance will hold a reference to FrozenJSON�even though the
actual construction is performed by object.__new__, implemented in C, in the guts of
the interpreter.
There is an obvious shortcoming in the way the OSCON JSON feed is structured: the
event at index 40, titled 'There *Will* Be Bugs' has two speakers, 3471 and 5199, but
finding them is not easy, because those are serial numbers, and the Schedule.speak
ers list is not indexed by them. The venue field, present in every event record, also
holds the a serial number, but finding the corresponding venue record requires a linear
scan of the Schedule.venues list. Our next task is restructuring the data, and then
automating the retrieval of linked records.
Restructuring the OSCON Feed with shelve
The funny name of the standard shelve module makes sense when you realize that
pickle is the name of the Python object serialization format�and of the module that
converts objects to/from that format. Because pickle jars are kept in shelves, it makes
sense that shelve provides pickle storage.
The shelve.open high-level function returns a shelve.Shelf instance�a simple keyvalue
object database backed by the dbm module, with these characteristics:
594 | Chapter 19: Dynamic Attributes and Properties
7. I could also do len(db), but that would be costly in a large dbm database.
� shelve.Shelf subclasses abc.MutableMapping, so it provides the essential methods
we expect of a mapping type
� In addition, shelve.Shelf provides a few other I/O management methods, like
sync and close; it�s also a context manager.
� Keys and values are saved whenever a new value is assigned to a key.
� The keys must be strings.
� The values must be objects that the pickle module can handle.
Consult the documentation for the shelve, dbm, and pickle modules for the details and
caveats. What matters to us now is that shelve provides a simple, efficient way to reorganize
the OSCON schedule data: we will read all records from the JSON file and save
them to a shelve.Shelf. Each key will be made from the record type and the serial
number (e.g., 'event.33950' or 'speaker.3471') and the value will be an instance of
a new Record class we are about to introduce.
Example 19-8 shows the doctests for the schedule1.py script using shelve. To try it out
interactively, run the script as python -i schedule1.py to get a console prompt with
the module loaded. The load_db function does the heavy work: it calls oscon
feed.load (from Example 19-2) to read the JSON data and saves each record as a Record
instance in the Shelf object passed as db. After that, retrieving a speaker record is as
easy as speaker = db['speaker.3471'].
Example 19-8. Trying out the functionality provided by schedule1.py (Example 19-9)
>>> import shelve
>>> db = shelve.open(DB_NAME)
>>> if CONFERENCE not in db:
... load_db(db)
...
>>> speaker = db['speaker.3471']
>>> type(speaker)
<class 'schedule1.Record'>
>>> speaker.name, speaker.twitter
('Anna Martelli Ravenscroft', 'annaraven')
>>> db.close()
shelve.open opens an existing or just-created database file.
A quick way to determine if the database is populated is to look for a known key,
in this case conference.115�the key to the single conference record.7
If the database is empty, call load_db(db) to load it.
Data Wrangling with Dynamic Attributes | 595
8. A fundamental weakness of doctest is the lack of proper resource setup and guaranteed tear-down. I wrote
most tests for schedule1.py using py.test, and you can see them at Example A-12.
Fetch a speaker record.
It�s an instance of the Record class defined in Example 19-9.
Each Record instance implements a custom set of attributes reflecting the fields
of the underlying JSON record.
Always remember to close a shelve.Shelf. If possible, use a with block to make
sure the Shelf is closed.8
The code for schedule1.py is in Example 19-9.
Example 19-9. schedule1.py: exploring OSCON schedule data saved to a shelve.Shelf
import warnings
import osconfeed
DB_NAME = 'data/schedule1_db'
CONFERENCE = 'conference.115'
class Record:
def __init__(self, **kwargs):
self.__dict__.update(kwargs)
def load_db(db):
raw_data = osconfeed.load()
warnings.warn('loading ' + DB_NAME)
for collection, rec_list in raw_data['Schedule'].items():
record_type = collection[:-1]
for record in rec_list:
key = '{}.{}'.format(record_type, record['serial'])
record['serial'] = key
db[key] = Record(**record)
Load the osconfeed.py module from Example 19-2.
This is a common shortcut to build an instance with attributes created from
keyword arguments (detailed explanation follows).
This may fetch the JSON feed from the Web, if there�s no local copy.
Iterate over the collections (e.g., 'conferences', 'events', etc.).
record_type is set to the collection name without the trailing 's' (i.e., 'events'
becomes 'event').
Build key from the record_type and the 'serial' field.
596 | Chapter 19: Dynamic Attributes and Properties
9. By the way, Bunch is the name of the class used by Alex Martelli to share this tip in a recipe from 2001 titled
�The simple but handy collector of a bunch of named stuff class�.
Update the 'serial' field with the full key.
Build Record instance and save it to the database under the key.
The Record.__init__ method illustrates a popular Python hack. Recall that the
__dict__ of an object is where its attributes are kept�unless __slots__ is declared in
the class, as we saw in �Saving Space with the __slots__ Class Attribute� on page 264.
So, updating an instance __dict__ with a mapping is a quick way to create a bunch of
attributes in that instance.9
I am not going to repeat the details we discussed earlier in �The
Invalid Attribute Name Problem� on page 591, but depending on
the application context, the Record class may need to deal with
keys that are not valid attribute names.
The definition of Record in Example 19-9 is so simple that you may be wondering why
we did not use it before, instead of the more complicated FrozenJSON. There are a couple
reasons. First, FrozenJSON works by recursively converting the nested mappings and
lists; Record doesn�t need that because our converted dataset doesn�t have mappings
nested in mappings or lists. The records contain only strings, integers, lists of strings,
and lists of integers. A second reason is that FrozenJSON provides access to the embedded
__data dict attributes�which we used to invoke methods like keys�and now we
don�t need that functionality either.
The Python standard library provides at least two classes similar
to our Record, where each instance has an arbitrary set of attributes
built from keyword arguments to the constructor: multi
processing.Namespace (documentation, source code), and arg
parse.Namespace (documentation, source code). I implemented
Record to highlight the essence of the idea: __init__ updating
the instance __dict__.
After reorganizing the schedule dataset as we just did, we can now extend the Record
class to provide a useful service: automatically retrieving venue and speaker records
referenced in an event record. This is similar to what the Django ORM does when you
access a models.ForeignKey field: instead of the key, you get the linked model object.
We�ll use properties to do that in the next example.
Data Wrangling with Dynamic Attributes | 597
Linked Record Retrieval with Properties
The goal of this next version is: given an event record retrieved from the shelf, reading
its venue or speakers attributes will not return serial numbers but full-fledged record
objects. See the partial interaction in Example 19-10 as an example.
Example 19-10. Extract from the doctests of schedule2.py
>>> DbRecord.set_db(db)
>>> event = DbRecord.fetch('event.33950')
>>> event
<Event 'There *Will* Be Bugs'>
>>> event.venue
<DbRecord serial='venue.1449'>
>>> event.venue.name
'Portland 251'
>>> for spkr in event.speakers:
... print('{0.serial}: {0.name}'.format(spkr))
...
speaker.3471: Anna Martelli Ravenscroft
speaker.5199: Alex Martelli
DbRecord extends Record, adding database support: to operate, DbRecord must
be given a reference to a database.
The DbRecord.fetch class method retrieves records of any type.
Note that event is an instance of the Event class, which extends DbRecord.
Accessing event.venue returns a DbRecord instance.
Now it�s easy to find out the name of an event.venue. This automatic
dereferencing is the goal of this example.
We can also iterate over the event.speakers list, retrieving DbRecords
representing each speaker.
Figure 19-1 Provides an overview of the classes we�ll be studying in this section:
Record
The __init__ method is the same as in schedule1.py (Example 19-9); the __eq__
method was added to facilitate testing.
DbRecord
Subclass of Record adding a __db class attribute, set_db and get_db static methods
to set/get that attribute, a fetch class method to retrieve records from the database,
and a __repr__ instance method to support debugging and testing.
Event
Subclass of DbRecord adding venue and speakers properties to retrieve linked records,
and a specialized __repr__ method.
598 | Chapter 19: Dynamic Attributes and Properties
10. The StackOverflow topic �Class-level read only properties in Python� has solutions to read-only attributes
in classes, including one by Alex Martelli. The solutions require metaclasses, so you may want to read
Chapter 21 before studying them.
11. The full listing for schedule2.py is in Example A-13, together with py.test scripts in �Chapter 19: OSCON
Schedule Scripts and Tests� on page 708.
Figure 19-1. UML class diagram for an enhanced Record class and two subclasses:
DbRecord and Event.
The DbRecord.__db class attribute exists to hold a reference to the opened
shelve.Shelf database, so it can be used by the DbRecord.fetch method and the
Event.venue and Event.speakers properties that depend on it. I coded __db as a private
class attribute with conventional getter and setter methods because I wanted to protect
it from accidental overwriting. I did not use a property to manage __db because of a
crucial fact: properties are class attributes designed to manage instance attributes.10
The code for this section is in the schedule2.py module in the Fluent Python code repository.
Because the module tops 100 lines, I�ll present it in parts.11
The first statements of schedule2.py are shown in Example 19-11.
Example 19-11. schedule2.py: imports, constants, and the enhanced Record class
import warnings
import inspect
import osconfeed
DB_NAME = 'data/schedule2_db'
CONFERENCE = 'conference.115'
class Record:
def __init__(self, **kwargs):
self.__dict__.update(kwargs)
def __eq__(self, other):
Data Wrangling with Dynamic Attributes | 599
12. Explicitly subclassing from object in Python 3 is not wrong, just redundant because all classes are newstyle
now. This is one example where breaking with the past made the language cleaner. If the same code
must run in Python 2 and Python 3, inheriting from object should be explicit.
if isinstance(other, Record):
return self.__dict__ == other.__dict__
else:
return NotImplemented
inspect will be used in the load_db function (Example 19-14).
Because we are storing instances of different classes, we create and use a different
database file, 'schedule2_db', instead of the 'schedule_db' of Example 19-9.
An __eq__ method is always handy for testing.
In Python 2, only �new style� classes support properties. To write
a new style class in Python 2 you must subclass directly or indirectly
from object. Record in Example 19-11 is the base class of a
hierarchy that will use properties, so in Python 2 its declaration
would start with:12
class Record(object):
# etc...
The next classes defined in schedule2.py are a custom exception type and DbRecord. See
Example 19-12.
Example 19-12. schedule2.py: MissingDatabaseError and DbRecord class
class MissingDatabaseError(RuntimeError):
"""Raised when a database is required but was not set."""
class DbRecord(Record):
__db = None
@staticmethod
def set_db(db):
DbRecord.__db = db
@staticmethod
def get_db():
return DbRecord.__db
@classmethod
def fetch(cls, ident):
db = cls.get_db()
try:
600 | Chapter 19: Dynamic Attributes and Properties
return db[ident]
except TypeError:
if db is None:
msg = "database not set; call '{}.set_db(my_db)'"
raise MissingDatabaseError(msg.format(cls.__name__))
else:
raise
def __repr__(self):
if hasattr(self, 'serial'):
cls_name = self.__class__.__name__
return '<{} serial={!r}>'.format(cls_name, self.serial)
else:
return super().__repr__()
Custom exceptions are usually marker classes, with no body. A docstring
explaining the usage of the exception is better than a mere pass statement.
DbRecord extends Record.
The __db class attribute will hold a reference to the opened shelve.Shelf
database.
set_db is a staticmethod to make it explicit that its effect is always exactly the
same, no matter how it�s called.
Even if this method is invoked as Event.set_db(my_db), the __db attribute will
be set in the DbRecord class.
get_db is also a staticmethod because it will always return the object referenced
by DbRecord.__db, no matter how it�s invoked.
fetch is a class method so that its behavior is easier to customize in subclasses.
This retrieves the record with the ident key from the database.
If we get a TypeError and db is None, raise a custom exception explaining that
the database must be set.
Otherwise, re-raise the exception because we don�t know how to handle it.
If the record has a serial attribute, use it in the string representation.
Otherwise, default to the inherited __repr__.
Now we get to the meat of the example: the Event class, listed in Example 19-13.
Example 19-13. schedule2.py: the Event class
class Event(DbRecord):
@property
def venue(self):
key = 'venue.{}'.format(self.venue_serial)
Data Wrangling with Dynamic Attributes | 601
return self.__class__.fetch(key)
@property
def speakers(self):
if not hasattr(self, '_speaker_objs'):
spkr_serials = self.__dict__['speakers']
fetch = self.__class__.fetch
self._speaker_objs = [fetch('speaker.{}'.format(key))
for key in spkr_serials]
return self._speaker_objs
def __repr__(self):
if hasattr(self, 'name'):
cls_name = self.__class__.__name__
return '<{} {!r}>'.format(cls_name, self.name)
else:
return super().__repr__()
Event extends DbRecord.
The venue property builds a key from the venue_serial attribute, and passes it
to the fetch class method, inherited from DbRecord (see explanation after this
example).
The speakers property checks if the record has a _speaker_objs attribute.
If it doesn�t, the 'speakers' attribute is retrieved directly from the instance
__dict__ to avoid an infinite recursion, because the public name of this property
is also speakers.
Get a reference to the fetch class method (the reason for this will be explained
shortly).
self._speaker_objs is loaded with a list of speaker records, using fetch.
That list is returned.
If the record has a name attribute, use it in the string representation.
Otherwise, default to the inherited __repr__.
In the venue property of Example 19-13, the last line returns
self.__class__.fetch(key). Why not write that simply as self.fetch(key)? The
simpler formula works with the specific dataset of the OSCON feed because there is no
event record with a 'fetch' key. If even a single event record had a key named
'fetch', then within that specific Event instance, the reference self.fetch would retrieve
the value of that field, instead of the fetch class method that Event inherits from
DbRecord. This is a subtle bug, and it could easily sneak through testing and blow up
only in production when the venue or speaker records linked to that specific Event
record are retrieved.
602 | Chapter 19: Dynamic Attributes and Properties
When creating instance attribute names from data, there is always
the risk of bugs due to shadowing of class attributes (such as
methods) or data loss through accidental overwriting of existing
instance attributes. This caveat is probably the main reason why,
by default, Python dicts are not like JavaScript objects in the first
place.
If the Record class behaved more like a mapping, implementing a dynamic __geti
tem__ instead of a dynamic __getattr__, there would be no risk of bugs from overwriting
or shadowing. A custom mapping is probably the Pythonic way to implement
Record. But if I took that road, we�d not be reflecting on the tricks and traps of dynamic
attribute programming.
The final piece of this example is the revised load_db function in Example 19-14.
Example 19-14. schedule2.py: the load_db function
def load_db(db):
raw_data = osconfeed.load()
warnings.warn('loading ' + DB_NAME)
for collection, rec_list in raw_data['Schedule'].items():
record_type = collection[:-1]
cls_name = record_type.capitalize()
cls = globals().get(cls_name, DbRecord)
if inspect.isclass(cls) and issubclass(cls, DbRecord):
factory = cls
else:
factory = DbRecord
for record in rec_list:
key = '{}.{}'.format(record_type, record['serial'])
record['serial'] = key
db[key] = factory(**record)
So far, no changes from the load_db in schedule1.py (Example 19-9).
Capitalize the record_type to get a potential class name (e.g., 'event' becomes
'Event').
Get an object by that name from the module global scope; get DbRecord if there�s
no such object.
If the object just retrieved is a class, and is a subclass of DbRecord�
�bind the factory name to it. This means factory may be any subclass of
DbRecord, depending on the record_type.
Otherwise, bind the factory name to DbRecord.
The for loop that creates the key and saves the records is the same as before,
except that�
Data Wrangling with Dynamic Attributes | 603
�the object stored in the database is constructed by factory, which may be
DbRecord or a subclass selected according to the record_type.
Note that the only record_type that has a custom class is Event, but if classes named
Speaker or Venue are coded, load_db will automatically use those classes when building
and saving records, instead of the default DbRecord class.
So far, the examples in this chapter were designed to show a variety of techniques for
implementing dynamic attributes using basic tools such as __getattr__, hasattr, get
attr, @property, and __dict__.
Properties are frequently used to enforce business rules by changing a public attribute
into an attribute managed by a getter and setter without affecting client code, as the next
section shows.
Using a Property for Attribute Validation
So far, we have only seen the @property decorator used to implement read-only properties.
In this section, we will create a read/write property.
LineItem Take #1: Class for an Item in an Order
Imagine an app for a store that sells organic food in bulk, where customers can order
nuts, dried fruit, or cereals by weight. In that system, each order would hold a sequence
of line items, and each line item could be represented by a class as in Example 19-15.
Example 19-15. bulkfood_v1.py: the simplest LineItem class
class LineItem:
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
That�s nice and simple. Perhaps too simple. Example 19-16 shows a problem.
Example 19-16. A negative weight results in a negative subtotal
>>> raisins = LineItem('Golden raisins', 10, 6.95)
>>> raisins.subtotal()
69.5
>>> raisins.weight = -20 # garbage in...
>>> raisins.subtotal() # garbage out...
-139.0
604 | Chapter 19: Dynamic Attributes and Properties
13. Direct quote by Jeff Bezos in the Wall Street Journal story �Birth of a Salesman� (October 15, 2011).
This is a toy example, but not as fanciful as you may think. Here is a true story from the
early days of Amazon.com:
We found that customers could order a negative quantity of books! And we would credit
their credit card with the price and, I assume, wait around for them to ship the books.13
� Jeff Bezos
Founder and CEO of Amazon.com
How do we fix this? We could change the interface of LineItem to use a getter and a
setter for the weight attribute. That would be the Java way, and it�s not wrong.
On the other hand, it�s natural to be able set the weight of an item by just assigning to
it; and perhaps the system is in production with other parts already accessing
item.weight directly. In this case, the Python way would be to replace the data attribute
with a property.
LineItem Take #2: A Validating Property
Implementing a property will allow us to use a getter and a setter, but the interface of
LineItem will not change (i.e., setting the weight of a LineItem will still be written as
raisins.weight = 12).
Example 19-17 lists the code for a read/write weight property.
Example 19-17. bulkfood_v2.py: a LineItem with a weight property
class LineItem:
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
@property
def weight(self):
return self.__weight
@weight.setter
def weight(self, value):
if value > 0:
self.__weight = value
else:
raise ValueError('value must be > 0')
Using a Property for Attribute Validation | 605
Here the property setter is already in use, making sure that no instances with
negative weight can be created.
@property decorates the getter method.
The methods that implement a property all have the name of the public attribute:
weight.
The actual value is stored in a private attribute __weight.
The decorated getter has a .setter attribute, which is also a decorator; this ties
the getter and setter together.
If the value is greater than zero, we set the private __weight.
Otherwise, ValueError is raised.
Note how a LineItem with an invalid weight cannot be created now:
>>> walnuts = LineItem('walnuts', 0, 10.00)
Traceback (most recent call last):
...
ValueError: value must be > 0
Now we have protected weight from users providing negative values. Although buyers
usually can�t set the price of an item, a clerical error or a bug may create a LineItem with
a negative price. To prevent that, we could also turn price into a property, but this
would entail some repetition in our code.
Remember the Paul Graham quote from Chapter 14: �When I see patterns in my programs,
I consider it a sign of trouble.� The cure for repetition is abstraction. There are
two ways to abstract away property definitions: using a property factory or a descriptor
class. The descriptor class approach is more flexible, and we�ll devote Chapter 20 to a
full discussion of it. Properties are in fact implemented as descriptor classes themselves.
But here we will continue our exploration of properties by implementing a property
factory as a function.
But before we can implement a property factory, we need to have a deeper understanding
of properties.
A Proper Look at Properties
Although often used as a decorator, the property built-in is actually a class. In Python,
functions and classes are often interchangeable, because both are callable and there is
no new operator for object instantiation, so invoking a constructor is no different than
invoking a factory function. And both can be used as decorators, as long as they return
a new callable that is a suitable replacement of the decorated function.
This is the full signature of the property constructor:
606 | Chapter 19: Dynamic Attributes and Properties
property(fget=None, fset=None, fdel=None, doc=None)
All arguments are optional, and if a function is not provided for one of them, the corresponding
operation is not allowed by the resulting property object.
The property type was added in Python 2.2, but the @ decorator syntax appeared only
in Python 2.4, so for a few years, properties were defined by passing the accessor functions
as the first two arguments.
The �classic� syntax for defining properties without decorators is illustrated in
Example 19-18.
Example 19-18. bulkfood_v2b.py: same as Example 19-17 but without using decorators
class LineItem:
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
def get_weight(self):
return self.__weight
def set_weight(self, value):
if value > 0:
self.__weight = value
else:
raise ValueError('value must be > 0')
weight = property(get_weight, set_weight)
A plain getter.
A plain setter.
Build the property and assign it to a public class attribute.
The classic form is better than the decorator syntax in some situations; the code of the
property factory we�ll discuss shortly is one example. On the other hand, in a class body
with many methods, the decorators make it explicit which are the getters and setters,
without depending on the convention of using get and set prefixes in their names.
The presence of a property in a class affects how attributes in instances of that class can
be found in a way that may be surprising at first. The next section explains.
A Proper Look at Properties | 607
Properties Override Instance Attributes
Properties are always class attributes, but they actually manage attribute access in the
instances of the class.
In �Overriding Class Attributes� on page 267 we saw that when an instance and its class
both have a data attribute by the same name, the instance attribute overrides, or shadows,
the class attribute�at least when read through that instance. Example 19-19 illustrates
this point.
Example 19-19. Instance attribute shadows class data attribute
>>> class Class: #
... data = 'the class data attr'
... @property
... def prop(self):
... return 'the prop value'
...
>>> obj = Class()
>>> vars(obj) #
{}
>>> obj.data #
'the class data attr'
>>> obj.data = 'bar' #
>>> vars(obj) #
{'data': 'bar'}
>>> obj.data #
'bar'
>>> Class.data #
'the class data attr'
Define Class with two class attributes: the data data attribute and the prop
property.
vars returns the __dict__ of obj, showing it has no instance attributes.
Reading from obj.data retrieves the value of Class.data.
Writing to obj.data creates an instance attribute.
Inspect the instance to see the instance attribute.
Now reading from obj.data retrieves the value of the instance attribute. When
read from the obj instance, the instance data shadows the class data.
The Class.data attribute is intact.
Now, let�s try to override the prop attribute on the obj instance. Resuming the previous
console session, we have Example 19-20.
608 | Chapter 19: Dynamic Attributes and Properties
Example 19-20. Instance attribute does not shadow class property (continued from
Example 19-19)
>>> Class.prop #
<property object at 0x1072b7408>
>>> obj.prop #
'the prop value'
>>> obj.prop = 'foo' #
Traceback (most recent call last):
...
AttributeError: can't set attribute
>>> obj.__dict__['prop'] = 'foo' #
>>> vars(obj) #
{'prop': 'foo', 'attr': 'bar'}
>>> obj.prop #
'the prop value'
>>> Class.prop = 'baz' #
>>> obj.prop #
'foo'
Reading prop directly from Class retrieves the property object itself, without
running its getter method.
Reading obj.prop executes the property getter.
Trying to set an instance prop attribute fails.
Putting 'prop' directly in the obj.__dict__ works.
We can see that obj now has two instance attributes: attr and prop.
However, reading obj.prop still runs the property getter. The property is not
shadowed by an instance attribute.
Overwriting Class.prop destroys the property object.
Now obj.prop retrieves the instance attribute. Class.prop is not a property
anymore, so it no longer overrides obj.prop.
As a final demonstration, we�ll add a new property to Class, and see it overriding an
instance attribute. Example 19-21 picks up where Example 19-20 left off.
Example 19-21. New class property shadows existing instance attribute (continued
from Example 19-20)
>>> obj.data #
'bar'
>>> Class.data #
'the class data attr'
>>> Class.data = property(lambda self: 'the "data" prop value') #
>>> obj.data #
'the "data" prop value'
>>> del Class.data #
A Proper Look at Properties | 609
>>> obj.data #
'bar'
obj.data retrieves the instance data attribute.
Class.data retrieves the class data attribute.
Overwrite Class.data with a new property.
obj.data is now shadowed by the Class.data property.
Delete the property.
obj.data now reads the instance data attribute again.
The main point of this section is that an expression like obj.attr does not search for
attr starting with obj. The search actually starts at obj.__class__, and only if there is
no property named attr in the class, Python looks in the obj instance itself. This rule
applies not only to properties but to a whole category of descriptors, the overriding
descriptors. Further treatment of descriptors must wait for Chapter 20, where we�ll see
that properties are in fact overriding descriptors.
Now back to properties. Every Python code unit�modules, functions, classes, methods
�can have a docstring. The next topic is how to attach documentation to properties.
Property Documentation
When tools such as the console help() function or IDEs need to display the documentation
of a property, they extract the information from the __doc__ attribute of the
property.
If used with the classic call syntax, property can get the documentation string as the
doc argument:
weight = property(get_weight, set_weight, doc='weight in kilograms')
When property is deployed as a decorator, the docstring of the getter method�the one
with the @property decorator itself�is used as the documentation of the property as a
whole. Figure 19-2 shows the help screens generated from the code in Example 19-22.
610 | Chapter 19: Dynamic Attributes and Properties
Figure 19-2. Screenshots of the Python console when issuing the commands
help(Foo.bar) and help(Foo). Source code in Example 19-22.
Example 19-22. Documentation for a property
class Foo:
@property
def bar(self):
'''The bar attribute'''
return self.__dict__['bar']
@bar.setter
def bar(self, value):
self.__dict__['bar'] = value
Now that we have these property essentials covered, let�s go back to the issue of protecting
both the weight and price attributes of LineItem so they only accept values greater
than zero�but without implementing two nearly identical pairs of getters/setters by
hand.
Coding a Property Factory
We�ll create a quantity property factory�so named because the managed attributes
represent quantities that can�t be negative or zero in the application. Example 19-23
shows the clean look of the LineItem class using two instances of quantity properties:
one for managing the weight attribute, the other for price.
Example 19-23. bulkfood_v2prop.py: the quantity property factory in use
class LineItem:
weight = quantity('weight')
price = quantity('price')
Coding a Property Factory | 611
14. This code is adapted from �Recipe 9.21. Avoiding Repetitive Property Methods� from Python Cookbook,
3E by David Beazley and Brian K. Jones (O�Reilly).
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
Use the factory to define the first custom property, weight, as a class attribute.
This second call builds another custom property, price.
Here the property is already active, making sure a negative or 0 weight is
rejected.
The properties are also in use here, retrieving the values stored in the instance.
Recall that properties are class attributes. When building each quantity property, we
need to pass the name of the LineItem attribute that will be managed by that specific
property. Having to type the word weight twice in this line is unfortunate:
weight = quantity('weight')
But avoiding that repetition is complicated because the property has no way of knowing
which class attribute name will be bound to it. Remember: the right side of an assignment
is evaluated first, so when quantity() is invoked, the price class attribute doesn�t even
exist.
Improving the quantity property so that the user doesn�t need to
retype the attribute name is a nontrivial metaprogramming problem.
We�ll see a workaround in Chapter 20, but real solutions will
have to wait until Chapter 21, because they require either a class
decorator or a metaclass.
Example 19-24 lists the implementation of the quantity property factory.14
Example 19-24. bulkfood_v2prop.py: the quantity property factory
def quantity(storage_name):
def qty_getter(instance):
return instance.__dict__[storage_name]
def qty_setter(instance, value):
if value > 0:
instance.__dict__[storage_name] = value
612 | Chapter 19: Dynamic Attributes and Properties
else:
raise ValueError('value must be > 0')
return property(qty_getter, qty_setter)
The storage_name argument determines where the data for each property is
stored; for the weight, the storage name will be 'weight'.
The first argument of the qty_getter could be named self, but that would be
strange because this is not a class body; instance refers to the LineItem instance
where the attribute will be stored.
qty_getter references storage_name, so it will be preserved in the closure of
this function; the value is retrieved directly from the instance.__dict__ to
bypass the property and avoid an infinite recursion.
qty_setter is defined, also taking instance as first argument.
The value is stored directly in the instance.__dict__, again bypassing the
property.
Build a custom property object and return it.
The bits of Example 19-24 that deserve careful study revolve around the stor
age_name variable. When you code each property in the traditional way, the name of
the attribute where you will store a value is hardcoded in the getter and setter methods.
But here, the qty_getter and qty_setter functions are generic, and they depend on
the storage_name variable to know where to get/set the managed attribute in the instance
__dict__. Each time the quantity factory is called to build a property, the
storage_name must be set to a unique value.
The functions qty_getter and qty_setter will be wrapped by the property object
created in the last line of the factory function. Later when called to perform their duties,
these functions will read the storage_name from their closures, to determine where to
retrieve/store the managed attribute values.
In Example 19-25, I create and inspect a LineItem instance, exposing the storage attributes.
Example 19-25. bulkfood_v2prop.py: the quantity property factory
>>> nutmeg = LineItem('Moluccan nutmeg', 8, 13.95)
>>> nutmeg.weight, nutmeg.price
(8, 13.95)
>>> sorted(vars(nutmeg).items())
[('description', 'Moluccan nutmeg'), ('price', 13.95), ('weight', 8)]
Reading the weight and price through the properties shadowing the namesake
instance attributes.
Coding a Property Factory | 613
Using vars to inspect the nutmeg instance: here we see the actual instance
attributes used to store the values.
Note how the properties built by our factory leverage the behavior described in �Properties
Override Instance Attributes� on page 608: the weight property overrides the
weight instance attribute so that every reference to self.weight or nutmeg.weight is
handled by the property functions, and the only way to bypass the property logic is to
access the instance __dict__ directly.
The code in Example 19-25 may be a bit tricky, but it�s concise: it�s identical in length to
the decorated getter/setter pair defining just the weight property in Example 19-17. The
LineItem definition in Example 19-23 looks much better without the noise of the getter/
setters.
In a real system, that same kind of validation may appear in many fields, across several
classes, and the quantity factory would be placed in a utility module to be used over
and over again. Eventually that simple factory could be refactored into a more extensible
descriptor class, with specialized subclasses performing different validations. We�ll do
that in Chapter 20.
Now let us wrap up the discussion of properties with the issue of attribute deletion.
Handling Attribute Deletion
Recall from the Python tutorial that object attributes can be deleted using the del
statement:
del my_object.an_attribute
In practice, deleting attributes is not something we do every day in Python, and the
requirement to handle it with a property is even more unusual. But it is supported, and
I can think of a silly example to demonstrate it.
In a property definition, the @my_propety.deleter decorator is used to wrap the method
in charge of deleting the attribute managed by the property. As promised,
Example 19-26 is a silly example showing how to code a property deleter.
Example 19-26. blackknight.py: inspired by the Black Knight character of �Monty
Python and the Holy Grail�
class BlackKnight:
def __init__(self):
self.members = ['an arm', 'another arm',
'a leg', 'another leg']
self.phrases = ["'Tis but a scratch.",
"It's just a flesh wound.",
"I'm invincible!",
614 | Chapter 19: Dynamic Attributes and Properties
"All right, we'll call it a draw."]
@property
def member(self):
print('next member is:')
return self.members[0]
@member.deleter
def member(self):
text = 'BLACK KNIGHT (loses {})\n-- {}'
print(text.format(self.members.pop(0), self.phrases.pop(0)))
The doctests in blackknight.py are in Example 19-27.
Example 19-27. blackknight.py: doctests for Example 19-26 (the Black Knight never
concedes defeat)
>>> knight = BlackKnight()
>>> knight.member
next member is:
'an arm'
>>> del knight.member
BLACK KNIGHT (loses an arm)
-- 'Tis but a scratch.
>>> del knight.member
BLACK KNIGHT (loses another arm)
-- It's just a flesh wound.
>>> del knight.member
BLACK KNIGHT (loses a leg)
-- I'm invincible!
>>> del knight.member
BLACK KNIGHT (loses another leg)
-- All right, we'll call it a draw.
Using the classic call syntax instead of decorators, the fdel argument is used to set the
deleter function. For example, the member property would be coded like this in the body
of the BlackKnight class:
member = property(member_getter, fdel=member_deleter)
If you are not using a property, attribute deletion can also be handled by implementing
the lower-level __delattr__ special method, presented in �Special Methods for Attribute
Handling� on page 617. Coding a silly class with __delattr__ is left as an exercise
to the procrastinating reader.
Properties are a powerful feature, but sometimes simpler or lower-level alternatives are
preferable. In the final section of this chapter, we�ll review some the core APIs that
Python offers for dynamic attribute programming.
Handling Attribute Deletion | 615
15. Alex Martelli points out that, although __slots__ can be coded as a list, it�s better to be explicit and always
use a tuple, because changing the list in the __slots__ after the class body is processed has no effect, so it
would be misleading to use a mutable sequence there.
Essential Attributes and Functions for Attribute Handling
Throughout this chapter, and even before in the book, we�ve used some of the built-in
functions and special methods Python provides for dealing with dynamic attributes.
This section gives an overview of them in one place, because their documentation is
scattered in the official docs.
Special Attributes that Affect Attribute Handling
The behavior of many of the functions and special methods listed in the following sections
depend on three special attributes:
__class__
A reference to the object�s class (i.e., obj.__class__ is the same as type(obj)).
Python looks for special methods such as __getattr__ only in an object�s class, and
not in the instances themselves.
__dict__
A mapping that stores the writable attributes of an object or class. An object that
has a __dict__ can have arbitrary new attributes set at any time. If a class has a
__slots__ attribute, then its instances may not have a __dict__. See __slots__
(next).
__slots__
An attribute that may be defined in a class to limit the attributes its instances can
have. __slots__ is a tuple of strings naming the allowed attributes.15 If the
'__dict__' name is not in __slots__, then the instances of that class will not have
a __dict__ of their own, and only the named attributes will be allowed in them.
Built-In Functions for Attribute Handling
These five built-in functions perform object attribute reading, writing, and introspection:
dir([object])
Lists most attributes of the object. The official docs say dir is intended for interactive
use so it does not provide a comprehensive list of attributes, but an �interesting�
set of names. dir can inspect objects implemented with or without a
__dict__. The __dict__ attribute itself is not listed by dir, but the __dict__ keys
are listed. Several special attributes of classes, such as __mro__, __bases__, and
616 | Chapter 19: Dynamic Attributes and Properties
__name__ are not listed by dir either. If the optional object argument is not given,
dir lists the names in the current scope.
getattr(object, name[, default])
Gets the attribute identified by the name string from the object. This may fetch an
attribute from the object�s class or from a superclass. If no such attribute exists,
getattr raises AttributeError or returns the default value, if given.
hasattr(object, name)
Returns True if the named attribute exists in the object, or can be somehow fetched
through it (by inheritance, for example). The documentation explains: �This is
implemented by calling getattr(object, name) and seeing whether it raises an AttributeError
or not.�
setattr(object, name, value)
Assigns the value to the named attribute of object, if the object allows it. This
may create a new attribute or overwrite an existing one.
vars([object])
Returns the __dict__ of object; vars can�t deal with instances of classes that define
__slots__ and don�t have a __dict__ (contrast with dir, which handles such instances).
Without an argument, vars() does the same as locals(): returns a dict
representing the local scope.
Special Methods for Attribute Handling
When implemented in a user-defined class, the special methods listed here handle attribute
retrieval, setting, deletion, and listing.
Attribute access using either dot notation or the built-in functions getattr, hasattr,
and setattr trigger the appropriate special methods listed here. Reading and writing
attributes directly in the instance __dict__ does not trigger these special methods�
and that�s the usual way to bypass them if needed.
�Section 3.3.9. Special method lookup� of the �Data model� chapter warns:
For custom classes, implicit invocations of special methods are only guaranteed to work
correctly if defined on an object�s type, not in the object�s instance dictionary.
In other words, assume that the special methods will be retrieved on the class itself, even
when the target of the action is an instance. For this reason, special methods are not
shadowed by instance attributes with the same name.
In the following examples, assume there is a class named Class, obj is an instance of
Class, and attr is an attribute of obj.
Essential Attributes and Functions for Attribute Handling | 617
For every one of these special methods, it doesn�t matter if the attribute access is done
using dot notation or one of the built-in functions listed in �Built-In Functions for
Attribute Handling� on page 616. For example, both obj.attr and getattr(obj,
'attr', 42) trigger Class.__getattribute__(obj, 'attr').
__delattr__(self, name)
Always called when there is an attempt to delete an attribute using the del statement;
e.g., del obj.attr triggers Class.__delattr__(obj, 'attr').
__dir__(self)
Called when dir is invoked on the object, to provide a listing of attributes; e.g.,
dir(obj) triggers Class.__dir__(obj).
__getattr__(self, name)
Called only when an attempt to retrieve the named attribute fails, after the obj,
Class, and its superclasses are searched. The expressions obj.no_such_attr, get
attr(obj, 'no_such_attr'), and hasattr(obj, 'no_such_attr') may trigger
Class.__getattr__(obj, 'no_such_attr'), but only if an attribute by that name
cannot be found in obj or in Class and its superclasses.
__getattribute__(self, name)
Always called when there is an attempt to retrieve the named attribute, except when
the attribute sought is a special attribute or method. Dot notation and the get
attr and hasattr built-ins trigger this method. __getattr__ is only invoked after
__getattribute__, and only when __getattribute__ raises AttributeError. To
retrieve attributes of the instance obj without triggering an infinite recursion, implementations
of __getattribute__ should use super().__getattri
bute__(obj, name).
__setattr__(self, name, value)
Always called when there is an attempt to set the named attribute. Dot notation and
the setattr built-in trigger this method; e.g., both obj.attr = 42 and
setattr(obj, 'attr', 42) trigger Class.__setattr__(obj, 'attr', 42).
In practice, because they are unconditionally called and affect
practically every attribute access, the __getattribute__ and
__setattr__ special methods are harder to use correctly than
__getattr__�which only handles nonexisting attribute names.
Using properties or descriptors is less error prone than defining
these special methods.
This concludes our dive into properties, special methods, and other techniques for
coding dynamic attributes.
618 | Chapter 19: Dynamic Attributes and Properties
Chapter Summary
We started our coverage of dynamic attributes by showing practical examples of simple
classes to make it easier to deal with a JSON data feed. The first example was the
FrozenJSON class that converted nested dicts and lists into nested FrozenJSON instances
and lists of them. The FrozenJSON code demonstrated the use of the __getattr__ special
method to convert data structures on the fly, whenever their attributes were read. The
last version of FrozenJSON showcased the use of the __new__ constructor method to
transform a class into a flexible factory of objects, not limited to instances of itself.
We then converted the JSON feed to a shelve.Shelf database storing serialized instances
of a Record class. The first rendition of Record was a few lines long and introduced
the �bunch� idiom: using self.__dict__.update(**kwargs) to build arbitrary
attributes from keyword arguments passed to __init__. The second iteration of this
example saw the extension of Record with a DbRecord class for database integration and
an Event class implementing automatic retrieval of linked records through properties.
Coverage of properties continued with the LineItem class, where a property was deployed
to protect a weight attribute from negative or zero values that make no business
sense. After a deeper look at property syntax and semantics, we created a property
factory to enforce the same validation on weight and price, without coding multiple
getters and setters. The property factory leveraged subtle concepts�such as closures
and the instance attribute overriding by properties�to provide an elegant generic solution
using the same number of lines as a single handcoded property definition.
Finally, we had a brief look at handling attribute deletion with properties, followed by
an overview of the key special attributes, built-in functions, and special methods that
support attribute metaprogramming in the core Python language.
Further Reading
The official documentation for the attribute handling and introspection built-in functions
is Chapter 2, �Built-in Functions� of The Python Standard Library. The related
special methods and the __slots__ special attribute are documented in The Python
Language Reference in �3.3.2. Customizing attribute access�. The semantics of how
special methods are invoked bypassing instances is explained in �3.3.9. Special method
lookup�. In Chapter 4, �Built-in Types,� of the Python Standard Library, �4.13. Special
Attributes� covers __class__ and __dict__ attributes.
Python Cookbook, 3E by David Beazley and Brian K. Jones (O�Reilly) has several recipes
covering the topics of this chapter, but I will highlight three that are outstanding: �Recipe
8.8. Extending a Property in a Subclass� addresses the thorny issue of overriding the
methods inside a property inherited from a superclass; �Recipe 8.15. Delegating Attribute
Access� implements a proxy class showcasing most special methods from �Spe?
Chapter Summary | 619
cial Methods for Attribute Handling� on page 617 in this book; and the awesome �Recipe
9.21. Avoiding Repetitive Property Methods,� which was the basis for the property factory
function presented in Example 19-24.
Python in a Nutshell, 2E (O�Reilly), by Alex Martelli, covers only Python 2.5 but the
fundamentals still apply to Python 3 and his treatment is rigorous and objective. Martelli
devotes only three pages to properties, but that�s because the book follows an axiomatic
presentation style: the previous 15 pages or so provide a thorough description of the
semantics of Python classes from the ground up, including descriptors, which are how
properties are actually implemented under the hood. So by the time he gets to properties,
he can pack a lot of insights in those three pages�including that which I selected to
open this chapter.
Bertrand Meyer, quoted in the Uniform Access Principle definition in this chapter opening,
wrote the excellent Object-Oriented Software Construction, 2E (Prentice-Hall). The
book is more than 1,250 pages long, and I confess I did not read it all, but the first six
chapters provide one of the best conceptual introductions to OO analysis and design
I�ve seen, Chapter 11 introduces Design by Contract (Meyer invented the method and
coined the term), and Chapter 35 offers his assessments of some key OO languages:
Simula, Smalltalk, CLOS (the Lisp OO extension), Objective-C, C++, and Java, with
brief comments on some others. Meyer is also the inventor of the pseudo-pseudocode:
only in the last page of the book he reveals that the �notation� he uses throughout as
pseudocode is in fact Eiffel.
Soapbox
Meyer�s Uniform Access Principle (sometimes called UAP by acronym-lovers) is aesthetically
appealing. As a programmer using an API, I shouldn�t have to care whether
coconut.price simply fetches a data attribute or performs a computation. As a consumer
and a citizen, I do care: in ecommerce today the value of coconut.price often
depends on who is asking, so it�s certainly not a mere data attribute. In fact, it�s common
practice that the price is lower if the query comes from outside the store�say, from a
price-comparison engine. This effectively punishes loyal customers who like to browse
within a particular store. But I digress.
The previous digression does raise a relevant point for programming: although the
Uniform Access Principle makes perfect sense in an ideal world, in reality users of an
API may need to know whether reading coconut.price is potentially too expensive or
time consuming. As usual in matters of software engineering, Ward Cunningham�s
original Wiki hosts insightful arguments about the merits of the Uniform Access Principle.
In object-oriented programming languages, application or violations of the Uniform
Access Principle usually revolve around the syntax of reading public data attributes
versus invoking getter/setter methods.
620 | Chapter 19: Dynamic Attributes and Properties
16. Including the no-name default that the Java Tutorial calls �package-private.�
17. Alex Martelli, Python in a Nutshell, 2E (O�Reilly), p. 101.
Smalltalk and Ruby address this issue in a simple and elegant way: they don�t support
public data attributes at all. Every instance attribute in these languages is private, so
every access to them must be through methods. But their syntax makes this painless: in
Ruby, coconut.price invokes the price getter; in Smalltalk, it�s simply coconut price.
At the other end of the spectrum, the Java language allows the programmer to choose
among four access level modifiers.16 The general practice does not agree with the syntax
established by the Java designers, though. Everybody in Java-land agrees that attributes
should be private, and you must spell it out every time, because it�s not the default.
When all attributes are private, all access to them from outside the class must go through
accessors. Java IDEs include shortcuts for generating accessor methods automatically.
Unfortunately, the IDE is not so helpful when you must read the code six months later.
It�s up to you to wade through a sea of do-nothing accessors to find those that add value
by implementing some business logic.
Alex Martelli speaks for the majority of the Python community when he calls accessors
�goofy idioms� and then provides these examples that look very different but do the
same thing:17
someInstance.widgetCounter += 1
# rather than...
someInstance.setWidgetCounter(someInstance.getWidgetCounter() + 1)
Sometimes when designing APIs, I�ve wondered whether every method that does not
take an argument (besides self), returns a value (other than None), and is a pure function
(i.e., has no side effects) should be replaced by a read-only property. In this chapter, the
LineItem.subtotal method (as in Example 19-23) would be a good candidate to become
a read-only property. Of course, this excludes methods that are designed to change
the object, such as my_list.clear(). It would be a terrible idea to turn that into a
property, so that merely acessing my_list.clear would delete the contents of the list!
In the Pingo.io GPIO library (mentioned in �The __missing__ Method� on page 72),
much of the user-level API is based on properties. For example, to read the current value
of an analog pin, the user writes pin.value, and setting a digital pin mode is written as
pin.mode = OUT. Behind the scenes, reading an analog pin value or setting a digital pin
mode may involve a lot of code, depending on the specific board driver. We decided to
use properties in Pingo because we want the API to be comfortable to use even in interactive
environments like iPython Notebook, and we feel pin.mode = OUT is easier on
the eyes and on the fingers than pin.set_mode(OUT).
Although I find the Smalltalk and Ruby solution cleaner, I think the Python approach
makes more sense than the Java one. We are allowed to start simple, coding data mem?
Further Reading | 621
18. The reasons I am about to mention are given in the Dr. Dobbs Journal article titled �Java�s new Considered
Harmful�, by Jonathan Amsterdam and in �Consider static factory methods instead of constructors�, which is
Item 1 of the award-winning book Effective Java (Addison-Wesley) by Joshua Bloch.
bers as public attributes, because we know they can always be wrapped by properties
(or descriptors, which we�ll talk about in the next chapter).
__new__ Is Better Than new
Another example of the Uniform Access Principle (or a variation of it) is the fact that
function calls and object instantiation use the same syntax in Python: my_obj =
foo(), where foo may be a class or any other callable.
Other languages influenced by C++ syntax have a new operator that makes instantiation
look different than a call. Most of the time, the user of an API doesn�t care whether foo
is a function or a class. Until recently, I was under the impression that property was a
function. In normal usage, it makes no difference.
There are many good reasons for replacing constructors with factories.18 A popular
motive is limiting the number of instances, by returning previously built ones (as in the
Singleton pattern). A related use is caching expensive object construction. Also, sometimes
it�s convenient to return objects of different types depending on the arguments
given.
Coding a constructor is simpler; providing a factory adds flexibility at the expense of
more code. In languages that have a new operator, the designer of an API must decide
in advance whether to stick with a simple constructor or invest in factory. If the initial
choice is wrong, the correction may be costly�all because new is an operator.
Sometimes it may also be convenient to go the other way, and replace a simple function
with a class.
In Python, classes and functions are interchangeable in many situations. Not only because
there�s no new operator, but also because there is the __new__ special method,
which can turn a class into a factory producing objects of different kinds (as we saw in
�Flexible Object Creation with __new__� on page 592) or returning prebuilt instances
instead of creating a new one every time.
This function-class duality would be easier to leverage if PEP 8 � Style Guide for Python
Code did not recommend CamelCase for class names. On the other hand, dozens of
classes in the standard library have lowercase names (e.g., property, str, defauldict,
etc.). So maybe the use of lowercase class names is a feature, and not a bug. But however
we look at it, the inconsistent capitalization of classes in the Python standard library
poses a usability problem.
Although calling a function is not different than calling a class, it�s good to know which
is which because of another thing we can do with a class: subclassing. So I personally
use CamelCase in every class that I code, and I wish all classes in the Python standard
622 | Chapter 19: Dynamic Attributes and Properties
library used the same convention. I am looking at you, collections.OrderedDict and
collections.defaultdict.
Further Reading | 623

1. Raymond Hettinger, Descriptor HowTo Guide.
CHAPTER 20
Attribute Descriptors
Learning about descriptors not only provides access to a larger toolset, it creates a deeper
understanding of how Python works and an appreciation for the elegance of its design.1
� Raymond Hettinger
Python core developer and guru
Descriptors are a way of reusing the same access logic in multiple attributes. For example,
field types in ORMs such as the Django ORM and SQL Alchemy are descriptors,
managing the flow of data from the fields in a database record to Python object attributes
and vice versa.
A descriptor is a class that implements a protocol consisting of the __get__, __set__,
and __delete__ methods. The property class implements the full descriptor protocol.
As usual with protocols, partial implementations are OK. In fact, most descriptors we
see in real code implement only __get__ and __set__, and many implement only one
of these methods.
Descriptors are a distinguishing feature of Python, deployed not only at the application
level but also in the language infrastructure. Besides properties, other Python features
that leverage descriptors are methods and the classmethod and staticmethod decorators.
Understanding descriptors is key to Python mastery. This is what this chapter is
about.
Descriptor Example: Attribute Validation
As we saw in �Coding a Property Factory� on page 611, a property factory is a way to
avoid repetitive coding of getters and setters by applying functional programming patterns.
A property factory is a higher-order function that creates a parameterized set of
625
accessor functions and builds a custom property instance from them, with closures to
hold settings like the storage_name. The object-oriented way of solving the same problem
is a descriptor class.
We�ll continue the series of LineItem examples where we left it, in �Coding a Property
Factory� on page 611, by refactoring the quantity property factory into a Quantity
descriptor class.
LineItem Take #3: A Simple Descriptor
A class implementing a __get__, a __set__, or a __delete__ method is a descriptor.
You use a descriptor by declaring instances of it as class attributes of another class.
We�ll create a Quantity descriptor and the LineItem class will use two instances of
Quantity: one for managing the weight attribute, the other for price. A diagram helps,
so take a look at Figure 20-1.
Figure 20-1. UML class diagram for LineItem using a descriptor class named Quantity.
Underlined attributes in UML are class attributes. Note that weight and price are instances
of Quantity attached to the LineItem class, but LineItem instances also have
their own weight and price attributes where those values are stored.
Note that the word weight appears twice in Figure 20-1, because there are really two
distinct attributes named weight: one is a class attribute of LineItem, the other is an
instance attribute that will exist in each LineItem object. This also applies to price.
From now on, I will use the following definitions:
Descriptor class
A class implementing the descriptor protocol. That�s Quantity in Figure 20-1.
Managed class
The class where the descriptor instances are declared as class attributes�LineI
tem in Figure 20-1.
626 | Chapter 20: Attribute Descriptors
Descriptor instance
Each instance of a descriptor class, declared as a class attribute of the managed class.
In Figure 20-1, each descriptor instance is represented by a composition arrow with
an underlined name (the underline means class attribute in UML). The black diamonds
touch the LineItem class, which contains the descriptor instances.
Managed instance
One instance of the managed class. In this example, LineItem instances will be the
managed instances (they are not shown in the class diagram).
Storage attribute
An attribute of the managed instance that will hold the value of a managed attribute
for that particular instance. In Figure 20-1, the LineItem instance attributes weight
and price will be the storage attributes. They are distinct from the descriptor instances,
which are always class attributes.
Managed attribute
A public attribute in the managed class that will be handled by a descriptor instance,
with values stored in storage attributes. In other words, a descriptor instance and
a storage attribute provide the infrastructure for a managed attribute.
It�s important to realize that Quantity instances are class attributes of LineItem. This
crucial point is highlighted by the mills and gizmos in Figure 20-2.
Figure 20-2. UML class diagram annotated with MGN (Mills & Gizmos Notation):
classes are mills that produce gizmos�the instances. The Quantity mill produces two
red gizmos, which are attached to the LineItem mill: weight and price. The LineItem
mill produces blue gizmos that have their own weight and price attributes where those
values are stored.
Descriptor Example: Attribute Validation | 627
2. Classes and instances are drawn as rectangles in UML class diagrams. There are visual differences, but instances
are rarely shown in class diagrams, so developers may not recognize them as such.
Introducing Mills & Gizmos Notation
After explaining descriptors many times, I realized UML is not very good at showing
relationships involving classes and instances, like the relationship between a managed
class and the descriptor instances.2 So I invented my own �language,� the Mills & Gizmos
Notation (MGN), which I use to annotate UML diagrams.
MGN is designed to make very clear the distinction between classes and instances. See
Figure 20-3. In MGN, a class is drawn as a �mill,� a complicated machine that produces
gizmos. Classes/mills are always machines with levers and dials. The gizmos are the
instances, and they look much simpler. A gizmo is the same color as the mill that made
it.
Figure 20-3. MGN sketch showing the LineItem class making three instances, and
Quantity making two. One instance of Quantity is retrieving a value stored in a
LineItem instance.
For this example, I drew LineItem instances as rows in a tabular invoice, with three cells
representing the three attributes (description, weight, and price). Because Quanti
ty instances are descriptors, they have a magnifying glass to __get__ values and a claw
to __set__ values. When we get to metaclasses, you�ll thank me for these doodles.
Enough doodling for now. Here is the code: Example 20-1 shows the Quantity descriptor
class and a new LineItem class using two instances of Quantity.
Example 20-1. bulkfood_v3.py: quantity descriptors manage attributes in LineItem
class Quantity:
def __init__(self, storage_name):
self.storage_name = storage_name
628 | Chapter 20: Attribute Descriptors
3. White truffles cost thousands of dollars per pound. Disallowing the sale of truffles for $0.01 is left as an
exercise for the enterprising reader. I know a person who actually bought an $1,800 encyclopedia of statistics
for $18 because of an error in an online store (not Amazon.com).
def __set__(self, instance, value):
if value > 0:
instance.__dict__[self.storage_name] = value
else:
raise ValueError('value must be > 0')
class LineItem:
weight = Quantity('weight')
price = Quantity('price')
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
Descriptor is a protocol-based feature; no subclassing is needed to implement
one.
Each Quantity instance will have a storage_name attribute: that�s the name of
the attribute that will hold the value in the managed instances.
__set__ is called when there is an attempt to assign to the managed attribute.
Here, self is the descriptor instance (i.e., LineItem.weight or LineI
tem.price), instance is the managed instance (a LineItem instance), and val
ue is the value being assigned.
Here, we must handle the managed instance __dict__ directly; trying to use the
setattr built-in would trigger the __set__ method again, leading to infinite
recursion.
The first descriptor instance is bound to the weight attribute.
The second descriptor instance is bound to the price attribute.
The rest of the class body is as simple and clean as the original code in
bulkfood_v1.py (Example 19-15).
In Example 20-1, each managed attribute has the same name as its storage attribute,
and there is no special getter logic, so Quantity doesn�t need a __get__ method.
The code in Example 20-1 works as intended, preventing the sale of truffles for $0:3
Descriptor Example: Attribute Validation | 629
>>> truffle = LineItem('White truffle', 100, 0)
Traceback (most recent call last):
...
ValueError: value must be > 0
When coding a __set__ method, you must keep in mind what the
self and instance arguments mean: self is the descriptor instance,
and instance is the managed instance. Descriptors managing
instance attributes should store values in the managed instances.
That�s why Python provides the instance argument to the
descriptor methods.
It may be tempting, but wrong, to store the value of each managed attribute in the
descriptor instance itself. In other words, in the __set__ method, instead of coding:
instance.__dict__[self.storage_name] = value
the tempting but bad alternative would be:
self.__dict__[self.storage_name] = value
To understand why this would be wrong, think about the meaning of the first two
arguments to __set__: self and instance. Here, self is the descriptor instance, which
is actually a class attribute of the managed class. You may have thousands of LineI
tem instances in memory at one time, but you�ll only have two instances of the descriptors:
LineItem.weight and LineItem.price. So anything you store in the descriptor
instances themselves is actually part of a LineItem class attribute, and therefore is shared
among all LineItem instances.
A drawback of Example 20-1 is the need to repeat the names of the attributes when the
descriptors are instantiated in the managed class body. It would be nice if the LineI
tem class could be declared like this:
class LineItem:
weight = Quantity()
price = Quantity()
# remaining methods as before
The problem is that�as we saw in Chapter 8�the righthand side of an assignment is
executed before the variable exists. The expression Quantity() is evaluated to create a
descriptor instance, and at this time there is no way the code in the Quantity class can
guess the name of the variable to which the descriptor will be bound (e.g., weight or
price).
As it stands, Example 20-1 requires naming each Quantity explicitly, which is not only
inconvenient but dangerous: if a programmer copy and pasting code forgets to edit both
630 | Chapter 20: Attribute Descriptors
names and writes something like price = Quantity('weight'), the program will misbehave
badly, clobbering the value of weight whenever the price is set.
A not-so-elegant but workable solution to the repeated name problem is presented next.
Better solutions require either a class decorator or a metaclass, so I�ll leave them for
Chapter 21.
LineItem Take #4: Automatic Storage Attribute Names
To avoid retyping the attribute name in the descriptor declarations, we�ll generate a
unique string for the storage_name of each Quantity instance. Figure 20-4 shows the
updated UML diagram for the Quantity and LineItem classes.
Figure 20-4. UML class diagram for Example 20-2. Now Quantity has both get and set
methods, and LineItem instances have storage attributes with generated names:
_Quantity#0 and _Quantity#1.
To generate the storage_name, we start with a '_Quantity#' prefix and concatenate an
integer: the current value of a Quantity.__counter class attribute that we�ll increment
every time a new Quantity descriptor instance is attached to a class. Using the hash
character in the prefix guarantees the storage_name will not clash with attributes created
by the user using dot notation, because nutmeg._Quantity#0 is not valid Python
syntax. But we can always get and set attributes with such �invalid� identifiers using the
getattr and setattr built-in functions, or by poking the instance __dict__.
Example 20-2 shows the new implementation.
Example 20-2. bulkfood_v4.py: each Quantity descriptor gets a unique storage_name
class Quantity:
__counter = 0
def __init__(self):
cls = self.__class__
prefix = cls.__name__
index = cls.__counter
Descriptor Example: Attribute Validation | 631
self.storage_name = '_{}#{}'.format(prefix, index)
cls.__counter += 1
def __get__(self, instance, owner):
return getattr(instance, self.storage_name)
def __set__(self, instance, value):
if value > 0:
setattr(instance, self.storage_name, value)
else:
raise ValueError('value must be > 0')
class LineItem:
weight = Quantity()
price = Quantity()
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
__counter is a class attribute of Quantity, counting the number of Quantity
instances.
cls is a reference to the Quantity class.
The storage_name for each descriptor instance is unique because it�s built from
the descriptor class name and the current __counter value (e.g., _Quantity#0).
Increment __counter.
We need to implement __get__ because the name of the managed attribute is
not the same as the storage_name. The owner argument will be explained shortly.
Use the getattr built-in function to retrieve the value from the instance.
Use the setattr built-in to store the value in the instance.
Now we don�t need to pass the managed attribute name to the Quantity
constructor. That was the goal for this version.
Here we can use the higher-level getattr and setattr built-ins to store the value�
instead of resorting to instance.__dict__�because the managed attribute and the
storage attribute have different names, so calling getattr on the storage attribute will
not trigger the descriptor, avoiding the infinite recursion discussed in Example 20-1.
632 | Chapter 20: Attribute Descriptors
If you test bulkfood_v4.py, you can see that the weight and price descriptors work as
expected, and the storage attributes can also be read directly, which is useful for debugging:
>>> from bulkfood_v4 import LineItem
>>> coconuts = LineItem('Brazilian coconut', 20, 17.95)
>>> coconuts.weight, coconuts.price
(20, 17.95)
>>> getattr(raisins, '_Quantity#0'), getattr(raisins, '_Quantity#1')
(20, 17.95)
If we wanted to follow the convention Python uses to do name
mangling (e.g., _LineItem__quantity0) we�d have to know the
name of the managed class (i.e., LineItem), but the body of a class
definition runs before the class itself is built by the interpreter, so
we don�t have that information when each descriptor instance is
created. However, in this case, there is no need to include the
managed class name to avoid accidental overwriting in subclasses:
the descriptor class __counter will be incremented every time
a new descriptor is instantiated, guaranteeing that each storage
name will be unique across all managed classes.
Note that __get__ receives three arguments: self, instance, and owner. The owner
argument is a reference to the managed class (e.g., LineItem), and it�s handy when the
descriptor is used to get attributes from the class. If a managed attribute, such as
weight, is retrieved via the class like LineItem.weight, the descriptor __get__ method
receives None as the value for the instance argument. This explains the Attribute error
in the next console session:
>>> from bulkfood_v4 import LineItem
>>> LineItem.weight
Traceback (most recent call last):
...
File ".../descriptors/bulkfood_v4.py", line 54, in __get__
return getattr(instance, self.storage_name)
AttributeError: 'NoneType' object has no attribute '_Quantity#0'
Raising AttributeError is an option when implementing __get__, but if you choose
to do so, the message should be fixed to remove the confusing mention of NoneType and
_Quantity#0, which are implementation details. A better message would be "'LineI
tem' class has no such attribute". Ideally, the name of the missing attribute should
be spelled out, but the descriptor doesn�t know the name of the managed attribute in
this example, so we can�t do better at this point.
On the other hand, to support introspection and other metaprogramming tricks by the
user, it�s a good practice to make __get__ return the descriptor instance when the man?
Descriptor Example: Attribute Validation | 633
aged attribute is accessed through the class. Example 20-3 is a minor variation of
Example 20-2, adding a bit of logic to Quantity.__get__.
Example 20-3. bulkfood_v4b.py (partial listing): when invoked through the managed
class, get returns a reference to the descriptor itself
class Quantity:
__counter = 0
def __init__(self):
cls = self.__class__
prefix = cls.__name__
index = cls.__counter
self.storage_name = '_{}#{}'.format(prefix, index)
cls.__counter += 1
def __get__(self, instance, owner):
if instance is None:
return self
else:
return getattr(instance, self.storage_name)
def __set__(self, instance, value):
if value > 0:
setattr(instance, self.storage_name, value)
else:
raise ValueError('value must be > 0')
If the call was not through an instance, return the descriptor itself.
Otherwise, return the managed attribute value, as usual.
Trying out Example 20-3, this is what we see:
>>> from bulkfood_v4b import LineItem
>>> LineItem.price
<bulkfood_v4b.Quantity object at 0x100721be0>
>>> br_nuts = LineItem('Brazil nuts', 10, 34.95)
>>> br_nuts.price
34.95
Looking at Example 20-2, you may think that�s a lot of code just for managing a couple
of attributes, but it�s important to realize that the descriptor logic is now abstracted into
a separate code unit: the Quantity class. Usually we do not define a descriptor in the
same module where it�s used, but in a separate utility module designed to be used across
the application�even in many applications, if you are developing a framework.
With this in mind, Example 20-4 better represents the typical usage of a descriptor.
634 | Chapter 20: Attribute Descriptors
Example 20-4. bulkfood_v4c.py: LineItem definition uncluttered; the Quantity descriptor
class now resides in the imported model_v4c module
import model_v4c as model
class LineItem:
weight = model.Quantity()
price = model.Quantity()
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
Import the model_v4c module, giving it a friendlier name.
Put model.Quantity to use.
Django users will notice that Example 20-4 looks a lot like a model definition. It�s no
coincidence: Django model fields are descriptors.
As implemented so far, the Quantity descriptor works pretty well.
Its only real drawback is the use of generated storage names like
_Quantity#0, making debugging hard for the users. But automatically
assigning storage names that resemble the managed
attribute names requires a class decorator or a metaclass, topics
we�ll defer to Chapter 21.
Because descriptors are defined in classes, we can leverage inheritance to reuse some of
the code we have for new descriptors. That�s what we�ll do in the following section.
Property Factory Versus Descriptor Class
It�s not hard to reimplement the enhanced descriptor class of Example 20-2 by adding
a few lines to the property factory shown in Example 19-24. The __counter variable
presents a difficulty, but we can make it persist across invocations of the factory by
defining it as an attribute of factory function object itself, as shown in Example 20-5.
Example 20-5. bulkfood_v4prop.py: same functionality as Example 20-2 with a
property factory instead of a descriptor class
def quantity():
try:
quantity.counter += 1
Descriptor Example: Attribute Validation | 635
except AttributeError:
quantity.counter = 0
storage_name = '_{}:{}'.format('quantity', quantity.counter)
def qty_getter(instance):
return getattr(instance, storage_name)
def qty_setter(instance, value):
if value > 0:
setattr(instance, storage_name, value)
else:
raise ValueError('value must be > 0')
return property(qty_getter, qty_setter)
No storage_name argument.
We can�t rely on class attributes to share the counter across invocations, so we
define it as an attribute of the quantity function itself.
If quantity.counter is undefined, set it to 0.
We also don�t have instance attributes, so we create storage_name as a local
variable and rely on closures to keep them alive for later use by qty_getter and
qty_setter.
The remaining code is identical to Example 19-24, except here we can use the
getattr and setattr built-ins instead of fiddling with instance.__dict__.
So, which do you prefer? Example 20-2 or Example 20-5?
I prefer the descriptor class approach mainly for two reasons:
� A descriptor class can be extended by subclassing; reusing code from a factory
function without copying and pasting is much harder.
� It�s more straightforward to hold state in class and instance attributes than in function
attributes and closures as we had to do in Example 20-5.
On the other hand, when I explain Example 20-5, I don�t feel the urge to draw mills and
gizmos. The property factory code does not depend on strange object relationships
evidenced by descriptor methods having arguments named self and instance.
To summarize, the property factory pattern is simpler in some regards, but the descriptor
class approach is more extensible. It�s also more widely used.
636 | Chapter 20: Attribute Descriptors
4. Gamma et al., Design Patterns: Elements of Reusable Object-Oriented Software, p. 326.
LineItem Take #5: A New Descriptor Type
he imaginary organic food store hits a snag: somehow a line item instance was created
with a blank description and the order could not be fulfilled. To prevent that, we�ll create
a new descriptor, NonBlank. As we design NonBlank, we realize it will be very much like
the Quantity descriptor, except for the validation logic.
Reflecting on the functionality of Quantity, we note it does two different things: it takes
care of the storage attributes in the managed instances, and it validates the value used
to set those attributes. This prompts a refactoring, producing two base classes:
AutoStorage
Descriptor class that manages storage attributes automatically.
Validated
AutoStorage abstract subclass that overrides the __set__ method, calling a vali
date method that must be implemented by subclasses.
We�ll then rewrite Quantity and implement NonBlank by inheriting from Validated
and just coding the validate methods. Figure 20-5 depicts the setup.
Figure 20-5. A hierarchy of descriptor classes. The AutoStorage base class manages the
automatic storage of the attribute, Validated handles validation by delegating to an abstract
validate method, Quantity and NonBlank are concrete subclasses of Validated.
The relationship between Validated, Quantity, and NonBlank is an application of the
Template Method design pattern. In particular, the Validated.__set__ is a clear example
of what the Gang of Four describe as a template method:
A template method defines an algorithm in terms of abstract operations that subclasses
override to provide concrete behavior.4
Descriptor Example: Attribute Validation | 637
In this case, the abstract operation is validation. Example 20-6 lists the implementation
of the classes in Figure 20-5.
Example 20-6. model_v5.py: the refactored descriptor classes
import abc
class AutoStorage:
__counter = 0
def __init__(self):
cls = self.__class__
prefix = cls.__name__
index = cls.__counter
self.storage_name = '_{}#{}'.format(prefix, index)
cls.__counter += 1
def __get__(self, instance, owner):
if instance is None:
return self
else:
return getattr(instance, self.storage_name)
def __set__(self, instance, value):
setattr(instance, self.storage_name, value)
class Validated(abc.ABC, AutoStorage):
def __set__(self, instance, value):
value = self.validate(instance, value)
super().__set__(instance, value)
@abc.abstractmethod
def validate(self, instance, value):
"""return validated value or raise ValueError"""
class Quantity(Validated):
"""a number greater than zero"""
def validate(self, instance, value):
if value <= 0:
raise ValueError('value must be > 0')
return value
class NonBlank(Validated):
"""a string with at least one non-space character"""
def validate(self, instance, value):
638 | Chapter 20: Attribute Descriptors
value = value.strip()
if len(value) == 0:
raise ValueError('value cannot be empty or blank')
return value
AutoStorage provides most of the functionality of the former Quantity
descriptor�
�except validation.
Validated is abstract but also inherits from AutoStorage.
__set__ delegates validation to a validate method�
�then uses the returned value to invoke __set__ on a superclass, which
performs the actual storage.
In this class, validate is an abstract method.
Quantity and NonBlank inherit from Validated.
Requiring the concrete validate methods to return the validated value gives
them an opportunity to clean up, convert, or normalize the data received. In this
case, the value is returned stripped of leading and trailing blanks.
Users of model_v5.py don�t need to know all these details. What matters is that they get
to use Quantity and NonBlank to automate the validation of instance attributes. See the
latest LineItem class in Example 20-7.
Example 20-7. bulkfood_v5.py: LineItem using Quantity and NonBlank descriptors
import model_v5 as model
class LineItem:
description = model.NonBlank()
weight = model.Quantity()
price = model.Quantity()
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
Import the model_v5 module, giving it a friendlier name.
Put model.NonBlank to use. The rest of the code is unchanged.
Descriptor Example: Attribute Validation | 639
The LineItem examples we�ve seen in this chapter demonstrate a typical use of descriptors
to manage data attributes. Such a descriptor is also called an overriding descriptor
because its __set__ method overrides (i.e., interrupts and overrules) the setting of an
attribute by the same name in the managed instance. However, there are also nonoverriding
descriptors. We�ll explore this distinction in detail in the next section.
Overriding Versus Nonoverriding Descriptors
Recall that there is an important asymmetry in the way Python handles attributes.
Reading an attribute through an instance normally returns the attribute defined in the
instance, but if there is no such attribute in the instance, a class attribute will be retrieved.
On the other hand, assigning to an attribute in an instance normally creates the attribute
in the instance, without affecting the class at all.
This asymmetry also affects descriptors, in effect creating two broad categories of descriptors
depending on whether the __set__ method is defined. Observing the different
behaviors requires a few classes, so we are going to use the code in Example 20-8 as our
testbed for the following sections.
Every __get__ and __set__ method in Example 20-8 calls
print_args so their invocations are displayed in a readable way.
Understanding print_args and the auxiliary functions cls_name
and display is not important, so don�t get distracted by them.
Example 20-8. descriptorkinds.py: simple classes for studying descriptor overriding behaviors
### auxiliary functions for display only ###
def cls_name(obj_or_cls):
cls = type(obj_or_cls)
if cls is type:
cls = obj_or_cls
return cls.__name__.split('.')[-1]
def display(obj):
cls = type(obj)
if cls is type:
return '<class {}>'.format(obj.__name__)
elif cls in [type(None), int]:
return repr(obj)
else:
return '<{} object>'.format(cls_name(obj))
def print_args(name, *args):
pseudo_args = ', '.join(display(x) for x in args)
640 | Chapter 20: Attribute Descriptors
print('-> {}.__{}__({})'.format(cls_name(args[0]), name, pseudo_args))
### essential classes for this example ###
class Overriding:
"""a.k.a. data descriptor or enforced descriptor"""
def __get__(self, instance, owner):
print_args('get', self, instance, owner)
def __set__(self, instance, value):
print_args('set', self, instance, value)
class OverridingNoGet:
"""an overriding descriptor without ``__get__``"""
def __set__(self, instance, value):
print_args('set', self, instance, value)
class NonOverriding:
"""a.k.a. non-data or shadowable descriptor"""
def __get__(self, instance, owner):
print_args('get', self, instance, owner)
class Managed:
over = Overriding()
over_no_get = OverridingNoGet()
non_over = NonOverriding()
def spam(self):
print('-> Managed.spam({})'.format(display(self)))
A typical overriding descriptor class with __get__ and __set__.
The print_args function is called by every descriptor method in this example.
An overriding descriptor without a __get__ method.
No __set__ method here, so this is a nonoverriding descriptor.
The managed class, using one instance of each of the descriptor classes.
The spam method is here for comparison, because methods are also descriptors.
In the following sections, we will examine the behavior of attribute reads and writes on
the Managed class and one instance of it, going through each of the different descriptors
defined.
Overriding Versus Nonoverriding Descriptors | 641
Overriding Descriptor
A descriptor that implements the __set__ method is called an overriding descriptor,
because although it is a class attribute, a descriptor implementing __set__ will override
attempts to assign to instance attributes. This is how Example 20-2 was implemented.
Properties are also overriding descriptors: if you don�t provide a setter function, the
default __set__ from the property class will raise AttributeError to signal that the
attribute is read-only. Given the code in Example 20-8, experiments with an overriding
descriptor can be seen in Example 20-9.
Example 20-9. Behavior of an overriding descriptor: obj.over is an instance of Overriding
(Example 20-8)
>>> obj = Managed()
>>> obj.over
-> Overriding.__get__(<Overriding object>, <Managed object>,
<class Managed>)
>>> Managed.over
-> Overriding.__get__(<Overriding object>, None, <class Managed>)
>>> obj.over = 7
-> Overriding.__set__(<Overriding object>, <Managed object>, 7)
>>> obj.over
-> Overriding.__get__(<Overriding object>, <Managed object>,
<class Managed>)
>>> obj.__dict__['over'] = 8
>>> vars(obj)
{'over': 8}
>>> obj.over
-> Overriding.__get__(<Overriding object>, <Managed object>,
<class Managed>)
Create Managed object for testing.
obj.over triggers the descriptor __get__ method, passing the managed instance
obj as the second argument.
Managed.over triggers the descriptor __get__ method, passing None as the
second argument (instance).
Assigning to obj.over triggers the descriptor __set__ method, passing the value
7 as the last argument.
Reading obj.over still invokes the descriptor __get__ method.
Bypassing the descriptor, setting a value directly to the obj.__dict__.
Verify that the value is in the obj.__dict__, under the over key.
However, even with an instance attribute named over, the Managed.over
descriptor still overrides attempts to read obj.over.
642 | Chapter 20: Attribute Descriptors
Overriding Descriptor Without __get__
Usually, overriding descriptors implement both __set__ and __get__, but it�s also possible
to implement only __set__, as we saw in Example 20-1. In this case, only writing
is handled by the descriptor. Reading the descriptor through an instance will return the
descriptor object itself because there is no __get__ to handle that access. If a namesake
instance attribute is created with a new value via direct access to the instance
__dict__, the __set__ method will still override further attempts to set that attribute,
but reading that attribute will simply return the new value from the instance, instead of
returning the descriptor object. In other words, the instance attribute will shadow the
descriptor, but only when reading. See Example 20-10.
Example 20-10. Overriding descriptor without get: obj.over_no_get is an instance of
OverridingNoGet (Example 20-8)
>>> obj.over_no_get
<__main__.OverridingNoGet object at 0x665bcc>
>>> Managed.over_no_get
<__main__.OverridingNoGet object at 0x665bcc>
>>> obj.over_no_get = 7
-> OverridingNoGet.__set__(<OverridingNoGet object>, <Managed object>, 7)
>>> obj.over_no_get
<__main__.OverridingNoGet object at 0x665bcc>
>>> obj.__dict__['over_no_get'] = 9
>>> obj.over_no_get
9
>>> obj.over_no_get = 7
-> OverridingNoGet.__set__(<OverridingNoGet object>, <Managed object>, 7)
>>> obj.over_no_get
9
This overriding descriptor doesn�t have a __get__ method, so reading
obj.over_no_get retrieves the descriptor instance from the class.
The same thing happens if we retrieve the descriptor instance directly from the
managed class.
Trying to set a value to obj.over_no_get invokes the __set__ descriptor
method.
Because our __set__ doesn�t make changes, reading obj.over_no_get again
retrieves the descriptor instance from the managed class.
Going through the instance __dict__ to set an instance attribute named
over_no_get.
Now that over_no_get instance attribute shadows the descriptor, but only for
reading.
Overriding Versus Nonoverriding Descriptors | 643
Trying to assign a value to obj.over_no_get still goes through the descriptor
set.
But for reading, that descriptor is shadowed as long as there is a namesake
instance attribute.
Nonoverriding Descriptor
If a descriptor does not implement __set__, then it�s a nonoverriding descriptor. Setting
an instance attribute with the same name will shadow the descriptor, rendering it ineffective
for handling that attribute in that specific instance. Methods are implemented
as nonoverriding descriptors. Example 20-11 shows the operation of a nonoverriding
descriptor.
Example 20-11. Behavior of a nonoverriding descriptor: obj.non_over is an instance of
NonOverriding (Example 20-8)
>>> obj = Managed()
>>> obj.non_over
-> NonOverriding.__get__(<NonOverriding object>, <Managed object>,
<class Managed>)
>>> obj.non_over = 7
>>> obj.non_over
7
>>> Managed.non_over
-> NonOverriding.__get__(<NonOverriding object>, None, <class Managed>)
>>> del obj.non_over
>>> obj.non_over
-> NonOverriding.__get__(<NonOverriding object>, <Managed object>,
<class Managed>)
obj.non_over triggers the descriptor __get__ method, passing obj as the second
argument.
Managed.non_over is a nonoverriding descriptor, so there is no __set__ to
interfere with this assignment.
The obj now has an instance attribute named non_over, which shadows the
namesake descriptor attribute in the Managed class.
The Managed.non_over descriptor is still there, and catches this access via the
class.
If the non_over instance attribute is deleted�
Then reading obj.non_over hits the __get__ method of the descriptor in the
class, but note that the second argument is the managed instance.
644 | Chapter 20: Attribute Descriptors
Python contributors and authors use different terms when discussing
these concepts. Overriding descriptors are also called data
descriptors or enforced descriptors. Nonoverriding descriptors are
also known as nondata descriptors or shadowable descriptors.
In the previous examples, we saw several assignments to an instance attribute with the
same name as a descriptor, and different results according to the presence of a __set__
method in the descriptor.
The setting of attributes in the class cannot be controlled by descriptors attached to the
same class. In particular, this means that the descriptor attributes themselves can be
clobbered by assigning to the class, as the next section explains.
Overwriting a Descriptor in the Class
Regardless of whether a descriptor is overriding or not, it can be overwritten by assignment
to the class. This is a monkey-patching technique, but in Example 20-12 the
descriptors are replaced by integers, which would effectively break any class that depended
on the descriptors for proper operation.
Example 20-12. Any descriptor can be overwritten on the class itself
>>> obj = Managed()
>>> Managed.over = 1
>>> Managed.over_no_get = 2
>>> Managed.non_over = 3
>>> obj.over, obj.over_no_get, obj.non_over
(1, 2, 3)
Create a new instance for later testing.
Overwrite the descriptor attributes in the class.
The descriptors are really gone.
Example 20-12 reveals another asymmetry regarding reading and writing attributes:
although the reading of a class attribute can be controlled by a descriptor with __get__
attached to the managed class, the writing of a class attribute cannot be handled by a
descriptor with __set__ attached to the same class.
In order to control the setting of attributes in a class, you have to
attach descriptors to the class of the class�in other words, the
metaclass. By default, the metaclass of user-defined classes is type,
and you cannot add attributes to type. But in Chapter 21, we�ll
create our own metaclasses.
Overriding Versus Nonoverriding Descriptors | 645
Let�s now focus on how descriptors are used to implement methods in Python.
Methods Are Descriptors
A function within a class becomes a bound method because all user-defined functions
have a __get__ method, therefore they operate as descriptors when attached to a class.
Example 20-13 demonstrates reading the spam method from the Managed class introduced
in Example 20-8.
Example 20-13. A method is a nonoverriding descriptor
>>> obj = Managed()
>>> obj.spam
<bound method Managed.spam of <descriptorkinds.Managed object at 0x74c80c>>
>>> Managed.spam
<function Managed.spam at 0x734734>
>>> obj.spam = 7
>>> obj.spam
7
Reading from obj.spam retrieves a bound method object.
But reading from Managed.spam retrieves a function.
Assigning a value to obj.spam shadows the class attribute, rendering the spam
method inaccessible from the obj instance.
Because functions do not implement __set__, they are nonoverriding descriptors, as
the last line of Example 20-13 shows.
The other key takeaway from Example 20-13 is that obj.spam and Managed.spam retrieve
different objects. As usual with descriptors, the __get__ of a function returns a
reference to itself when the access happens through the managed class. But when the
access goes through an instance, the __get__ of the function returns a bound method
object: a callable that wraps the function and binds the managed instance (e.g., obj) to
the first argument of the function (i.e., self), like the functools.partial function does
(as seen in �Freezing Arguments with functools.partial� on page 159).
For a deeper understanding of this mechanism, take a look at Example 20-14.
Example 20-14. method_is_descriptor.py: a Text class, derived from UserString
import collections
class Text(collections.UserString):
def __repr__(self):
return 'Text({!r})'.format(self.data)
646 | Chapter 20: Attribute Descriptors
def reverse(self):
return self[::-1]
Now let�s investigate the Text.reverse method. See Example 20-15.
Example 20-15. Experiments with a method
>>> word = Text('forward')
>>> word
Text('forward')
>>> word.reverse()
Text('drawrof')
>>> Text.reverse(Text('backward'))
Text('drawkcab')
>>> type(Text.reverse), type(word.reverse)
(<class 'function'>, <class 'method'>)
>>> list(map(Text.reverse, ['repaid', (10, 20, 30), Text('stressed')]))
['diaper', (30, 20, 10), Text('desserts')]
>>> Text.reverse.__get__(word)
<bound method Text.reverse of Text('forward')>
>>> Text.reverse.__get__(None, Text)
<function Text.reverse at 0x101244e18>
>>> word.reverse
<bound method Text.reverse of Text('forward')>
>>> word.reverse.__self__
Text('forward')
>>> word.reverse.__func__ is Text.reverse
True
The repr of a Text instance looks like a Text constructor call that would make
an equal instance.
The reverse method returns the text spelled backward.
A method called on the class works as a function.
Note the different types: a function and a method.
Text.reverse operates as a function, even working with objects that are not
instances of Text.
Any function is a nonoverriding descriptor. Calling its __get__ with an instance
retrieves a method bound to that instance.
Calling the function�s __get__ with None as the instance argument retrieves the
function itself.
The expression word.reverse actually invokes Text.reverse.__get__(word),
returning the bound method.
The bound method object has a __self__ attribute holding a reference to the
instance on which the method was called.
Methods Are Descriptors | 647
5. Python is not consistent in such messages. Trying to change the c.real attribute of a complex number gets
AttributeError: read-only attribute, but an attempt to change c.conjugate (a method of com
plex), results in AttributeError: 'complex' object attribute 'conjugate' is read-only.
The __func__ attribute of the bound method is a reference to the original
function attached to the managed class.
The bound method object also has a __call__ method, which handles the actual invocation.
This method calls the original function referenced in __func__, passing the
__self__ attribute of the method as the first argument. That�s how the implicit binding
of the conventional self argument works.
The way functions are turned into bound methods is a prime example of how descriptors
are used as infrastructure in the language.
After this deep dive into how descriptors and methods work, let�s go through some
practical advice about their use.
Descriptor Usage Tips
The following list addresses some practical consequences of the descriptor characteristics
just described:
Use property to Keep It Simple
The property built-in actually creates overriding descriptors implementing both
__set__ and __get__, even if you do not define a setter method. The default
__set__ of a property raises AttributeError: can't set attribute, so a property
is the easiest way to create a read-only attribute, avoiding the issue described
next.
Read-only descriptors require __set__
If you use a descriptor class to implement a read-only attribute, you must remember
to code both __get__ and __set__, otherwise setting a namesake attribute on an
instance will shadow the descriptor. The __set__ method of a read-only attribute
should just raise AttributeError with a suitable message.5
Validation descriptors can work with __set__ only
In a descriptor designed only for validation, the __set__ method should check the
value argument it gets, and if valid, set it directly in the instance __dict__ using
the descriptor instance name as key. That way, reading the attribute with the same
name from the instance will be as fast as possible, because it will not require a
__get__. See the code for Example 20-1.
648 | Chapter 20: Attribute Descriptors
Caching can be done efficiently with __get__ only
If you code just the __get__ method, you have a nonoverriding descriptor. These
are useful to make some expensive computation and then cache the result by setting
an attribute by the same name on the instance. The namesake instance attribute
will shadow the descriptor, so subsequent access to that attribute will fetch it directly
from the instance __dict__ and not trigger the descriptor __get__ anymore.
Nonspecial methods can be shadowed by instance attributes
Because functions and methods only implement __get__, they do not handle attempts
at setting instance attributes with the same name, so a simple assignment
like my_obj.the_method = 7 means that further access to the_method through that
instance will retrieve the number 7�without affecting the class or other instances.
However, this issue does not interfere with special methods. The interpreter only
looks for special methods in the class itself, in other words, repr(x) is executed as
x.__class__.__repr__(x), so a __repr__ attribute defined in x has no effect on
repr(x). For the same reason, the existence of an attribute named __getattr__ in
an instance will not subvert the usual attribute access algorithm.
The fact that nonspecial methods can be overridden so easily in instances may sound
fragile and error-prone, but I personally have never been bitten by this in more than 15
years of Python coding. On the other hand, if you are doing a lot of dynamic attribute
creation, where the attribute names come from data you don�t control (as we did in the
earlier parts of this chapter), then you should be aware of this and perhaps implement
some filtering or escaping of the dynamic attribute names to preserve your sanity.
The FrozenJSON class in Example 19-6 is safe from instance attribute
shadowing methods because its only methods are special
methods and the build class method. Class methods are safe as
long as they are always accessed through the class, as I did with
FrozenJSON.build in Example 19-6�later replaced by __new__
in Example 19-7. The Record class (Examples 19-9 and 19-11)
and subclasses are also safe: they use only special methods, class
methods, static methods, and properties. Properties are data descriptors,
so cannot be overridden by instance attributes.
To close this chapter, we�ll cover two features we saw with properties that we have not
addressed in the context of descriptors: documentation and handling attempts to delete
a managed attribute.
Descriptor Usage Tips | 649
6. Customizing the help text for each descriptor instance is surprisingly hard. One solution requires dynamically
building a wrapper class for each descriptor instance.
Descriptor docstring and Overriding Deletion
The docstring of a descriptor class is used to document every instance of the descriptor
in the managed class. See Figure 20-6 for the help displays for the LineItem class with
the Quantity and NonBlank descriptors from Examples 20-6 and 20-7.
Figure 20-6. Screenshots of the Python console when issuing the commands help(LineItem.
weight) and help(LineItem)
That is somewhat unsatisfactory. In the case of LineItem, it would be good to add, for
example, the information that weight must be in kilograms. That would be trivial with
properties, because each property handles a specific managed attribute. But with descriptors,
the same Quantity descriptor class is used for weight and price.6
The second detail we discussed with properties but have not addressed with descriptors
is handling attempts to delete a managed attribute. That can be done by implementing
a __delete__ method alongside or instead of the usual __get__ and/or __set__ in the
650 | Chapter 20: Attribute Descriptors
descriptor class. Coding a silly descriptor class with __delete__ is left as an exercise to
the leisurely reader.
Chapter Summary
The first example of this chapter was a continuation of the LineItem examples from
Chapter 19. In Example 20-1, we replaced properties with descriptors. We saw that a
descriptor is a class that provides instances that are deployed as attributes in the managed
class. Discussing this mechanism required special terminology, introducing terms
such as managed instance and storage attribute.
In �LineItem Take #4: Automatic Storage Attribute Names� on page 631, we removed
the requirement that Quantity descriptors were declared with an explicit stor
age_name, which was redundant and error-prone, because that name should always
match the attribute name on the left of the assignment in the descriptor instantiation.
The solution was to generate unique storage_names by combining the descriptor class
name with a counter at the class level (e.g., '_Quantity#1').
Next, we compared the code size, strengths, and weaknesses of a descriptor class with
a property factory built on functional programming idioms. The latter works perfectly
well and is simpler in some ways, but the former is more flexible and is the standard
solution. A key advantage of the descriptor class was exploited in �LineItem Take #5: A
New Descriptor Type� on page 637: subclassing to share code while building specialized
descriptors with some common functionality.
We then looked at the different behavior of descriptors providing or omitting the
__set__ method, making the crucial distinction between overriding and nonoverriding
descriptors. Through detailed testing we uncovered when descriptors are in
control and when they are shadowed, bypassed, or overwritten.
Following that, we studied a particular category of nonoverriding descriptors: methods.
Console testing revealed how a function attached to a class becomes a method when
accessed through an instance, by leveraging the descriptor protocol.
To conclude the chapter, �Descriptor Usage Tips� on page 648 provided a brief look at
how descriptor deletion and documentation work.
Throughout this chapter, we faced a few issues that only class metaprogramming can
solve, and we deferred those to Chapter 21.
Further Reading
Besides the obligatory reference to the �Data Model� chapter, Raymond Hettinger�s
Descriptor HowTo Guide is a valuable resource�part of the HowTo collection in the
official Python documentation.
Chapter Summary | 651
As usual with Python object model subjects, Alex Martelli�s Python in a Nutshell, 2E
(O�Reilly) is authoritative and objective, even if somewhat dated: the key mechanisms
discussed in this chapter were introduced in Python 2.2, long before the 2.5 version
covered by that book. Martelli also has a presentation titled Python�s Object Model,
which covers properties and descriptors in depth (slides, video). Highly recommended.
For Python 3 coverage with practical examples, Python Cookbook, 3E by David Beazley
and Brian K. Jones (O�Reilly), has many recipes illustrating descriptors, of which I want
to highlight �6.12. Reading Nested and Variable-Sized Binary Structures,� �8.10. Using
Lazily Computed Properties,� �8.13. Implementing a Data Model or Type System,� and
�9.9. Defining Decorators As Classes��the latter of which addresses deep issues with
the interaction of function decorators, descriptors, and methods, explaining how a
function decorator implemented as a class with __call__ also needs to implement
__get__ if it wants to work with decorating methods as well as functions.
Soapbox
The Problem with self
�Worse is Better� is a design philosophy described by Richard P. Gabriel in �The Rise
of Worse is Better�. The first priority of this philosophy is �Simplicity,� which Gabriel
states as:
The design must be simple, both in implementation and interface. It is more important
for the implementation to be simple than the interface. Simplicity is the most important
consideration in a design.
I believe the requirement to explicitly declare self as a first argument in methods is an
application of �Worse is Better� in Python. The implementation is simple�elegant even
�at the expense of the user interface: a method signature like def zfill(self,
width): doesn�t visually match the invocation pobox.zfill(8).
Modula-3 introduced that convention�and the use of the self identifier�but there is
a difference: in Modula-3, interfaces are declared separately from their implementation,
and in the interface declaration the self argument is omitted, so from the user�s perspective,
a method appears in an interface declaration exactly with the same number of
explicit arguments it takes.
One improvement in this regard has been the error messages: for a user-defined method
with one argument besides self, if the user invokes obj.meth(), Python 2.7 raises
TypeError: meth() takes exactly 2 arguments (1 given), but in Python 3.4 the
message is less confusing, sidestepping the issue of the argument count and naming the
missing argument: meth() missing 1 required positional argument: 'x'.
652 | Chapter 20: Attribute Descriptors
7. See, for example, A. M. Kuchling�s famous Python Warts post (archived); Kuchling himself is not so bothered
by the self qualifier, but he mentions it�probably echoing opinions from comp.lang.python.
Besides the use of self as an explicit argument, the requirement to qualify all access to
instance attributes with self is also criticized.7 I personally don�t mind typing the self
qualifier: it�s good to distinguish local variables from attributes. My issue is with the use
of self in the def statement. But I got used to it.
Anyone who is unhappy about the explicit self in Python can feel a lot better by considering
the baffling semantics of the implicit this in JavaScript. Guido had some good
reasons to make self work as it does, and he wrote about them in �Adding Support for
User-Defined Classes�, a post on his blog, The History of Python.
Further Reading | 653

1. Message to comp.lang.python, subject: �Acrimony in c.l.p.�. This is another part of the same message from
December 23, 2002, quoted in the Preface. The TimBot was inspired that day.
CHAPTER 21
Class Metaprogramming
[Metaclasses] are deeper magic than 99% of users should ever worry about. If you wonder
whether you need them, you don�t (the people who actually need them know with certainty
that they need them, and don�t need an explanation about why).1
� Tim Peters
Inventor of the timsort algorithm and prolific Python contributor
Class metaprogramming is the art of creating or customizing classes at runtime. Classes
are first-class objects in Python, so a function can be used to create a new class at any
time, without using the class keyword. Class decorators are also functions, but capable
of inspecting, changing, and even replacing the decorated class with another class. Finally,
metaclasses are the most advanced tool for class metaprogramming: they let you
create whole new categories of classes with special traits, such as the abstract base classes
we�ve already seen.
Metaclasses are powerful, but hard to get right. Class decorators solve many of the same
problems more simply. In fact, metaclasses are now so hard to justify in real code that
my favorite motivating example lost much of its appeal with the introduction of class
decorators in Python 2.6.
Also covered here is the distinction between import time and runtime: a crucial prerequisite
for effective Python metaprogramming.
This is an exciting topic, and it�s easy to get carried away. So I must
start this chapter with the following admonition:
If you are not authoring a framework, you should not be writing
metaclasses�unless you�re doing it for fun or to practice the concepts.
655
We�ll get started by reviewing how to create a class at runtime.
A Class Factory
The standard library has a class factory that we�ve seen several times in this book:
collections.namedtuple. It�s a function that, given a class name and attribute names
creates a subclass of tuple that allows retrieving items by name and provides a nice
__repr__ for debugging.
Sometimes I�ve felt the need for a similar factory for mutable objects. Suppose I�m writing
a pet shop application and I want to process data for dogs as simple records. It�s bad
to have to write boilerplate like this:
class Dog:
def __init__(self, name, weight, owner):
self.name = name
self.weight = weight
self.owner = owner
Boring� the field names appear three times each. All that boilerplate doesn�t even buy
us a nice repr:
>>> rex = Dog('Rex', 30, 'Bob')
>>> rex
<__main__.Dog object at 0x2865bac>
Taking a hint from collections.namedtuple, let�s create a record_factory that creates
simple classes like Dog on the fly. Example 21-1 shows how it should work.
Example 21-1. Testing record_factory, a simple class factory
>>> Dog = record_factory('Dog', 'name weight owner')
>>> rex = Dog('Rex', 30, 'Bob')
>>> rex
Dog(name='Rex', weight=30, owner='Bob')
>>> name, weight, _ = rex
>>> name, weight
('Rex', 30)
>>> "{2}'s dog weighs {1}kg".format(*rex)
"Bob's dog weighs 30kg"
>>> rex.weight = 32
>>> rex
Dog(name='Rex', weight=32, owner='Bob')
>>> Dog.__mro__
(<class 'factories.Dog'>, <class 'object'>)
Factory signature is similar to that of namedtuple: class name, followed by
attribute names in a single string, separated by spaces or commas.
Nice repr.
656 | Chapter 21: Class Metaprogramming
2. Thanks to my friend J.S. Bueno for suggesting this solution.
Instances are iterable, so they can be conveniently unpacked on assignment�
�or when passing to functions like format.
A record instance is mutable.
The newly created class inherits from object�no relationship to our factory.
The code for record_factory is in Example 21-2.2
Example 21-2. record_factory.py: a simple class factory
def record_factory(cls_name, field_names):
try:
field_names = field_names.replace(',', ' ').split()
except AttributeError: # no .replace or .split
pass # assume it's already a sequence of identifiers
field_names = tuple(field_names)
def __init__(self, *args, **kwargs):
attrs = dict(zip(self.__slots__, args))
attrs.update(kwargs)
for name, value in attrs.items():
setattr(self, name, value)
def __iter__(self):
for name in self.__slots__:
yield getattr(self, name)
def __repr__(self):
values = ', '.join('{}={!r}'.format(*i) for i
in zip(self.__slots__, self))
return '{}({})'.format(self.__class__.__name__, values)
cls_attrs = dict(__slots__ = field_names,
__init__ = __init__,
__iter__ = __iter__,
__repr__ = __repr__)
return type(cls_name, (object,), cls_attrs)
Duck typing in practice: try to split field_names by commas or spaces; if that
fails, assume it�s already an iterable, with one name per item.
Build a tuple of attribute names, this will be the __slots__ attribute of the new
class; this also sets the order of the fields for unpacking and __repr__.
This function will become the __init__ method in the new class. It accepts
positional and/or keyword arguments.
A Class Factory | 657
Implement an __iter__, so the class instances will be iterable; yield the field
values in the order given by __slots__.
Produce the nice repr, iterating over __slots__ and self.
Assemble dictionary of class attributes.
Build and return the new class, calling the type constructor.
We usually think of type as a function, because we use it like one, e.g., type(my_ob
ject) to get the class of the object�same as my_object.__class__. However, type is
a class. It behaves like a class that creates a new class when invoked with three arguments:
MyClass = type('MyClass', (MySuperClass, MyMixin),
{'x': 42, 'x2': lambda self: self.x * 2})
The three arguments of type are named name, bases, and dict�the latter being a mapping
of attribute names and attributes for the new class. The preceding code is functionally
equivalent to this:
class MyClass(MySuperClass, MyMixin):
x = 42
def x2(self):
return self.x * 2
The novelty here is that the instances of type are classes, like MyClass here, or the Dog
class in Example 21-1.
In summary, the last line of record_factory in Example 21-2 builds a class named by
the value of cls_name, with object as its single immediate superclass and with class
attributes named __slots__, __init__, __iter__, and __repr__, of which the last three
are instance methods.
We could have named the __slots__ class attribute anything else, but then we�d have
to implement __setattr__ to validate the names of attributes being assigned, because
for our record-like classes we want the set of attributes to be always the same and in the
same order. However, recall that the main feature of __slots__ is saving memory when
you are dealing with millions of instances, and using __slots__ has some drawbacks,
discussed in �Saving Space with the __slots__ Class Attribute� on page 264.
Invoking type with three arguments is a common way of creating a class dynamically.
If you peek at the source code for collections.namedtuple, you�ll see a different approach:
there is _class_template, a source code template as a string, and the namedtu
ple function fills its blanks calling _class_template.format(�). The resulting source
code string is then evaluated with the exec built-in function.
658 | Chapter 21: Class Metaprogramming
It�s good practice to avoid exec or eval for metaprogramming in
Python. These functions pose serious security risks if they are fed
strings (even fragments) from untrusted sources. Python offers
sufficient introspection tools to make exec and eval unnecessary
most of the time. However, the Python core developers chose to
use exec when implementing namedtuple. The chosen approach
makes the code generated for the class available in the ._source
attribute.
Instances of classes created by record_factory have a limitation: they are not serializable�
that is, they can�t be used with the dump/load functions from the pickle module.
Solving this problem is beyond the scope of this example, which aims to show the type
class in action in a simple use case. For the full solution, study the source code for
collections.nameduple; search for the word �pickling.�
A Class Decorator for Customizing Descriptors
When we left the LineItem example in �LineItem Take #5: A New Descriptor Type� on
page 637, the issue of descriptive storage names was still pending: the value of attributes
such as weight was stored in an instance attribute named _Quantity#0, which made
debugging a bit hard. You can retrieve the storage name from a descriptor in
Example 20-7 with the following lines:
>>> LineItem.weight.storage_name
'_Quantity#0'
However, it would be better if the storage names actually included the name of the
managed attribute, like this:
>>> LineItem.weight.storage_name
'_Quantity#weight'
Recall from �LineItem Take #4: Automatic Storage Attribute Names� on page 631 that
we could not use descriptive storage names because when the descriptor is instantiated
it has no way of knowing the name of the managed attribute (i.e., the class attribute to
which the descriptor will be bound, such as weight in the preceding examples). But
once the whole class is assembled and the descriptors are bound to the class attributes,
we can inspect the class and set proper storage names to the descriptors. This could be
done in the __new__ method of the LineItem class, so that by the time the descriptors
are used in the __init__ method, the correct storage names are set. The problem of
using __new__ for that purpose is wasted effort: the logic of __new__ will run every time
a new LineItem instance is created, but the binding of the descriptor to the managed
attribute will never change once the LineItem class itself is built. So we need to set the
A Class Decorator for Customizing Descriptors | 659
storage names when the class is created. That can be done with a class decorator or a
metaclass. We�ll do it first in the easier way.
A class decorator is very similar to a function decorator: it�s a function that gets a class
object and returns the same class or a modified one.
In Example 21-3, the LineItem class will be evaluated by the interpreter and the resulting
class object will be passed to the model.entity function. Python will bind the global
name LineItem to whatever the model.entity function returns. In this example, mod
el.entity returns the same LineItem class with the storage_name attribute of each
descriptor instance changed.
Example 21-3. bulkfood_v6.py: LineItem using Quantity and NonBlank descriptors
import model_v6 as model
@model.entity
class LineItem:
description = model.NonBlank()
weight = model.Quantity()
price = model.Quantity()
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
The only change in this class is the added decorator.
Example 21-4 shows the implementation of the decorator. Only the new code at the
bottom of model_v6.py is listed here; the rest of the module is identical to model_
v5.py (Example 20-6).
Example 21-4. model_v6.py: a class decorator
def entity(cls):
for key, attr in cls.__dict__.items():
if isinstance(attr, Validated):
type_name = type(attr).__name__
attr.storage_name = '_{}#{}'.format(type_name, key)
return cls
Decorator gets class as argument.
Iterate over dict holding the class attributes.
If the attribute is one of our Validated descriptors�
660 | Chapter 21: Class Metaprogramming
3. Contrast with the import statement in Java, which is just a declaration to let the compiler know that certain
packages are required.
�set the storage_name to use the descriptor class name and the managed
attribute name (e.g., _NonBlank#description).
Return the modified class.
The doctests in bulkfood_v6.py prove that the changes are successful. For example,
Example 21-5 shows the names of the storage attributes in a LineItem instance.
Example 21-5. bulkfood_v6.py: doctests for new storage_name descriptor attributes
>>> raisins = LineItem('Golden raisins', 10, 6.95)
>>> dir(raisins)[:3]
['_NonBlank#description', '_Quantity#price', '_Quantity#weight']
>>> LineItem.description.storage_name
'_NonBlank#description'
>>> raisins.description
'Golden raisins'
>>> getattr(raisins, '_NonBlank#description')
'Golden raisins'
That�s not too complicated. Class decorators are a simpler way of doing something that
previously required a metaclass: customizing a class the moment it�s created.
A significant drawback of class decorators is that they act only on the class where they
are directly applied. This means subclasses of the decorated class may or may not inherit
the changes made by the decorator, depending on what those changes are. We�ll explore
the problem and see how it�s solved in the following sections.
What Happens When: Import Time Versus Runtime
For successful metaprogramming, you must be aware of when the Python interpreter
evaluates each block of code. Python programmers talk about �import time� versus
�runtime� but the terms are not strictly defined and there is a gray area between them.
At import time, the interpreter parses the source code of a .py module in one pass from
top to bottom, and generates the bytecode to be executed. That�s when syntax errors
may occur. If there is an up-to-date .pyc file available in the local __pycache__, those
steps are skipped because the bytecode is ready to run.
Although compiling is definitely an import-time activity, other things may happen at
that time, because almost every statement in Python is executable in the sense that they
potentially run user code and change the state of the user program. In particular, the
import statement is not merely a declaration3 but it actually runs all the top-level code
of the imported module when it�s imported for the first time in the process�further
imports of the same module will use a cache, and only name binding occurs then. That
What Happens When: Import Time Versus Runtime | 661
4. I�m not saying starting a database connection just because a module is imported is a good idea, only pointing
out it can be done.
top-level code may do anything, including actions typical of �runtime�, such as connecting
to a database.4 That�s why the border between �import time� and �runtime� is
fuzzy: the import statement can trigger all sorts of �runtime� behavior.
In the previous paragraph, I wrote that importing �runs all the top-level code,� but �toplevel
code� requires some elaboration. The interpreter executes a def statement on the
top level of a module when the module is imported, but what does that achieve? The
interpreter compiles the function body (if it�s the first time that module is imported),
and binds the function object to its global name, but it does not execute the body of the
function, obviously. In the usual case, this means that the interpreter defines top-level
functions at import time, but executes their bodies only when�and if�the functions
are invoked at runtime.
For classes, the story is different: at import time, the interpreter executes the body of
every class, even the body of classes nested in other classes. Execution of a class body
means that the attributes and methods of the class are defined, and then the class object
itself is built. In this sense, the body of classes is �top-level code�: it runs at import time.
This is all rather subtle and abstract, so here is an exercise to help you see what happens
when.
The Evaluation Time Exercises
Consider a script, evaltime.py, which imports a module evalsupport.py. Both modules
have several print calls to output markers in the format <[N]>, where N is a number.
The goal of this pair of exercises is to determine when each of theses calls will be made.
Students have reported these exercises are helpful to better appreciate
how Python evaluates the source code. Do take the time
to solve them with paper and pencil before looking at �Solution
for scenario #1� on page 664.
The listings are Examples 21-6 and 21-7. Grab paper and pencil and�without running
the code�write down the markers in the order they will appear in the output, in two
scenarios:
Scenario #1
The module evaltime.py is imported interactively in the Python console:
>>> import evaltime
662 | Chapter 21: Class Metaprogramming
Scenario #2
The module evaltime.py is run from the command shell:
$ python3 evaltime.py
Example 21-6. evaltime.py: write down the numbered <[N]> markers in the order they
will appear in the output
from evalsupport import deco_alpha
print('<[1]> evaltime module start')
class ClassOne():
print('<[2]> ClassOne body')
def __init__(self):
print('<[3]> ClassOne.__init__')
def __del__(self):
print('<[4]> ClassOne.__del__')
def method_x(self):
print('<[5]> ClassOne.method_x')
class ClassTwo(object):
print('<[6]> ClassTwo body')
@deco_alpha
class ClassThree():
print('<[7]> ClassThree body')
def method_y(self):
print('<[8]> ClassThree.method_y')
class ClassFour(ClassThree):
print('<[9]> ClassFour body')
def method_y(self):
print('<[10]> ClassFour.method_y')
if __name__ == '__main__':
print('<[11]> ClassOne tests', 30 * '.')
one = ClassOne()
one.method_x()
print('<[12]> ClassThree tests', 30 * '.')
three = ClassThree()
three.method_y()
print('<[13]> ClassFour tests', 30 * '.')
four = ClassFour()
What Happens When: Import Time Versus Runtime | 663
four.method_y()
print('<[14]> evaltime module end')
Example 21-7. evalsupport.py: module imported by evaltime.py
print('<[100]> evalsupport module start')
def deco_alpha(cls):
print('<[200]> deco_alpha')
def inner_1(self):
print('<[300]> deco_alpha:inner_1')
cls.method_y = inner_1
return cls
# BEGIN META_ALEPH
class MetaAleph(type):
print('<[400]> MetaAleph body')
def __init__(cls, name, bases, dic):
print('<[500]> MetaAleph.__init__')
def inner_2(self):
print('<[600]> MetaAleph.__init__:inner_2')
cls.method_z = inner_2
# END META_ALEPH
print('<[700]> evalsupport module end')
Solution for scenario #1
Example 21-8 is the output of importing the evaltime.py module in the Python console.
Example 21-8. Scenario #1: importing evaltime in the Python console
>>> import evaltime
<[100]> evalsupport module start
<[400]> MetaAleph body
<[700]> evalsupport module end
<[1]> evaltime module start
<[2]> ClassOne body
<[6]> ClassTwo body
<[7]> ClassThree body
<[200]> deco_alpha
<[9]> ClassFour body
<[14]> evaltime module end
664 | Chapter 21: Class Metaprogramming
All top-level code in evalsupport runs when the module is imported; the
deco_alpha function is compiled, but its body does not execute.
The body of the MetaAleph function does run.
The body of every class is executed�
�including nested classes.
The decorator function runs after the body of the decorated ClassThree is
evaluated.
In this scenario, the evaltime is imported, so the if __name__ == '__main__':
block never runs.
Notes about scenario #1:
1. This scenario is triggered by a simple import evaltime statement.
2. The interpreter executes every class body of the imported module and its dependency,
evalsupport.
3. It makes sense that the interpreter evaluates the body of a decorated class before it
invokes the decorator function that is attached on top of it: the decorator must get
a class object to process, so the class object must be built first.
4. The only user-defined function or method that runs in this scenario is the deco_al
pha decorator.
Now let�s see what happens in scenario #2.
Solution for scenario #2
Example 21-9 is the output of running python evaltime.py.
Example 21-9. Scenario #2: running evaltime.py from the shell
$ python3 evaltime.py
<[100]> evalsupport module start
<[400]> MetaAleph body
<[700]> evalsupport module end
<[1]> evaltime module start
<[2]> ClassOne body
<[6]> ClassTwo body
<[7]> ClassThree body
<[200]> deco_alpha
<[9]> ClassFour body
<[11]> ClassOne tests ..............................
<[3]> ClassOne.__init__
<[5]> ClassOne.method_x
<[12]> ClassThree tests ..............................
<[300]> deco_alpha:inner_1
What Happens When: Import Time Versus Runtime | 665
<[13]> ClassFour tests ..............................
<[10]> ClassFour.method_y
<[14]> evaltime module end
<[4]> ClassOne.__del__
Same output as Example 21-8 so far.
Standard behavior of a class.
ClassThree.method_y was changed by the deco_alpha decorator, so the call
three.method_y() runs the body of the inner_1 function.
The ClassOne instance bound to one global variable is garbage-collected only
when the program ends.
The main point of scenario #2 is to show that the effects of a class decorator may not
affect subclasses. In Example 21-6, ClassFour is defined as a subclass of ClassThree.
The @deco_alpha decorator is applied to ClassThree, replacing its method_y, but that
does not affect ClassFour at all. Of course, if the ClassFour.method_y did invoke the
ClassThree.method_y with super(�), we would see the effect of the decorator, as the
inner_1 function executed.
In contrast, the next section will show that metaclasses are more effective when we want
to customize a whole class hierarchy, and not one class at a time.
Metaclasses 101
A metaclass is a class factory, except that instead of a function, like record_factory
from Example 21-2, a metaclass is written as a class. Figure 21-1 depicts a metaclass
using the Mills & Gizmos Notation: a mill producing another mill.
666 | Chapter 21: Class Metaprogramming
Figure 21-1. A metaclass is a class that builds classes
Consider the Python object model: classes are objects, therefore each class must be an
instance of some other class. By default, Python classes are instances of type. In other
words, type is the metaclass for most built-in and user-defined classes:
>>> 'spam'.__class__
<class 'str'>
>>> str.__class__
<class 'type'>
>>> from bulkfood_v6 import LineItem
>>> LineItem.__class__
<class 'type'>
>>> type.__class__
<class 'type'>
To avoid infinite regress, type is an instance of itself, as the last line shows.
Note that I am not saying that str or LineItem inherit from type. What I am saying is
that str and LineItem are instances of type. They all are subclasses of object.
Figure 21-2 may help you confront this strange reality.
Metaclasses 101 | 667
Figure 21-2. Both diagrams are true. The left one emphasizes that str, type, and LineItem
are subclasses of object. The right one makes it clear that str, object, and LineItem
are instances of type, because they are all classes.
The classes object and type have a unique relationship: object
is an instance of type, and type is a subclass of object. This
relationship is �magic�: it cannot be expressed in Python because
either class would have to exist before the other could be
defined. The fact that type is an instance of itself is also magical.
Besides type, a few other metaclasses exist in the standard library, such as ABCMeta and
Enum. The next snippet shows that the class of collections.Iterable is abc.ABCMeta.
The class Iterable is abstract, but ABCMeta is not�after all, Iterable is an instance of
ABCMeta:
>>> import collections
>>> collections.Iterable.__class__
<class 'abc.ABCMeta'>
>>> import abc
>>> abc.ABCMeta.__class__
<class 'type'>
>>> abc.ABCMeta.__mro__
(<class 'abc.ABCMeta'>, <class 'type'>, <class 'object'>)
Ultimately, the class of ABCMeta is also type. Every class is an instance of type, directly
or indirectly, but only metaclasses are also subclasses of type. That�s the most important
relationship to understand metaclasses: a metaclass, such as ABCMeta, inherits from type
the power to construct classes. Figure 21-3 illustrates this crucial relationship.
668 | Chapter 21: Class Metaprogramming
Figure 21-3. Iterable is a subclass of object and an instance of ABCMeta. Both object
and ABCMeta are instances of type, but the key relationship here is that ABCMeta is
also a subclass of type, because ABCMeta is a metaclass. In this diagram, Iterable is the
only abstract class.
The important takeaway here is that all classes are instances of type, but metaclasses
are also subclasses of type, so they act as class factories. In particular, a metaclass can
customize its instances by implementing __init__. A metaclass __init__ method can
do everything a class decorator can do, but its effects are more profound, as the next
exercise demonstrates.
The Metaclass Evaluation Time Exercise
This is a variation of �The Evaluation Time Exercises� on page 662. The evalsupport.
py module is the same as Example 21-7, but the main script is now evaltime_
meta.py, listed in Example 21-10.
Example 21-10. evaltime_meta.py: ClassFive is an instance of the MetaAleph metaclass
from evalsupport import deco_alpha
from evalsupport import MetaAleph
print('<[1]> evaltime_meta module start')
@deco_alpha
class ClassThree():
print('<[2]> ClassThree body')
def method_y(self):
print('<[3]> ClassThree.method_y')
Metaclasses 101 | 669
class ClassFour(ClassThree):
print('<[4]> ClassFour body')
def method_y(self):
print('<[5]> ClassFour.method_y')
class ClassFive(metaclass=MetaAleph):
print('<[6]> ClassFive body')
def __init__(self):
print('<[7]> ClassFive.__init__')
def method_z(self):
print('<[8]> ClassFive.method_y')
class ClassSix(ClassFive):
print('<[9]> ClassSix body')
def method_z(self):
print('<[10]> ClassSix.method_y')
if __name__ == '__main__':
print('<[11]> ClassThree tests', 30 * '.')
three = ClassThree()
three.method_y()
print('<[12]> ClassFour tests', 30 * '.')
four = ClassFour()
four.method_y()
print('<[13]> ClassFive tests', 30 * '.')
five = ClassFive()
five.method_z()
print('<[14]> ClassSix tests', 30 * '.')
six = ClassSix()
six.method_z()
print('<[15]> evaltime_meta module end')
Again, grab pencil and paper and write down the numbered <[N]> markers in the order
they will appear in the output, considering these two scenarios:
Scenario #3
The module evaltime_meta.py is imported interactively in the Python console.
Scenario #4
The module evaltime_meta.py is run from the command shell.
Solutions and analysis are next.
670 | Chapter 21: Class Metaprogramming
Solution for scenario #3
Example 21-11 shows the output of importing evaltime_meta.py in the Python console.
Example 21-11. Scenario #3: importing evaltime_meta in the Python console
>>> import evaltime_meta
<[100]> evalsupport module start
<[400]> MetaAleph body
<[700]> evalsupport module end
<[1]> evaltime_meta module start
<[2]> ClassThree body
<[200]> deco_alpha
<[4]> ClassFour body
<[6]> ClassFive body
<[500]> MetaAleph.__init__
<[9]> ClassSix body
<[500]> MetaAleph.__init__
<[15]> evaltime_meta module end
The key difference from scenario #1 is that the MetaAleph.__init__ method is
invoked to initialize the just-created ClassFive.
And MetaAleph.__init__ also initializes ClassSix, which is a subclass of Class
Five.
The Python interpreter evaluates the body of ClassFive but then, instead of calling
type to build the actual class body, it calls MetaAleph. Looking at the definition of
MetaAleph in Example 21-12, you�ll see that the __init__ method gets four arguments:
self
That�s the class object being initialized (e.g., ClassFive)
name, bases, dic
The same arguments passed to type to build a class
Example 21-12. evalsupport.py: definition of the metaclass MetaAleph from
Example 21-7
class MetaAleph(type):
print('<[400]> MetaAleph body')
def __init__(cls, name, bases, dic):
print('<[500]> MetaAleph.__init__')
def inner_2(self):
print('<[600]> MetaAleph.__init__:inner_2')
cls.method_z = inner_2
Metaclasses 101 | 671
When coding a metaclass, it�s conventional to replace self with
cls. For example, in the __init__ method of the metaclass, using
cls as the name of the first argument makes it clear that the
instance under construction is a class.
The body of __init__ defines an inner_2 function, then binds it to cls.method_z. The
name cls in the signature of MetaAleph.__init__ refers to the class being created (e.g.,
ClassFive). On the other hand, the name self in the signature of inner_2 will eventually
refer to an instance of the class we are creating (e.g., an instance of ClassFive).
Solution for scenario #4
Example 21-13 shows the output of running python evaltime.py from the command
line.
Example 21-13. Scenario #4: running evaltime_meta.py from the shell
$ python3 evaltime.py
<[100]> evalsupport module start
<[400]> MetaAleph body
<[700]> evalsupport module end
<[1]> evaltime_meta module start
<[2]> ClassThree body
<[200]> deco_alpha
<[4]> ClassFour body
<[6]> ClassFive body
<[500]> MetaAleph.__init__
<[9]> ClassSix body
<[500]> MetaAleph.__init__
<[11]> ClassThree tests ..............................
<[300]> deco_alpha:inner_1
<[12]> ClassFour tests ..............................
<[5]> ClassFour.method_y
<[13]> ClassFive tests ..............................
<[7]> ClassFive.__init__
<[600]> MetaAleph.__init__:inner_2
<[14]> ClassSix tests ..............................
<[7]> ClassFive.__init__
<[600]> MetaAleph.__init__:inner_2
<[15]> evaltime_meta module end
When the decorator is applied to ClassThree, its method_y is replaced by the
inner_1 method�
But this has no effect on the undecorated ClassFour, even though ClassFour is
a subclass of ClassThree.
672 | Chapter 21: Class Metaprogramming
The __init__ method of MetaAleph replaces ClassFive.method_z with its
inner_2 function.
The same happens with the ClassFive subclass, ClassSix: its method_z is
replaced by inner_2.
Note that ClassSix makes no direct reference to MetaAleph, but it is affected by it
because it�s a subclass of ClassFive and therefore it is also an instance of MetaAleph, so
it�s initialized by MetaAleph.__init__.
Further class customization can be done by implementing __new__
in a metaclass. But more often than not, implementing __init__
is enough.
We can now put all this theory in practice by creating a metaclass to provide a definitive
solution to the descriptors with automatic storage attribute names.
A Metaclass for Customizing Descriptors
Back to the LineItem examples. It would be nice if the user did not have to be aware of
decorators or metaclasses at all, and could just inherit from a class provided by our
library, like in Example 21-14.
Example 21-14. bulkfood_v7.py: inheriting from model.Entity can work, if a metaclass
is behind the scenes
import model_v7 as model
class LineItem(model.Entity):
description = model.NonBlank()
weight = model.Quantity()
price = model.Quantity()
def __init__(self, description, weight, price):
self.description = description
self.weight = weight
self.price = price
def subtotal(self):
return self.weight * self.price
LineItem is a subclass of model.Entity.
A Metaclass for Customizing Descriptors | 673
Example 21-14 looks pretty harmless. No strange syntax to be seen at all. However, it
only works because model_v7.py defines a metaclass, and model.Entity is an instance
of that metaclass. Example 21-15 shows the implementation of the Entity class in the
model_v7.py module.
Example 21-15. model_v7.py: the EntityMeta metaclass and one instance of it, Entity
class EntityMeta(type):
"""Metaclass for business entities with validated fields"""
def __init__(cls, name, bases, attr_dict):
super().__init__(name, bases, attr_dict)
for key, attr in attr_dict.items():
if isinstance(attr, Validated):
type_name = type(attr).__name__
attr.storage_name = '_{}#{}'.format(type_name, key)
class Entity(metaclass=EntityMeta):
"""Business entity with validated fields"""
Call __init__ on the superclass (type in this case).
Same logic as the @entity decorator in Example 21-4.
This class exists for convenience only: the user of this module can just subclass
Entity and not worry about EntityMeta�or even be aware of its existence.
The code in Example 21-14 passes the tests in Example 21-3. The support module,
model_v7.py, is harder to understand than model_v6.py, but the user-level code is simpler:
just inherit from model_v7.entity and you get custom storage names for your
Validated fields.
Figure 21-4 is a simplified depiction of what we just implemented. There is a lot going
on, but the complexity is hidden inside the model_v7 module. From the user perspective,
LineItem is simply a subclass of Entity, as coded in Example 21-14. This is the power
of abstraction.
674 | Chapter 21: Class Metaprogramming
5. Recall from �ABC Syntax Details� on page 328 that in Python 2.7 the __metaclass__ class attribute is used,
and the metaclass= keyword argument is not supported in the class declaration.
Figure 21-4. UML class diagram annotated with MGN (Mills & Gizmos Notation): the
EntityMeta meta-mill builds the LineItem mill. Configuration of the descriptors (e.g.,
weight and price) is done by EntityMeta.__init__. Note the package boundary of model_
v7.
Except for the syntax for linking a class to the metaclass,5 everything written so far about
metaclasses applies to versions of Python as early as 2.2, when Python types underwent
a major overhaul. The next section covers a feature that is only available in Python 3.
The Metaclass __prepare__ Special Method
In some applications it�s interesting to be able to know the order in which the attributes
of a class are defined. For example, a library to read/write CSV files driven by userdefined
classes may want to map the order of the fields declared in the class to the order
of the columns in the CSV file.
As we�ve seen, both the type constructor and the __new__ and __init__ methods of
metaclasses receive the body of the class evaluated as a mapping of names to attributes.
However, by default, that mapping is a dict, which means the order of the attributes as
they appear in the class body is lost by the time our metaclass or class decorator can
look at them.
The solution to this problem is the __prepare__ special method, introduced in Python
3. This special method is relevant only in metaclasses, and it must be a class method
(i.e., defined with the @classmethod decorator). The __prepare__ method is invoked
The Metaclass __prepare__ Special Method | 675
by the interpreter before the __new__ method in the metaclass to create the mapping
that will be filled with the attributes from the class body. Besides the metaclass as first
argument, __prepare__ gets the name of the class to be constructed and its tuple of base
classes, and it must return a mapping, which will be received as the last argument by
__new__ and then __init__ when the metaclass builds a new class.
It sounds complicated in theory, but in practice, every time I�ve seen __prepare__ being
used it was very simple. Take a look at Example 21-16.
Example 21-16. model_v8.py: the EntityMeta metaclass uses prepare, and Entity now
has a field_names class method
class EntityMeta(type):
"""Metaclass for business entities with validated fields"""
@classmethod
def __prepare__(cls, name, bases):
return collections.OrderedDict()
def __init__(cls, name, bases, attr_dict):
super().__init__(name, bases, attr_dict)
cls._field_names = []
for key, attr in attr_dict.items():
if isinstance(attr, Validated):
type_name = type(attr).__name__
attr.storage_name = '_{}#{}'.format(type_name, key)
cls._field_names.append(key)
class Entity(metaclass=EntityMeta):
"""Business entity with validated fields"""
@classmethod
def field_names(cls):
for name in cls._field_names:
yield name
Return an empty OrderedDict instance, where the class attributes will be stored.
Create a _field_names attribute in the class under construction.
This line is unchanged from the previous version, but attr_dict here is the
OrderedDict obtained by the interpreter when it called __prepare__ before
calling __init__. Therefore, this for loop will go over the attributes in the order
they were added.
Add the name of each Validated field found to _field_names.
The field_names class method simply yields the names of the fields in the order
they were added.
676 | Chapter 21: Class Metaprogramming
With the simple additions made in Example 21-16, we are now able to iterate over the
Validated fields of any Entity subclass using the field_names class method.
Example 21-17 demonstrates this new feature.
Example 21-17. bulkfood_v8.py: doctest showing the use of field_names�no changes
are needed in the LineItem class; field_names is inherited from model.Entity
>>> for name in LineItem.field_names():
... print(name)
...
description
weight
price
This wraps up our coverage of metaclasses. In the real world, metaclasses are used in
frameworks and libraries that help programmers perform, among other tasks:
� Attribute validation
� Applying decorators to many methods at once
� Object serialization or data conversion
� Object-relational mapping
� Object-based persistency
� Dynamic translation of class structures from other languages
We�ll now have a brief overview of methods defined in the Python data model for all
classes.
Classes as Objects
Every class has a number of attributes defined in the Python data model, documented
in �4.13. Special Attributes� of the �Built-in Types� chapter in the Library Reference.
Three of those attributes we�ve seen several times in the book already: __mro__,
__class__, and __name__. Other class attributes are:
cls.__bases__
The tuple of base classes of the class.
cls.__qualname__
A new attribute in Python 3.3 holding the qualified name of a class or function,
which is a dotted path from the global scope of the module to the class definition.
For example, in Example 21-6, the __qualname__ of the inner class ClassTwo is the
string 'ClassOne.ClassTwo', while its __name__ is just 'ClassTwo'. The specification
for this attribute is PEP-3155 � Qualified name for classes and functions.
Classes as Objects | 677
cls.__subclasses__()
This method returns a list of the immediate subclasses of the class. The implementation
uses weak references to avoid circular references between the superclass and
its subclasses�which hold a strong reference to the superclasses in their __bases__
attribute. The method returns the list of subclasses that currently exist in memory.
cls.mro()
The interpreter calls this method when building a class to obtain the tuple of superclasses
that is stored in the __mro__ attribute of the class. A metaclass can override
this method to customize the method resolution order of the class under construction.
None of the attributes mentioned in this section are listed by the
dir(�) function.
With this, our study of class metaprogramming ends. This is a vast topic and I only
scratched the surface. That�s why we have �Further Reading� sections in this book.
Chapter Summary
Class metaprogramming is about creating or customizing classes dynamically. Classes
in Python are first-class objects, so we started the chapter by showing how a class can
be created by a function invoking the type built-in metaclass.
In the next section, we went back to the LineItem class with descriptors from Chapter
20 to solve a lingering issue: how to generate names for the storage attributes that
reflected the names of the managed attributes (e.g., _Quantity#price instead of _Quan
tity#1). The solution was to use a class decorator, essentially a function that gets a justbuilt
class and has the opportunity to inspect it, change it, and even replace it with a
different class.
We then moved to a discussion of when different parts of the source code of a module
actually run. We saw that there is some overlap between the so-called �import time� and
�runtime,� but clearly a lot of code runs triggered by the import statement. Understanding
what runs when is crucial, and there are some subtle rules, so we used the
evaluation-time exercises to cover this topic.
The following subject was an introduction to metaclasses. We saw that all classes are
instances of type, directly or indirectly, so that is the �root metaclass� of the language.
A variation of the evaluation-time exercise was designed to show that a metaclass can
678 | Chapter 21: Class Metaprogramming
customize a hierarchy of classes�in contrast with a class decorator, which affects a single
class and may have no impact on its descendants.
The first practical application of a metaclass was to solve the issue of the storage attribute
names in LineItem. The resulting code is a bit trickier than the class decorator solution,
but it can be encapsulated in a module so that the user merely subclasses an apparently
plain class (model.Entity) without being aware that it is an instance of a custom metaclass
(model.EntityMeta). The end result is reminiscent of the ORM APIs in Django
and SQLAlchemy, which use metaclasses in their implementations but don�t require the
user to know anything about them.
The second metaclass we implemented added a small feature to model.EntityMeta: a
__prepare__ method to provide an OrderedDict to serve as the mapping from names
to attributes. This preserves the order in which those attributes are bound in the body
of the class under construction, so that metaclass methods like __new__ and __init__
can use that information. In the example, we implemented a _field_names class attribute,
which made possible an Entity.field_names() so users could retrieve the
Validated descriptors in the same order they appear in the source code.
The last section was a brief overview of attributes and methods available in all Python
classes.
Metaclasses are challenging, exciting, and�sometimes�abused by programmers trying
to be too clever. To wrap up, let�s recall Alex Martelli�s final advice from his essay
�Waterfowl and ABCs� on page 314:
And, don�t define custom ABCs (or metaclasses) in production code� if you feel the
urge to do so, I�d bet it�s likely to be a case of �all problems look like a nail�-syndrome for
somebody who just got a shiny new hammer�you (and future maintainers of your code)
will be much happier sticking with straightforward and simple code, eschewing such
depths.
� Alex Martelli
Wise words from a man who is not only a master of Python metaprogramming but also
an accomplished software engineer working on some of the largest mission-critical
Python deployments in the world.
Further Reading
The essential references for this chapter in the Python documentation are �3.3.3. Customizing
class creation� in the �Data Model� chapter of The Python Language Reference,
the type class documentation in the �Built-in Functions� page, and �4.13. Special Attributes�
of the �Built-in Types� chapter in the Library Reference. Also, in the Library
Reference, the types module documentation covers two functions that are new in
Further Reading | 679
6. Amazon.com catalog page for Putting Metaclasses to Work. You can still buy it used. I bought it and found
it a hard read, but I will probably go back to it later.
Python 3.3 and are designed to help with class metaprogramming: types.new_class(�)
and types.prepare_class(�).
Class decorators were formalized in PEP 3129 - Class Decorators, written by Collin
Winter, with the reference implementation authored by Jack Diederich. The PyCon 2009
talk �Class Decorators: Radically Simple� (video), also by Jack Diederich, is a quick
introduction to the feature.
Python in a Nutshell, 2E by Alex Martelli features outstanding coverage of metaclasses,
including a metaMetaBunch metaclass that aims to solve the same problem as our simple
record_factory from Example 21-2 but is much more sophisticated. Martelli does not
address class decorators because the feature appeared later than his book. Beazley and
Jones provide excellent examples of class decorators and metaclasses in their Python
Cookbook, 3E (O�Reilly). Michael Foord wrote an intriguing post titled �Meta-classes
Made Easy: Eliminating self with Metaclasses�. The subtitle says it all.
For metaclasses, the main references are PEP 3115 � Metaclasses in Python 3000, in
which the __prepare__ special method was introduced and Unifying types and classes
in Python 2.2, authored by Guido van Rossum. The text applies to Python 3 as well, and
it covers what were then called the �new-style� class semantics, including descriptors
and metaclasses. It�s a must-read. One of the references cited by Guido is Putting Metaclasses
to Work: a New Dimension in Object-Oriented Programming, by Ira R. Forman
and Scott H. Danforth (Addison-Wesley, 1998), a book to which he gave 5 stars on
Amazon.com, adding the following review:
This book contributed to the design for metaclasses in Python 2.2
Too bad this is out of print; I keep referring to it as the best tutorial I know for the difficult
subject of cooperative multiple inheritance, supported by Python via the super() function.
6.
For Python 3.5�in alpha as I write this�PEP 487 - Simpler customization of class
creation puts forward a new special method, __init_subclass__ that will allow a regular
class (i.e., not a metaclass) to customize the initialization of its subclasses. As with
class decorators, __init_subclass__ will make class metaprogramming more accessible
and also make it that much harder to justify the deployment of the nuclear option
�metaclasses.
If you are into metaprogramming, you may wish Python had the ultimate metaprogramming
feature: syntactic macros, as offered by Elixir and the Lisp family of languages.
Be careful what you wish for. I�ll just say one word: MacroPy.
680 | Chapter 21: Class Metaprogramming
7. Brian Harvey and Matthew Wright, Simply Scheme (MIT Press, 1999), p. xvii. Full text available
at Berkeley.edu.
8. Machine Beauty by David Gelernter (Basic Books) is an intriguing short book about elegance and aesthetics
in works of engineering, from bridges to software.
Soapbox
I will start the last soapbox in the book with a long quote from Brian Harvey and Matthew
Wright, two computer science professors from the University of California (Berkeley
and Santa Barbara). In their book, Simply Scheme, Harvey and Wright wrote:
There are two schools of thought about teaching computer science. We might caricature
the two views this way:
1. The conservative view: Computer programs have become too large and complex
to encompass in a human mind. Therefore, the job of computer science education
is to teach people how to discipline their work in such a way that 500 mediocre
programmers can join together and produce a program that correctly meets its
specification.
2. The radical view: Computer programs have become too large and complex to
encompass in a human mind. Therefore, the job of computer science education is
to teach people how to expand their minds so that the programs can fit, by learning
to think in a vocabulary of larger, more powerful, more flexible ideas than the
obvious ones. Each unit of programming thought must have a big payoff in the
capabilities of the program.7
� Brian Harvey and Matthew Wright
Preface to Simply Scheme
Harvey and Wright�s exaggerated descriptions are about teaching computer science, but
they also apply to programming language design. By now, you should have guessed that
I subscribe to the �radical� view, and I believe Python was designed in that spirit.
The property idea is a great step forward compared to the accessors-from-the-start
approach practically demanded by Java and supported by Java IDEs generating getters/
setters with a keyboard shortcut. The main advantage of properties is to let us start our
programs simply exposing attributes as public�in the spirit of KISS�knowing a public
attribute can become a property at any time without much pain. But the descriptor idea
goes way beyond that, providing a framework for abstracting away repetitive accessor
logic. That framework is so effective that essential Python constructs use it behind the
scenes.
Another powerful idea is functions as first-class objects, paving the way to higher-order
functions. Turns out the combination of descriptors and higher-order functions enable
the unification of functions and methods. A function�s __get__ produces a method
object on the fly by binding the instance to the self argument. This is elegant.8
Further Reading | 681
Finally, we have the idea of classes as first-class objects. It�s an outstanding feat of design
that a beginner-friendly language provides powerful abstractions such as class decorators
and full-fledged, user-defined metaclasses. Best of all: the advanced features are
integrated in a way that does not complicate Python�s suitability for casual programming
(they actually help it, under the covers). The convenience and success of frameworks
such as Django and SQLAlchemy owes much to metaclasses, even if many users of these
tools aren�t aware of them. But they can always learn and create the next great library.
I haven�t yet found a language that manages to be easy for beginners, practical for professionals,
and exciting for hackers in the way that Python is. Thanks, Guido van Rossum
and everybody else who makes it so.
682 | Chapter 21: Class Metaprogramming
Afterword
Python is a language for consenting adults.
� Alan Runyan
Cofounder of Plone
Alan�s pithy definition expresses one of the best qualities of Python: it gets out of the
way and lets you do what you must. This also means it doesn�t give you tools to restrict
what others can do with your code and the objects it builds.
Of course, Python is not perfect. Among the top irritants to me is the inconsistent use
of CamelCase, snake_case and joinedwords in the standard library. But the language
definition and the standard library are only part of an ecosystem. The community of
users and contributors is the best part of the Python ecosystem.
Here is one example of the community at its best: one morning while writing about
asyncio I was frustrated because the API has many functions, dozens of which are
coroutines, and you have to call the coroutines with yield from but you can�t do that
with regular functions. This was documented in the asyncio pages, but sometimes you
had to read a few paragraphs to find out whether a particular function was a coroutine.
So I sent a message to python-tulip titled �Proposal: make coroutines stand out in the
asyncio docs�. Victor Stinner, an asyncio core developer, Andrew Svetlov, main author
of aiohttp, Ben Darnell, lead developer of Tornado, and Glyph Lefkowitz, inventor of
Twisted, joined the conversation. Darnell suggested a solution, Alexander Shorin explained
how to implement it in Sphinx, and Stinner added the necessary configuration
and markup. Less than 12 hours after I raised the issue, the entire asyncio documentation
set online was updated with the coroutine tags you can see today.
That story did not happen in an exclusive club. Anybody can join the python-tulip list,
and I had posted only a few times when I wrote the proposal. The story illustrates a
community that is really open to new ideas and new members. Guido van Rossum hangs
out in python-tulip and can regularly be seen answering even simple questions.
683
Another example of openness: the Python Software Foundation (PSF) has been working
to increase diversity in the Python community. Some encouraging results are already
in. The 2013�2014 PSF board saw the first women elected directors: Jessica McKellar
and Lynn Root. And in the 2015 PyCon North America in Montreal�chaired by Diana
Clarke�about 1/3 of the speakers were women. I am unaware of any other major IT
conference that has gone so far in the pursuit of gender equality.
If you are a Pythonista but you have not engaged with the community, I encourage you
to do so. Seek the Python Users Group (PUG) in your area. If there isn�t one, create it.
Python is everywhere, so you will not be alone. Travel to events if you can. Come to a
PythonBrasil conference�we�ve had international speakers regularly for many years
now. Meeting fellow Pythonistas in person beats any online interaction and is known
to bring real benefits besides all the knowledge sharing. Like real jobs and real friendships.
I know I could not have written this book without the help of many friends I made over
the years in the Python community.
My father Jairo Ramalho used to say �So erra quem trabalha��Portuguese for �Only
those who work make mistakes��great advice to avoid being paralyzed by the fear of
making errors. I certainly made my share of mistakes while writing this book. The
reviewers, editors, and Early Release readers caught many of them. Within hours of the
first Early Release, a reader was reporting typos in the errata page for the book. Other
readers contributed more reports, and friends contacted me directly to offer suggestions
and corrections. The O�Reilly copyeditors will catch other errors during the production
process, which will start as soon as I manage to stop writing. I take responsibility and
apologize for any errors and suboptimal prose that remains.
I am very happy to bring this work to conclusion, mistakes and all, and I am very grateful
to everybody who helped along the way.
I hope to see you soon at some live event. Please come say hi if you see me around!
Further Reading
I will wrap up the book with references regarding what it its to be �Pythonic��the main
question this book tried to address.
Brandon Rhodes is an awesome Python teacher, and his talk �A Python ?sthetic: Beauty
and Why I Python� is beautiful, starting with the use of Unicode U+00C6 (LATIN CAP
ITAL LETTER AE) in the title. Another awesome teacher, Raymond Hettinger, spoke of
beauty in Python at PyCon US 2013: �Transforming Code into Beautiful, Idiomatic
Python�.
The Evolution of Style Guides thread that Ian Lee started on Python-ideas is worth
reading. Lee is the maintainer of the pep8 package that checks Python source code for
684 | Afterword
PEP 8 compliance. To check the code in this book, I used flake8, which wraps pep8,
pyflakes, and Ned Batchelder�s McCabe complexity plug-in.
Besides PEP 8, other influential style guides are the Google Python Style Guide and the
Pocoo style guide, from the team who brings us Flake, Sphinx, Jinja 2, and other great
Python libraries.
The Hitchhiker�s Guide to Python! is a collective work about writing Pythonic code. Its
most prolific contributor is Kenneth Reitz, a community hero thanks to his beautifully
Pythonic requests package. David Goodger presented a tutorial at PyCon US 2008
titled �Code Like a Pythonista: Idiomatic Python�. If printed, the tutorial notes are 30
pages long. Of course, the reStructuredText source is available and can be rendered to
HTML and S5 slides by docutils. After all, Goodger created both reStructuredText and
docutils�the foundations of Sphinx, Python�s excellent documentation system
(which, by the way, is also the official documentation system for MongoDB and many
other projects).
Martijn Faassen tackles the question head-on in �What is Pythonic?� In the pythonlist,
there is a thread with that same title. Martijn�s post is from 2005, and the thread
from 2003, but the Pythonic ideal hasn�t changed much�neither has the language, for
that matter. A great thread with �Pythonic� in the title is �Pythonic way to sum n-th list
element?�, from which I quoted extensively in �Soapbox� on page 302.
PEP 3099 � Things that will Not Change in Python 3000 explains why many things are
the way they are, even after the major overhaul that was Python 3. For a long time,
Python 3 was nicknamed Python 3000, but it arrived a few centuries sooner�to the
dismay of some. PEP 3099 was written by Georg Brandl, compiling many opinions
expressed by the BDFL, Guido van Rossum. The Python Essays page lists several texts
by Guido himself.
Afterword | 685

APPENDIX A
Support Scripts
Here are full listings for some scripts that were too long to fit in the main text. Also
included are scripts used to generate some of the tables and data fixtures used in this
book.
These scripts are also available in the Fluent Python code repository, along with almost
every other code snippet that appears in the book.
Chapter 3: in Operator Performance Test
Example A-1 is the code I used to produce the timings in Table 3-6 using the timeit
module. The script mostly deals with setting up the haystack and needles samples and
with formatting output.
While coding Example A-1, I found something that really puts dict performance in
perspective. If the script is run in �verbose mode� (with the -v command-line option),
the timings I get are nearly twice those in Table 3-5. But note that, in this script, �verbose
mode� means only four calls to print while setting up the test, and one additional print
to show the number of needles found when each test finishes. No output happens within
the loop that does the actual search of the needles in the haystack, but these five print
calls take about as much time as searching for 1,000 needles.
Example A-1. container_perftest.py: run it with the name of a built-in collection type as
a command-line argument (e.g., container_perftest.py dict)
"""
Container ``in`` operator performance test
"""
import sys
import timeit
SETUP = '''
687
import array
selected = array.array('d')
with open('selected.arr', 'rb') as fp:
selected.fromfile(fp, {size})
if {container_type} is dict:
haystack = dict.fromkeys(selected, 1)
else:
haystack = {container_type}(selected)
if {verbose}:
print(type(haystack), end=' ')
print('haystack: %10d' % len(haystack), end=' ')
needles = array.array('d')
with open('not_selected.arr', 'rb') as fp:
needles.fromfile(fp, 500)
needles.extend(selected[::{size}//500])
if {verbose}:
print(' needles: %10d' % len(needles), end=' ')
'''
TEST = '''
found = 0
for n in needles:
if n in haystack:
found += 1
if {verbose}:
print(' found: %10d' % found)
'''
def test(container_type, verbose):
MAX_EXPONENT = 7
for n in range(3, MAX_EXPONENT + 1):
size = 10**n
setup = SETUP.format(container_type=container_type,
size=size, verbose=verbose)
test = TEST.format(verbose=verbose)
tt = timeit.repeat(stmt=test, setup=setup, repeat=5, number=1)
print('|{:{}d}|{:f}'.format(size, MAX_EXPONENT + 1, min(tt)))
if __name__=='__main__':
if '-v' in sys.argv:
sys.argv.remove('-v')
verbose = True
else:
verbose = False
if len(sys.argv) != 2:
print('Usage: %s <container_type>' % sys.argv[0])
else:
test(sys.argv[1], verbose)
The script container_perftest_datagen.py (Example A-2) generates the data fixture for
the script in Example A-1.
688 | Appendix A: Support Scripts
Example A-2. container_perftest_datagen.py: generate files with arrays of unique floating
point numbers for use in Example A-1
"""
Generate data for container performance test
"""
import random
import array
MAX_EXPONENT = 7
HAYSTACK_LEN = 10 ** MAX_EXPONENT
NEEDLES_LEN = 10 ** (MAX_EXPONENT - 1)
SAMPLE_LEN = HAYSTACK_LEN + NEEDLES_LEN // 2
needles = array.array('d')
sample = {1/random.random() for i in range(SAMPLE_LEN)}
print('initial sample: %d elements' % len(sample))
# complete sample, in case duplicate random numbers were discarded
while len(sample) < SAMPLE_LEN:
sample.add(1/random.random())
print('complete sample: %d elements' % len(sample))
sample = array.array('d', sample)
random.shuffle(sample)
not_selected = sample[:NEEDLES_LEN // 2]
print('not selected: %d samples' % len(not_selected))
print(' writing not_selected.arr')
with open('not_selected.arr', 'wb') as fp:
not_selected.tofile(fp)
selected = sample[NEEDLES_LEN // 2:]
print('selected: %d samples' % len(selected))
print(' writing selected.arr')
with open('selected.arr', 'wb') as fp:
selected.tofile(fp)
Chapter 3: Compare the Bit Patterns of Hashes
Example A-3 is a simple script to visually show how different are the bit patterns for the
hashes of similiar floating-point numbers (e.g., 1.0001, 1.0002, etc.). Its output appears
in Example 3-16.
Example A-3. hashdiff.py: display the difference of bit paterns from hash values
import sys
MAX_BITS = len(format(sys.maxsize, 'b'))
Chapter 3: Compare the Bit Patterns of Hashes | 689
print('%s-bit Python build' % (MAX_BITS + 1))
def hash_diff(o1, o2):
h1 = '{:>0{}b}'.format(hash(o1), MAX_BITS)
h2 = '{:>0{}b}'.format(hash(o2), MAX_BITS)
diff = ''.join('!' if b1 != b2 else ' ' for b1, b2 in zip(h1, h2))
count = '!= {}'.format(diff.count('!'))
width = max(len(repr(o1)), len(repr(o2)), 8)
sep = '-' * (width * 2 + MAX_BITS)
return '{!r:{width}} {}\n{:{width}} {} {}\n{!r:{width}} {}\n{}'.format(
o1, h1, ' ' * width, diff, count, o2, h2, sep, width=width)
if __name__ == '__main__':
print(hash_diff(1, 1.0))
print(hash_diff(1.0, 1.0001))
print(hash_diff(1.0001, 1.0002))
print(hash_diff(1.0002, 1.0003))
Chapter 9: RAM Usage With and Without __slots__
The memtest.py script was used for a demostration in �Saving Space with the __slots__
Class Attribute� on page 264: Example 9-12.
The memtest.py script takes a module name in the command line and loads it. Assuming
the module defines a class named Vector, memtest.py creates a list with 10 million
instances, reporting the memory usage before and after the list is created.
Example A-4. memtest.py: create lots of Vector instances reporting memory usage
import importlib
import sys
import resource
NUM_VECTORS = 10**7
if len(sys.argv) == 2:
module_name = sys.argv[1].replace('.py', '')
module = importlib.import_module(module_name)
else:
print('Usage: {} <vector-module-to-test>'.format())
sys.exit(1)
fmt = 'Selected Vector2d type: {.__name__}.{.__name__}'
print(fmt.format(module, module.Vector2d))
mem_init = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
print('Creating {:,} Vector2d instances'.format(NUM_VECTORS))
vectors = [module.Vector2d(3.0, 4.0) for i in range(NUM_VECTORS)]
mem_final = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
690 | Appendix A: Support Scripts
print('Initial RAM usage: {:14,}'.format(mem_init))
print(' Final RAM usage: {:14,}'.format(mem_final))
Chapter 14: isis2json.py Database Conversion Script
Example A-5 is the isis2json.py script discussed in �Case Study: Generators in a Database
Conversion Utility� on page 437 (Chapter 14). It uses generator functions to lazily convert
CDS/ISIS databases to JSON for loading to CouchDB or MongoDB.
Note that this is a Python 2 script, designed to run on CPython or Jython, versions 2.5
to 2.7, but not on Python 3. Under CPython it can read only .iso files; with Jython it can
also read .mst files, using the Bruma library available on the fluentpython/isis2json
repository in GitHub. See usage documentation in that repository.
Example A-5. isis2json.py: dependencies and documentation available on GitHub
repository fluentpython/isis2json
# this script works with Python or Jython (versions >=2.5 and <3)
import sys
import argparse
from uuid import uuid4
import os
try:
import json
except ImportError:
if os.name == 'java': # running Jython
from com.xhaus.jyson import JysonCodec as json
else:
import simplejson as json
SKIP_INACTIVE = True
DEFAULT_QTY = 2**31
ISIS_MFN_KEY = 'mfn'
ISIS_ACTIVE_KEY = 'active'
SUBFIELD_DELIMITER = '^'
INPUT_ENCODING = 'cp1252'
def iter_iso_records(iso_file_name, isis_json_type):
from iso2709 import IsoFile
from subfield import expand
iso = IsoFile(iso_file_name)
for record in iso:
fields = {}
for field in record.directory:
field_key = str(int(field.tag)) # remove leading zeroes
field_occurrences = fields.setdefault(field_key, [])
content = field.value.decode(INPUT_ENCODING, 'replace')
Chapter 14: isis2json.py Database Conversion Script | 691
if isis_json_type == 1:
field_occurrences.append(content)
elif isis_json_type == 2:
field_occurrences.append(expand(content))
elif isis_json_type == 3:
field_occurrences.append(dict(expand(content)))
else:
raise NotImplementedError('ISIS-JSON type %s conversion '
'not yet implemented for .iso input' % isis_json_type)
yield fields
iso.close()
def iter_mst_records(master_file_name, isis_json_type):
try:
from bruma.master import MasterFactory, Record
except ImportError:
print('IMPORT ERROR: Jython 2.5 and Bruma.jar '
'are required to read .mst files')
raise SystemExit
mst = MasterFactory.getInstance(master_file_name).open()
for record in mst:
fields = {}
if SKIP_INACTIVE:
if record.getStatus() != Record.Status.ACTIVE:
continue
else: # save status only there are non-active records
fields[ISIS_ACTIVE_KEY] = (record.getStatus() ==
Record.Status.ACTIVE)
fields[ISIS_MFN_KEY] = record.getMfn()
for field in record.getFields():
field_key = str(field.getId())
field_occurrences = fields.setdefault(field_key, [])
if isis_json_type == 3:
content = {}
for subfield in field.getSubfields():
subfield_key = subfield.getId()
if subfield_key == '*':
content['_'] = subfield.getContent()
else:
subfield_occurrences = content.setdefault(subfield_key, [])
subfield_occurrences.append(subfield.getContent())
field_occurrences.append(content)
elif isis_json_type == 1:
content = []
for subfield in field.getSubfields():
subfield_key = subfield.getId()
if subfield_key == '*':
content.insert(0, subfield.getContent())
else:
content.append(SUBFIELD_DELIMITER + subfield_key +
692 | Appendix A: Support Scripts
subfield.getContent())
field_occurrences.append(''.join(content))
else:
raise NotImplementedError('ISIS-JSON type %s conversion '
'not yet implemented for .mst input' % isis_json_type)
yield fields
mst.close()
def write_json(input_gen, file_name, output, qty, skip, id_tag,
gen_uuid, mongo, mfn, isis_json_type, prefix,
constant):
start = skip
end = start + qty
if id_tag:
id_tag = str(id_tag)
ids = set()
else:
id_tag = ''
for i, record in enumerate(input_gen):
if i >= end:
break
if not mongo:
if i == 0:
output.write('[')
elif i > start:
output.write(',')
if start <= i < end:
if id_tag:
occurrences = record.get(id_tag, None)
if occurrences is None:
msg = 'id tag #%s not found in record %s'
if ISIS_MFN_KEY in record:
msg = msg + (' (mfn=%s)' % record[ISIS_MFN_KEY])
raise KeyError(msg % (id_tag, i))
if len(occurrences) > 1:
msg = 'multiple id tags #%s found in record %s'
if ISIS_MFN_KEY in record:
msg = msg + (' (mfn=%s)' % record[ISIS_MFN_KEY])
raise TypeError(msg % (id_tag, i))
else: # ok, we have one and only one id field
if isis_json_type == 1:
id = occurrences[0]
elif isis_json_type == 2:
id = occurrences[0][0][1]
elif isis_json_type == 3:
id = occurrences[0]['_']
if id in ids:
msg = 'duplicate id %s in tag #%s, record %s'
if ISIS_MFN_KEY in record:
msg = msg + (' (mfn=%s)' % record[ISIS_MFN_KEY])
raise TypeError(msg % (id, id_tag, i))
Chapter 14: isis2json.py Database Conversion Script | 693
record['_id'] = id
ids.add(id)
elif gen_uuid:
record['_id'] = unicode(uuid4())
elif mfn:
record['_id'] = record[ISIS_MFN_KEY]
if prefix:
# iterate over a fixed sequence of tags
for tag in tuple(record):
if str(tag).isdigit():
record[prefix+tag] = record[tag]
del record[tag] # this is why we iterate over a tuple
# with the tags, and not directly on the record dict
if constant:
constant_key, constant_value = constant.split(':')
record[constant_key] = constant_value
output.write(json.dumps(record).encode('utf-8'))
output.write('\n')
if not mongo:
output.write(']\n')
def main():
# create the parser
parser = argparse.ArgumentParser(
description='Convert an ISIS .mst or .iso file to a JSON array')
# add the arguments
parser.add_argument(
'file_name', metavar='INPUT.(mst|iso)',
help='.mst or .iso file to read')
parser.add_argument(
'-o', '--out', type=argparse.FileType('w'), default=sys.stdout,
metavar='OUTPUT.json',
help='the file where the JSON output should be written'
' (default: write to stdout)')
parser.add_argument(
'-c', '--couch', action='store_true',
help='output array within a "docs" item in a JSON document'
' for bulk insert to CouchDB via POST to db/_bulk_docs')
parser.add_argument(
'-m', '--mongo', action='store_true',
help='output individual records as separate JSON dictionaries, one'
' per line for bulk insert to MongoDB via mongoimport utility')
parser.add_argument(
'-t', '--type', type=int, metavar='ISIS_JSON_TYPE', default=1,
help='ISIS-JSON type, sets field structure: 1=string, 2=alist,'
' 3=dict (default=1)')
parser.add_argument(
'-q', '--qty', type=int, default=DEFAULT_QTY,
help='maximum quantity of records to read (default=ALL)')
parser.add_argument(
694 | Appendix A: Support Scripts
'-s', '--skip', type=int, default=0,
help='records to skip from start of .mst (default=0)')
parser.add_argument(
'-i', '--id', type=int, metavar='TAG_NUMBER', default=0,
help='generate an "_id" from the given unique TAG field number'
' for each record')
parser.add_argument(
'-u', '--uuid', action='store_true',
help='generate an "_id" with a random UUID for each record')
parser.add_argument(
'-p', '--prefix', type=str, metavar='PREFIX', default='',
help='concatenate prefix to every numeric field tag'
' (ex. 99 becomes "v99")')
parser.add_argument(
'-n', '--mfn', action='store_true',
help='generate an "_id" from the MFN of each record'
' (available only for .mst input)')
parser.add_argument(
'-k', '--constant', type=str, metavar='TAG:VALUE', default='',
help='Include a constant tag:value in every record (ex. -k type:AS)')
'''
# TODO: implement this to export large quantities of records to CouchDB
parser.add_argument(
'-r', '--repeat', type=int, default=1,
help='repeat operation, saving multiple JSON files'
' (default=1, use -r 0 to repeat until end of input)')
'''
# parse the command line
args = parser.parse_args()
if args.file_name.lower().endswith('.mst'):
input_gen_func = iter_mst_records
else:
if args.mfn:
print('UNSUPORTED: -n/--mfn option only available for .mst input.')
raise SystemExit
input_gen_func = iter_iso_records
input_gen = input_gen_func(args.file_name, args.type)
if args.couch:
args.out.write('{ "docs" : ')
write_json(input_gen, args.file_name, args.out, args.qty,
args.skip, args.id, args.uuid, args.mongo, args.mfn,
args.type, args.prefix, args.constant)
if args.couch:
args.out.write('}\n')
args.out.close()
if __name__ == '__main__':
main()
iter_iso_records generator function reads .iso file, yields records.
Chapter 14: isis2json.py Database Conversion Script | 695
iter_mst_records generator function reads .mst file, yields records.
write_json iterates over input_gen generator and outputs the .json file.
Main function reads command-line arguments then�
�selects iter_iso_records or�
�iter_mst_records depending on input file extension.
A generator object is built from the selected generator function.
write_json is called with the generator as the first argument.
Chapter 16: Taxi Fleet Discrete Event Simulation
Example A-6 is the full listing for taxi_sim.py discussed in �The Taxi Fleet Simulation�
on page 490.
Example A-6. taxi_sim.py: the taxi fleet simulator
"""
Taxi simulator
==============
Driving a taxi from the console::
>>> from taxi_sim import taxi_process
>>> taxi = taxi_process(ident=13, trips=2, start_time=0)
>>> next(taxi)
Event(time=0, proc=13, action='leave garage')
>>> taxi.send(_.time + 7)
Event(time=7, proc=13, action='pick up passenger')
>>> taxi.send(_.time + 23)
Event(time=30, proc=13, action='drop off passenger')
>>> taxi.send(_.time + 5)
Event(time=35, proc=13, action='pick up passenger')
>>> taxi.send(_.time + 48)
Event(time=83, proc=13, action='drop off passenger')
>>> taxi.send(_.time + 1)
Event(time=84, proc=13, action='going home')
>>> taxi.send(_.time + 10)
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
StopIteration
Sample run with two cars, random seed 10. This is a valid doctest::
>>> main(num_taxis=2, seed=10)
taxi: 0 Event(time=0, proc=0, action='leave garage')
taxi: 0 Event(time=5, proc=0, action='pick up passenger')
taxi: 1 Event(time=5, proc=1, action='leave garage')
taxi: 1 Event(time=10, proc=1, action='pick up passenger')
696 | Appendix A: Support Scripts
taxi: 1 Event(time=15, proc=1, action='drop off passenger')
taxi: 0 Event(time=17, proc=0, action='drop off passenger')
taxi: 1 Event(time=24, proc=1, action='pick up passenger')
taxi: 0 Event(time=26, proc=0, action='pick up passenger')
taxi: 0 Event(time=30, proc=0, action='drop off passenger')
taxi: 0 Event(time=34, proc=0, action='going home')
taxi: 1 Event(time=46, proc=1, action='drop off passenger')
taxi: 1 Event(time=48, proc=1, action='pick up passenger')
taxi: 1 Event(time=110, proc=1, action='drop off passenger')
taxi: 1 Event(time=139, proc=1, action='pick up passenger')
taxi: 1 Event(time=140, proc=1, action='drop off passenger')
taxi: 1 Event(time=150, proc=1, action='going home')
*** end of events ***
See longer sample run at the end of this module.
"""
import random
import collections
import queue
import argparse
import time
DEFAULT_NUMBER_OF_TAXIS = 3
DEFAULT_END_TIME = 180
SEARCH_DURATION = 5
TRIP_DURATION = 20
DEPARTURE_INTERVAL = 5
Event = collections.namedtuple('Event', 'time proc action')
# BEGIN TAXI_PROCESS
def taxi_process(ident, trips, start_time=0):
"""Yield to simulator issuing event at each state change"""
time = yield Event(start_time, ident, 'leave garage')
for i in range(trips):
time = yield Event(time, ident, 'pick up passenger')
time = yield Event(time, ident, 'drop off passenger')
yield Event(time, ident, 'going home')
# end of taxi process
# END TAXI_PROCESS
# BEGIN TAXI_SIMULATOR
class Simulator:
def __init__(self, procs_map):
self.events = queue.PriorityQueue()
self.procs = dict(procs_map)
Chapter 16: Taxi Fleet Discrete Event Simulation | 697
def run(self, end_time):
"""Schedule and display events until time is up"""
# schedule the first event for each cab
for _, proc in sorted(self.procs.items()):
first_event = next(proc)
self.events.put(first_event)
# main loop of the simulation
sim_time = 0
while sim_time < end_time:
if self.events.empty():
print('*** end of events ***')
break
current_event = self.events.get()
sim_time, proc_id, previous_action = current_event
print('taxi:', proc_id, proc_id * ' ', current_event)
active_proc = self.procs[proc_id]
next_time = sim_time + compute_duration(previous_action)
try:
next_event = active_proc.send(next_time)
except StopIteration:
del self.procs[proc_id]
else:
self.events.put(next_event)
else:
msg = '*** end of simulation time: {} events pending ***'
print(msg.format(self.events.qsize()))
# END TAXI_SIMULATOR
def compute_duration(previous_action):
"""Compute action duration using exponential distribution"""
if previous_action in ['leave garage', 'drop off passenger']:
# new state is prowling
interval = SEARCH_DURATION
elif previous_action == 'pick up passenger':
# new state is trip
interval = TRIP_DURATION
elif previous_action == 'going home':
interval = 1
else:
raise ValueError('Unknown previous_action: %s' % previous_action)
return int(random.expovariate(1/interval)) + 1
def main(end_time=DEFAULT_END_TIME, num_taxis=DEFAULT_NUMBER_OF_TAXIS,
seed=None):
"""Initialize random generator, build procs and run siIntroduction xix
� Discussions between proponents and opponents of agile approaches
are so heated and frequently emotional when both sides seem to
agree, on the surface, on so many things?
� �Better people� has been recognized as the most promising silver
bullet for addressing the software crisis, and yet almost all of our
energy has been spent on creating better tools, methods, and processes
instead of better people?
� Every effort to advance nonformal, iterative, artistic, and humane
ways to develop software seem to be resisted and then co-opted and
debased by formal software engineering?
� So few developers seem to be able to adopt and practice innovations
such as objects and agile methods in a manner consistent with the
intent and example of those who first advanced the innovation?
(Don�t believe this is a fair statement? Then why, two to four years
after XP and agile were introduced, is that community spending so
much time and effort wrestling with questions of certification? Why
does Alan Kay say the object revolution has yet to occur?)
The long answer to these and similar questions is this book. The short
answer, and hopefully part of your motivation for reading this book, is that software
developers tend to be so focused on what and how that they forget to
explore questions of why.
The �People Issue�
Fact 1: The most important factor in software work is not the
tools and techniques used by the programmers but rather the
quality of the programmers themselves.
Fact 2: The best programmers are up to 28 times better than the
worst programmers, according to �individual differences�
research. Given that their pay is never commensurate, they are
the biggest bargains in the software field.
�Robert L. Glass3
3. Glass, Robert L., Facts and Fallacies of Software Engineering. Boston: Addison-Wesley, 2003.mulation"""
Introduction xxv
It�s also reasonable to assume that behavioral object thinking is only implicit
in the XP/agile culture because so few books or texts were ever written in support
of this approach. Neither Beck nor Cunningham ever wrote such a book.
Rebecca Wirfs-Brock didn�t update her 1991 book describing behavior-based
object design until this year. Other efforts in this area (for example, Nancy Wilkerson�s
book on CRC cards) were not widely known and have not been updated.
This is particularly unfortunate because the CRC card �method� as described
in the early 1990s did not incorporate all aspects of object thinking. In fact, I
believe that object thinking transcends the notion of method just as it transcended
programming languages. (You can do good object programming in almost any
language, even though some languages offer you more support than others.)
Object thinking requires more than an understanding of CRC cards as presented
circa 1990. It also requires understanding some of the history and some
of the philosophical presuppositions behind object behavioralism, CRC cards,
and languages such as Smalltalk. It requires an understanding of the metaphors
that assist in good object thinking and an extension of the CRC card metaphor,
in particular, to include more than object identification and responsibility
assignment.
It is my hope that this book will promote such an understanding by capturing
at least part of the oral tradition of behavioral objects and making it
explicit.
XP and Object Thinking
This book is based on the following beliefs:
� Agility, especially in the form of extreme programming, is essential if
the software development profession, and industry, are to improve
themselves.
� XP offers a way to create �better developers� to improve the intrinsic
abilities of human beings to develop software for use by other
human beings�a way to produce �master� developers.
� XP cannot be understood, and those practicing XP will not realize
the full potential of the approach, until they understand object thinking
and the shared historical and philosophical roots of both object
thinking and XP core values and practices.
� In particular, programmers and technical developers will fail to realize
the full potential of XP-based development without a thorough
understanding of object orientation�the �object thinking� promised
by the title of this book.
2 Object Thinking
The short quote from Grady Booch that opens this chapter contains three
key phrases that provide a framework for the discussion in the remainder of
this chapter:
� �fundamentally different� Which raises the question of how
object-oriented design�s difference manifests itself and whether, in
some aspects of development, this difference is more obvious or
important than in other aspects.
� �different way of thinking� Not only about decomposition but
about all aspects of development. Of course, this is the theme of this
entire book.
� �structured design culture� By explicitly mentioning a structured
design culture, Grady implies the existence of a corresponding object
culture. There is an object culture and an extreme programming culture,
and both cultures share a common heritage. Culture is a critical factor in
how and what we think as human beings and as software developers.
Observing the Object Difference
Is Figure 1-1 or Figure 1-2 a better visual depiction of an object?
F01MQ01 Figure 1-1 A graphical object model using UML graphical syntax.
Data
Structure
operationX
operationY
operationZ
Encapsulation
BarrierDive into Deep Learning
Release 0.14.3
Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola
Aug 11, 2020

Contents
Preface 1
Installation 9
Notation 13
1 Introduction 17
1.1 A Motivating Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.2 The Key Components: Data, Models, and Algorithms . . . . . . . . . . . . . . . . 20
1.3 Kinds of Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.4 Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
1.5 The Road to Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.6 Success Stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2 Preliminaries 43
2.1 Data Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.1.1 Getting Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
2.1.2 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.1.3 Broadcasting Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.1.4 Indexing and Slicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.1.5 Saving Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
2.1.6 Conversion to Other Python Objects . . . . . . . . . . . . . . . . . . . . . 50
2.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.2.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.2.2 Handling Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.2.3 Conversion to the Tensor Format . . . . . . . . . . . . . . . . . . . . . . . 53
2.3 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.3.1 Scalars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.3.2 Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.3.3 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.3.4 Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.3.5 Basic Properties of Tensor Arithmetic . . . . . . . . . . . . . . . . . . . . 58
2.3.6 Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2.3.7 Dot Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
2.3.8 Matrix-Vector Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
2.3.9 Matrix-Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . 62
2.3.10 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
2.3.11 More on Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
2.4 Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
2.4.1 Derivatives and Differentiation . . . . . . . . . . . . . . . . . . . . . . . . 67
2.4.2 Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
i
2.4.3 Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
2.4.4 Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
2.5 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
2.5.1 A Simple Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
2.5.2 Backward for Non-Scalar Variables . . . . . . . . . . . . . . . . . . . . . . 74
2.5.3 Detaching Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.5.4 Computing the Gradient of Python Control Flow . . . . . . . . . . . . . . 75
2.6 Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
2.6.1 Basic Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
2.6.2 Dealing with Multiple Random Variables . . . . . . . . . . . . . . . . . . 81
2.6.3 Expectation and Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
2.7 Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
2.7.1 Finding All the Functions and Classes in a Module . . . . . . . . . . . . . 85
2.7.2 Finding the Usage of Specific Functions and Classes . . . . . . . . . . . . 85
3 Linear Neural Networks 89
3.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
3.1.1 Basic Elements of Linear Regression . . . . . . . . . . . . . . . . . . . . . 89
3.1.2 Vectorization for Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
3.1.3 The Normal Distribution and Squared Loss . . . . . . . . . . . . . . . . . 95
3.1.4 From Linear Regression to Deep Networks . . . . . . . . . . . . . . . . . 96
3.2 Linear Regression Implementation from Scratch . . . . . . . . . . . . . . . . . . 99
3.2.1 Generating the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
3.2.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.2.3 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 101
3.2.4 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.2.5 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.2.6 Defining the Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . 102
3.2.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
3.3 Concise Implementation of Linear Regression . . . . . . . . . . . . . . . . . . . . 105
3.3.1 Generating the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.3.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.3.3 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
3.3.4 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 107
3.3.5 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.3.6 Defining the Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . 107
3.3.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.4 Softmax Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.4.1 Classification Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.4.2 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.4.3 Softmax Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
3.4.4 Vectorization for Minibatches . . . . . . . . . . . . . . . . . . . . . . . . 112
3.4.5 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.4.6 Information Theory Basics . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.4.7 Model Prediction and Evaluation . . . . . . . . . . . . . . . . . . . . . . . 115
3.5 The Image Classification Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.5.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.5.2 Reading a Minibatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.5.3 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 118
3.6 Implementation of Softmax Regression from Scratch . . . . . . . . . . . . . . . . 119
3.6.1 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 119
ii
3.6.2 Defining the Softmax Operation . . . . . . . . . . . . . . . . . . . . . . . 120
3.6.3 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.6.4 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.6.5 Classification Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
3.6.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.6.7 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
3.7 Concise Implementation of Softmax Regression . . . . . . . . . . . . . . . . . . . 126
3.7.1 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 127
3.7.2 Softmax Implementation Revisited . . . . . . . . . . . . . . . . . . . . . . 127
3.7.3 Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.7.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
4 Multilayer Perceptrons 131
4.1 Multilayer Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.1.1 Hidden Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.1.2 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
4.2 Implementation of Multilayer Perceptrons from Scratch . . . . . . . . . . . . . . 139
4.2.1 Initializing Model Parameters . . . . . . . . . . . . . . . . . . . . . . . . 139
4.2.2 Activation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.2.3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.2.4 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.2.5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.3 Concise Implementation of Multilayer Perceptrons . . . . . . . . . . . . . . . . . 142
4.3.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
4.4 Model Selection, Underfitting, and Overfitting . . . . . . . . . . . . . . . . . . . . 144
4.4.1 Training Error and Generalization Error . . . . . . . . . . . . . . . . . . . 144
4.4.2 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
4.4.3 Underfitting or Overfitting? . . . . . . . . . . . . . . . . . . . . . . . . . . 148
4.4.4 Polynomial Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
4.5 Weight Decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.5.1 Norms and Weight Decay . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.5.2 High-Dimensional Linear Regression . . . . . . . . . . . . . . . . . . . . 156
4.5.3 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 156
4.5.4 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
4.6 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
4.6.1 Overfitting Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
4.6.2 Robustness through Perturbations . . . . . . . . . . . . . . . . . . . . . . 162
4.6.3 Dropout in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
4.6.4 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 163
4.6.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.7 Forward Propagation, Backward Propagation, and Computational Graphs . . . . . 167
4.7.1 Forward Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
4.7.2 Computational Graph of Forward Propagation . . . . . . . . . . . . . . . . 168
4.7.3 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
4.7.4 Training Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
4.8 Numerical Stability and Initialization . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.8.1 Vanishing and Exploding Gradients . . . . . . . . . . . . . . . . . . . . . 172
4.8.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
4.9 Environment and Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . . . 176
4.9.1 Types of Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . . . 177
4.9.2 Examples of Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . 179
iii
4.9.3 Correction of Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . 181
4.9.4 A Taxonomy of Learning Problems . . . . . . . . . . . . . . . . . . . . . . 184
4.9.5 Fairness, Accountability, and Transparency in Machine Learning . . . . . 186
4.10 Predicting House Prices on Kaggle . . . . . . . . . . . . . . . . . . . . . . . . . . 187
4.10.1 Downloading and Caching Datasets . . . . . . . . . . . . . . . . . . . . . 187
4.10.2 Kaggle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
4.10.3 Accessing and Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . 190
4.10.4 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
4.10.5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
4.10.6 K-Fold Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
4.10.7 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
4.10.8 Submitting Predictions on Kaggle . . . . . . . . . . . . . . . . . . . . . . 195
5 Deep Learning Computation 199
5.1 Layers and Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
5.1.1 A Custom Block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
5.1.2 The Sequential Block . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
5.1.3 Executing Code in the Forward Propagation Function . . . . . . . . . . . . 204
5.1.4 Compilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
5.2 Parameter Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
5.2.1 Parameter Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
5.2.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
5.2.3 Tied Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
5.3 Deferred Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
5.3.1 Instantiating a Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
5.4 Custom Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
5.4.1 Layers without Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 216
5.4.2 Layers with Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
5.5 File I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.5.1 Loading and Saving Tensors . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.5.2 Loading and Saving Model Parameters . . . . . . . . . . . . . . . . . . . . 219
5.6 GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
5.6.1 Computing Devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
5.6.2 Tensors and GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
5.6.3 Neural Networks and GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6 Convolutional Neural Networks 227
6.1 From Fully-Connected Layers to Convolutions . . . . . . . . . . . . . . . . . . . . 228
6.1.1 Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.1.2 Constraining the MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
6.1.3 Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
6.1.4 �Where?s Waldo� Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . 231
6.2 Convolutions for Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
6.2.1 The Cross-Correlation Operation . . . . . . . . . . . . . . . . . . . . . . . 233
6.2.2 Convolutional Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
6.2.3 Object Edge Detection in Images . . . . . . . . . . . . . . . . . . . . . . . 235
6.2.4 Learning a Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6.2.5 Cross-Correlation and Convolution . . . . . . . . . . . . . . . . . . . . . . 237
6.2.6 Feature Map and Receptive Field . . . . . . . . . . . . . . . . . . . . . . . 238
6.3 Padding and Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
6.3.1 Padding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
iv
6.3.2 Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
6.4 Multiple Input and Multiple Output Channels . . . . . . . . . . . . . . . . . . . . 243
6.4.1 Multiple Input Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
6.4.2 Multiple Output Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
6.4.3 1  1 Convolutional Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
6.5 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
6.5.1 Maximum Pooling and Average Pooling . . . . . . . . . . . . . . . . . . . 247
6.5.2 Padding and Stride . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
6.5.3 Multiple Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
6.6 Convolutional Neural Networks (LeNet) . . . . . . . . . . . . . . . . . . . . . . . 251
6.6.1 LeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
6.6.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
7 Modern Convolutional Neural Networks 257
7.1 Deep Convolutional Neural Networks (AlexNet) . . . . . . . . . . . . . . . . . . . 257
7.1.1 Learning Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
7.1.2 AlexNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
7.1.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.1.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.2 Networks Using Blocks (VGG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.2.1 VGG Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
7.2.2 VGG Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
7.2.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
7.3 Network in Network (NiN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.3.1 NiN Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.3.2 NiN Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
7.3.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.4 Networks with Parallel Concatenations (GoogLeNet) . . . . . . . . . . . . . . . . 274
7.4.1 Inception Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
7.4.2 GoogLeNet Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
7.4.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
7.5 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
7.5.1 Training Deep Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
7.5.2 Batch Normalization Layers . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.5.3 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 282
7.5.4 Applying Batch Normalization in LeNet . . . . . . . . . . . . . . . . . . . 283
7.5.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
7.5.6 Controversy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
7.6 Residual Networks (ResNet) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
7.6.1 Function Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
7.6.2 Residual Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
7.6.3 ResNet Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
7.6.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
7.7 Densely Connected Networks (DenseNet) . . . . . . . . . . . . . . . . . . . . . . 294
7.7.1 From ResNet to DenseNet . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
7.7.2 Dense Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
7.7.3 Transition Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
7.7.4 DenseNet Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
7.7.5 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
8 Recurrent Neural Networks 299
v
8.1 Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.1.1 Statistical Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
8.1.2 A Toy Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
8.1.3 Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
8.2 Text Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
8.2.1 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
8.2.2 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.2.3 Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.2.4 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 310
8.3 Language Models and the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
8.3.1 Estimating a Language Model . . . . . . . . . . . . . . . . . . . . . . . . . 311
8.3.2 Markov Models and n-grams . . . . . . . . . . . . . . . . . . . . . . . . . 312
8.3.3 Natural Language Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 312
8.3.4 Training Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . 315
8.4 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
8.4.1 Recurrent Networks Without Hidden States . . . . . . . . . . . . . . . . . 319
8.4.2 Recurrent Networks with Hidden States . . . . . . . . . . . . . . . . . . . 320
8.4.3 Steps in a Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . 321
8.4.4 Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
8.5 Implementation of Recurrent Neural Networks from Scratch . . . . . . . . . . . . 323
8.5.1 One-hot Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
8.5.2 Initializing the Model Parameters . . . . . . . . . . . . . . . . . . . . . . 324
8.5.3 RNN Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
8.5.4 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
8.5.5 Gradient Clipping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
8.5.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
8.6 Concise Implementation of Recurrent Neural Networks . . . . . . . . . . . . . . . 331
8.6.1 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
8.6.2 Training and Predicting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
8.7 Backpropagation Through Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
8.7.1 A Simplified Recurrent Network . . . . . . . . . . . . . . . . . . . . . . . 334
8.7.2 The Computational Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
8.7.3 BPTT in Detail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9 Modern Recurrent Neural Networks 339
9.1 Gated Recurrent Units (GRU) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
9.1.1 Gating the Hidden State . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
9.1.2 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 342
9.1.3 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
9.2 Long Short Term Memory (LSTM) . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
9.2.1 Gated Memory Cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
9.2.2 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 350
9.2.3 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.3 Deep Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
9.3.1 Functional Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
9.3.2 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.3.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.4 Bidirectional Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 356
9.4.1 Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
9.4.2 Bidirectional Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
9.5 Machine Translation and the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . 362
vi
9.5.1 Reading and Preprocessing the Dataset . . . . . . . . . . . . . . . . . . . 363
9.5.2 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
9.5.3 Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
9.5.4 Loading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
9.5.5 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 366
9.6 Encoder-Decoder Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.6.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.6.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.6.3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
9.7 Sequence to Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
9.7.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
9.7.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
9.7.3 The Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
9.7.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
9.7.5 Predicting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
9.8 Beam Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
9.8.1 Greedy Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
9.8.2 Exhaustive Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
9.8.3 Beam Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
10 Attention Mechanisms 381
10.1 Attention Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
10.1.1 Dot Product Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
10.1.2 MLP Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
10.2 Sequence to Sequence with Attention Mechanisms . . . . . . . . . . . . . . . . . 386
10.2.1 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
10.2.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
10.3 Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
10.3.1 Multi-Head Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
10.3.2 Position-wise Feed-Forward Networks . . . . . . . . . . . . . . . . . . . . 394
10.3.3 Add and Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
10.3.4 Positional Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
10.3.5 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
10.3.6 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
10.3.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
11 Optimization Algorithms 405
11.1 Optimization and Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
11.1.1 Optimization and Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 406
11.1.2 Optimization Challenges in Deep Learning . . . . . . . . . . . . . . . . . 407
11.2 Convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
11.2.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
11.2.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
11.2.3 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
11.3 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
11.3.1 Gradient Descent in One Dimension . . . . . . . . . . . . . . . . . . . . . 420
11.3.2 Multivariate Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 423
11.3.3 Adaptive Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
11.4 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
11.4.1 Stochastic Gradient Updates . . . . . . . . . . . . . . . . . . . . . . . . . 429
11.4.2 Dynamic Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
vii
11.4.3 Convergence Analysis for Convex Objectives . . . . . . . . . . . . . . . . 433
11.4.4 Stochastic Gradients and Finite Samples . . . . . . . . . . . . . . . . . . . 434
11.5 Minibatch Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 436
11.5.1 Vectorization and Caches . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
11.5.2 Minibatches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
11.5.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
11.5.4 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 439
11.5.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
11.6 Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
11.6.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
11.6.2 Practical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
11.6.3 Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
11.7 Adagrad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
11.7.1 Sparse Features and Learning Rates . . . . . . . . . . . . . . . . . . . . . 455
11.7.2 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
11.7.3 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
11.7.4 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 458
11.7.5 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
11.8 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
11.8.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
11.8.2 Implementation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . 462
11.8.3 Concise Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
11.9 Adadelta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
11.9.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
11.9.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
11.10 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
11.10.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
11.10.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
11.10.3 Yogi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
11.11 Learning Rate Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
11.11.1 Toy Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
11.11.2 Schedulers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
11.11.3 Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
12 Computational Performance 483
12.1 Compilers and Interpreters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483
12.1.1 Symbolic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
12.1.2 Hybrid Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
12.1.3 HybridSequential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
12.2 Asynchronous Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
12.2.1 Asynchrony via Backend . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
12.2.2 Barriers and Blockers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
12.2.3 Improving Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
12.2.4 Improving Memory Footprint . . . . . . . . . . . . . . . . . . . . . . . . . 494
12.3 Automatic Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
12.3.1 Parallel Computation on CPUs and GPUs . . . . . . . . . . . . . . . . . . . 497
12.3.2 Parallel Computation and Communication . . . . . . . . . . . . . . . . . 498
12.4 Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
12.4.1 Computers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
12.4.2 Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
12.4.3 Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
viii
12.4.4 CPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
12.4.5 GPUs and other Accelerators . . . . . . . . . . . . . . . . . . . . . . . . . 507
12.4.6 Networks and Buses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
12.4.7 More Latency Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
12.5 Training on Multiple GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
12.5.1 Splitting the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
12.5.2 Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
12.5.3 A Toy Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
12.5.4 Data Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
12.5.5 Distributing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
12.5.6 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
12.5.7 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
12.6 Concise Implementation for Multiple GPUs . . . . . . . . . . . . . . . . . . . . . 522
12.6.1 A Toy Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
12.6.2 Parameter Initialization and Logistics . . . . . . . . . . . . . . . . . . . . 523
12.6.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
12.6.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526
12.7 Parameter Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
12.7.1 Data Parallel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
12.7.2 Ring Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
12.7.3 Multi-Machine Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533
12.7.4 (key,value) Stores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
13 Computer Vision 537
13.1 Image Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
13.1.1 Common Image Augmentation Method . . . . . . . . . . . . . . . . . . . 538
13.1.2 Using an Image Augmentation Training Model . . . . . . . . . . . . . . . 542
13.2 Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
13.2.1 Hot Dog Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
13.3 Object Detection and Bounding Boxes . . . . . . . . . . . . . . . . . . . . . . . . 552
13.3.1 Bounding Box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
13.4 Anchor Boxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
13.4.1 Generating Multiple Anchor Boxes . . . . . . . . . . . . . . . . . . . . . . 555
13.4.2 Intersection over Union . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
13.4.3 Labeling Training Set Anchor Boxes . . . . . . . . . . . . . . . . . . . . . 557
13.4.4 Bounding Boxes for Prediction . . . . . . . . . . . . . . . . . . . . . . . . 561
13.5 Multiscale Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
13.6 The Object Detection Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
13.6.1 Downloading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
13.6.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
13.6.3 Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
13.7 Single Shot Multibox Detection (SSD) . . . . . . . . . . . . . . . . . . . . . . . . . 570
13.7.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
13.7.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575
13.7.3 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578
13.8 Region-based CNNs (R-CNNs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
13.8.1 R-CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
13.8.2 Fast R-CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582
13.8.3 Faster R-CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
13.8.4 Mask R-CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
13.9 Semantic Segmentation and the Dataset . . . . . . . . . . . . . . . . . . . . . . . 587
ix
13.9.1 Image Segmentation and Instance Segmentation . . . . . . . . . . . . . . 587
13.9.2 The Pascal VOC2012 Semantic Segmentation Dataset . . . . . . . . . . . . 588
13.10 Transposed Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
13.10.1 Basic 2D Transposed Convolution . . . . . . . . . . . . . . . . . . . . . . 593
13.10.2 Padding, Strides, and Channels . . . . . . . . . . . . . . . . . . . . . . . . 594
13.10.3 Analogy to Matrix Transposition . . . . . . . . . . . . . . . . . . . . . . . 595
13.11 Fully Convolutional Networks (FCN) . . . . . . . . . . . . . . . . . . . . . . . . . 596
13.11.1 Constructing a Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597
13.11.2 Initializing the Transposed Convolution Layer . . . . . . . . . . . . . . . . 599
13.11.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
13.11.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
13.11.5 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601
13.12 Neural Style Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
13.12.1 Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604
13.12.2 Reading the Content and Style Images . . . . . . . . . . . . . . . . . . . . 605
13.12.3 Preprocessing and Postprocessing . . . . . . . . . . . . . . . . . . . . . . 606
13.12.4 Extracting Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
13.12.5 Defining the Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . 607
13.12.6 Creating and Initializing the Composite Image . . . . . . . . . . . . . . . 609
13.12.7 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610
13.13 Image Classification (CIFAR-10) on Kaggle . . . . . . . . . . . . . . . . . . . . . . 613
13.13.1 Obtaining and Organizing the Dataset . . . . . . . . . . . . . . . . . . . . 614
13.13.2 Image Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616
13.13.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617
13.13.4 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618
13.13.5 Defining the Training Functions . . . . . . . . . . . . . . . . . . . . . . . 619
13.13.6 Training and Validating the Model . . . . . . . . . . . . . . . . . . . . . . 620
13.13.7 Classifying the Testing Set and Submitting Results on Kaggle . . . . . . . . 620
13.14 Dog Breed Identification (ImageNet Dogs) on Kaggle . . . . . . . . . . . . . . . . 622
13.14.1 Obtaining and Organizing the Dataset . . . . . . . . . . . . . . . . . . . . 623
13.14.2 Image Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
13.14.3 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625
13.14.4 Defining the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625
13.14.5 Defining the Training Functions . . . . . . . . . . . . . . . . . . . . . . . 626
13.14.6 Training and Validating the Model . . . . . . . . . . . . . . . . . . . . . . 627
13.14.7 Classifying the Testing Set and Submitting Results on Kaggle . . . . . . . . 627
14 Natural Language Processing: Pretraining 631
14.1 Word Embedding (word2vec) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
14.1.1 Why Not Use One-hot Vectors? . . . . . . . . . . . . . . . . . . . . . . . . 632
14.1.2 The Skip-Gram Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
14.1.3 The Continuous Bag of Words (CBOW) Model . . . . . . . . . . . . . . . . 634
14.2 Approximate Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636
14.2.1 Negative Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637
14.2.2 Hierarchical Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638
14.3 The Dataset for Pretraining Word Embedding . . . . . . . . . . . . . . . . . . . . 639
14.3.1 Reading and Preprocessing the Dataset . . . . . . . . . . . . . . . . . . . 639
14.3.2 Subsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640
14.3.3 Loading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642
14.3.4 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 645
14.4 Pretraining word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646
x
14.4.1 The Skip-Gram Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647
14.4.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648
14.4.3 Applying the Word Embedding Model . . . . . . . . . . . . . . . . . . . . 650
14.5 Word Embedding with Global Vectors (GloVe) . . . . . . . . . . . . . . . . . . . . 651
14.5.1 The GloVe Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652
14.5.2 Understanding GloVe from Conditional Probability Ratios . . . . . . . . . 653
14.6 Subword Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
14.6.1 fastText . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
14.6.2 Byte Pair Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
14.7 Finding Synonyms and Analogies . . . . . . . . . . . . . . . . . . . . . . . . . . . 658
14.7.1 Using Pretrained Word Vectors . . . . . . . . . . . . . . . . . . . . . . . . 659
14.7.2 Applying Pretrained Word Vectors . . . . . . . . . . . . . . . . . . . . . . 660
14.8 Bidirectional Encoder Representations from Transformers (BERT) . . . . . . . . . 663
14.8.1 From Context-Independent to Context-Sensitive . . . . . . . . . . . . . . 663
14.8.2 From Task-Specific to Task-Agnostic . . . . . . . . . . . . . . . . . . . . . 663
14.8.3 BERT: Combining the Best of Both Worlds . . . . . . . . . . . . . . . . . . 664
14.8.4 Input Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 665
14.8.5 Pretraining Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667
14.8.6 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 670
14.9 The Dataset for Pretraining BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . 671
14.9.1 Defining Helper Functions for Pretraining Tasks . . . . . . . . . . . . . . 672
14.9.2 Transforming Text into the Pretraining Dataset . . . . . . . . . . . . . . . 674
14.10 Pretraining BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677
14.10.1 Pretraining BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677
14.10.2 Representing Text with BERT . . . . . . . . . . . . . . . . . . . . . . . . . 679
15 Natural Language Processing: Applications 683
15.1 Sentiment Analysis and the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 684
15.1.1 The Sentiment Analysis Dataset . . . . . . . . . . . . . . . . . . . . . . . 684
15.1.2 Putting All Things Together . . . . . . . . . . . . . . . . . . . . . . . . . . 687
15.2 Sentiment Analysis: Using Recurrent Neural Networks . . . . . . . . . . . . . . . 687
15.2.1 Using a Recurrent Neural Network Model . . . . . . . . . . . . . . . . . . 688
15.3 Sentiment Analysis: Using Convolutional Neural Networks . . . . . . . . . . . . . 691
15.3.1 One-Dimensional Convolutional Layer . . . . . . . . . . . . . . . . . . . . 692
15.3.2 Max-Over-Time Pooling Layer . . . . . . . . . . . . . . . . . . . . . . . . 694
15.3.3 The TextCNN Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695
15.4 Natural Language Inference and the Dataset . . . . . . . . . . . . . . . . . . . . . 698
15.4.1 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . 698
15.4.2 The Stanford Natural Language Inference (SNLI) Dataset . . . . . . . . . . 699
15.5 Natural Language Inference: Using Attention . . . . . . . . . . . . . . . . . . . . 703
15.5.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703
15.5.2 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 707
15.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications . . . . . . . . 710
15.6.1 Single Text Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 710
15.6.2 Text Pair Classification or Regression . . . . . . . . . . . . . . . . . . . . 711
15.6.3 Text Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 712
15.6.4 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
15.7 Natural Language Inference: Fine-Tuning BERT . . . . . . . . . . . . . . . . . . . 715
15.7.1 Loading Pretrained BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . 716
15.7.2 The Dataset for Fine-Tuning BERT . . . . . . . . . . . . . . . . . . . . . . 717
15.7.3 Fine-Tuning BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718
xi
16 Recommender Systems 721
16.1 Overview of Recommender Systems . . . . . . . . . . . . . . . . . . . . . . . . . 721
16.1.1 Collaborative Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722
16.1.2 Explicit Feedback and Implicit Feedback . . . . . . . . . . . . . . . . . . 723
16.1.3 Recommendation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723
16.2 The MovieLens Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724
16.2.1 Getting the Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724
16.2.2 Statistics of the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 725
16.2.3 Splitting the dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 726
16.2.4 Loading the data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727
16.3 Matrix Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 728
16.3.1 The Matrix Factorization Model . . . . . . . . . . . . . . . . . . . . . . . 729
16.3.2 Model Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730
16.3.3 Evaluation Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730
16.3.4 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 731
16.4 AutoRec: Rating Prediction with Autoencoders . . . . . . . . . . . . . . . . . . . 733
16.4.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733
16.4.2 Implementing the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 734
16.4.3 Reimplementing the Evaluator . . . . . . . . . . . . . . . . . . . . . . . . 734
16.4.4 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 735
16.5 Personalized Ranking for Recommender Systems . . . . . . . . . . . . . . . . . . 736
16.5.1 Bayesian Personalized Ranking Loss and its Implementation . . . . . . . 737
16.5.2 Hinge Loss and its Implementation . . . . . . . . . . . . . . . . . . . . . 738
16.6 Neural Collaborative Filtering for Personalized Ranking . . . . . . . . . . . . . . 739
16.6.1 The NeuMF model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 740
16.6.2 Model Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 741
16.6.3 Customized Dataset with Negative Sampling . . . . . . . . . . . . . . . . . 742
16.6.4 Evaluator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742
16.6.5 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 744
16.7 Sequence-Aware Recommender Systems . . . . . . . . . . . . . . . . . . . . . . . 746
16.7.1 Model Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746
16.7.2 Model Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748
16.7.3 Sequential Dataset with Negative Sampling . . . . . . . . . . . . . . . . . 749
16.7.4 Load the MovieLens 100K dataset . . . . . . . . . . . . . . . . . . . . . . 750
16.7.5 Train the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 751
16.8 Feature-Rich Recommender Systems . . . . . . . . . . . . . . . . . . . . . . . . . 752
16.8.1 An Online Advertising Dataset . . . . . . . . . . . . . . . . . . . . . . . . 753
16.8.2 Dataset Wrapper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753
16.9 Factorization Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 755
16.9.1 2-Way Factorization Machines . . . . . . . . . . . . . . . . . . . . . . . . 755
16.9.2 An Efficient Optimization Criterion . . . . . . . . . . . . . . . . . . . . . 756
16.9.3 Model Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 756
16.9.4 Load the Advertising Dataset . . . . . . . . . . . . . . . . . . . . . . . . . 757
16.9.5 Train the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757
16.10 Deep Factorization Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 758
16.10.1 Model Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 759
16.10.2 Implemenation of DeepFM . . . . . . . . . . . . . . . . . . . . . . . . . . 760
16.10.3 Training and Evaluating the Model . . . . . . . . . . . . . . . . . . . . . . 761
17 Generative Adversarial Networks 763
17.1 Generative Adversarial Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 763
xii
17.1.1 Generate some �real� data . . . . . . . . . . . . . . . . . . . . . . . . . . . 765
17.1.2 Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766
17.1.3 Discriminator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766
17.1.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766
17.2 Deep Convolutional Generative Adversarial Networks . . . . . . . . . . . . . . . . 769
17.2.1 The Pokemon Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 769
17.2.2 The Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770
17.2.3 Discriminator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 772
17.2.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773
18 Appendix: Mathematics for Deep Learning 777
18.1 Geometry and Linear Algebraic Operations . . . . . . . . . . . . . . . . . . . . . 778
18.1.1 Geometry of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778
18.1.2 Dot Products and Angles . . . . . . . . . . . . . . . . . . . . . . . . . . . 780
18.1.3 Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 782
18.1.4 Geometry of Linear Transformations . . . . . . . . . . . . . . . . . . . . 785
18.1.5 Linear Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787
18.1.6 Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787
18.1.7 Invertibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 788
18.1.8 Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 789
18.1.9 Tensors and Common Linear Algebra Operations . . . . . . . . . . . . . . 790
18.2 Eigendecompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794
18.2.1 Finding Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794
18.2.2 Decomposing Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795
18.2.3 Operations on Eigendecompositions . . . . . . . . . . . . . . . . . . . . . 795
18.2.4 Eigendecompositions of Symmetric Matrices . . . . . . . . . . . . . . . . 796
18.2.5 Gershgorin Circle Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 796
18.2.6 A Useful Application: The Growth of Iterated Maps . . . . . . . . . . . . . 797
18.2.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
18.3 Single Variable Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803
18.3.1 Differential Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803
18.3.2 Rules of Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 806
18.4 Multivariable Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 813
18.4.1 Higher-Dimensional Differentiation . . . . . . . . . . . . . . . . . . . . . 814
18.4.2 Geometry of Gradients and Gradient Descent . . . . . . . . . . . . . . . . 815
18.4.3 A Note on Mathematical Optimization . . . . . . . . . . . . . . . . . . . . 816
18.4.4 Multivariate Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
18.4.5 The Backpropagation Algorithm . . . . . . . . . . . . . . . . . . . . . . . 819
18.4.6 Hessians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 822
18.4.7 A Little Matrix Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 824
18.5 Integral Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 829
18.5.1 Geometric Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . 829
18.5.2 The Fundamental Theorem of Calculus . . . . . . . . . . . . . . . . . . . 831
18.5.3 Change of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833
18.5.4 A Comment on Sign Conventions . . . . . . . . . . . . . . . . . . . . . . . 834
18.5.5 Multiple Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 835
18.5.6 Change of Variables in Multiple Integrals . . . . . . . . . . . . . . . . . . 837
18.6 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 838
18.6.1 Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . . . . 838
18.7 Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855
18.7.1 The Maximum Likelihood Principle . . . . . . . . . . . . . . . . . . . . . 856
xiii
18.7.2 Numerical Optimization and the Negative Log-Likelihood . . . . . . . . . 857
18.7.3 Maximum Likelihood for Continuous Variables . . . . . . . . . . . . . . . 859
18.8 Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 861
18.8.1 Bernoulli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 861
18.8.2 Discrete Uniform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863
18.8.3 Continuous Uniform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 864
18.8.4 Binomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 866
18.8.5 Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 868
18.8.6 Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 871
18.8.7 Exponential Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 874
18.9 Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 875
18.9.1 Optical Character Recognition . . . . . . . . . . . . . . . . . . . . . . . . 876
18.9.2 The Probabilistic Model for Classification . . . . . . . . . . . . . . . . . . 877
18.9.3 The Naive Bayes Classifier . . . . . . . . . . . . . . . . . . . . . . . . . . 877
18.9.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 878
18.10 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882
18.10.1 Evaluating and Comparing Estimators . . . . . . . . . . . . . . . . . . . . 882
18.10.2 Conducting Hypothesis Tests . . . . . . . . . . . . . . . . . . . . . . . . . 886
18.10.3 Constructing Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . 890
18.11 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 893
18.11.1 Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 893
18.11.2 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 895
18.11.3 Mutual Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 897
18.11.4 Kullback�Leibler Divergence . . . . . . . . . . . . . . . . . . . . . . . . . 901
18.11.5 Cross Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 903
19 Appendix: Tools for Deep Learning 907
19.1 Using Jupyter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 907
19.1.1 Editing and Running the Code Locally . . . . . . . . . . . . . . . . . . . . 907
19.1.2 Advanced Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 911
19.2 Using Amazon SageMaker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912
19.2.1 Registering and Logging In . . . . . . . . . . . . . . . . . . . . . . . . . . 912
19.2.2 Creating a SageMaker Instance . . . . . . . . . . . . . . . . . . . . . . . . 913
19.2.3 Running and Stopping an Instance . . . . . . . . . . . . . . . . . . . . . . 914
19.2.4 Updating Notebooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 915
19.3 Using AWS EC2 Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 916
19.3.1 Creating and Running an EC2 Instance . . . . . . . . . . . . . . . . . . . . 916
19.3.2 Installing CUDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921
19.3.3 Installing MXNet and Downloading the D2L Notebooks . . . . . . . . . . . 922
19.3.4 Running Jupyter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 923
19.3.5 Closing Unused Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . 924
19.4 Using Google Colab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 924
19.5 Selecting Servers and GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 925
19.5.1 Selecting Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 926
19.5.2 Selecting GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927
19.6 Contributing to This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 930
19.6.1 Minor Text Changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 930
19.6.2 Propose a Major Change . . . . . . . . . . . . . . . . . . . . . . . . . . . 930
19.6.3 Adding a New Section or a New Framework Implementation . . . . . . . . 931
19.6.4 Submitting a Major Change . . . . . . . . . . . . . . . . . . . . . . . . . . 931
19.7 d2l API Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 935
xiv
Bibliography 955
Python Module Index 965
Index 967
xv
xvi
Preface
Just a few years ago, there were no legions of deep learning scientists developing intelligent products
and services at major companies and startups. When the youngest among us (the authors)
entered the field, machine learning did not command headlines in daily newspapers. Our parents
had no idea what machine learning was, let alone why we might prefer it to a career in medicine or
law. Machine learning was a forward-looking academic discipline with a narrow set of real-world
applications. And those applications, e.g., speech recognition and computer vision, required so
much domain knowledge that they were often regarded as separate areas entirely for which machine
learning was one small component. Neural networks then, the antecedents of the deep
learning models that we focus on in this book, were regarded as outmoded tools.
In just the past five years, deep learning has taken the world by surprise, driving rapid progress
in fields as diverse as computer vision, natural language processing, automatic speech recognition,
reinforcement learning, and statistical modeling. With these advances in hand, we can now
build cars that drive themselves with more autonomy than ever before (and less autonomy than
some companies might have you believe), smart reply systems that automatically draft the most
mundane emails, helping people dig out from oppressively large inboxes, and software agents that
dominate the world?s best humans at board games like Go, a feat once thought to be decades away.
Already, these tools exert ever-wider impacts on industry and society, changing the way movies
are made, diseases are diagnosed, and playing a growing role in basic sciences�from astrophysics
to biology.
About This Book
This book represents our attempt to make deep learning approachable, teaching you the concepts,
the context, and the code.
One Medium Combining Code, Math, and HTML
For any computing technology to reach its full impact, it must be well-understood, welldocumented,
and supported by mature, well-maintained tools. The key ideas should be clearly
distilled, minimizing the onboarding time needing to bring new practitioners up to date. Mature
libraries should automate common tasks, and exemplar code should make it easy for practitioners
to modify, apply, and extend common applications to suit their needs. Take dynamic web applications
as an example. Despite a large number of companies, like Amazon, developing successful
database-driven web applications in the 1990s, the potential of this technology to aid creative entrepreneurs
has been realized to a far greater degree in the past ten years, owing in part to the
development of powerful, well-documented frameworks.
1
Testing the potential of deep learning presents unique challenges because any single application
brings together various disciplines. Applying deep learning requires simultaneously understanding
(i) the motivations for casting a problem in a particular way; (ii) the mathematics of a given
modeling approach; (iii) the optimization algorithms for fitting the models to data; and (iv) and the
engineering required to train models efficiently, navigating the pitfalls of numerical computing
and getting the most out of available hardware. Teaching both the critical thinking skills required
to formulate problems, the mathematics to solve them, and the software tools to implement those
solutions all in one place presents formidable challenges. Our goal in this book is to present a
unified resource to bring would-be practitioners up to speed.
At the time we started this book project, there were no resources that simultaneously (i) were
up to date; (ii) covered the full breadth of modern machine learning with substantial technical
depth; and (iii) interleaved exposition of the quality one expects from an engaging textbook with
the clean runnable code that one expects to find in hands-on tutorials. We found plenty of code
examples for how to use a given deep learning framework (e.g., how to do basic numerical computing
with matrices in TensorFlow) or for implementing particular techniques (e.g., code snippets
for LeNet, AlexNet, ResNets, etc) scattered across various blog posts and GitHub repositories.
However, these examples typically focused on how to implement a given approach, but left out the
discussion of why certain algorithmic decisions are made. While some interactive resources have
popped up sporadically to address a particular topic, e.g., the engaging blog posts published on
the website Distill3, or personal blogs, they only covered selected topics in deep learning, and
often lacked associated code. On the other hand, while several textbooks have emerged, most notably
(Goodfellow et al., 2016), which offers a comprehensive survey of the concepts behind deep
learning, these resources do not marry the descriptions to realizations of the concepts in code,
sometimes leaving readers clueless as to how to implement them. Moreover, too many resources
are hidden behind the paywalls of commercial course providers.
We set out to create a resource that could (i) be freely available for everyone; (ii) offer sufficient
technical depth to provide a starting point on the path to actually becoming an applied machine
learning scientist; (iii) include runnable code, showing readers how to solve problems in practice;
(iv) allow for rapid updates, both by us and also by the community at large; and (v) be complemented
by a forum4 for interactive discussion of technical details and to answer questions.
These goals were often in conflict. Equations, theorems, and citations are best managed and laid
out in LaTeX. Code is best described in Python. And webpages are native in HTML and JavaScript.
Furthermore, we want the content to be accessible both as executable code, as a physical book,
as a downloadable PDF, and on the Internet as a website. At present there exist no tools and no
workflow perfectly suited to these demands, so we had to assemble our own. We describe our
approach in detail in Section 19.6. We settled on GitHub to share the source and to allow for edits,
Jupyter notebooks for mixing code, equations and text, Sphinx as a rendering engine to generate
multiple outputs, and Discourse for the forum. While our system is not yet perfect, these choices
provide a good compromise among the competing concerns. We believe that this might be the
first book published using such an integrated workflow.
3 http://distill.pub
4 http://discuss.d2l.ai
2 Contents
Learning by Doing
Many textbooks teach a series of topics, each in exhaustive detail. For example, Chris Bishop?s
excellent textbook (Bishop, 2006), teaches each topic so thoroughly, that getting to the chapter on
linear regression requires a non-trivial amount of work. While experts love this book precisely
for its thoroughness, for beginners, this property limits its usefulness as an introductory text.
In this book, we will teach most concepts just in time. In other words, you will learn concepts at the
very moment that they are needed to accomplish some practical end. While we take some time at
the outset to teach fundamental preliminaries, like linear algebra and probability, we want you to
taste the satisfaction of training your first model before worrying about more esoteric probability
distributions.
Aside from a few preliminary notebooks that provide a crash course in the basic mathematical
background, each subsequent chapter introduces both a reasonable number of new concepts and
provides single self-contained working examples�using real datasets. This presents an organizational
challenge. Some models might logically be grouped together in a single notebook. And
some ideas might be best taught by executing several models in succession. On the other hand,
there is a big advantage to adhering to a policy of 1 working example, 1 notebook: This makes it as
easy as possible for you to start your own research projects by leveraging our code. Just copy a
notebook and start modifying it.
We will interleave the runnable code with background material as needed. In general, we will
often err on the side of making tools available before explaining them fully (and we will follow up
by explaining the background later). For instance, we might use stochastic gradient descent before
fully explaining why it is useful or why it works. This helps to give practitioners the necessary
ammunition to solve problems quickly, at the expense of requiring the reader to trust us with
some curatorial decisions.
This book will teach deep learning concepts from scratch. Sometimes, we want to delve into fine
details about the models that would typically be hidden from the user by deep learning frameworks
? advanced abstractions. This comes up especially in the basic tutorials, where we want you
to understand everything that happens in a given layer or optimizer. In these cases, we will often
present two versions of the example: one where we implement everything from scratch, relying
only on the NumPy interface and automatic differentiation, and another, more practical example,
where we write succinct code using Gluon. Once we have taught you how some component
works, we can just use the Gluon version in subsequent tutorials.
Contents 3
Content and Structure
The book can be roughly divided into three parts, which are presented by different colors in Fig.
1:
Fig. 1: Book structure
� The first part covers basics and preliminaries. Chapter 1 offers an introduction to deep learning.
Then, in Chapter 2, we quickly bring you up to speed on the prerequisites required for
hands-on deep learning, such as how to store and manipulate data, and how to apply various
numerical operations based on basic concepts from linear algebra, calculus, and probability.
Chapter 3 and Chapter 4 cover the most basic concepts and techniques of deep learning,
such as linear regression, multilayer perceptrons and regularization.
� The next five chapters focus on modern deep learning techniques. Chapter 5 describes the
various key components of deep learning calculations and lays the groundwork for us to
subsequently implement more complex models. Next, in Chapter 6 and Chapter 7, we introduce
convolutional neural networks (CNNs), powerful tools that form the backbone of most
modern computer vision systems. Subsequently, in Chapter 8 and Chapter 9, we introduce
recurrent neural networks (RNNs), models that exploit temporal or sequential structure in
data, and are commonly used for natural language processing and time series prediction.
In Chapter 10, we introduce a new class of models that employ a technique called attention
mechanisms and they have recently begun to displace RNNs in natural language processing.
These sections will get you up to speed on the basic tools behind most modern applications
of deep learning.
� Part three discusses scalability, efficiency, and applications. First, in Chapter 11, we discuss
several common optimization algorithms used to train deep learning models. The next
4 Contents
chapter, Chapter 12 examines several key factors that influence the computational performance
of your deep learning code. In Chapter 13, we illustrate major applications of deep
learning in computer vision. In Chapter 14 and Chapter 15, we show how to pretrain language
representation models and apply them to natural language processing tasks.
Code
Most sections of this book feature executable code because of our belief in the importance of an
interactive learning experience in deep learning. At present, certain intuitions can only be developed
through trial and error, tweaking the code in small ways and observing the results. Ideally,
an elegant mathematical theory might tell us precisely how to tweak our code to achieve a desired
result. Unfortunately, at present, such elegant theories elude us. Despite our best attempts, formal
explanations for various techniques are still lacking, both because the mathematics to characterize
these models can be so difficult and also because serious inquiry on these topics has only
just recently kicked into high gear. We are hopeful that as the theory of deep learning progresses,
future editions of this book will be able to provide insights in places the present edition cannot.
At times, to avoid unnecessary repetition, we encapsulate the frequently-imported and referred-to
functions, classes, etc. in this book in the d2l package. For any block such as a function, a class,
or multiple imports to be saved in the package, we will mark it with #@save. We offer a detailed
overview of these functions and classes in Section 19.7. The d2l package is light-weight and only
requires the following packages and modules as dependencies:
#@save
import collections
from collections import defaultdict
from IPython import display
import math
from matplotlib import pyplot as plt
import os
import pandas as pd
import random
import re
import shutil
import sys
import tarfile
import time
import requests
import zipfile
import hashlib
d2l = sys.modules[__name__]
Most of the code in this book is based on Apache MXNet. MXNet is an open-source framework for
deep learning and the preferred choice of AWS (Amazon Web Services), as well as many colleges
and companies. All of the code in this book has passed tests under the newest MXNet version.
However, due to the rapid development of deep learning, some code in the print edition may not
work properly in future versions of MXNet. However, we plan to keep the online version up-todate.
In case you encounter any such problems, please consult Installation (page 9) to update your
code and runtime environment.
Here is how we import modules from MXNet.
Contents 5
#@save
from mxnet import autograd, context, gluon, image, init, np, npx
from mxnet.gluon import nn, rnn
Target Audience
This book is for students (undergraduate or graduate), engineers, and researchers, who seek a
solid grasp of the practical techniques of deep learning. Because we explain every concept from
scratch, no previous background in deep learning or machine learning is required. Fully explaining
the methods of deep learning requires some mathematics and programming, but we will only
assume that you come in with some basics, including (the very basics of) linear algebra, calculus,
probability, and Python programming. Moreover, in the Appendix, we provide a refresher
on most of the mathematics covered in this book. Most of the time, we will prioritize intuition
and ideas over mathematical rigor. There are many terrific books which can lead the interested
reader further. For instance, Linear Analysis by Bela Bollobas (Bollobas, 1999) covers linear algebra
and functional analysis in great depth. All of Statistics (Wasserman, 2013) is a terrific guide to
statistics. And if you have not used Python before, you may want to peruse this Python tutorial5.
Forum
Associated with this book, we have launched a discussion forum, located at discuss.d2l.ai6. When
you have questions on any section of the book, you can find the associated discussion page link at
the end of each chapter.
Acknowledgments
We are indebted to the hundreds of contributors for both the English and the Chinese drafts. They
helped improve the content and offered valuable feedback. Specifically, we thank every contributor
of this English draft for making it better for everyone. Their GitHub IDs or names are
(in no particular order): alxnorden, avinashingit, bowen0701, brettkoonce, Chaitanya Prakash
Bapat, cryptonaut, Davide Fiocco, edgarroman, gkutiel, John Mitro, Liang Pu, Rahul Agarwal,
Mohamed Ali Jamaoui, Michael (Stu) Stewart, Mike Muller, NRauschmayr, Prakhar Srivastav,
sad-, sfermigier, Sheng Zha, sundeepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh
Ravi Shrimali, YaYaB, Yuhong Chen, Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor
Dzreyev, Ha Nguyen, pmuens, Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad
Mahdi Rahimi, Abhishek Gupta, uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth
Buddareddygari, brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete Luer, Surbhi Vijayvargeeya,
Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pedro Larroy,
lgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner, Maximilian Bother,
Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong, Steve Sedlmeyer, Ruslan Baratov,
Rafael Schlatter, liusy182, Giannis Pappas, ati-ozgur, qbaza, dchoi77, Adam Gerson, Phuc
Le, Mark Atwood, christabella, vn09, Haibin Lin, jjangga0214, RichyChen, noelo, hansent, Giel
Dops, dvincent1337, WhiteD3vil, Peter Kulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle,
Peter Goetz, rigtorp, tiepvupsu, sfilip, mlxd, Kale-ab Tessera, Sanjar Adilov, MatteoFerrara,
hsneto, Katarzyna Biesialska, Gregory Bruss, duythanhvn, paulaurel, graytowne, minhduc0711,
5 http://learnpython.org/
6 https://discuss.d2l.ai/
6 Contents
sl7423, Jaedong Hwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums,
cuongvng, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic, thegreat-
shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, biagiom, abhinavsp0730,
jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang Kang, StevenJokes,
Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl, SportsTHU, nguyenhoa93,
manuel-arno-korfmann-webentwicklung, aterzis-personal, nxby, Xiaoting He, yoderj, mathresearch.
We thank Amazon Web Services, especially Swami Sivasubramanian, Raju Gulabani, Charlie Bell,
and Andrew Jassy for their generous support in writing this book. Without the available time,
resources, discussions with colleagues, and continuous encouragement this book would not have
happened.
Summary
� Deep learning has revolutionized pattern recognition, introducing technology that now
powers a wide range of technologies, including computer vision, natural language processing,
automatic speech recognition.
� To successfully apply deep learning, you must understand how to cast a problem, the mathematics
of modeling, the algorithms for fitting your models to data, and the engineering
techniques to implement it all.
� This book presents a comprehensive resource, including prose, figures, mathematics, and
code, all in one place.
� To answer questions related to this book, visit our forum at https://discuss.d2l.ai/.
� All notebooks are available for download on GitHub.
Exercises
1. Register an account on the discussion forum of this book discuss.d2l.ai7.
2. Install Python on your computer.
3. Follow the links at the bottom of the section to the forum, where you will be able to seek out
help and discuss the book and find answers to your questions by engaging the authors and
broader community.
Discussions8
7 https://discuss.d2l.ai/
8 https://discuss.d2l.ai/t/18
Contents 7
8 Contents
Installation
In order to get you up and running for hands-on learning experience, we need to set you up with an
environment for running Python, Jupyter notebooks, the relevant libraries, and the code needed
to run the book itself.
Installing Miniconda
The simplest way to get going will be to install Miniconda9. The Python 3.x version is required.
You can skip the following steps if conda has already been installed. Download the corresponding
Miniconda sh file from the website and then execute the installation from the command line using
sh <FILENAME> -b. For macOS users:
# The file name is subject to changes
sh Miniconda3-latest-MacOSX-x86_64.sh -b
For Linux users:
# The file name is subject to changes
sh Miniconda3-latest-Linux-x86_64.sh -b
Next, initialize the shell so we can run conda directly.
~/miniconda3/bin/conda init
Now close and re-open your current shell. You should be able to create a new environment as
following:
conda create --name d2l -y
9 https://conda.io/en/latest/miniconda.html
9
Downloading the D2L Notebooks
Next, we need to download the code of this book. You can click the �All Notebooks� tab on the top
of any HTML page to download and unzip the code. Alternatively, if you have unzip (otherwise
run sudo apt install unzip) available:
mkdir d2l-en && cd d2l-en
curl https://d2l.ai/d2l-en.zip -o d2l-en.zip
unzip d2l-en.zip && rm d2l-en.zip
Now we will want to activate the d2l environment and install pip. Enter y for the queries that
follow this command.
conda activate d2l
conda install python=3.7 pip -y
Installing the Framework and the d2l Package
Before installing the deep learning framework, please first check whether or not you have proper
GPUs on your machine (the GPUs that power the display on a standard laptop do not count for our
purposes). If you are installing on a GPU server, proceed to GPU Support (page 11) for instructions
to install a GPU-supported version.
Otherwise, you can install the CPU version. That will be more than enough horsepower to get you
through the first few chapters but you will want to access GPUs before running larger models.
pip install mxnet==1.6.0
We also install the d2l package that encapsulates frequently used functions and classes in this
book.
# -U: Upgrade all packages to the newest available version
pip install -U d2l
Once they are installed, we now open the Jupyter notebook by running:
jupyter notebook
At this point, you can open http://localhost:8888 (it usually opens automatically) in your Web
browser. Then we can run the code for each section of the book. Please always execute conda activate
d2l to activate the runtime environment before running the code of the book or updating
the deep learning framework or the d2l package. To exit the environment, run conda deactivate.
10 Contents
GPU Support
By default, the deep learning framework is installed without GPU support to ensure that it will
run on any computer (including most laptops). Part of this book requires or recommends running
with GPU. If your computer has NVIDIA graphics cards and has installed CUDA10, then you should
install a GPU-enabled version. If you have installed the CPU-only version, you may need to remove
it first by running:
pip uninstall mxnet
Then we need to find the CUDA version you installed. You may check it through nvcc --version
or cat /usr/local/cuda/version.txt. Assume that you have installed CUDA 10.1, then you can
install with the following command:
# For Windows users
pip install mxnet-cu101==1.6.0b20190926
# For Linux and macOS users
pip install mxnet-cu101==1.6.0
You may change the last digits according to your CUDA version, e.g., cu100 for CUDA 10.0 and cu90
for CUDA 9.0.
Exercises
1. Download the code for the book and install the runtime environment.
Discussions11
10 https://developer.nvidia.com/cuda-downloads
11 https://discuss.d2l.ai/t/23
Contents 11
12 Contents
Notation
The notation used throughout this book is summarized below.
Numbers
� x: A scalar
� x: A vector
� X: A matrix
� X: A tensor
� I: An identity matrix
� xi, [x]i: The ith element of vector x
� xij , xi;j ,[X]ij , [X]i;j : The element of matrix X at row i and column j
Set Theory
� X: A set
� Z: The set of integers
� R: The set of real numbers
� Rn: The set of n-dimensional vectors of real numbers
� Rab: The set of matrices of real numbers with a rows and b columns
� jX j: Cardinality (number of elements) of set X
� A [ B: Union of sets A and B
� A \ B: Intersection of sets A and B
� A n B: Subtraction of set B from set A
13
Functions and Operators
� f(): A function
� log(): The natural logarithm
� exp(): The exponential function
� 1X : The indicator function
� ()
?: Transpose of a vector or a matrix
� X??1: Inverse of matrix X
� ?: Hadamard (elementwise) product
� [; ]: Concatenation
� jX j: Cardinality of set X
� ?  ?p: Lp norm
� ?  ?: L2 norm
� ?x; y?: Dot product of vectors x and y
�
?
: Series addition
�
?
: Series multiplication
� def
=: Definition
Calculus
� dy
dx : Derivative of y with respect to x
� @y
@x : Partial derivative of y with respect to x
� ?xy: Gradient of y with respect to x
�
? b
a f(x) dx: Definite integral of f from a to b with respect to x
�
?
f(x) dx: Indefinite integral of f with respect to x
Probability and Information Theory
� P(): Probability distribution
� z  P: Random variable z has probability distribution P
� P(X j Y ): Conditional probability of X j Y
� p(x): Probability density function
� Ex[f(x)]: Expectation of f with respect to x
� X ? Y : Random variables X and Y are independent
14 Contents
� X ? Y j Z: Random variables X and Y are conditionally independent given random variable
Z
� Var(X): Variance of random variable X
� X: Standard deviation of random variable X
� Cov(X; Y ): Covariance of random variables X and Y
� (X; Y ): Correlation of random variables X and Y
� H(X): Entropy of random variable X
� DKL(P?Q): KL-divergence of distributions P and Q
Complexity
� O: Big O notation
Discussions12
12 https://discuss.d2l.ai/t/25
Contents 15
16 Contents
1 | Introduction
Until recently, nearly every computer program that we interact with daily was coded by software
developers from first principles. Say that we wanted to write an application to manage an ecommerce
platform. After huddling around a whiteboard for a few hours to ponder the problem,
we would come up with the broad strokes of a working solution that might probably look
something like this: (i) users interact with the application through an interface running in a web
browser or mobile application; (ii) our application interacts with a commercial-grade database
engine to keep track of each user?s state and maintain records of historical transactions; and (iii)
at the heart of our application, the business logic (you might say, the brains) of our application spells
out in methodical detail the appropriate action that our program should take in every conceivable
circumstance.
To build the brains of our application, we?d have to step through every possible corner case that
we anticipate encountering, devising appropriate rules. Each time a customer clicks to add an
item to their shopping cart, we add an entry to the shopping cart database table, associating that
user?s ID with the requested product?s ID. While few developers ever get it completely right the
first time (it might take some test runs to work out the kinks), for the most part, we could write
such a program from first principles and confidently launch it before ever seeing a real customer.
Our ability to design automated systems from first principles that drive functioning products and
systems, often in novel situations, is a remarkable cognitive feat. And when you are able to devise
solutions that work 100% of the time, you should not be using machine learning.
Fortunately for the growing community of machine learning (ML) scientists, many tasks that we
would like to automate do not bend so easily to human ingenuity. Imagine huddling around the
whiteboard with the smartest minds you know, but this time you are tackling one of the following
problems:
� Write a program that predicts tomorrow?s weather given geographic information, satellite
images, and a trailing window of past weather.
� Write a program that takes in a question, expressed in free-form text, and answers it correctly.
� Write a program that given an image can identify all the people it contains, drawing outlines
around each.
� Write a program that presents users with products that they are likely to enjoy but unlikely,
in the natural course of browsing, to encounter.
In each of these cases, even elite programmers are incapable of coding up solutions from scratch.
The reasons for this can vary. Sometimes the program that we are looking for follows a pattern
that changes over time, and we need our programs to adapt. In other cases, the relationship (say
between pixels, and abstract categories) may be too complicated, requiring thousands or millions
of computations that are beyond our conscious understanding (even if our eyes manage the task
17
effortlessly). ML is the study of powerful techniques that can learn from experience. As an ML
algorithm accumulates more experience, typically in the form of observational data or interactions
with an environment, its performance improves. Contrast this with our deterministic ecommerce
platform, which performs according to the same business logic, no matter how much
experience accrues, until the developers themselves learn and decide that it is time to update the
software. In this book, we will teach you the fundamentals of machine learning, and focus in particular
on deep learning, a powerful set of techniques driving innovations in areas as diverse as
computer vision, natural language processing, healthcare, and genomics.
1.1 A Motivating Example
Before we could begin writing, the authors of this book, like much of the work force, had to become
caffeinated. We hopped in the car and started driving. Using an iPhone, Alex called out
�Hey Siri�, awakening the phone?s voice recognition system. Then Mu commanded �directions to
Blue Bottle coffee shop�. The phone quickly displayed the transcription of his command. It also
recognized that we were asking for directions and launched the Maps application to fulfill our request.
Once launched, the Maps app identified a number of routes. Next to each route, the phone
displayed a predicted transit time. While we fabricated this story for pedagogical convenience, it
demonstrates that in the span of just a few seconds, our everyday interactions with a smart phone
can engage several machine learning models.
Imagine just writing a program to respond to a wake word like �Alexa�, �Okay, Google� or �Siri�. Try
coding it up in a room by yourself with nothing but a computer and a code editor, as illustrated
in Fig. 1.1.1. How would you write such a program from first principles? Think about it� the
problem is hard. Every second, the microphone will collect roughly 44,000 samples. Each sample
is a measurement of the amplitude of the sound wave. What rule could map reliably from a snippet
of raw audio to confident predictions {yes, no} on whether the snippet contains the wake word?
If you are stuck, do not worry. We do not know how to write such a program from scratch either.
That is why we use ML.
Fig. 1.1.1: Identify an awake word.
Here?s the trick. Often, even when we do not know how to tell a computer explicitly how to map
from inputs to outputs, we are nonetheless capable of performing the cognitive feat ourselves. In
other words, even if you do not know how to program a computer to recognize the word �Alexa�,
you yourself are able to recognize the word �Alexa�. Armed with this ability, we can collect a huge
dataset containing examples of audio and label those that do and that do not contain the wake word.
In the ML approach, we do not attempt to design a system explicitly to recognize wake words.
Instead, we define a flexible program whose behavior is determined by a number of parameters.
Then we use the dataset to determine the best possible set of parameters, those that improve the
performance of our program with respect to some measure of performance on the task of interest.
You can think of the parameters as knobs that we can turn, manipulating the behavior of the program.
Fixing the parameters, we call the program a model. The set of all distinct programs (inputoutput
mappings) that we can produce just by manipulating the parameters is called a family of
18 Chapter 1. Introduction
models. And the meta-program that uses our dataset to choose the parameters is called a learning
algorithm.
Before we can go ahead and engage the learning algorithm, we have to define the problem precisely,
pinning down the exact nature of the inputs and outputs, and choosing an appropriate
model family. In this case, our model receives a snippet of audio as input, and it generates a selection
among {yes, no} as output. If all goes according to plan the model?s guesses will typically
be correct as to whether (or not) the snippet contains the wake word.
If we choose the right family of models, then there should exist one setting of the knobs such
that the model fires yes every time it hears the word �Alexa�. Because the exact choice of the
wake word is arbitrary, we will probably need a model family sufficiently rich that, via another
setting of the knobs, it could fire yes only upon hearing the word �Apricot�. We expect that the
same model family should be suitable for �Alexa� recognition and �Apricot� recognition because
they seem, intuitively, to be similar tasks. However, we might need a different family of models
entirely if we want to deal with fundamentally different inputs or outputs, say if we wanted to map
from images to captions, or from English sentences to Chinese sentences.
As you might guess, if we just set all of the knobs randomly, it is not likely that our model will recognize
�Alexa�, �Apricot�, or any other English word. In deep learning, the learning is the process
by which we discover the right setting of the knobs coercing the desired behavior from our model.
As shown in Fig. 1.1.2, the training process usually looks like this:
1. Start off with a randomly initialized model that cannot do anything useful.
2. Grab some of your labeled data (e.g., audio snippets and corresponding {yes, no} labels)
3. Tweak the knobs so the model sucks less with respect to those examples
4. Repeat until the model is awesome.
Fig. 1.1.2: A typical training process.
To summarize, rather than code up a wake word recognizer, we code up a program that can learn
to recognize wake words, if we present it with a large labeled dataset. You can think of this act of
determining a program?s behavior by presenting it with a dataset as programming with data. We
can �program� a cat detector by providing our machine learning system with many examples of
cats and dogs, such as the images below:
1.1. A Motivating Example 19
cat cat dog dog
This way the detector will eventually learn to emit a very large positive number if it is a cat, a very
large negative number if it is a dog, and something closer to zero if it is not sure, and this barely
scratches the surface of what ML can do.
Deep learning is just one among many popular methods for solving machine learning problems.
Thus far, we have only talked about machine learning broadly and not deep learning. To see why
deep learning is important, we should pause for a moment to highlight a couple of crucial points.
First, the problems that we have discussed thus far�learning from the raw audio signal, the raw
pixel values of images, or mapping between sentences of arbitrary lengths and their counterparts
in foreign languages�are problems where deep learning excels and where traditional ML methods
faltered. Deep models are deep in precisely the sense that they learn many layers of computation.
It turns out that these many-layered (or hierarchical) models are capable of addressing
low-level perceptual data in a way that previous tools could not. In bygone days, the crucial part
of applying ML to these problems consisted of coming up with manually-engineered ways of transforming
the data into some form amenable to shallow models. One key advantage of deep learning
is that it replaces not only the shallow models at the end of traditional learning pipelines, but also
the labor-intensive process of feature engineering. Second, by replacing much of the domainspecific
preprocessing, deep learning has eliminated many of the boundaries that previously separated
computer vision, speech recognition, natural language processing, medical informatics,
and other application areas, offering a unified set of tools for tackling diverse problems.
1.2 The Key Components: Data, Models, and Algorithms
In our wake-word example, we described a dataset consisting of audio snippets and binary labels,
and we gave a hand-wavy sense of how we might train a model to approximate a mapping from
snippets to classifications. This sort of problem, where we try to predict a designated unknown
label given known inputs, given a dataset consisting of examples, for which the labels are known
is called supervised learning, and it is just one among many kinds of machine learning problems.
In the next section, we will take a deep dive into the different ML problems. First, we?d like to
shed more light on some core components that will follow us around, no matter what kind of ML
problem we take on:
1. The data that we can learn from.
2. A model of how to transform the data.
3. A loss function that quantifies the badness of our model.
4. An algorithm to adjust the model?s parameters to minimize the loss.
20 Chapter 1. Introduction
1.2.1 Data
It might go without saying that you cannot do data science without data. We could lose hundreds of
pages pondering what precisely constitutes data, but for now, we will err on the practical side and
focus on the key properties to be concerned with. Generally, we are concerned with a collection
of examples. In order to work with data usefully, we typically need to come up with a suitable
numerical representation. Each example typically consists of a collection of numerical attributes
called features. In the supervised learning problems above, a special feature is designated as the
prediction target, (sometimes called the label or dependent variable). The given features from which
the model must make its predictions can then simply be called the features, (or often, the inputs,
covariates, or independent variables).
If we were working with image data, each individual photograph might constitute an example,
each represented by an ordered list of numerical values corresponding to the brightness of each
pixel. A 200  200 color photograph would consist of 200  200  3 = 120000 numerical values,
corresponding to the brightness of the red, green, and blue channels for each spatial location.
In a more traditional task, we might try to predict whether or not a patient will survive, given a
standard set of features such as age, vital signs, diagnoses, etc.
When every example is characterized by the same number of numerical values, we say that the
data consists of fixed-length vectors and we describe the (constant) length of the vectors as the
dimensionality of the data. As you might imagine, fixed-length can be a convenient property. If we
wanted to train a model to recognize cancer in microscopy images, fixed-length inputs mean we
have one less thing to worry about.
However, not all data can easily be represented as fixed-length vectors. While we might expect
microscope images to come from standard equipment, we cannot expect images mined from the
Internet to all show up with the same resolution or shape. For images, we might consider cropping
them all to a standard size, but that strategy only gets us so far. We risk losing information
in the cropped out portions. Moreover, text data resists fixed-length representations even more
stubbornly. Consider the customer reviews left on e-commerce sites like Amazon, IMDB, or TripAdvisor.
Some are short: �it stinks!�. Others ramble for pages. One major advantage of deep
learning over traditional methods is the comparative grace with which modern models can handle
varying-length data.
Generally, the more data we have, the easier our job becomes. When we have more data, we
can train more powerful models and rely less heavily on pre-conceived assumptions. The regime
change from (comparatively) small to big data is a major contributor to the success of modern
deep learning. To drive the point home, many of the most exciting models in deep learning do
not work without large datasets. Some others work in the low-data regime, but are no better than
traditional approaches.
Finally, it is not enough to have lots of data and to process it cleverly. We need the right data. If
the data is full of mistakes, or if the chosen features are not predictive of the target quantity of
interest, learning is going to fail. The situation is captured well by the cliche: garbage in, garbage
out. Moreover, poor predictive performance is not the only potential consequence. In sensitive
applications of machine learning, like predictive policing, resume screening, and risk models
used for lending, we must be especially alert to the consequences of garbage data. One common
failure mode occurs in datasets where some groups of people are unrepresented in the training
data. Imagine applying a skin cancer recognition system in the wild that had never seen black
skin before. Failure can also occur when the data does not merely under-represent some groups
but reflects societal prejudices. For example, if past hiring decisions are used to train a predictive
model that will be used to screen resumes, then machine learning models could inadvertently
1.2. The Key Components: Data, Models, and Algorithms 21
capture and automate historical injustices. Note that this can all happen without the data scientist
actively conspiring, or even being aware.
1.2.2 Models
Most machine learning involves transforming the data in some sense. We might want to build a
system that ingests photos and predicts smiley-ness. Alternatively, we might want to ingest a set of
sensor readings and predict how normal vs anomalous the readings are. By model, we denote the
computational machinery for ingesting data of one type, and spitting out predictions of a possibly
different type. In particular, we are interested in statistical models that can be estimated from
data. While simple models are perfectly capable of addressing appropriately simple problems the
problems that we focus on in this book stretch the limits of classical methods. Deep learning is
differentiated from classical approaches principally by the set of powerful models that it focuses
on. These models consist of many successive transformations of the data that are chained together
top to bottom, thus the name deep learning. On our way to discussing deep neural networks, we
will discuss some more traditional methods.
1.2.3 Objective functions
Earlier, we introduced machine learning as �learning from experience�. By learning here, we mean
improving at some task over time. But who is to say what constitutes an improvement? You might
imagine that we could propose to update our model, and some people might disagree on whether
the proposed update constituted an improvement or a decline.
In order to develop a formal mathematical system of learning machines, we need to have formal
measures of how good (or bad) our models are. In machine learning, and optimization more
generally, we call these objective functions. By convention, we usually define objective functions
so that lower is better. This is merely a convention. You can take any function f for which higher is
better, and turn it into a new function f? that is qualitatively identical but for which lower is better
by setting f? = ??f. Because lower is better, these functions are sometimes called loss functions or
cost functions.
When trying to predict numerical values, the most common objective function is squared error
(y??^y)2. For classification, the most common objective is to minimize error rate, i.e., the fraction of
instances on which our predictions disagree with the ground truth. Some objectives (like squared
error) are easy to optimize. Others (like error rate) are difficult to optimize directly, owing to
non-differentiability or other complications. In these cases, it is common to optimize a surrogate
objective.
Typically, the loss function is defined with respect to the model?s parameters and depends upon the
dataset. The best values of our model?s parameters are learned by minimizing the loss incurred on
a training set consisting of some number of examples collected for training. However, doing well on
the training data does not guarantee that we will do well on (unseen) test data. So we will typically
want to split the available data into two partitions: the training data (for fitting model parameters)
and the test data (which is held out for evaluation), reporting the following two quantities:
� Training Error: The error on that data on which the model was trained. You could think of
this as being like a student?s scores on practice exams used to prepare for some real exam.
Even if the results are encouraging, that does not guarantee success on the final exam.
� Test Error: This is the error incurred on an unseen test set. This can deviate significantly
from the training error. When a model performs well on the training data but fails to gen-
22 Chapter 1. Introduction
eralize to unseen data, we say that it is overfitting. In real-life terms, this is like flunking the
real exam despite doing well on practice exams.
1.2.4 Optimization algorithms
Once we have got some data source and representation, a model, and a well-defined objective
function, we need an algorithm capable of searching for the best possible parameters for minimizing
the loss function. The most popular optimization algorithms for neural networks follow
an approach called gradient descent. In short, at each step, they check to see, for each parameter,
which way the training set loss would move if you perturbed that parameter just a small amount.
They then update the parameter in the direction that reduces the loss.
1.3 Kinds of Machine Learning
In the following sections, we discuss a few kinds of machine learning problems in greater detail.
We begin with a list of objectives, i.e., a list of things that we would like machine learning to do.
Note that the objectives are complemented with a set of techniques of how to accomplish them,
including types of data, models, training techniques, etc. The list below is just a sampling of the
problems ML can tackle to motivate the reader and provide us with some common language for
when we talk about more problems throughout the book.
1.3.1 Supervised learning
Supervised learning addresses the task of predicting targets given inputs. The targets, which we
often call labels, are generally denoted by y. The input data, also called the features or covariates,
are typically denoted x. Each (input, target) pair is called an example or instance. Sometimes, when
the context is clear, we may use the term examples, to refer to a collection of inputs, even when the
corresponding targets are unknown. We denote any particular instance with a subscript, typically
i, for instance (xi; yi). A dataset is a collection of n instances fxi; yigni
=1. Our goal is to produce a
model f that maps any input xi to a prediction f(xi).
To ground this description in a concrete example, if we were working in healthcare, then we might
want to predict whether or not a patient would have a heart attack. This observation, heart attack
or no heart attack, would be our label y. The input data x might be vital signs such as heart rate,
diastolic and systolic blood pressure, etc.
The supervision comes into play because for choosing the parameters , we (the supervisors)
provide the model with a dataset consisting of labeled examples (xi; yi), where each example xi
is matched with the correct label.
In probabilistic terms, we typically are interested in estimating the conditional probability P(yjx).
While it is just one among several paradigms within machine learning, supervised learning accounts
for the majority of successful applications of machine learning in industry. Partly, that is
because many important tasks can be described crisply as estimating the probability of something
unknown given a particular set of available data:
� Predict cancer vs not cancer, given a CT image.
� Predict the correct translation in French, given a sentence in English.
� Predict the price of a stock next month based on this month?s financial reporting data.
1.3. Kinds of Machine Learning 23
Even with the simple description �predict targets from inputs� supervised learning can take a great
many forms and require a great many modeling decisions, depending on (among other considerations)
the type, size, and the number of inputs and outputs. For example, we use different models
to process sequences (like strings of text or time series data) and for processing fixed-length vector
representations. We will visit many of these problems in depth throughout the first 9 parts of
this book.
Informally, the learning process looks something like this: Grab a big collection of examples for
which the covariates are known and select from them a random subset, acquiring the ground truth
labels for each. Sometimes these labels might be available data that has already been collected
(e.g., did a patient die within the following year?) and other times we might need to employ human
annotators to label the data, (e.g., assigning images to categories).
Together, these inputs and corresponding labels comprise the training set. We feed the training
dataset into a supervised learning algorithm, a function that takes as input a dataset and outputs
another function, the learned model. Finally, we can feed previously unseen inputs to the learned
model, using its outputs as predictions of the corresponding label. The full process is drawn in
Fig. 1.3.1.
Fig. 1.3.1: Supervised learning.
Regression
Perhaps the simplest supervised learning task to wrap your head around is regression. Consider,
for example, a set of data harvested from a database of home sales. We might construct a table,
where each row corresponds to a different house, and each column corresponds to some relevant
attribute, such as the square footage of a house, the number of bedrooms, the number of bathrooms,
and the number of minutes (walking) to the center of town. In this dataset, each example
would be a specific house, and the corresponding feature vector would be one row in the table.
If you live in New York or San Francisco, and you are not the CEO of Amazon, Google, Microsoft, or
Facebook, the (sq. footage, no. of bedrooms, no. of bathrooms, walking distance) feature vector
for your home might look something like: [100; 0; :5; 60]. However, if you live in Pittsburgh, it
might look more like [3000; 4; 3; 10]. Feature vectors like this are essential for most classic machine
learning algorithms. We will continue to denote the feature vector corresponding to any example
i as xi and we can compactly refer to the full table containing all of the feature vectors as X.
What makes a problem a regression is actually the outputs. Say that you are in the market for a new
home. You might want to estimate the fair market value of a house, given some features like these.
The target value, the price of sale, is a real number. If you remember the formal definition of the
reals you might be scratching your head now. Homes probably never sell for fractions of a cent,
let alone prices expressed as irrational numbers. In cases like this, when the target is actually
discrete, but where the rounding takes place on a sufficiently fine scale, we will abuse language
just a bit and continue to describe our outputs and targets as real-valued numbers.
24 Chapter 1. Introduction
We denote any individual target yi (corresponding to example xi) and the set of all targets y (corresponding
to all examples X). When our targets take on arbitrary values in some range, we call
this a regression problem. Our goal is to produce a model whose predictions closely approximate
the actual target values. We denote the predicted target for any instance ^yi. Do not worry if the
notation is bogging you down. We will unpack it more thoroughly in the subsequent chapters.
Lots of practical problems are well-described regression problems. Predicting the rating that a
user will assign to a movie can be thought of as a regression problem and if you designed a great
algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar Netflix prize13.
Predicting the length of stay for patients in the hospital is also a regression problem. A good rule
of thumb is that any How much? or How many? problem should suggest regression.
� �How many hours will this surgery take?�: regression
� �How many dogs are in this photo?�: regression.
However, if you can easily pose your problem as �Is this a _ ?�, then it is likely, classification, a
different kind of supervised problem that we will cover next. Even if you have never worked with
machine learning before, you have probably worked through a regression problem informally.
Imagine, for example, that you had your drains repaired and that your contractor spent x1 = 3
hours removing gunk from your sewage pipes. Then he sent you a bill of y1 = $350. Now imagine
that your friend hired the same contractor for x2 = 2 hours and that he received a bill of y2 = $250.
If someone then asked you how much to expect on their upcoming gunk-removal invoice you
might make some reasonable assumptions, such as more hours worked costs more dollars. You
might also assume that there is some base charge and that the contractor then charges per hour.
If these assumptions held true, then given these two data examples, you could already identify the
contractor?s pricing structure: $100 per hour plus $50 to show up at your house. If you followed
that much then you already understand the high-level idea behind linear regression (and you just
implicitly designed a linear model with a bias term).
In this case, we could produce the parameters that exactly matched the contractor?s prices. Sometimes
that is not possible, e.g., if some of the variance owes to some factors besides your two features.
In these cases, we will try to learn models that minimize the distance between our predictions
and the observed values. In most of our chapters, we will focus on one of two very common
losses, the L1 loss where
l(y; y
?
) =
?
i
jyi ?? y
?
i
j (1.3.1)
and the least mean squares loss, or L2 loss where
l(y; y
?
) =
?
i
(yi ?? y
?
i)2: (1.3.2)
As we will see later, the L2 loss corresponds to the assumption that our data was corrupted by
Gaussian noise, whereas the L1 loss corresponds to an assumption of noise from a Laplace distribution.
13 https://en.wikipedia.org/wiki/Netflix_Prize
1.3. Kinds of Machine Learning 25
Classification
While regression models are great for addressing how many? questions, lots of problems do not
bend comfortably to this template. For example, a bank wants to add check scanning to its mobile
app. This would involve the customer snapping a photo of a check with their smart phone?s camera
and the machine learning model would need to be able to automatically understand text seen in
the image. It would also need to understand hand-written text to be even more robust. This kind of
system is referred to as optical character recognition (OCR), and the kind of problem it addresses
is called classification. It is treated with a different set of algorithms than those used for regression
(although many techniques will carry over).
In classification, we want our model to look at a feature vector, e.g., the pixel values in an image,
and then predict which category (formally called classes), among some (discrete) set of options, an
example belongs. For hand-written digits, we might have 10 classes, corresponding to the digits 0
through 9. The simplest form of classification is when there are only two classes, a problem which
we call binary classification. For example, our dataset X could consist of images of animals and
our labels Y might be the classes fcat; dogg. While in regression, we sought a regressor to output a
real value ^y, in classification, we seek a classifier, whose output ^y is the predicted class assignment.
For reasons that we will get into as the book gets more technical, it can be hard to optimize a
model that can only output a hard categorical assignment, e.g., either cat or dog. In these cases,
it is usually much easier to instead express our model in the language of probabilities. Given an
example x, our model assigns a probability ^yk to each label k. Because these are probabilities,
they need to be positive numbers and add up to 1 and thus we only need K ??1 numbers to assign
probabilities of K categories. This is easy to see for binary classification. If there is a 0:6 (60%)
probability that an unfair coin comes up heads, then there is a 0:4 (40%) probability that it comes
up tails. Returning to our animal classification example, a classifier might see an image and output
the probability that the image is a cat P(y = cat j x) = 0:9. We can interpret this number by saying
that the classifier is 90% sure that the image depicts a cat. The magnitude of the probability for
the predicted class conveys one notion of uncertainty. It is not the only notion of uncertainty and
we will discuss others in more advanced chapters.
When we have more than two possible classes, we call the problem multiclass classification. Common
examples include hand-written character recognition [0, 1, 2, 3 ... 9, a, b, c, ...].
While we attacked regression problems by trying to minimize the L1 or L2 loss functions, the
common loss function for classification problems is called cross-entropy.
Note that the most likely class is not necessarily the one that you are going to use for your decision.
Assume that you find this beautiful mushroom in your backyard as shown in Fig. 1.3.2.
26 Chapter 1. Introduction
Fig. 1.3.2: Death cap�do not eat!
Now, assume that you built a classifier and trained it to predict if a mushroom is poisonous based
on a photograph. Say our poison-detection classifier outputs P(y = deathcapjimage) = 0:2. In
other words, the classifier is 80% sure that our mushroom is not a death cap. Still, you would have
to be a fool to eat it. That is because the certain benefit of a delicious dinner is not worth a 20%
risk of dying from it. In other words, the effect of the uncertain risk outweighs the benefit by far.
We can look at this more formally. Basically, we need to compute the expected risk that we incur,
i.e., we need to multiply the probability of the outcome with the benefit (or harm) associated with
it:
L(actionjx) = Eyp(yjx)[loss(action; y)]: (1.3.3)
Hence, the loss L incurred by eating the mushroom is L(a = eatjx) = 0:2  1 + 0:8  0 = 1,
whereas the cost of discarding it is L(a = discardjx) = 0:2  0 + 0:8  1 = 0:8.
Our caution was justified: as any mycologist would tell us, the above mushroom actually is a death
cap. Classification can get much more complicated than just binary, multiclass, or even multilabel
classification. For instance, there are some variants of classification for addressing hierarchies.
Hierarchies assume that there exist some relationships among the many classes. So not all
errors are equal�if we must err, we would prefer to misclassify to a related class rather than to a
distant class. Usually, this is referred to as hierarchical classification. One early example is due to
Linnaeus14, who organized the animals in a hierarchy.
In the case of animal classification, it might not be so bad to mistake a poodle for a schnauzer,
but our model would pay a huge penalty if it confused a poodle for a dinosaur. Which hierarchy
is relevant might depend on how you plan to use the model. For example, rattle snakes and garter
snakes might be close on the phylogenetic tree, but mistaking a rattler for a garter could be deadly.
14 https://en.wikipedia.org/wiki/Carl_Linnaeus
1.3. Kinds of Machine Learning 27
Tagging
Some classification problems do not fit neatly into the binary or multiclass classification setups.
For example, we could train a normal binary classifier to distinguish cats from dogs. Given the
current state of computer vision, we can do this easily, with off-the-shelf tools. Nonetheless, no
matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters
an image of the Town Musicians of Bremen.
Fig. 1.3.3: A cat, a rooster, a dog and a donkey
As you can see, there is a cat in the picture, and a rooster, a dog, a donkey, and a bird, with some
trees in the background. Depending on what we want to do with our model ultimately, treating
this as a binary classification problem might not make a lot of sense. Instead, we might want to
give the model the option of saying the image depicts a cat and a dog and a donkey and a rooster
and a bird.
The problem of learning to predict classes that are not mutually exclusive is called multi-label classification.
Auto-tagging problems are typically best described as multi-label classification problems.
Think of the tags people might apply to posts on a tech blog, e.g., �machine learning�, �technology�,
�gadgets�, �programming languages�, �linux�, �cloud computing�, �AWS�. A typical article
might have 5-10 tags applied because these concepts are correlated. Posts about �cloud computing�
are likely to mention �AWS� and posts about �machine learning� could also deal with �programming
languages�.
We also have to deal with this kind of problem when dealing with the biomedical literature, where
correctly tagging articles is important because it allows researchers to do exhaustive reviews of
the literature. At the National Library of Medicine, a number of professional annotators go over
28 Chapter 1. Introduction
each article that gets indexed in PubMed to associate it with the relevant terms from MeSH, a
collection of roughly 28k tags. This is a time-consuming process and the annotators typically have
a one year lag between archiving and tagging. Machine learning can be used here to provide
provisional tags until each article can have a proper manual review. Indeed, for several years, the
BioASQ organization has hosted a competition15 to do precisely this.
Search and ranking
Sometimes we do not just want to assign each example to a bucket or to a real value. In the field of
information retrieval, we want to impose a ranking on a set of items. Take web search for example,
the goal is less to determine whether a particular page is relevant for a query, but rather, which
one of the plethora of search results is most relevant for a particular user. We really care about
the ordering of the relevant search results and our learning algorithm needs to produce ordered
subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters
from the alphabet, there is a difference between returning A B C D E and C A B E D. Even if the
result set is the same, the ordering within the set matters.
One possible solution to this problem is to first assign to every element in the set a corresponding
relevance score and then to retrieve the top-rated elements. PageRank16, the original secret sauce
behind the Google search engine was an early example of such a scoring system but it was peculiar
in that it did not depend on the actual query. Here they relied on a simple relevance filter to
identify the set of relevant items and then on PageRank to order those results that contained the
query term. Nowadays, search engines use machine learning and behavioral models to obtain
query-dependent relevance scores. There are entire academic conferences devoted to this subject.
Recommender systems
Recommender systems are another problem setting that is related to search and ranking. The
problems are similar insofar as the goal is to display a set of relevant items to the user. The main
difference is the emphasis on personalization to specific users in the context of recommender systems.
For instance, for movie recommendations, the results page for a SciFi fan and the results
page for a connoisseur of Peter Sellers comedies might differ significantly. Similar problems pop
up in other recommendation settings, e.g., for retail products, music, or news recommendation.
In some cases, customers provide explicit feedback communicating how much they liked a particular
product (e.g., the product ratings and reviews on Amazon, IMDB, GoodReads, etc.). In some
other cases, they provide implicit feedback, e.g., by skipping titles on a playlist, which might indicate
dissatisfaction but might just indicate that the song was inappropriate in context. In the
simplest formulations, these systems are trained to estimate some score yij , such as an estimated
rating or the probability of purchase, given a user ui and product pj .
Given such a model, then for any given user, we could retrieve the set of objects with the largest
scores yij , which could then be recommended to the customer. Production systems are considerably
more advanced and take detailed user activity and item characteristics into account when
computing such scores. Fig. 1.3.4 is an example of deep learning books recommended by Amazon
based on personalization algorithms tuned to capture the author?s preferences.
15 http://bioasq.org/
16 https://en.wikipedia.org/wiki/PageRank
1.3. Kinds of Machine Learning 29
Fig. 1.3.4: Deep learning books recommended by Amazon.
Despite their tremendous economic value, recommendation systems naively built on top of predictive
models suffer some serious conceptual flaws. To start, we only observe censored feedback.
Users preferentially rate movies that they feel strongly about: you might notice that items receive
many 5 and 1 star ratings but that there are conspicuously few 3-star ratings. Moreover, current
purchase habits are often a result of the recommendation algorithm currently in place, but learning
algorithms do not always take this detail into account. Thus it is possible for feedback loops
to form where a recommender system preferentially pushes an item that is then taken to be better
(due to greater purchases) and in turn is recommended even more frequently. Many of these
problems about how to deal with censoring, incentives, and feedback loops, are important open
research questions.
Sequence Learning
So far, we have looked at problems where we have some fixed number of inputs and produce a fixed
number of outputs. Before we considered predicting home prices from a fixed set of features:
square footage, number of bedrooms, number of bathrooms, walking time to downtown. We
also discussed mapping from an image (of fixed dimension) to the predicted probabilities that it
belongs to each of a fixed number of classes, or taking a user ID and a product ID, and predicting
a star rating. In these cases, once we feed our fixed-length input into the model to generate an
output, the model immediately forgets what it just saw.
This might be fine if our inputs truly all have the same dimensions and if successive inputs truly
have nothing to do with each other. But how would we deal with video snippets? In this case,
each snippet might consist of a different number of frames. And our guess of what is going on in
each frame might be much stronger if we take into account the previous or succeeding frames.
30 Chapter 1. Introduction
Same goes for language. One popular deep learning problem is machine translation: the task of
ingesting sentences in some source language and predicting their translation in another language.
These problems also occur in medicine. We might want a model to monitor patients in the intensive
care unit and to fire off alerts if their risk of death in the next 24 hours exceeds some
threshold. We definitely would not want this model to throw away everything it knows about the
patient history each hour and just make its predictions based on the most recent measurements.
These problems are among the most exciting applications of machine learning and they are instances
of sequence learning. They require a model to either ingest sequences of inputs or to emit
sequences of outputs (or both!). These latter problems are sometimes referred to as seq2seq problems.
Language translation is a seq2seq problem. Transcribing text from the spoken speech is
also a seq2seq problem. While it is impossible to consider all types of sequence transformations,
a number of special cases are worth mentioning:
Tagging and Parsing. This involves annotating a text sequence with attributes. In other words,
the number of inputs and outputs is essentially the same. For instance, we might want to know
where the verbs and subjects are. Alternatively, we might want to know which words are the
named entities. In general, the goal is to decompose and annotate text based on structural and
grammatical assumptions to get some annotation. This sounds more complex than it actually is.
Below is a very simple example of annotating a sentence with tags indicating which words refer
to named entities.
Tom has dinner in Washington with Sally.
Ent - - - Ent - Ent
Automatic Speech Recognition. With speech recognition, the input sequence x is an audio
recording of a speaker (shown in Fig. 1.3.5), and the output y is the textual transcript of what the
speaker said. The challenge is that there are many more audio frames (sound is typically sampled
at 8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text, since
thousands of samples correspond to a single spoken word. These are seq2seq problems where
the output is much shorter than the input.
Fig. 1.3.5: -D-e-e-p- L-ea-r-ni-ng-
Text to Speech. Text-to-Speech (TTS) is the inverse of speech recognition. In other words, the
input x is text and the output y is an audio file. In this case, the output is much longer than the
input. While it is easy for humans to recognize a bad audio file, this is not quite so trivial for
computers.
Machine Translation. Unlike the case of speech recognition, where corresponding inputs and
outputs occur in the same order (after alignment), in machine translation, order inversion can be
vital. In other words, while we are still converting one sequence into another, neither the number
of inputs and outputs nor the order of corresponding data examples are assumed to be the same.
1.3. Kinds of Machine Learning 31
Consider the following illustrative example of the peculiar tendency of Germans to place the verbs
at the end of sentences.
German: Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
English: Did you already check out this excellent tutorial?
Wrong alignment: Did you yourself already this excellent tutorial looked-at?
Many related problems pop up in other learning tasks. For instance, determining the order in
which a user reads a Webpage is a two-dimensional layout analysis problem. Dialogue problems
exhibit all kinds of additional complications, where determining what to say next requires taking
into account real-world knowledge and the prior state of the conversation across long temporal
distances. This is an active area of research.
1.3.2 Unsupervised learning
All the examples so far were related to Supervised Learning, i.e., situations where we feed the model
a giant dataset containing both the features and corresponding target values. You could think of
the supervised learner as having an extremely specialized job and an extremely anal boss. The
boss stands over your shoulder and tells you exactly what to do in every situation until you learn
to map from situations to actions. Working for such a boss sounds pretty lame. On the other hand,
it is easy to please this boss. You just recognize the pattern as quickly as possible and imitate their
actions.
In a completely opposite way, it could be frustrating to work for a boss who has no idea what
they want you to do. However, if you plan to be a data scientist, you?d better get used to it. The
boss might just hand you a giant dump of data and tell you to do some data science with it! This
sounds vague because it is. We call this class of problems unsupervised learning, and the type and
number of questions we could ask is limited only by our creativity. We will address a number of
unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a
few of the questions you might ask:
� Can we find a small number of prototypes that accurately summarize the data? Given a set of
photos, can we group them into landscape photos, pictures of dogs, babies, cats, mountain
peaks, etc.? Likewise, given a collection of users? browsing activity, can we group them into
users with similar behavior? This problem is typically known as clustering.
� Can we find a small number of parameters that accurately capture the relevant properties of
the data? The trajectories of a ball are quite well described by velocity, diameter, and mass
of the ball. Tailors have developed a small number of parameters that describe human body
shape fairly accurately for the purpose of fitting clothes. These problems are referred to
as subspace estimation problems. If the dependence is linear, it is called principal component
analysis.
� Is there a representation of (arbitrarily structured) objects in Euclidean space (i.e., the space
of vectors in Rn) such that symbolic properties can be well matched? This is called representation
learning and it is used to describe entities and their relations, such as Rome ?? Italy +
France = Paris.
� Is there a description of the root causes of much of the data that we observe? For instance,
if we have demographic data about house prices, pollution, crime, location, education,
salaries, etc., can we discover how they are related simply based on empirical data? The
fields concerned with causality and probabilistic graphical models address this problem.
32 Chapter 1. Introduction
� Another important and exciting recent development in unsupervised learning is the advent
of generative adversarial networks (GANs). These give us a procedural way to synthesize data,
even complicated structured data like images and audio. The underlying statistical mechanisms
are tests to check whether real and fake data are the same. We will devote a few
notebooks to them.
1.3.3 Interacting with an Environment
So far, we have not discussed where data actually comes from, or what actually happens when a
machine learning model generates an output. That is because supervised learning and unsupervised
learning do not address these issues in a very sophisticated way. In either case, we grab a big
pile of data upfront, then set our pattern recognition machines in motion without ever interacting
with the environment again. Because all of the learning takes place after the algorithm is disconnected
from the environment, this is sometimes called offline learning. For supervised learning,
the process looks like Fig. 1.3.6.
Fig. 1.3.6: Collect data for supervised learning from an environment.
This simplicity of offline learning has its charms. The upside is we can worry about pattern recognition
in isolation, without any distraction from these other problems. But the downside is that
the problem formulation is quite limiting. If you are more ambitious, or if you grew up reading
Asimov?s Robot Series, then you might imagine artificially intelligent bots capable not only of making
predictions, but of taking actions in the world. We want to think about intelligent agents, not
just predictive models. That means we need to think about choosing actions, not just making predictions.
Moreover, unlike predictions, actions actually impact the environment. If we want to train
an intelligent agent, we must account for the way its actions might impact the future observations
of the agent.
Considering the interaction with an environment opens a whole set of new modeling questions.
Does the environment:
� Remember what we did previously?
� Want to help us, e.g., a user reading text into a speech recognizer?
� Want to beat us, i.e., an adversarial setting like spam filtering (against spammers) or playing
a game (vs an opponent)?
� Not care (as in many cases)?
� Have shifting dynamics (does future data always resemble the past or do the patterns change
over time, either naturally or in response to our automated tools)?
1.3. Kinds of Machine Learning 33
This last question raises the problem of distribution shift, (when training and test data are different).
It is a problem that most of us have experienced when taking exams written by a lecturer,
while the homeworks were composed by his TAs. We will briefly describe reinforcement learning
and adversarial learning, two settings that explicitly consider interaction with an environment.
1.3.4 Reinforcement learning
If you are interested in using machine learning to develop an agent that interacts with an environment
and takes actions, then you are probably going to wind up focusing on reinforcement learning
(RL). This might include applications to robotics, to dialogue systems, and even to developing AI
for video games. Deep reinforcement learning (DRL), which applies deep neural networks to RL
problems, has surged in popularity. The breakthrough deep Q-network that beat humans at Atari
games using only the visual input17, and the AlphaGo program that dethroned the world champion
at the board game Go18 are two prominent examples.
Reinforcement learning gives a very general statement of a problem, in which an agent interacts
with an environment over a series of timesteps. At each timestep t, the agent receives some observation
ot from the environment and must choose an action at that is subsequently transmitted
back to the environment via some mechanism (sometimes called an actuator). Finally, the agent
receives a reward rt from the environment. The agent then receives a subsequent observation,
and chooses a subsequent action, and so on. The behavior of an RL agent is governed by a policy.
In short, a policy is just a function that maps from observations (of the environment) to actions.
The goal of reinforcement learning is to produce a good policy.
Fig. 1.3.7: The interaction between reinforcement learning and an environment.
It is hard to overstate the generality of the RL framework. For example, we can cast any supervised
learning problem as an RL problem. Say we had a classification problem. We could create an RL
agent with one action corresponding to each class. We could then create an environment which
gave a reward that was exactly equal to the loss function from the original supervised problem.
That being said, RL can also address many problems that supervised learning cannot. For example,
in supervised learning we always expect that the training input comes associated with the
correct label. But in RL, we do not assume that for each observation, the environment tells us the
optimal action. In general, we just get some reward. Moreover, the environment may not even
tell us which actions led to the reward.
Consider for example the game of chess. The only real reward signal comes at the end of the
game when we either win, which we might assign a reward of 1, or when we lose, which we could
17 https://www.wired.com/2015/02/google-ai-plays-atari-like-pros/
18 https://www.wired.com/2017/05/googles-alphago-trounces-humans-also-gives-boost/
34 Chapter 1. Introduction
assign a reward of -1. So reinforcement learners must deal with the credit assignment problem:
determining which actions to credit or blame for an outcome. The same goes for an employee
who gets a promotion on October 11. That promotion likely reflects a large number of well-chosen
actions over the previous year. Getting more promotions in the future requires figuring out what
actions along the way led to the promotion.
Reinforcement learners may also have to deal with the problem of partial observability. That is,
the current observation might not tell you everything about your current state. Say a cleaning
robot found itself trapped in one of many identical closets in a house. Inferring the precise location
(and thus state) of the robot might require considering its previous observations before
entering the closet.
Finally, at any given point, reinforcement learners might know of one good policy, but there might
be many other better policies that the agent has never tried. The reinforcement learner must
constantly choose whether to exploit the best currently-known strategy as a policy, or to explore
the space of strategies, potentially giving up some short-run reward in exchange for knowledge.
MDPs, bandits, and friends
The general reinforcement learning problem is a very general setting. Actions affect subsequent
observations. Rewards are only observed corresponding to the chosen actions. The environment
may be either fully or partially observed. Accounting for all this complexity at once may ask too
much of researchers. Moreover, not every practical problem exhibits all this complexity. As a
result, researchers have studied a number of special cases of reinforcement learning problems.
When the environment is fully observed, we call the RL problem a Markov Decision Process (MDP).
When the state does not depend on the previous actions, we call the problem a contextual bandit
problem. When there is no state, just a set of available actions with initially unknown rewards, this
problem is the classic multi-armed bandit problem.
1.4 Roots
Although many deep learning methods are recent inventions, humans have held the desire to analyze
data and to predict future outcomes for centuries. In fact, much of natural science has its
roots in this. For instance, the Bernoulli distribution is named after Jacob Bernoulli (1655-1705)19,
and the Gaussian distribution was discovered by Carl Friedrich Gauss (1777-1855)20. He invented,
for instance, the least mean squares algorithm, which is still used today for countless problems
from insurance calculations to medical diagnostics. These tools gave rise to an experimental approach
in the natural sciences�for instance, Ohm?s law relating current and voltage in a resistor
is perfectly described by a linear model.
Even in the middle ages, mathematicians had a keen intuition of estimates. For instance, the
geometry book of Jacob Kobel (1460-1533)21 illustrates averaging the length of 16 adult men?s feet
to obtain the average foot length.
19 https://en.wikipedia.org/wiki/Jacob_Bernoulli
20 https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss
21 https://www.maa.org/press/periodicals/convergence/mathematical-treasures-jacob-kobels-geometry
1.4. Roots 35
Fig. 1.4.1: Estimating the length of a foot
Fig. 1.4.1 illustrates how this estimator works. The 16 adult men were asked to line up in a row,
when leaving church. Their aggregate length was then divided by 16 to obtain an estimate for
what now amounts to 1 foot. This �algorithm� was later improved to deal with misshapen feet�
the 2 men with the shortest and longest feet respectively were sent away, averaging only over the
remainder. This is one of the earliest examples of the trimmed mean estimate.
Statistics really took off with the collection and availability of data. One of its titans, Ronald Fisher
(1890-1962)22, contributed significantly to its theory and also its applications in genetics. Many of
his algorithms (such as Linear Discriminant Analysis) and formula (such as the Fisher Information
Matrix) are still in frequent use today (even the Iris dataset that he released in 1936 is still used
sometimes to illustrate machine learning algorithms). Fisher was also a proponent of eugenics,
which should remind us that the morally dubious use of data science has as long and enduring a
history as its productive use in industry and the natural sciences.
A second influence for machine learning came from Information Theory (Claude Shannon, 1916-
2001)23 and the Theory of computation via Alan Turing (1912-1954)24. Turing posed the question
�can machines think?� in his famous paper Computing machinery and intelligence25 (Mind, October
1950). In what he described as the Turing test, a machine can be considered intelligent if it
is difficult for a human evaluator to distinguish between the replies from a machine and a human
based on textual interactions.
Another influence can be found in neuroscience and psychology. After all, humans clearly exhibit
intelligent behavior. It is thus only reasonable to ask whether one could explain and possibly re-
22 https://en.wikipedia.org/wiki/Ronald_Fisher
23 https://en.wikipedia.org/wiki/Claude_Shannon
24 https://en.wikipedia.org/wiki/Alan_Turing
25 https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence
36 Chapter 1. Introduction
verse engineer this capacity. One of the oldest algorithms inspired in this fashion was formulated
by Donald Hebb (1904-1985)26. In his groundbreaking book The Organization of Behavior (Hebb
& Hebb, 1949), he posited that neurons learn by positive reinforcement. This became known as
the Hebbian learning rule. It is the prototype of Rosenblatt?s perceptron learning algorithm and it
laid the foundations of many stochastic gradient descent algorithms that underpin deep learning
today: reinforce desirable behavior and diminish undesirable behavior to obtain good settings of
the parameters in a neural network.
Biological inspiration is what gave neural networks their name. For over a century (dating back
to the models of Alexander Bain, 1873 and James Sherrington, 1890), researchers have tried to
assemble computational circuits that resemble networks of interacting neurons. Over time, the
interpretation of biology has become less literal but the name stuck. At its heart, lie a few key
principles that can be found in most networks today:
� The alternation of linear and nonlinear processing units, often referred to as layers.
� The use of the chain rule (also known as backpropagation) for adjusting parameters in the
entire network at once.
After initial rapid progress, research in neural networks languished from around 1995 until 2005.
This was due to a number of reasons. Training a network is computationally very expensive.
While RAM was plentiful at the end of the past century, computational power was scarce. Second,
datasets were relatively small. In fact, Fisher?s Iris dataset from 1932 was a popular tool for
testing the efficacy of algorithms. MNIST with its 60,000 handwritten digits was considered huge.
Given the scarcity of data and computation, strong statistical tools such as Kernel Methods, Decision
Trees and Graphical Models proved empirically superior. Unlike neural networks, they did
not require weeks to train and provided predictable results with strong theoretical guarantees.
1.5 The Road to Deep Learning
Much of this changed with the ready availability of large amounts of data, due to the World Wide
Web, the advent of companies serving hundreds of millions of users online, a dissemination of
cheap, high-quality sensors, cheap data storage (Kryder?s law), and cheap computation (Moore?s
law), in particular in the form of GPUs, originally engineered for computer gaming. Suddenly
algorithms and models that seemed computationally infeasible became relevant (and vice versa).
This is best illustrated in Table 1.5.1.
Table 1.5.1: Dataset vs. computer memory and computational
power
Decade Dataset Memory
Floating Point Calculations per Second
1970 100 (Iris) 1 KB 100 KF (Intel 8080)
1980 1 K (House prices in Boston) 100 KB 1 MF (Intel 80186)
1990 10 K (optical character recognition) 10 MB 10 MF (Intel 80486)
2000 10 M (web pages) 100 MB 1 GF (Intel Core)
2010 10 G (advertising) 1 GB 1 TF (Nvidia C2050)
2020 1 T (social network) 100 GB 1 PF (Nvidia DGX-2)
It is evident that RAM has not kept pace with the growth in data. At the same time, the increase
26 https://en.wikipedia.org/wiki/Donald_O._Hebb
1.5. The Road to Deep Learning 37
in computational power has outpaced that of the data available. This means that statistical models
needed to become more memory efficient (this is typically achieved by adding nonlinearities)
while simultaneously being able to spend more time on optimizing these parameters, due
to an increased compute budget. Consequently, the sweet spot in machine learning and statistics
moved from (generalized) linear models and kernel methods to deep networks. This is also
one of the reasons why many of the mainstays of deep learning, such as multilayer perceptrons
(McCulloch & Pitts, 1943), convolutional neural networks (LeCun et al., 1998), Long Short-Term
Memory (Hochreiter & Schmidhuber, 1997), and Q-Learning (Watkins & Dayan, 1992), were essentially
�rediscovered� in the past decade, after laying comparatively dormant for considerable
time.
The recent progress in statistical models, applications, and algorithms, has sometimes been
likened to the Cambrian Explosion: a moment of rapid progress in the evolution of species. Indeed,
the state of the art is not just a mere consequence of available resources, applied to decades
old algorithms. Note that the list below barely scratches the surface of the ideas that have helped
researchers achieve tremendous progress over the past decade.
� Novel methods for capacity control, such as Dropout (Srivastava et al., 2014) have helped
to mitigate the danger of overfitting. This was achieved by applying noise injection (Bishop,
1995) throughout the network, replacing weights by random variables for training purposes.
� Attention mechanisms solved a second problem that had plagued statistics for over a century:
how to increase the memory and complexity of a system without increasing the number
of learnable parameters. (Bahdanau et al., 2014) found an elegant solution by using what
can only be viewed as a learnable pointer structure. Rather than having to remember an entire
sentence, e.g., for machine translation in a fixed-dimensional representation, all that
needed to be stored was a pointer to the intermediate state of the translation process. This
allowed for significantly increased accuracy for long sentences, since the model no longer
needed to remember the entire sentence before commencing the generation of a new sentence.
� Multi-stage designs, e.g., via the Memory Networks (MemNets) (Sukhbaatar et al., 2015) and
the Neural Programmer-Interpreter (Reed & DeFreitas, 2015) allowed statistical modelers
to describe iterative approaches to reasoning. These tools allow for an internal state of the
deep network to be modified repeatedly, thus carrying out subsequent steps in a chain of
reasoning, similar to how a processor can modify memory for a computation.
� Another key development was the invention of GANs (Goodfellow et al., 2014). Traditionally,
statistical methods for density estimation and generative models focused on finding proper
probability distributions and (often approximate) algorithms for sampling from them. As
a result, these algorithms were largely limited by the lack of flexibility inherent in the statistical
models. The crucial innovation in GANs was to replace the sampler by an arbitrary
algorithm with differentiable parameters. These are then adjusted in such a way that the discriminator
(effectively a two-sample test) cannot distinguish fake from real data. Through
the ability to use arbitrary algorithms to generate data, it opened up density estimation to
a wide variety of techniques. Examples of galloping Zebras (Zhu et al., 2017) and of fake
celebrity faces (Karras et al., 2017) are both testimony to this progress. Even amateur doodlers
can produce photorealistic images based on just sketches that describe how the layout
of a scene looks like (Park et al., 2019).
� In many cases, a single GPU is insufficient to process the large amounts of data available for
training. Over the past decade the ability to build parallel distributed training algorithms
has improved significantly. One of the key challenges in designing scalable algorithms is
that the workhorse of deep learning optimization, stochastic gradient descent, relies on rel-
38 Chapter 1. Introduction
atively small minibatches of data to be processed. At the same time, small batches limit the
efficiency of GPUs. Hence, training on 1024 GPUs with a minibatch size of, say 32 images
per batch amounts to an aggregate minibatch of 32k images. Recent work, first by Li (Li,
2017), and subsequently by (You et al., 2017) and (Jia et al., 2018) pushed the size up to 64k
observations, reducing training time for ResNet50 on ImageNet to less than 7 minutes. For
comparison�initially training times were measured in the order of days.
� The ability to parallelize computation has also contributed quite crucially to progress in reinforcement
learning, at least whenever simulation is an option. This has led to significant
progress in computers achieving superhuman performance in Go, Atari games, Starcraft,
and in physics simulations (e.g., using MuJoCo). See e.g., (Silver et al., 2016) for a description
of how to achieve this in AlphaGo. In a nutshell, reinforcement learning works best if
plenty of (state, action, reward) triples are available, i.e., whenever it is possible to try out
lots of things to learn how they relate to each other. Simulation provides such an avenue.
� Deep Learning frameworks have played a crucial role in disseminating ideas. The first
generation of frameworks allowing for easy modeling encompassed Caffe27, Torch28, and
Theano29. Many seminal papers were written using these tools. By now, they have been superseded
by TensorFlow30, often used via its high level API Keras31, CNTK32, Caffe 233, and
Apache MxNet34. The third generation of tools, namely imperative tools for deep learning,
was arguably spearheaded by Chainer35, which used a syntax similar to Python NumPy to
describe models. This idea was adopted by both PyTorch36, the Gluon API37 of MXNet, and
Jax38. It is the latter group that this course uses to teach deep learning.
The division of labor between systems researchers building better tools and statistical modelers
building better networks has greatly simplified things. For instance, training a linear logistic regression
model used to be a nontrivial homework problem, worthy to give to new machine learning
PhD students at Carnegie Mellon University in 2014. By now, this task can be accomplished
with less than 10 lines of code, putting it firmly into the grasp of programmers.
1.6 Success Stories
Artificial Intelligence has a long history of delivering results that would be difficult to accomplish
otherwise. For instance, mail is sorted using optical character recognition. These systems have
been deployed since the 90s (this is, after all, the source of the famous MNIST and USPS sets of
handwritten digits). The same applies to reading checks for bank deposits and scoring creditworthiness
of applicants. Financial transactions are checked for fraud automatically. This forms the
backbone of many e-commerce payment systems, such as PayPal, Stripe, AliPay, WeChat, Apple,
Visa, MasterCard. Computer programs for chess have been competitive for decades. Machine
learning feeds search, recommendation, personalization and ranking on the Internet. In other
words, artificial intelligence and machine learning are pervasive, albeit often hidden from sight.
27 https://github.com/BVLC/caffe
28 https://github.com/torch
29 https://github.com/Theano/Theano
30 https://github.com/tensorflow/tensorflow
31 https://github.com/keras-team/keras
32 https://github.com/Microsoft/CNTK
33 https://github.com/caffe2/caffe2
34 https://github.com/apache/incubator-mxnet
35 https://github.com/chainer/chainer
36 https://github.com/pytorch/pytorch
37 https://github.com/apache/incubator-mxnet
38 https://github.com/google/jax
1.6. Success Stories 39
It is only recently that AI has been in the limelight, mostly due to solutions to problems that were
considered intractable previously.
� Intelligent assistants, such as Apple?s Siri, Amazon?s Alexa, or Google?s assistant are able to
answer spoken questions with a reasonable degree of accuracy. This includes menial tasks
such as turning on light switches (a boon to the disabled) up to making barber?s appointments
and offering phone support dialog. This is likely the most noticeable sign that AI is
affecting our lives.
� A key ingredient in digital assistants is the ability to recognize speech accurately. Gradually
the accuracy of such systems has increased to the point where they reach human parity
(Xiong et al., 2018) for certain applications.
� Object recognition likewise has come a long way. Estimating the object in a picture was a
fairly challenging task in 2010. On the ImageNet benchmark (Lin et al., 2010) achieved a
top-5 error rate of 28%. By 2017, (Hu et al., 2018) reduced this error rate to 2.25%. Similarly,
stunning results have been achieved for identifying birds, or diagnosing skin cancer.
� Games used to be a bastion of human intelligence. Starting from TDGammon [23], a program
for playing Backgammon using temporal difference (TD) reinforcement learning, algorithmic
and computational progress has led to algorithms for a wide range of applications.
Unlike Backgammon, chess has a much more complex state space and set of actions. Deep-
Blue beat Garry Kasparov, Campbell et al. (Campbell et al., 2002), using massive parallelism,
special purpose hardware and efficient search through the game tree. Go is more difficult
still, due to its huge state space. AlphaGo reached human parity in 2015, (Silver et al., 2016)
using Deep Learning combined with Monte Carlo tree sampling. The challenge in Poker
was that the state space is large and it is not fully observed (we do not know the opponents?
cards). Libratus exceeded human performance in Poker using efficiently structured strategies
(Brown & Sandholm, 2017). This illustrates the impressive progress in games and the
fact that advanced algorithms played a crucial part in them.
� Another indication of progress in AI is the advent of self-driving cars and trucks. While
full autonomy is not quite within reach yet, excellent progress has been made in this direction,
with companies such as Tesla, NVIDIA, and Waymo shipping products that enable at
least partial autonomy. What makes full autonomy so challenging is that proper driving requires
the ability to perceive, to reason and to incorporate rules into a system. At present,
deep learning is used primarily in the computer vision aspect of these problems. The rest is
heavily tuned by engineers.
Again, the above list barely scratches the surface of where machine learning has impacted practical
applications. For instance, robotics, logistics, computational biology, particle physics, and
astronomy owe some of their most impressive recent advances at least in parts to machine learning.
ML is thus becoming a ubiquitous tool for engineers and scientists.
Frequently, the question of the AI apocalypse, or the AI singularity has been raised in nontechnical
articles on AI. The fear is that somehow machine learning systems will become sentient
and decide independently from their programmers (and masters) about things that directly affect
the livelihood of humans. To some extent, AI already affects the livelihood of humans in an
immediate way�creditworthiness is assessed automatically, autopilots mostly navigate vehicles,
decisions about whether to grant bail use statistical data as input. More frivolously, we can ask
Alexa to switch on the coffee machine.
Fortunately, we are far from a sentient AI system that is ready to manipulate its human creators
(or burn their coffee). First, AI systems are engineered, trained and deployed in a specific, goaloriented
manner. While their behavior might give the illusion of general intelligence, it is a com-
40 Chapter 1. Introduction
bination of rules, heuristics and statistical models that underlie the design. Second, at present
tools for artificial general intelligence simply do not exist that are able to improve themselves, reason
about themselves, and that are able to modify, extend and improve their own architecture
while trying to solve general tasks.
A much more pressing concern is how AI is being used in our daily lives. It is likely that many menial
tasks fulfilled by truck drivers and shop assistants can and will be automated. Farm robots will
likely reduce the cost for organic farming but they will also automate harvesting operations. This
phase of the industrial revolution may have profound consequences on large swaths of society
(truck drivers and shop assistants are some of the most common jobs in many states). Furthermore,
statistical models, when applied without care can lead to racial, gender or age bias and raise
reasonable concerns about procedural fairness if automated to drive consequential decisions. It
is important to ensure that these algorithms are used with care. With what we know today, this
strikes us a much more pressing concern than the potential of malevolent superintelligence to
destroy humanity.
Summary
� Machine learning studies how computer systems can leverage experience (often data) to improve
performance at specific tasks. It combines ideas from statistics, data mining, artificial
intelligence, and optimization. Often, it is used as a means of implementing artificiallyintelligent
solutions.
� As a class of machine learning, representational learning focuses on how to automatically
find the appropriate way to represent data. This is often accomplished by a progression of
learned transformations.
� Much of the recent progress in deep learning has been triggered by an abundance of data
arising from cheap sensors and Internet-scale applications, and by significant progress in
computation, mostly through GPUs.
� Whole system optimization is a key component in obtaining good performance. The availability
of efficient deep learning frameworks has made design and implementation of this
significantly easier.
Exercises
1. Which parts of code that you are currently writing could be �learned�, i.e., improved by
learning and automatically determining design choices that are made in your code? Does
your code include heuristic design choices?
2. Which problems that you encounter have many examples for how to solve them, yet no specific
way to automate them? These may be prime candidates for using deep learning.
3. Viewing the development of artificial intelligence as a new industrial revolution, what is the
relationship between algorithms and data? Is it similar to steam engines and coal (what is
the fundamental difference)?
4. Where else can you apply the end-to-end training approach (such as in Fig. 1.1.2)? Physics?
Engineering? Econometrics?
1.6. Success Stories 41
Discussions39
39 https://discuss.d2l.ai/t/22
42 Chapter 1. Introduction
2 | Preliminaries
To get started with deep learning, we will need to develop a few basic skills. All machine learning
is concerned with extracting information from data. So we will begin by learning the practical
skills for storing, manipulating, and preprocessing data.
Moreover, machine learning typically requires working with large datasets, which we can think
of as tables, where the rows correspond to examples and the columns correspond to attributes.
Linear algebra gives us a powerful set of techniques for working with tabular data. We will not go
too far into the weeds but rather focus on the basic of matrix operations and their implementation.
Additionally, deep learning is all about optimization. We have a model with some parameters and
we want to find those that fit our data the best. Determining which way to move each parameter at
each step of an algorithm requires a little bit of calculus, which will be briefly introduced. Fortunately,
the autograd package automatically computes differentiation for us, and we will cover it
next.
Next, machine learning is concerned with making predictions: what is the likely value of some unknown
attribute, given the information that we observe? To reason rigorously under uncertainty
we will need to invoke the language of probability.
In the end, the official documentation provides plenty of descriptions and examples that are beyond
this book. To conclude the chapter, we will show you how to look up documentation for the
needed information.
This book has kept the mathematical content to the minimum necessary to get a proper understanding
of deep learning. However, it does not mean that this book is mathematics free. Thus,
this chapter provides a rapid introduction to basic and frequently-used mathematics to allow anyone
to understand at least most of the mathematical content of the book. If you wish to understand
all of the mathematical content, further reviewing the online appendix on mathematics40 should
be sufficient.
2.1 Data Manipulation
In order to get anything done, we need some way to store and manipulate data. Generally, there
are two important things we need to do with data: (i) acquire them; and (ii) process them once they
are inside the computer. There is no point in acquiring data without some way to store it, so let us
get our hands dirty first by playing with synthetic data. To start, we introduce the n-dimensional
array, which is also called the tensor.
If you have worked with NumPy, the most widely-used scientific computing package in Python,
then you will find this section familiar. No matter which framework you use, its tensor class
40 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html
43
(ndarray in MXNet, Tensor in both PyTorch and TensorFlow) is similar to NumPy?s ndarray with
a few killer features. First, GPU is well-supported to accelerate the computation whereas NumPy
only supports CPU computation. Second, the tensor class supports automatic differentiation.
These properties make the tensor class suitable for deep learning. Throughout the book, when
we say tensors, we are referring to instances of the tensor class unless otherwise stated.
2.1.1 Getting Started
In this section, we aim to get you up and running, equipping you with the basic math and numerical
computing tools that you will build on as you progress through the book. Do not worry if you
struggle to grok some of the mathematical concepts or library functions. The following sections
will revisit this material in the context of practical examples and it will sink. On the other hand,
if you already have some background and want to go deeper into the mathematical content, just
skip this section.
To start, we import the np (numpy) and npx (numpy_extension) modules from MXNet. Here, the np
module includes functions supported by NumPy, while the npx module contains a set of extensions
developed to empower deep learning within a NumPy-like environment. When using tensors, we
almost always invoke the set_np function: this is for compatibility of tensor processing by other
components of MXNet.
from mxnet import np, npx
npx.set_np()
A tensor represents a (possibly multi-dimensional) array of numerical values. With one axis, a
tensor corresponds (in math) to a vector. With two axes, a tensor corresponds to a matrix. Tensors
with more than two axes do not have special mathematical names.
To start, we can use arange to create a row vector x containing the first 12 integers starting with 0,
though they are created as floats by default. Each of the values in a tensor is called an element of
the tensor. For instance, there are 12 elements in the tensor x. Unless otherwise specified, a new
tensor will be stored in main memory and designated for CPU-based computation.
x = np.arange(12)
x
array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.])
We can access a tensor?s shape (the length along each axis) by inspecting its shape property.
x.shape
(12,)
If we just want to know the total number of elements in a tensor, i.e., the product of all of the shape
elements, we can inspect its size. Because we are dealing with a vector here, the single element
of its shape is identical to its size.
x.size
44 Chapter 2. Preliminaries
12
To change the shape of a tensor without altering either the number of elements or their values, we
can invoke the reshape function. For example, we can transform our tensor, x, from a row vector
with shape (12,) to a matrix with shape (3, 4). This new tensor contains the exact same values, but
views them as a matrix organized as 3 rows and 4 columns. To reiterate, although the shape has
changed, the elements have not. Note that the size is unaltered by reshaping.
X = x.reshape(3, 4)
X
array([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.]])
Reshaping by manually specifying every dimension is unnecessary. If our target shape is a matrix
with shape (height, width), then after we know the width, the height is given implicitly. Why
should we have to perform the division ourselves? In the example above, to get a matrix with 3
rows, we specified both that it should have 3 rows and 4 columns. Fortunately, tensors can automatically
work out one dimension given the rest. We invoke this capability by placing -1 for
the dimension that we would like tensors to automatically infer. In our case, instead of calling
x.reshape(3, 4), we could have equivalently called x.reshape(-1, 4) or x.reshape(3, -1).
Typically, we will want our matrices initialized either with zeros, ones, some other constants, or
numbers randomly sampled from a specific distribution. We can create a tensor representing a
tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:
np.zeros((2, 3, 4))
array([[[0., 0., 0., 0.],
[0., 0., 0., 0.],
[0., 0., 0., 0.]],
[[0., 0., 0., 0.],
[0., 0., 0., 0.],
[0., 0., 0., 0.]]])
Similarly, we can create tensors with each element set to 1 as follows:
np.ones((2, 3, 4))
array([[[1., 1., 1., 1.],
[1., 1., 1., 1.],
[1., 1., 1., 1.]],
[[1., 1., 1., 1.],
[1., 1., 1., 1.],
[1., 1., 1., 1.]]])
Often, we want to randomly sample the values for each element in a tensor from some probability
distribution. For example, when we construct arrays to serve as parameters in a neural network,
2.1. Data Manipulation 45
we will typically initialize their values randomly. The following snippet creates a tensor with shape
(3, 4). Each of its elements is randomly sampled from a standard Gaussian (normal) distribution
with a mean of 0 and a standard deviation of 1.
np.random.normal(0, 1, size=(3, 4))
array([[ 2.2122064 , 1.1630787 , 0.7740038 , 0.4838046 ],
[ 1.0434405 , 0.29956347, 1.1839255 , 0.15302546],
[ 1.8917114 , -1.1688148 , -1.2347414 , 1.5580711 ]])
We can also specify the exact values for each element in the desired tensor by supplying a Python
list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis
0, and the inner list to axis 1.
np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
array([[2., 1., 4., 3.],
[1., 2., 3., 4.],
[4., 3., 2., 1.]])
2.1.2 Operations
This book is not about software engineering. Our interests are not limited to simply reading and
writing data from/to arrays. We want to perform mathematical operations on those arrays. Some
of the simplest and most useful operations are the elementwise operations. These apply a standard
scalar operation to each element of an array. For functions that take two arrays as inputs,
elementwise operations apply some standard binary operator on each pair of corresponding elements
from the two arrays. We can create an elementwise function from any function that maps
from a scalar to a scalar.
In mathematical notation, we would denote such a unary scalar operator (taking one input) by the
signature f : R ! R. This just means that the function is mapping from any real number (R) onto
another. Likewise, we denote a binary scalar operator (taking two real inputs, and yielding one
output) by the signature f : R;R ! R. Given any two vectors u and v of the same shape, and a binary
operator f, we can produce a vector c = F(u; v) by setting ci   f(ui; vi) for all i, where ci; ui, and
vi are the ith elements of vectors c; u, and v. Here, we produced the vector-valued F : Rd;Rd ! Rd
by lifting the scalar function to an elementwise vector operation.
The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise
operations for any identically-shaped tensors of arbitrary shape. We can call elementwise
operations on any two tensors of the same shape. In the following example, we use commas to
formulate a 5-element tuple, where each element is the result of an elementwise operation.
x = np.array([1, 2, 4, 8])
y = np.array([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y # The ** operator is exponentiation
(array([ 3., 4., 6., 10.]),
array([-1., 0., 2., 6.]),
(continues on next page)
46 Chapter 2. Preliminaries
(continued from previous page)
array([ 2., 4., 8., 16.]),
array([0.5, 1. , 2. , 4. ]),
array([ 1., 4., 16., 64.]))
Many more operations can be applied elementwise, including unary operators like exponentiation.
np.exp(x)
array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03])
In addition to elementwise computations, we can also perform linear algebra operations, including
vector dot products and matrix multiplication. We will explain the crucial bits of linear algebra
(with no assumed prior knowledge) in Section 2.3.
We can also concatenate multiple tensors together, stacking them end-to-end to form a larger tensor.
We just need to provide a list of tensors and tell the system along which axis to concatenate.
The example below shows what happens when we concatenate two matrices along rows (axis 0,
the first element of the shape) vs. columns (axis 1, the second element of the shape). We can see
that the first output tensor?s axis-0 length (6) is the sum of the two input tensors? axis-0 lengths
(3+3); while the second output tensor?s axis-1 length (8) is the sum of the two input tensors? axis-1
lengths (4 + 4).
X = np.arange(12).reshape(3, 4)
Y = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
np.concatenate([X, Y], axis=0), np.concatenate([X, Y], axis=1)
(array([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[ 2., 1., 4., 3.],
[ 1., 2., 3., 4.],
[ 4., 3., 2., 1.]]),
array([[ 0., 1., 2., 3., 2., 1., 4., 3.],
[ 4., 5., 6., 7., 1., 2., 3., 4.],
[ 8., 9., 10., 11., 4., 3., 2., 1.]]))
Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example.
For each position, if X and Y are equal at that position, the corresponding entry in the new tensor
takes a value of 1, meaning that the logical statement X == Y is true at that position; otherwise that
position takes 0.
X == Y
array([[False, True, False, True],
[False, False, False, False],
[False, False, False, False]])
Summing all the elements in the tensor yields a tensor with only one element.
2.1. Data Manipulation 47
X.sum()
array(66.)
2.1.3 Broadcasting Mechanism
In the above section, we saw how to perform elementwise operations on two tensors of the same
shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations
by invoking the broadcasting mechanism. This mechanism works in the following way:
First, expand one or both arrays by copying elements appropriately so that after this transformation,
the two tensors have the same shape. Second, carry out the elementwise operations on the
resulting arrays.
In most cases, we broadcast along an axis where an array initially only has length 1, such as in the
following example:
a = np.arange(3).reshape(3, 1)
b = np.arange(2).reshape(1, 2)
a, b
(array([[0.],
[1.],
[2.]]),
array([[0., 1.]]))
Since a and b are 3  1 and 1  2 matrices respectively, their shapes do not match up if we want
to add them. We broadcast the entries of both matrices into a larger 3  2 matrix as follows: for
matrix a it replicates the columns and for matrix b it replicates the rows before adding up both
elementwise.
a + b
array([[0., 1.],
[1., 2.],
[2., 3.]])
2.1.4 Indexing and Slicing
Just as in any other Python array, elements in a tensor can be accessed by index. As in any Python
array, the first element has index 0 and ranges are specified to include the first but before the last
element. As in standard Python lists, we can access elements according to their relative position
to the end of the list by using negative indices.
Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:
X[-1], X[1:3]
48 Chapter 2. Preliminaries
(array([ 8., 9., 10., 11.]),
array([[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.]]))
Beyond reading, we can also write elements of a matrix by specifying indices.
X[1, 2] = 9
X
array([[ 0., 1., 2., 3.],
[ 4., 5., 9., 7.],
[ 8., 9., 10., 11.]])
If we want to assign multiple elements the same value, we simply index all of them and then assign
them the value. For instance, [0:2, :] accesses the first and second rows, where : takes all the
elements along axis 1 (column). While we discussed indexing for matrices, this obviously also
works for vectors and for tensors of more than 2 dimensions.
X[0:2, :] = 12
X
array([[12., 12., 12., 12.],
[12., 12., 12., 12.],
[ 8., 9., 10., 11.]])
2.1.5 Saving Memory
Running operations can cause new memory to be allocated to host results. For example, if we
write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at
the newly allocated memory. In the following example, we demonstrate this with Python?s id()
function, which gives us the exact address of the referenced object in memory. After running Y =
Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y
+ X, allocating new memory for the result and then makes Y point to this new location in memory.
before = id(Y)
Y = Y + X
id(Y) == before
False
This might be undesirable for two reasons. First, we do not want to run around allocating memory
unnecessarily all the time. In machine learning, we might have hundreds of megabytes of
parameters and update all of them multiple times per second. Typically, we will want to perform
these updates in place. Second, we might point at the same parameters from multiple variables.
If we do not update in place, other references will still point to the old memory location, making
it possible for parts of our code to inadvertently reference stale parameters.
Fortunately, performing in-place operations is easy. We can assign the result of an operation to
a previously allocated array with slice notation, e.g., Y[:] = <expression>. To illustrate this
2.1. Data Manipulation 49
concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to
allocate a block of 0 entries.
Z = np.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
id(Z): 140228072655200
id(Z): 140228072655200
If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X +=
Y to reduce the memory overhead of the operation.
before = id(X)
X += Y
id(X) == before
True
2.1.6 Conversion to Other Python Objects
Converting to a NumPy tensor, or vice versa, is easy. The converted result does not share memory.
This minor inconvenience is actually quite important: when you perform operations on the CPU
or on GPUs, you do not want to halt computation, waiting to see whether the NumPy package of
Python might want to be doing something else with the same chunk of memory.
A = X.asnumpy()
B = np.array(A)
type(A), type(B)
(numpy.ndarray, mxnet.numpy.ndarray)
To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python?s built-in
functions.
a = np.array([3.5])
a, a.item(), float(a), int(a)
(array([3.5]), 3.5, 3.5, 3)
50 Chapter 2. Preliminaries
Summary
� The main interface to store and manipulate data for deep learning is the tensor (n-
dimensional array). It provides a variety of functionalities including basic mathematics operations,
broadcasting, indexing, slicing, memory saving, and conversion to other Python
objects.
Exercises
1. Run the code in this section. Change the conditional statement X == Y in this section to X <
Y or X > Y, and then see what kind of tensor you can get.
2. Replace the two tensors that operate by element in the broadcasting mechanism with other
shapes, e.g., 3-dimensional tensors. Is the result the same as expected?
Discussions41
2.2 Data Preprocessing
So far we have introduced a variety of techniques for manipulating data that are already stored in
tensors. To apply deep learning to solving real-world problems, we often begin with preprocessing
raw data, rather than those nicely prepared data in the tensor format. Among popular data
analytic tools in Python, the pandas package is commonly used. Like many other extension packages
in the vast ecosystem of Python, pandas can work together with tensors. So, we will briefly
walk through steps for preprocessing raw data with pandas and converting them into the tensor
format. We will cover more data preprocessing techniques in later chapters.
2.2.1 Reading the Dataset
As an example, we begin by creating an artificial dataset that is stored in a csv (comma-separated
values) file ../data/house_tiny.csv. Data stored in other formats may be processed in similar
ways. The following mkdir_if_not_exist function ensures that the directory ../data exists.
Note that the comment #@save is a special mark where the following function, class,
or statements are saved in the d2l package so later they can be directly invoked (e.g., d2l.
mkdir_if_not_exist(path)) without being redefined.
import os
def mkdir_if_not_exist(path): #@save
"""Make a directory if it does not exist."""
if not isinstance(path, str):
path = os.path.join(*path)
if not os.path.exists(path):
os.makedirs(path)
Below we write the dataset row by row into a csv file.
41 https://discuss.d2l.ai/t/26
2.2. Data Preprocessing 51
data_file = '../data/house_tiny.csv'
mkdir_if_not_exist('../data')
with open(data_file, 'w') as f:
f.write('NumRooms,Alley,Price\n') # Column names
f.write('NA,Pave,127500\n') # Each row represents a data example
f.write('2,NA,106000\n')
f.write('4,NA,178100\n')
f.write('NA,NA,140000\n')
To load the raw dataset from the created csv file, we import the pandas package and invoke the
read_csv function. This dataset has four rows and three columns, where each row describes the
number of rooms (�NumRooms�), the alley type (�Alley�), and the price (�Price�) of a house.
# If pandas is not installed, just uncomment the following line:
# !pip install pandas
import pandas as pd
data = pd.read_csv(data_file)
print(data)
NumRooms Alley Price
0 NaN Pave 127500
1 2.0 NaN 106000
2 4.0 NaN 178100
3 NaN NaN 140000
2.2.2 Handling Missing Data
Note that �NaN� entries are missing values. To handle missing data, typical methods include imputation
and deletion, where imputation replaces missing values with substituted ones, while deletion
ignores missing values. Here we will consider imputation.
By integer-location based indexing (iloc), we split data into inputs and outputs, where the former
takes the first two columns while the latter only keeps the last column. For numerical values in
inputs that are missing, we replace the �NaN� entries with the mean value of the same column.
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
NumRooms Alley
0 3.0 Pave
1 2.0 NaN
2 4.0 NaN
3 3.0 NaN
For categorical or discrete values in inputs, we consider �NaN� as a category. Since the �Alley�
column only takes two types of categorical values �Pave� and �NaN�, pandas can automatically
convert this column to two columns �Alley_Pave� and �Alley_nan�. A row whose alley type is �Pave�
will set values of �Alley_Pave� and �Alley_nan� to 1 and 0. A row with a missing alley type will set
their values to 0 and 1.
52 Chapter 2. Preliminaries
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
NumRooms Alley_Pave Alley_nan
0 3.0 1 0
1 2.0 0 1
2 4.0 0 1
3 3.0 0 1
2.2.3 Conversion to the Tensor Format
Now that all the entries in inputs and outputs are numerical, they can be converted to the tensor
format. Once data are in this format, they can be further manipulated with those tensor functionalities
that we have introduced in Section 2.1.
from mxnet import np
X, y = np.array(inputs.values), np.array(outputs.values)
X, y
(array([[3., 1., 0.],
[2., 0., 1.],
[4., 0., 1.],
[3., 0., 1.]], dtype=float64),
array([127500, 106000, 178100, 140000], dtype=int64))
Summary
� Like many other extension packages in the vast ecosystem of Python, pandas can work together
with tensors.
� Imputation and deletion can be used to handle missing data.
Exercises
Create a raw dataset with more rows and columns.
1. Delete the column with the most missing values.
2. Convert the preprocessed dataset to the tensor format.
Discussions42
42 https://discuss.d2l.ai/t/28
2.2. Data Preprocessing 53
2.3 Linear Algebra
Now that you can store and manipulate data, let us briefly review the subset of basic linear algebra
that you will need to understand and implement most of models covered in this book. Below, we
introduce the basic mathematical objects, arithmetic, and operations in linear algebra, expressing
each of them through mathematical notation and the corresponding implementation in code.
2.3.1 Scalars
If you never studied linear algebra or machine learning, then your past experience with math
probably consisted of thinking about one number at a time. And, if you ever balanced a checkbook
or even paid for dinner at a restaurant then you already know how to do basic things like
adding and multiplying pairs of numbers. For example, the temperature in Palo Alto is 52 degrees
Fahrenheit. Formally, we call values consisting of just one numerical quantity scalars. If
you wanted to convert this value to Celsius (the metric system?s more sensible temperature scale),
you would evaluate the expression c = 5
9 (f ?? 32), setting f to 52. In this equation, each of the
terms�5, 9, and 32�are scalar values. The placeholders c and f are called variables and they represent
unknown scalar values.
In this book, we adopt the mathematical notation where scalar variables are denoted by ordinary
lower-cased letters (e.g., x, y, and z). We denote the space of all (continuous) real-valued scalars
by R. For expedience, we will punt on rigorous definitions of what precisely space is, but just
remember for now that the expression x 2 R is a formal way to say that x is a real-valued scalar.
The symbol 2 can be pronounced �in� and simply denotes membership in a set. Analogously, we
could write x; y 2 f0; 1g to state that x and y are numbers whose value can only be 0 or 1.
A scalar is represented by a tensor with just one element. In the next snippet, we instantiate two
scalars and perform some familiar arithmetic operations with them, namely addition, multiplication,
division, and exponentiation.
from mxnet import np, npx
npx.set_np()
x = np.array(3.0)
y = np.array(2.0)
x + y, x * y, x / y, x ** y
(array(5.), array(6.), array(1.5), array(9.))
2.3.2 Vectors
You can think of a vector as simply a list of scalar values. We call these values the elements (entries
or components) of the vector. When our vectors represent examples from our dataset, their values
hold some real-world significance. For example, if we were training a model to predict the risk that
a loan defaults, we might associate each applicant with a vector whose components correspond
to their income, length of employment, number of previous defaults, and other factors. If we
were studying the risk of heart attacks hospital patients potentially face, we might represent each
patient by a vector whose components capture their most recent vital signs, cholesterol levels,
54 Chapter 2. Preliminaries
minutes of exercise per day, etc. In math notation, we will usually denote vectors as bold-faced,
lower-cased letters (e.g., x, y, and z).
We work with vectors via one-dimensional tensors. In general tensors can have arbitrary lengths,
subject to the memory limits of your machine.
x = np.arange(4)
x
array([0., 1., 2., 3.])
We can refer to any element of a vector by using a subscript. For example, we can refer to the ith
element of x by xi. Note that the element xi is a scalar, so we do not bold-face the font when referring
to it. Extensive literature considers column vectors to be the default orientation of vectors,
so does this book. In math, a vector x can be written as
x =
2
6664
x1
x2
...
xn
3
7775
; (2.3.1)
where x1; : : : ; xn are elements of the vector. In code, we access any element by indexing into the
tensor.
x[3]
array(3.)
Length, Dimensionality, and Shape
Let us revisit some concepts from Section 2.1. A vector is just an array of numbers. And just as
every array has a length, so does every vector. In math notation, if we want to say that a vector x
consists of n real-valued scalars, we can express this as x 2 Rn. The length of a vector is commonly
called the dimension of the vector.
As with an ordinary Python array, we can access the length of a tensor by calling Python?s built-in
len() function.
len(x)
4
When a tensor represents a vector (with precisely one axis), we can also access its length via the
.shape attribute. The shape is a tuple that lists the length (dimensionality) along each axis of the
tensor. For tensors with just one axis, the shape has just one element.
x.shape
2.3. Linear Algebra 55
(4,)
Note that the word �dimension� tends to get overloaded in these contexts and this tends to confuse
people. To clarify, we use the dimensionality of a vector or an axis to refer to its length, i.e., the
number of elements of a vector or an axis. However, we use the dimensionality of a tensor to refer
to the number of axes that a tensor has. In this sense, the dimensionality of some axis of a tensor
will be the length of that axis.
2.3.3 Matrices
Just as vectors generalize scalars from order zero to order one, matrices generalize vectors from
order one to order two. Matrices, which we will typically denote with bold-faced, capital letters
(e.g., X, Y, and Z), are represented in code as tensors with two axes.
In math notation, we use A 2 Rmn to express that the matrix A consists ofmrows and n columns
of real-valued scalars. Visually, we can illustrate any matrix A 2 Rmn as a table, where each
element aij belongs to the ith row and jth column:
A =
2
6664
a11 a12    a1n
a21 a22    a2n
...
...
. .. ...
am1 am2    amn
3
7775
: (2.3.2)
For any A 2 Rmn, the shape of A is (m, n) or m  n. Specifically, when a matrix has the same
number of rows and columns, its shape becomes a square; thus, it is called a square matrix.
We can create an m n matrix by specifying a shape with two components m and n when calling
any of our favorite functions for instantiating a tensor.
A = np.arange(20).reshape(5, 4)
A
array([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.],
[16., 17., 18., 19.]])
We can access the scalar element aij of a matrix A in :eqref:eq_matrix_def by specifying the indices
for the row (i) and column (j), such as [A]ij . When the scalar elements of a matrix A, such
as in :eqref:eq_matrix_def, are not given, we may simply use the lower-case letter of the matrix
A with the index subscript, aij , to refer to [A]ij . To keep notation simple, commas are inserted to
separate indices only when necessary, such as a2;3j and [A]2i??1;3.
Sometimes, we want to flip the axes. When we exchange a matrix?s rows and columns, the result is
called the transpose of the matrix. Formally, we signify a matrix A?s transpose by A? and if B = A?,
then bij = aji for any i and j. Thus, the transpose of A in :eqref:eq_matrix_def is a n  m matrix:
A?
=
2
6664
a11 a21 : : : am1
a12 a22 : : : am2
...
...
. .. ...
a1n a2n : : : amn
3
7775
: (2.3.3)
56 Chapter 2. Preliminaries
Now we access a matrix?s transpose in code.
A.T
array([[ 0., 4., 8., 12., 16.],
[ 1., 5., 9., 13., 17.],
[ 2., 6., 10., 14., 18.],
[ 3., 7., 11., 15., 19.]])
As a special type of the square matrix, a symmetric matrix A is equal to its transpose: A = A?. Here
we define a symmetric matrix B.
B = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
B
array([[1., 2., 3.],
[2., 0., 4.],
[3., 4., 5.]])
Now we compare B with its transpose.
B == B.T
array([[ True, True, True],
[ True, True, True],
[ True, True, True]])
Matrices are useful data structures: they allow us to organize data that have different modalities
of variation. For example, rows in our matrix might correspond to different houses (data examples),
while columns might correspond to different attributes. This should sound familiar if you
have ever used spreadsheet software or have read Section 2.2. Thus, although the default orientation
of a single vector is a column vector, in a matrix that represents a tabular dataset, it is more
conventional to treat each data example as a row vector in the matrix. And, as we will see in later
chapters, this convention will enable common deep learning practices. For example, along the
outermost axis of a tensor, we can access or enumerate minibatches of data examples, or just data
examples if no minibatch exists.
2.3.4 Tensors
Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures
with even more axes. Tensors (�tensors� in this subsection refer to algebraic objects) give us a
generic way of describing n-dimensional arrays with an arbitrary number of axes. Vectors, for
example, are first-order tensors, and matrices are second-order tensors. Tensors are denoted with
capital letters of a special font face (e.g., X, Y, and Z) and their indexing mechanism (e.g., xijk and
[X]1;2i??1;3) is similar to that of matrices.
Tensors will become more important when we start working with images, which arrive as n-
dimensional arrays with 3 axes corresponding to the height, width, and a channel axis for stacking
the color channels (red, green, and blue). For now, we will skip over higher order tensors and
focus on the basics.
2.3. Linear Algebra 57
X = np.arange(24).reshape(2, 3, 4)
X
array([[[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.]],
[[12., 13., 14., 15.],
[16., 17., 18., 19.],
[20., 21., 22., 23.]]])
2.3.5 Basic Properties of Tensor Arithmetic
Scalars, vectors, matrices, and tensors (�tensors� in this subsection refer to algebraic objects) of an
arbitrary number of axes have some nice properties that often come in handy. For example, you
might have noticed from the definition of an elementwise operation that any elementwise unary
operation does not change the shape of its operand. Similarly, given any two tensors with the
same shape, the result of any binary elementwise operation will be a tensor of that same shape.
For example, adding two matrices of the same shape performs elementwise addition over these
two matrices.
A = np.arange(20).reshape(5, 4)
B = A.copy() # Assign a copy of `A` to `B` by allocating new memory
A, A + B
(array([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.],
[16., 17., 18., 19.]]),
array([[ 0., 2., 4., 6.],
[ 8., 10., 12., 14.],
[16., 18., 20., 22.],
[24., 26., 28., 30.],
[32., 34., 36., 38.]]))
Specifically, elementwise multiplication of two matrices is called their Hadamard product (math
notation?). Consider matrix B 2 Rmn whose element of row i and column j is bij . The Hadamard
product of matrices A (defined in :eqref:eq_matrix_def) and B
A ? B =
2
6664
a11b11 a12b12 : : : a1nb1n
a21b21 a22b22 : : : a2nb2n
...
...
. .. ...
am1bm1 am2bm2 : : : amnbmn
3
7775
: (2.3.4)
A * B
58 Chapter 2. Preliminaries
array([[ 0., 1., 4., 9.],
[ 16., 25., 36., 49.],
[ 64., 81., 100., 121.],
[144., 169., 196., 225.],
[256., 289., 324., 361.]])
Multiplying or adding a tensor by a scalar also does not change the shape of the tensor, where each
element of the operand tensor will be added or multiplied by the scalar.
a = 2
X = np.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
(array([[[ 2., 3., 4., 5.],
[ 6., 7., 8., 9.],
[10., 11., 12., 13.]],
[[14., 15., 16., 17.],
[18., 19., 20., 21.],
[22., 23., 24., 25.]]]),
(2, 3, 4))
2.3.6 Reduction
One useful operation that we can perform with arbitrary tensors is to calculate the sum of their
elements. In mathematical notation, we express sums using the
?
symbol. To express the sum
of the elements in a vector x of length d, we write
?d
i=1 xi. In code, we can just call the function
for calculating the sum.
x = np.arange(4)
x, x.sum()
(array([0., 1., 2., 3.]), array(6.))
We can express sums over the elements of tensors of arbitrary shape. For example, the sum of the
elements of an m  n matrix A could be written
?m
i=1
?n
j=1 aij .
A.shape, A.sum()
((5, 4), array(190.))
By default, invoking the function for calculating the sum reduces a tensor along all its axes to a
scalar. We can also specify the axes along which the tensor is reduced via summation. Take matrices
as an example. To reduce the row dimension (axis 0) by summing up elements of all the
rows, we specify axis=0 when invoking the function. Since the input matrix reduces along axis 0
to generate the output vector, the dimension of axis 0 of the input is lost in the output shape.
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
2.3. Linear Algebra 59
(array([40., 45., 50., 55.]), (4,))
Specifying axis=1 will reduce the column dimension (axis 1) by summing up elements of all the
columns. Thus, the dimension of axis 1 of the input is lost in the output shape.
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
(array([ 6., 22., 38., 54., 70.]), (5,))
Reducing a matrix along both rows and columns via summation is equivalent to summing up all
the elements of the matrix.
A.sum(axis=[0, 1]) # Same as `A.sum()`
array(190.)
A related quantity is the mean, which is also called the average. We calculate the mean by dividing
the sum by the total number of elements. In code, we could just call the function for calculating
the mean on tensors of arbitrary shape.
A.mean(), A.sum() / A.size
(array(9.5), array(9.5))
Likewise, the function for calculating the mean can also reduce a tensor along the specified axes.
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
(array([ 8., 9., 10., 11.]), array([ 8., 9., 10., 11.]))
Non-Reduction Sum
However, sometimes it can be useful to keep the number of axes unchanged when invoking the
function for calculating the sum or mean.
sum_A = A.sum(axis=1, keepdims=True)
sum_A
array([[ 6.],
[22.],
[38.],
[54.],
[70.]])
For instance, since sum_A still keeps its two axes after summing each row, we can divide A by sum_A
with broadcasting.
60 Chapter 2. Preliminaries
A / sum_A
array([[0. , 0.16666667, 0.33333334, 0.5 ],
[0.18181819, 0.22727273, 0.27272728, 0.3181818 ],
[0.21052632, 0.23684211, 0.2631579 , 0.28947368],
[0.22222222, 0.24074075, 0.25925925, 0.2777778 ],
[0.22857143, 0.24285714, 0.25714287, 0.27142859]])
If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by
row), we can call the cumsum function. This function will not reduce the input tensor along any
axis.
A.cumsum(axis=0)
array([[ 0., 1., 2., 3.],
[ 4., 6., 8., 10.],
[12., 15., 18., 21.],
[24., 28., 32., 36.],
[40., 45., 50., 55.]])
2.3.7 Dot Products
So far, we have only performed elementwise operations, sums, and averages. And if this was all
we could do, linear algebra probably would not deserve its own section. However, one of the most
fundamental operations is the dot product. Given two vectors x; y 2 Rd, their dot product x?y (or
?x; y?) is a sum over the products of the elements at the same position: x?y =
?d
i=1 xiyi.
y = np.ones(4)
x, y, np.dot(x, y)
(array([0., 1., 2., 3.]), array([1., 1., 1., 1.]), array(6.))
Note that we can express the dot product of two vectors equivalently by performing an elementwise
multiplication and then a sum:
np.sum(x * y)
array(6.)
Dot products are useful in a wide range of contexts. For example, given some set of values, denoted
by a vector x 2 Rd and a set of weights denoted by w 2 Rd, the weighted sum of the values in x
according to the weights w could be expressed as the dot product x?w. When the weights are
non-negative and sum to one (i.e.,
(?d
i=1 wi = 1
)
), the dot product expresses a weighted average.
After normalizing two vectors to have the unit length, the dot products express the cosine of the
angle between them. We will formally introduce this notion of length later in this section.
2.3. Linear Algebra 61
2.3.8 Matrix-Vector Products
Now that we know how to calculate dot products, we can begin to understand matrix-vector products.
Recall the matrix A 2 Rmn and the vector x 2 Rn defined and visualized in (2.3.2) and (2.3.1)
respectively. Let us start off by visualizing the matrix A in terms of its row vectors
A =
2
6664
a?
1
a?
2...
a?
m
3
7775
; (2.3.5)
where each a?
i
2 Rn is a row vector representing the ith row of the matrix A. The matrix-vector
product Ax is simply a column vector of length m, whose ith element is the dot product a?
i x:
Ax =
2
6664
a?
1
a?
2...
a?
m
3
7775
x =
2
6664
a?
1 x
a?
2 x
...
a?
mx
3
7775
: (2.3.6)
We can think of multiplication by a matrix A 2 Rmn as a transformation that projects vectors
from Rn to Rm. These transformations turn out to be remarkably useful. For example, we can
represent rotations as multiplications by a square matrix. As we will see in subsequent chapters,
we can also use matrix-vector products to describe the most intensive calculations required when
computing each layer in a neural network given the values of the previous layer.
Expressing matrix-vector products in code with tensors, we use the same dot function as for dot
products. When we call np.dot(A, x) with a matrix A and a vector x, the matrix-vector product is
performed. Note that the column dimension of A (its length along axis 1) must be the same as the
dimension of x (its length).
A.shape, x.shape, np.dot(A, x)
((5, 4), (4,), array([ 14., 38., 62., 86., 110.]))
2.3.9 Matrix-Matrix Multiplication
If you have gotten the hang of dot products and matrix-vector products, then matrix-matrix multiplication
should be straightforward.
Say that we have two matrices A 2 Rnk and B 2 Rkm:
A =
2
6664
a11 a12    a1k
a21 a22    a2k
...
...
.. . ...
an1 an2    ank
3
7775
; B =
2
6664
b11 b12    b1m
b21 b22    b2m
...
...
. .. ...
bk1 bk2    bkm
3
7775
: (2.3.7)
Denote by a?
i
2 Rk the row vector representing the ith row of the matrix A, and let bj 2 Rk be the
column vector from the jth column of the matrix B. To produce the matrix product C = AB, it is
62 Chapter 2. Preliminaries
easiest to think of A in terms of its row vectors and B in terms of its column vectors:
A =
2
6664
a?
1
a?
2...
a?
n
3
7775
; B =
[
b1 b2    bm
]
: (2.3.8)
Then the matrix product C 2 Rnm is produced as we simply compute each element cij as the dot
product a?
i bj :
C = AB =
2
6664
a?
1
a?
2...
a?
n
3
7775
[
b1 b2    bm
]
=
2
6664
a?
1 b1 a?
1 b2    a?
1 bm
a?
2 b1 a?
2 b2    a?
2 bm
...
...
.. . ...
a?
n b1 a?
n b2    a?
n bm
3
7775
: (2.3.9)
We can think of the matrix-matrix multiplication AB as simply performingmmatrix-vector products
and stitching the results together to form an n  m matrix. In the following snippet, we
perform matrix multiplication on A and B. Here, A is a matrix with 5 rows and 4 columns, and B is
a matrix with 4 rows and 3 columns. After multiplication, we obtain a matrix with 5 rows and 3
columns.
B = np.ones(shape=(4, 3))
np.dot(A, B)
array([[ 6., 6., 6.],
[22., 22., 22.],
[38., 38., 38.],
[54., 54., 54.],
[70., 70., 70.]])
Matrix-matrix multiplication can be simply called matrix multiplication, and should not be confused
with the Hadamard product.
2.3.10 Norms
Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector
tells us how big a vector is. The notion of size under consideration here concerns not dimensionality
but rather the magnitude of the components.
In linear algebra, a vector norm is a function f that maps a vector to a scalar, satisfying a handful
of properties. Given any vector x, the first property says that if we scale all the elements of a vector
by a constant factor , its norm also scales by the absolute value of the same constant factor:
f(x) = jjf(x): (2.3.10)
The second property is the familiar triangle inequality:
f(x + y)  f(x) + f(y): (2.3.11)
The third property simply says that the norm must be non-negative:
f(x)  0: (2.3.12)
2.3. Linear Algebra 63
That makes sense, as in most contexts the smallest size for anything is 0. The final property requires
that the smallest norm is achieved and only achieved by a vector consisting of all zeros.
8i; [x]i = 0 , f(x) = 0: (2.3.13)
You might notice that norms sound a lot like measures of distance. And if you remember Euclidean
distances (think Pythagoras? theorem) from grade school, then the concepts of non-negativity and
the triangle inequality might ring a bell. In fact, the Euclidean distance is a norm: specifically it
is the L2 norm. Suppose that the elements in the n-dimensional vector x are x1; : : : ; xn. The L2
norm of x is the square root of the sum of the squares of the vector elements:
?x?2 =
vuut
?n
i=1
x2i
; (2.3.14)
where the subscript 2 is often omitted in L2 norms, i.e., ?x? is equivalent to ?x?2. In code, we can
calculate the L2 norm of a vector as follows.
u = np.array([3, -4])
np.linalg.norm(u)
array(5.)
In deep learning, we work more often with the squared L2 norm. You will also frequently encounter
the L1 norm, which is expressed as the sum of the absolute values of the vector elements:
?x?1 =
?n
i=1
jxij : (2.3.15)
As compared with the L2 norm, it is less influenced by outliers. To calculate the L1 norm, we
compose the absolute value function with a sum over the elements.
np.abs(u).sum()
array(7.)
Both the L2 norm and the L1 norm are special cases of the more general Lp norm:
?x?p =
(
?n
i=1
jxijp
)1/p
: (2.3.16)
Analogous to L2 norms of vectors, the Frobenius norm of a matrix X 2 Rmn is the square root of
the sum of the squares of the matrix elements:
?X?F =
vuut
?m
i=1
?n
j=1
x2
ij : (2.3.17)
The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an L2 norm
of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a
matrix.
64 Chapter 2. Preliminaries
np.linalg.norm(np.ones((4, 9)))
array(6.)
Norms and Objectives
While we do not want to get too far ahead of ourselves, we can plant some intuition already about
why these concepts are useful. In deep learning, we are often trying to solve optimization problems:
maximize the probability assigned to observed data; minimize the distance between predictions
and the ground-truth observations. Assign vector representations to items (like words,
products, or news articles) such that the distance between similar items is minimized, and the
distance between dissimilar items is maximized. Oftentimes, the objectives, perhaps the most
important components of deep learning algorithms (besides the data), are expressed as norms.
2.3.11 More on Linear Algebra
In just this section, we have taught you all the linear algebra that you will need to understand a
remarkable chunk of modern deep learning. There is a lot more to linear algebra and a lot of
that mathematics is useful for machine learning. For example, matrices can be decomposed into
factors, and these decompositions can reveal low-dimensional structure in real-world datasets.
There are entire subfields of machine learning that focus on using matrix decompositions and
their generalizations to high-order tensors to discover structure in datasets and solve prediction
problems. But this book focuses on deep learning. And we believe you will be much more inclined
to learn more mathematics once you have gotten your hands dirty deploying useful machine learning
models on real datasets. So while we reserve the right to introduce more mathematics much
later on, we will wrap up this section here.
If you are eager to learn more about linear algebra, you may refer to either the online appendix
on linear algebraic operations43 or other excellent resources (Strang, 1993; Kolter, 2008; Petersen
et al., 2008).
Summary
� Scalars, vectors, matrices, and tensors are basic mathematical objects in linear algebra.
� Vectors generalize scalars, and matrices generalize vectors.
� Scalars, vectors, matrices, and tensors have zero, one, two, and an arbitrary number of axes,
respectively.
� A tensor can be reduced along the specified axes by sum and mean.
� Elementwise multiplication of two matrices is called their Hadamard product. It is different
from matrix multiplication.
� In deep learning, we often work with norms such as the L1 norm, the L2 norm, and the
Frobenius norm.
� We can perform a variety of operations over scalars, vectors, matrices, and tensors.
43 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html
2.3. Linear Algebra 65
Exercises
1. Prove that the transpose of a matrix A?s transpose is A: (A?)? = A.
2. Given two matrices A and B, show that the sum of transposes is equal to the transpose of a
sum: A? + B? = (A + B)?.
3. Given any square matrix A, is A + A? always symmetric? Why?
4. We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)?
5. For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain
axis of X? What is that axis?
6. Run A / A.sum(axis=1) and see what happens. Can you analyze the reason?
7. When traveling between two points in Manhattan, what is the distance that you need to cover
in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?
8. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along
axis 0, 1, and 2?
9. Feed a tensor with 3 or more axes to the linalg.norm function and observe its output. What
does this function compute for tensors of arbitrary shape?
Discussions44
2.4 Calculus
Finding the area of a polygon had remained mysterious until at least 2,500 years ago, when ancient
Greeks divided a polygon into triangles and summed their areas. To find the area of curved shapes,
such as a circle, ancient Greeks inscribed polygons in such shapes. As shown in Fig. 2.4.1, an
inscribed polygon with more sides of equal length better approximates the circle. This process is
also known as the method of exhaustion.
Fig. 2.4.1: Find the area of a circle with the method of exhaustion.
In fact, the method of exhaustion is where integral calculus (will be described in Section 18.5) originates
from. More than 2,000 years later, the other branch of calculus, differential calculus, was
invented. Among the most critical applications of differential calculus, optimization problems
consider how to do something the best. As discussed in Section 2.3.10, such problems are ubiquitous
in deep learning.
In deep learning, we train models, updating them successively so that they get better and better
as they see more and more data. Usually, getting better means minimizing a loss function, a score
44 https://discuss.d2l.ai/t/30
66 Chapter 2. Preliminaries
that answers the question �how bad is our model?� This question is more subtle than it appears.
Ultimately, what we really care about is producing a model that performs well on data that we have
never seen before. But we can only fit the model to data that we can actually see. Thus we can
decompose the task of fitting models into two key concerns: i) optimization: the process of fitting
our models to observed data; ii) generalization: the mathematical principles and practitioners?
wisdom that guide as to how to produce models whose validity extends beyond the exact set of
data examples used to train them.
To help you understand optimization problems and methods in later chapters, here we give a very
brief primer on differential calculus that is commonly used in deep learning.
2.4.1 Derivatives and Differentiation
We begin by addressing the calculation of derivatives, a crucial step in nearly all deep learning
optimization algorithms. In deep learning, we typically choose loss functions that are differentiable
with respect to our model?s parameters. Put simply, this means that for each parameter,
we can determine how rapidly the loss would increase or decrease, were we to increase or decrease
that parameter by an infinitesimally small amount.
Suppose that we have a function f : R ! R, whose input and output are both scalars. The derivative
of f is defined as
f
?
(x) = lim
h!0
f(x + h) ?? f(x)
h
; (2.4.1)
if this limit exists. If f?(a) exists, f is said to be differentiable at a. If f is differentiable at every
number of an interval, then this function is differentiable on this interval. We can interpret the
derivative f?(x) in (2.4.1) as the instantaneous rate of change of f(x) with respect to x. The so-called
instantaneous rate of change is based on the variation h in x, which approaches 0.
To illustrate derivatives, let us experiment with an example. Define u = f(x) = 3x2 ?? 4x.
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mxnet import np, npx
npx.set_np()
def f(x):
return 3 * x ** 2 - 4 * x
By setting x = 1 and letting h approach 0, the numerical result of f(x+h)??f(x)
h in (2.4.1) approaches
2. Though this experiment is not a mathematical proof, we will see later that the derivative u? is 2
when x = 1.
def numerical_lim(f, x, h):
return (f(x + h) - f(x)) / h
h = 0.1
for i in range(5):
print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
h *= 0.1
2.4. Calculus 67
h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
Let us familiarize ourselves with a few equivalent notations for derivatives. Given y = f(x), where
x and y are the independent variable and the dependent variable of the function f, respectively.
The following expressions are equivalent:
f
?
(x) = y
?
=
dy
dx
=
df
dx
=
d
dx
f(x) = Df(x) = Dxf(x); (2.4.2)
where symbols d
dx and D are differentiation operators that indicate operation of differentiation. We
can use the following rules to differentiate common functions:
� DC = 0 (C is a constant),
� Dxn = nxn??1 (the power rule, n is any real number),
� Dex = ex,
� Dln(x) = 1/x:
To differentiate a function that is formed from a few simpler functions such as the above common
functions, the following rules can be handy for us. Suppose that functions f and g are both
differentiable and C is a constant, we have the constant multiple rule
d
dx
[Cf(x)] = C
d
dx
f(x); (2.4.3)
the sum rule
d
dx
[f(x) + g(x)] =
d
dx
f(x) +
d
dx
g(x); (2.4.4)
the product rule
d
dx
[f(x)g(x)] = f(x)
d
dx
[g(x)] + g(x)
d
dx
[f(x)]; (2.4.5)
and the quotient rule
d
dx
[
f(x)
g(x)
]
=
g(x) d
dx [f(x)] ?? f(x) d
dx [g(x)]
[g(x)]2 : (2.4.6)
Now we can apply a few of the above rules to find u? = f?(x) = 3 d
dxx2 ?? 4 d
dxx = 6x ?? 4. Thus, by
setting x = 1, we have u? = 2: this is supported by our earlier experiment in this section where
the numerical result approaches 2. This derivative is also the slope of the tangent line to the curve
u = f(x) when x = 1.
To visualize such an interpretation of derivatives, we will use matplotlib, a popular plotting library
in Python. To configure properties of the figures produced by matplotlib, we need to define
a few functions. In the following, the use_svg_display function specifies the matplotlib package
to output the svg figures for sharper images.
68 Chapter 2. Preliminaries
def use_svg_display(): #@save
"""Use the svg format to display a plot in Jupyter."""
display.set_matplotlib_formats('svg')
We define the set_figsize function to specify the figure sizes. Note that here we directly use d2l.
plt since the import statement from matplotlib import pyplot as plt has been marked for
being saved in the d2l package in the preface.
def set_figsize(figsize=(3.5, 2.5)): #@save
"""Set the figure size for matplotlib."""
use_svg_display()
d2l.plt.rcParams['figure.figsize'] = figsize
The following set_axes function sets properties of axes of figures produced by matplotlib.
#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
"""Set the axes for matplotlib."""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()
With these three functions for figure configurations, we define the plot function to plot multiple
curves succinctly since we will need to visualize many curves throughout the book.
#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
"""Plot data points."""
if legend is None:
legend = []
set_figsize(figsize)
axes = axes if axes else d2l.plt.gca()
# Return True if `X` (tensor or list) has 1 axis
def has_one_axis(X):
return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
and not hasattr(X[0], "__len__"))
if has_one_axis(X):
X = [X]
if Y is None:
X, Y = [[]] * len(X), X
elif has_one_axis(Y):
Y = [Y]
if len(X) != len(Y):
(continues on next page)
2.4. Calculus 69
(continued from previous page)
X = X * len(Y)
axes.cla()
for x, y, fmt in zip(X, Y, fmts):
if len(x):
axes.plot(x, y, fmt)
else:
axes.plot(y, fmt)
set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
Now we can plot the function u = f(x) and its tangent line y = 2x??3 at x = 1, where the coefficient
2 is the slope of the tangent line.
x = np.arange(0, 3, 0.1)
plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
2.4.2 Partial Derivatives
So far we have dealt with the differentiation of functions of just one variable. In deep learning,
functions often depend on many variables. Thus, we need to extend the ideas of differentiation to
these multivariate functions.
Let y = f(x1; x2; : : : ; xn) be a function with n variables. The partial derivative of y with respect to
its ith parameter xi is
@y
@xi
= lim
h!0
f(x1; : : : ; xi??1; xi + h; xi+1; : : : ; xn) ?? f(x1; : : : ; xi; : : : ; xn)
h
: (2.4.7)
To calculate @y
@xi , we can simply treat x1; : : : ; xi??1; xi+1; : : : ; xn as constants and calculate the derivative
of y with respect to xi. For notation of partial derivatives, the following are equivalent:
@y
@xi
=
@f
@xi
= fxi = fi = Dif = Dxif: (2.4.8)
70 Chapter 2. Preliminaries
2.4.3 Gradients
We can concatenate partial derivatives of a multivariate function with respect to all its variables
to obtain the gradient vector of the function. Suppose that the input of function f : Rn ! R is an
n-dimensional vector x = [x1; x2; : : : ; xn]? and the output is a scalar. The gradient of the function
f(x) with respect to x is a vector of n partial derivatives:
?xf(x) =
[
@f(x)
@x1
;
@f(x)
@x2
; : : : ;
@f(x)
@xn
]?
; (2.4.9)
where ?xf(x) is often replaced by ?f(x) when there is no ambiguity.
Let x be an n-dimensional vector, the following rules are often used when differentiating multivariate
functions:
� For all A 2 Rmn, ?xAx = A?,
� For all A 2 Rnm, ?xx?A = A,
� For all A 2 Rnn, ?xx?Ax = (A + A?)x,
� ?x?x?2 = ?xx?x = 2x.
Similarly, for any matrix X, we have ?X?X?2
F = 2X. As we will see later, gradients are useful for
designing optimization algorithms in deep learning.
2.4.4 Chain Rule
However, such gradients can be hard to find. This is because multivariate functions in deep learning
are often composite, so we may not apply any of the aforementioned rules to differentiate these
functions. Fortunately, the chain rule enables us to differentiate composite functions.
Let us first consider functions of a single variable. Suppose that functions y = f(u) and u = g(x)
are both differentiable, then the chain rule states that
dy
dx
=
dy
du
du
dx
: (2.4.10)
Now let us turn our attention to a more general scenario where functions have an arbitrary
number of variables. Suppose that the differentiable function y has variables u1; u2; : : : ; um,
where each differentiable function ui has variables x1; x2; : : : ; xn. Note that y is a function of
x1; x2; : : : ; xn. Then the chain rule gives
dy
dxi
=
dy
du1
du1
dxi
+
dy
du2
du2
dxi
+    +
dy
dum
dum
dxi
(2.4.11)
for any i = 1; 2; : : : ; n.
Summary
� Differential calculus and integral calculus are two branches of calculus, where the former
can be applied to the ubiquitous optimization problems in deep learning.
� A derivative can be interpreted as the instantaneous rate of change of a function with respect
to its variable. It is also the slope of the tangent line to the curve of the function.
2.4. Calculus 71
� A gradient is a vector whose components are the partial derivatives of a multivariate function
with respect to all its variables.
� The chain rule enables us to differentiate composite functions.
Exercises
1. Plot the function y = f(x) = x3 ?? 1
x and its tangent line when x = 1.
2. Find the gradient of the function f(x) = 3x21
+ 5ex2 .
3. What is the gradient of the function f(x) = ?x?2?
4. Can you write out the chain rule for the case where u = f(x; y; z) and x = x(a; b), y = y(a; b),
and z = z(a; b)?
Discussions45
2.5 Automatic Differentiation
As we have explained in Section 2.4, differentiation is a crucial step in nearly all deep learning
optimization algorithms. While the calculations for taking these derivatives are straightforward,
requiring only some basic calculus, for complex models, working out the updates by hand can be
a pain (and often error-prone).
Deep learning frameworks expedite this work by automatically calculating derivatives, i.e., automatic
differentiation. In practice, based on our designed model the system builds a computational
graph, tracking which data combined through which operations to produce the output. Automatic
differentiation enables the system to subsequently backpropagate gradients. Here, backpropagate
simply means to trace through the computational graph, filling in the partial derivatives with respect
to each parameter.
from mxnet import autograd, np, npx
npx.set_np()
2.5.1 A Simple Example
As a toy example, say that we are interested in differentiating the function y = 2x?x with respect
to the column vector x. To start, let us create the variable x and assign it an initial value.
x = np.arange(4.0)
x
array([0., 1., 2., 3.])
Before we even calculate the gradient of y with respect to x, we will need a place to store it. It is
important that we do not allocate new memory every time we take a derivative with respect to a
parameter because we will often update the same parameters thousands or millions of times and
45 https://discuss.d2l.ai/t/32
72 Chapter 2. Preliminaries
could quickly run out of memory. Note that a gradient of a scalar-valued function with respect to
a vector x is itself vector-valued and has the same shape as x.
# We allocate memory for a tensor's gradient by invoking `attach_grad`
x.attach_grad()
# After we calculate a gradient taken with respect to `x`, we will be able to
# access it via the `grad` attribute, whose values are initialized with 0s
x.grad
array([0., 0., 0., 0.])
Now let us calculate y.
# Place our code inside an `autograd.record` scope to build the computational
# graph
with autograd.record():
y = 2 * np.dot(x, x)
y
array(28.)
Since x is a vector of length 4, an inner product of x and x is performed, yielding the scalar output
that we assign to y. Next, we can automatically calculate the gradient of y with respect to each
component of x by calling the function for backpropagation and printing the gradient.
y.backward()
x.grad
array([ 0., 4., 8., 12.])
The gradient of the function y = 2x?x with respect to x should be 4x. Let us quickly verify that
our desired gradient was calculated correctly.
x.grad == 4 * x
array([ True, True, True, True])
Now let us calculate another function of x.
with autograd.record():
y = x.sum()
y.backward()
x.grad # Overwritten by the newly calculated gradient
array([1., 1., 1., 1.])
2.5. Automatic Differentiation 73
2.5.2 Backward for Non-Scalar Variables
Technically, when y is not a scalar, the most natural interpretation of the differentiation of a vector
y with respect to a vector x is a matrix. For higher-order and higher-dimensional y and x, the
differentiation result could be a high-order tensor.
However, while these more exotic objects do show up in advanced machine learning (including in
deep learning), more often when we are calling backward on a vector, we are trying to calculate
the derivatives of the loss functions for each constituent of a batch of training examples. Here, our
intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives
computed individually for each example in the batch.
# When we invoke `backward` on a vector-valued variable `y` (function of `x`),
# a new scalar variable is created by summing the elements in `y`. Then the
# gradient of that scalar variable with respect to `x` is computed
with autograd.record():
y = x * x # `y` is a vector
y.backward()
x.grad # Equals to y = sum(x * x)
array([0., 2., 4., 6.])
2.5.3 Detaching Computation
Sometimes, we wish to move some calculations outside of the recorded computational graph. For
example, say that y was calculated as a function of x, and that subsequently z was calculated as a
function of both y and x. Now, imagine that we wanted to calculate the gradient of z with respect
to x, but wanted for some reason to treat y as a constant, and only take into account the role that
x played after y was calculated.
Here, we can detach y to return a new variable u that has the same value as y but discards any
information about how y was computed in the computational graph. In other words, the gradient
will not flow backwards through u to x. Thus, the following backpropagation function computes
the partial derivative of z = u * x with respect to x while treating u as a constant, instead of the
partial derivative of z = x * x * x with respect to x.
with autograd.record():
y = x * x
u = y.detach()
z = u * x
z.backward()
x.grad == u
array([ True, True, True, True])
Since the computation of y was recorded, we can subsequently invoke backpropagation on y to
get the derivative of y = x * x with respect to x, which is 2 * x.
y.backward()
x.grad == 2 * x
74 Chapter 2. Preliminaries
array([ True, True, True, True])
2.5.4 Computing the Gradient of Python Control Flow
One benefit of using automatic differentiation is that even if building the computational graph
of a function required passing through a maze of Python control flow (e.g., conditionals, loops,
and arbitrary function calls), we can still calculate the gradient of the resulting variable. In the
following snippet, note that the number of iterations of the while loop and the evaluation of the
if statement both depend on the value of the input a.
def f(a):
b = a * 2
while np.linalg.norm(b) < 1000:
b = b * 2
if b.sum() > 0:
c = b
else:
c = 100 * b
return c
Let us compute the gradient.
a = np.random.normal()
a.attach_grad()
with autograd.record():
d = f(a)
d.backward()
We can now analyze the f function defined above. Note that it is piecewise linear in its input a. In
other words, for any a there exists some constant scalar k such that f(a) = k * a, where the value
of k depends on the input a. Consequently d / a allows us to verify that the gradient is correct.
a.grad == d / a
array(True)
Summary
� Deep learning frameworks can automate the calculation of derivatives. To use it, we first
attach gradients to those variables with respect to which we desire partial derivatives. We
then record the computation of our target value, execute its function for backpropagation,
and access the resulting gradient.
2.5. Automatic Differentiation 75
Exercises
1. Why is the second derivative much more expensive to compute than the first derivative?
2. After running the function for backpropagation, immediately run it again and see what happens.
3. In the control flow example where we calculate the derivative of d with respect to a, what
would happen if we changed the variable a to a random vector or matrix. At this point, the
result of the calculation f(a) is no longer a scalar. What happens to the result? How do we
analyze this?
4. Redesign an example of finding the gradient of the control flow. Run and analyze the result.
5. Let f(x) = sin(x). Plot f(x) and df(x)
dx , where the latter is computed without exploiting that
f?(x) = cos(x).
Discussions46
2.6 Probability
In some form or another, machine learning is all about making predictions. We might want to
predict the probability of a patient suffering a heart attack in the next year, given their clinical history.
In anomaly detection, we might want to assess how likely a set of readings from an airplane?s
jet engine would be, were it operating normally. In reinforcement learning, we want an agent to
act intelligently in an environment. This means we need to think about the probability of getting
a high reward under each of the available actions. And when we build recommender systems we
also need to think about probability. For example, say hypothetically that we worked for a large
online bookseller. We might want to estimate the probability that a particular user would buy
a particular book. For this we need to use the language of probability. Entire courses, majors,
theses, careers, and even departments, are devoted to probability. So naturally, our goal in this
section is not to teach the whole subject. Instead we hope to get you off the ground, to teach you
just enough that you can start building your first deep learning models, and to give you enough of
a flavor for the subject that you can begin to explore it on your own if you wish.
We have already invoked probabilities in previous sections without articulating what precisely
they are or giving a concrete example. Let us get more serious now by considering the first case:
distinguishing cats and dogs based on photographs. This might sound simple but it is actually a
formidable challenge. To start with, the difficulty of the problem may depend on the resolution
of the image.
46 https://discuss.d2l.ai/t/34
76 Chapter 2. Preliminaries
Fig. 2.6.1: Images of varying resolutions (10  10, 20  20, 40  40, 80  80, and 160  160 pixels).
As shown in Fig. 2.6.1, while it is easy for humans to recognize cats and dogs at the resolution of
160160 pixels, it becomes challenging at 4040 pixels and next to impossible at 1010 pixels.
In other words, our ability to tell cats and dogs apart at a large distance (and thus low resolution)
might approach uninformed guessing. Probability gives us a formal way of reasoning about our
level of certainty. If we are completely sure that the image depicts a cat, we say that the probability
that the corresponding label y is �cat�, denoted P(y = �cat�) equals 1. If we had no evidence to
suggest that y = �cat� or that y = �dog�, then we might say that the two possibilities were equally
likely expressing this as P(y = �cat�) = P(y = �dog�) = 0:5. If we were reasonably confident, but
not sure that the image depicted a cat, we might assign a probability 0:5 < P(y = �cat�) < 1.
Now consider the second case: given some weather monitoring data, we want to predict the probability
that it will rain in Taipei tomorrow. If it is summertime, the rain might come with probability
0.5.
In both cases, we have some value of interest. And in both cases we are uncertain about the outcome.
But there is a key difference between the two cases. In this first case, the image is in fact
either a dog or a cat, and we just do not know which. In the second case, the outcome may actually
be a random event, if you believe in such things (and most physicists do). So probability is a
flexible language for reasoning about our level of certainty, and it can be applied effectively in a
broad set of contexts.
2.6.1 Basic Probability Theory
Say that we cast a die and want to know what the chance is of seeing a 1 rather than another digit.
If the die is fair, all the six outcomes f1; : : : ; 6g are equally likely to occur, and thus we would see
a 1 in one out of six cases. Formally we state that 1 occurs with probability 1
6 .
For a real die that we receive from a factory, we might not know those proportions and we would
need to check whether it is tainted. The only way to investigate the die is by casting it many times
and recording the outcomes. For each cast of the die, we will observe a value in f1; : : : ; 6g. Given
these outcomes, we want to investigate the probability of observing each outcome.
2.6. Probability 77
One natural approach for each value is to take the individual count for that value and to divide it
by the total number of tosses. This gives us an estimate of the probability of a given event. The law
of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer
to the true underlying probability. Before going into the details of what is going here, let us try it
out.
To start, let us import the necessary packages.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
import random
npx.set_np()
Next, we will want to be able to cast the die. In statistics we call this process of drawing examples
from probability distributions sampling. The distribution that assigns probabilities to a number
of discrete choices is called the multinomial distribution. We will give a more formal definition of
distribution later, but at a high level, think of it as just an assignment of probabilities to events.
To draw a single sample, we simply pass in a vector of probabilities. The output is another vector
of the same length: its value at index i is the number of times the sampling outcome corresponds
to i.
fair_probs = [1.0 / 6] * 6
np.random.multinomial(1, fair_probs)
array([0, 0, 0, 1, 0, 0], dtype=int64)
If you run the sampler a bunch of times, you will find that you get out random values each time.
As with estimating the fairness of a die, we often want to generate many samples from the same
distribution. It would be unbearably slow to do this with a Python for loop, so the function we are
using supports drawing multiple samples at once, returning an array of independent samples in
any shape we might desire.
np.random.multinomial(10, fair_probs)
array([1, 1, 5, 1, 1, 1], dtype=int64)
Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We can then go through
and count, after each of the 1000 rolls, how many times each number was rolled. Specifically, we
calculate the relative frequency as the estimate of the true probability.
counts = np.random.multinomial(1000, fair_probs).astype(np.float32)
counts / 1000
array([0.162, 0.149, 0.178, 0.17 , 0.166, 0.175])
Because we generated the data from a fair die, we know that each outcome has true probability 1
6 ,
roughly 0:167, so the above output estimates look good.
We can also visualize how these probabilities converge over time towards the true probability. Let
us conduct 500 groups of experiments where each group draws 10 samples.
78 Chapter 2. Preliminaries
counts = np.random.multinomial(10, fair_probs, size=500)
cum_counts = counts.astype(np.float32).cumsum(axis=0)
estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)
d2l.set_figsize((6, 4.5))
for i in range(6):
d2l.plt.plot(estimates[:, i].asnumpy(),
label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Groups of experiments')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
Each solid curve corresponds to one of the six values of the die and gives our estimated probability
that the die turns up that value as assessed after each group of experiments. The dashed black line
gives the true underlying probability. As we get more data by conducting more experiments, the
6 solid curves converge towards the true probability.
Axioms of Probability Theory
When dealing with the rolls of a die, we call the set S = f1; 2; 3; 4; 5; 6g the sample space or outcome
space, where each element is an outcome. An event is a set of outcomes from a given sample space.
For instance, �seeing a 5� (f5g) and �seeing an odd number� (f1; 3; 5g) are both valid events of
rolling a die. Note that if the outcome of a random experiment is in event A, then event A has
occurred. That is to say, if 3 dots faced up after rolling a die, since 3 2 f1; 3; 5g, we can say that the
event �seeing an odd number� has occurred.
Formally, probability can be thought of a function that maps a set to a real value. The probability
2.6. Probability 79
of an event A in the given sample space S, denoted as P(A), satisfies the following properties:
� For any event A, its probability is never negative, i.e., P(A)  0;
� Probability of the entire sample space is 1, i.e., P(S) = 1;
� For any countable sequence of eventsA1;A2; : : : that are mutually exclusive (Ai\Aj = ? for all
i ?= j), the probability that any happens is equal to the sum of their individual probabilities,
i.e., P(
?1
i=1
Ai) =
?1
i=1 P(Ai).
These are also the axioms of probability theory, proposed by Kolmogorov in 1933. Thanks to this
axiom system, we can avoid any philosophical dispute on randomness; instead, we can reason
rigorously with a mathematical language. For instance, by letting event A1 be the entire sample
space and Ai = ? for all i > 1, we can prove that P(?) = 0, i.e., the probability of an impossible
event is 0.
Random Variables
In our random experiment of casting a die, we introduced the notion of a random variable. A random
variable can be pretty much any quantity and is not deterministic. It could take one value
among a set of possibilities in a random experiment. Consider a random variable X whose value
is in the sample space S = f1; 2; 3; 4; 5; 6g of rolling a die. We can denote the event �seeing a 5�
as fX = 5g or X = 5, and its probability as P(fX = 5g) or P(X = 5). By P(X = a), we make a
distinction between the random variableX and the values (e.g., a) thatX can take. However, such
pedantry results in a cumbersome notation. For a compact notation, on one hand, we can just denote
P(X) as the distribution over the random variable X: the distribution tells us the probability
thatX takes any value. On the other hand, we can simply write P(a) to denote the probability that
a random variable takes the value a. Since an event in probability theory is a set of outcomes from
the sample space, we can specify a range of values for a random variable to take. For example,
P(1  X  3) denotes the probability of the event f1  X  3g, which means fX = 1; 2; or; 3g.
Equivalently, P(1  X  3) represents the probability that the random variable X can take a
value from f1; 2; 3g.
Note that there is a subtle difference between discrete random variables, like the sides of a die,
and continuous ones, like the weight and the height of a person. There is little point in asking
whether two people have exactly the same height. If we take precise enough measurements
you will find that no two people on the planet have the exact same height. In fact, if
we take a fine enough measurement, you will not have the same height when you wake up and
when you go to sleep. So there is no purpose in asking about the probability that someone is
1.80139278291028719210196740527486202 meters tall. Given the world population of humans the
probability is virtually 0. It makes more sense in this case to ask whether someone?s height falls
into a given interval, say between 1.79 and 1.81 meters. In these cases we quantify the likelihood
that we see a value as a density. The height of exactly 1.80 meters has no probability, but nonzero
density. In the interval between any two different heights we have nonzero probability. In the rest
of this section, we consider probability in discrete space. For probability over continuous random
variables, you may refer to Section 18.6.
80 Chapter 2. Preliminaries
2.6.2 Dealing with Multiple Random Variables
Very often, we will want to consider more than one random variable at a time. For instance, we
may want to model the relationship between diseases and symptoms. Given a disease and a symptom,
say �flu� and �cough�, either may or may not occur in a patient with some probability. While
we hope that the probability of both would be close to zero, we may want to estimate these probabilities
and their relationships to each other so that we may apply our inferences to effect better
medical care.
As a more complicated example, images contain millions of pixels, thus millions of random variables.
And in many cases images will come with a label, identifying objects in the image. We can
also think of the label as a random variable. We can even think of all the metadata as random
variables such as location, time, aperture, focal length, ISO, focus distance, and camera type. All
of these are random variables that occur jointly. When we deal with multiple random variables,
there are several quantities of interest.
Joint Probability
The first is called the joint probability P(A = a;B = b). Given any values a and b, the joint probability
lets us answer, what is the probability that A = a and B = b simultaneously? Note that for
any values a and b, P(A = a;B = b)  P(A = a). This has to be the case, since for A = a and
B = b to happen, A = a has to happen and B = b also has to happen (and vice versa). Thus, A = a
and B = b cannot be more likely than A = a or B = b individually.
Conditional Probability
This brings us to an interesting ratio: 0  P(A=a;B=b)
P(A=a)
 1. We call this ratio a conditional probability
and denote it by P(B = b j A = a): it is the probability ofB = b, provided thatA = a has occurred.
Bayes� theorem
Using the definition of conditional probabilities, we can derive one of the most useful and celebrated
equations in statistics: Bayes� theorem. It goes as follows. By construction, we have the
multiplication rule that P(A;B) = P(B j A)P(A). By symmetry, this also holds for P(A;B) =
P(A j B)P(B). Assume that P(B) > 0. Solving for one of the conditional variables we get
P(A j B) =
P(B j A)P(A)
P(B)
: (2.6.1)
Note that here we use the more compact notation where P(A;B) is a joint distribution and P(A j B)
is a conditional distribution. Such distributions can be evaluated for particular valuesA = a;B = b.
2.6. Probability 81
Marginalization
Bayes? theorem is very useful if we want to infer one thing from the other, say cause and effect,
but we only know the properties in the reverse direction, as we will see later in this section. One
important operation that we need, to make this work, is marginalization. It is the operation of
determining P(B) from P(A;B). We can see that the probability of B amounts to accounting for
all possible choices of A and aggregating the joint probabilities over all of them:
P(B) =
?
A
P(A;B); (2.6.2)
which is also known as the sum rule. The probability or distribution as a result of marginalization
is called a marginal probability or a marginal distribution.
Independence
Another useful property to check for is dependence vs. independence. Two random variables A and
B being independent means that the occurrence of one event ofAdoes not reveal any information
about the occurrence of an event of B. In this case P(B j A) = P(B). Statisticians typically
express this as A ? B. From Bayes? theorem, it follows immediately that also P(A j B) = P(A).
In all the other cases we call A and B dependent. For instance, two successive rolls of a die are
independent. In contrast, the position of a light switch and the brightness in the room are not
(they are not perfectly deterministic, though, since we could always have a broken light bulb,
power failure, or a broken switch).
Since P(A j B) = P(A;B)
P(B) = P(A) is equivalent to P(A;B) = P(A)P(B), two random variables are
independent if and only if their joint distribution is the product of their individual distributions.
Likewise, two random variables A and B are conditionally independent given another random variable
C if and only if P(A;B j C) = P(A j C)P(B j C). This is expressed as A ? B j C.
Application
Let us put our skills to the test. Assume that a doctor administers an AIDS test to a patient. This
test is fairly accurate and it fails only with 1% probability if the patient is healthy but reporting
him as diseased. Moreover, it never fails to detect HIV if the patient actually has it. We use D1 to
indicate the diagnosis (1 if positive and 0 if negative) and H to denote the HIV status (1 if positive
and 0 if negative). Table 2.6.1 lists such conditional probabilities.
Table 2.6.1: Conditional probability of P(D1 j H).
Conditional probability H = 1 H = 0
P(D1 = 1 j H) 1 0.01
P(D1 = 0 j H) 0 0.99
Note that the column sums are all 1 (but the row sums are not), since the conditional probability
needs to sum up to 1, just like the probability. Let us work out the probability of the patient
having AIDS if the test comes back positive, i.e., P(H = 1 j D1 = 1). Obviously this is going to
depend on how common the disease is, since it affects the number of false alarms. Assume that
the population is quite healthy, e.g., P(H = 1) = 0:0015. To apply Bayes? theorem, we need to
82 Chapter 2. Preliminaries
apply marginalization and the multiplication rule to determine
P(D1 = 1)
=P(D1 = 1;H = 0) + P(D1 = 1;H = 1)
=P(D1 = 1 j H = 0)P(H = 0) + P(D1 = 1 j H = 1)P(H = 1)
=0:011485:
(2.6.3)
Thus, we get
P(H = 1 j D1 = 1)
=
P(D1 = 1 j H = 1)P(H = 1)
P(D1 = 1)
=0:1306
: (2.6.4)
In other words, there is only a 13.06% chance that the patient actually has AIDS, despite using a
very accurate test. As we can see, probability can be counterintuitive.
What should a patient do upon receiving such terrifying news? Likely, the patient would ask the
physician to administer another test to get clarity. The second test has different characteristics
and it is not as good as the first one, as shown in Table 2.6.2.
Table 2.6.2: Conditional probability of P(D2 j H).
Conditional probability H = 1 H = 0
P(D2 = 1 j H) 0.98 0.03
P(D2 = 0 j H) 0.02 0.97
Unfortunately, the second test comes back positive, too. Let us work out the requisite probabilities
to invoke Bayes? theorem by assuming the conditional independence:
P(D1 = 1;D2 = 1 j H = 0)
=P(D1 = 1 j H = 0)P(D2 = 1 j H = 0)
=0:0003;
(2.6.5)
P(D1 = 1;D2 = 1 j H = 1)
=P(D1 = 1 j H = 1)P(D2 = 1 j H = 1)
=0:98:
(2.6.6)
Now we can apply marginalization and the multiplication rule:
P(D1 = 1;D2 = 1)
=P(D1 = 1;D2 = 1;H = 0) + P(D1 = 1;D2 = 1;H = 1)
=P(D1 = 1;D2 = 1 j H = 0)P(H = 0) + P(D1 = 1;D2 = 1 j H = 1)P(H = 1)
=0:00176955:
(2.6.7)
In the end, the probability of the patient having AIDS given both positive tests is
P(H = 1 j D1 = 1;D2 = 1)
=
P(D1 = 1;D2 = 1 j H = 1)P(H = 1)
P(D1 = 1;D2 = 1)
=0:8307:
(2.6.8)
That is, the second test allowed us to gain much higher confidence that not all is well. Despite the
second test being considerably less accurate than the first one, it still significantly improved our
estimate.
2.6. Probability 83
2.6.3 Expectation and Variance
To summarize key characteristics of probability distributions, we need some measures. The expectation
(or average) of the random variable X is denoted as
E[X] =
?
x
xP(X = x): (2.6.9)
When the input of a function f(x) is a random variable drawn from the distribution P with different
values x, the expectation of f(x) is computed as
ExP [f(x)] =
?
x
f(x)P(x): (2.6.10)
In many cases we want to measure by how much the random variable X deviates from its expectation.
This can be quantified by the variance
Var[X] = E
[
(X ?? E[X])2]
= E[X2] ?? E[X]2: (2.6.11)
Its square root is called the standard deviation. The variance of a function of a random variable
measures by how much the function deviates from the expectation of the function, as different
values x of the random variable are sampled from its distribution:
Var[f(x)] = E
[
(f(x) ?? E[f(x)])2
]
: (2.6.12)
Summary
� We can sample from probability distributions.
� We can analyze multiple random variables using joint distribution, conditional distribution,
Bayes? theorem, marginalization, and independence assumptions.
� Expectation and variance offer useful measures to summarize key characteristics of probability
distributions.
Exercises
1. We conducted m = 500 groups of experiments where each group draws n = 10 samples.
Vary m and n. Observe and analyze the experimental results.
2. Given two events with probability P(A) and P(B), compute upper and lower bounds on
P(A [ B) and P(A \ B). (Hint: display the situation using a Venn Diagram47.)
3. Assume that we have a sequence of random variables, say A, B, and C, where B only depends
on A, and C only depends on B, can you simplify the joint probability P(A;B;C)?
(Hint: this is a Markov Chain48.)
4. In Section 2.6.2, the first test is more accurate. Why not just run the first test a second time?
Discussions49
47 https://en.wikipedia.org/wiki/Venn_diagram
48 https://en.wikipedia.org/wiki/Markov_chain
49 https://discuss.d2l.ai/t/36
84 Chapter 2. Preliminaries
2.7 Documentation
Due to constraints on the length of this book, we cannot possibly introduce every single MXNet
function and class (and you probably would not want us to). The API documentation and additional
tutorials and examples provide plenty of documentation beyond the book. In this section
we provide you with some guidance to exploring the MXNet API.
2.7.1 Finding All the Functions and Classes in a Module
In order to know which functions and classes can be called in a module, we invoke the dir function.
For instance, we can query all properties in the module for generating random numbers:
from mxnet import np
print(dir(np.random))
['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '_
,!_package__', '__spec__', '_mx_nd_np', 'absolute_import', 'choice', 'multinomial', 'normal',
,! 'rand', 'randint', 'shuffle', 'uniform']
Generally, we can ignore functions that start and end with __ (special objects in Python) or functions
that start with a single _(usually internal functions). Based on the remaining function or
attribute names, we might hazard a guess that this module offers various methods for generating
random numbers, including sampling from the uniform distribution (uniform), normal distribution
(normal), and multinomial distribution (multinomial).
2.7.2 Finding the Usage of Specific Functions and Classes
For more specific instructions on how to use a given function or class, we can invoke the help
function. As an example, let us explore the usage instructions for tensors? ones function.
help(np.ones)
Help on function ones in module mxnet.numpy:
ones(shape, dtype=<class 'numpy.float32'>, order='C', ctx=None)
Return a new array of given shape and type, filled with ones.
This function currently only supports storing multi-dimensional data
in row-major (C-style).
Parameters
----------
shape : int or tuple of int
The shape of the empty array.
dtype : str or numpy.dtype, optional
An optional value type. Default is numpy.float32. Note that this
behavior is different from NumPy's ones function where float64
is the default value, because float32 is considered as the default
data type in deep learning.
order : {'C'}, optional, default: 'C'
2.7. Documentation 85
How to store multi-dimensional data in memory, currently only row-major
(C-style) is supported.
ctx : Context, optional
An optional device context (default is the current default context).
Returns
-------
out : ndarray
Array of ones with the given shape, dtype, and ctx.
Examples
--------
>>> np.ones(5)
array([1., 1., 1., 1., 1.])
>>> np.ones((5,), dtype=int)
array([1, 1, 1, 1, 1], dtype=int64)
>>> np.ones((2, 1))
array([[1.],
[1.]])
>>> s = (2,2)
>>> np.ones(s)
array([[1., 1.],
[1., 1.]])
From the documentation, we can see that the ones function creates a new tensor with the specified
shape and sets all the elements to the value of 1. Whenever possible, you should run a quick test
to confirm your interpretation:
np.ones(4)
array([1., 1., 1., 1.])
In the Jupyter notebook, we can use ? to display the document in another window. For example,
list? will create content that is almost identical to help(list), displaying it in a new browser
window. In addition, if we use two question marks, such as list??, the Python code implementing
the function will also be displayed.
Summary
� The official documentation provides plenty of descriptions and examples that are beyond
this book.
� We can look up documentation for the usage of an API by calling the dir and help functions,
or ? and ?? in Jupyter notebooks.
86 Chapter 2. Preliminaries
Exercises
1. Look up the documentation for any function or class in the deep learning framework. Can
you also find the documentation on the official website of the framework?
Discussions50
50 https://discuss.d2l.ai/t/38
2.7. Documentation 87
88 Chapter 2. Preliminaries
3 | Linear Neural Networks
Before we get into the details of deep neural networks, we need to cover the basics of neural network
training. In this chapter, we will cover the entire training process, including defining simple
neural network architectures, handling data, specifying a loss function, and training the model.
In order to make things easier to grasp, we begin with the simplest concepts. Fortunately, classic
statistical learning techniques such as linear and softmax regression can be cast as linear neural
networks. Starting from these classic algorithms, we will introduce you to the basics, providing
the basis for more complex techniques in the rest of the book.
3.1 Linear Regression
Regression refers to a set of methods for modeling the relationship between one or more independent
variables and a dependent variable. In the natural sciences and social sciences, the purpose
of regression is most often to characterize the relationship between the inputs and outputs. Machine
learning, on the other hand, is most often concerned with prediction.
Regression problems pop up whenever we want to predict a numerical value. Common examples
include predicting prices (of homes, stocks, etc.), predicting length of stay (for patients in
the hospital), demand forecasting (for retail sales), among countless others. Not every prediction
problem is a classic regression problem. In subsequent sections, we will introduce classification
problems, where the goal is to predict membership among a set of categories.
3.1.1 Basic Elements of Linear Regression
Linear regression may be both the simplest and most popular among the standard tools to regression.
Dating back to the dawn of the 19th century, linear regression flows from a few simple
assumptions. First, we assume that the relationship between the independent variables x and the
dependent variable y is linear, i.e., that y can be expressed as a weighted sum of the elements
in x, given some noise on the observations. Second, we assume that any noise is well-behaved
(following a Gaussian distribution).
To motivate the approach, let us start with a running example. Suppose that we wish to estimate
the prices of houses (in dollars) based on their area (in square feet) and age (in years). To actually
develop a model for predicting house prices, we would need to get our hands on a dataset consisting
of sales for which we know the sale price, area, and age for each home. In the terminology of
machine learning, the dataset is called a training dataset or training set, and each row (here the data
corresponding to one sale) is called an example (or data point, data instance, sample). The thing we
are trying to predict (price) is called a label (or target). The independent variables (age and area)
upon which the predictions are based are called features (or covariates).
89
Typically, we will use n to denote the number of examples in our dataset. We index the data examples
by i, denoting each input as x(i) = [x(i)
1 ; x(i)
2 ]? and the corresponding label as y(i).
Linear Model
The linearity assumption just says that the target (price) can be expressed as a weighted sum of
the features (area and age):
price = warea  area + wage  age + b: (3.1.1)
In (3.1.1), warea and wage are called weights, and b is called a bias (also called an offset or intercept).
The weights determine the influence of each feature on our prediction and the bias just says what
value the predicted price should take when all of the features take value 0. Even if we will never
see any homes with zero area, or that are precisely zero years old, we still need the bias or else we
will limit the expressivity of our model. Strictly speaking, (3.1.1) is an affine transformation of input
features, which is characterized by a linear transformation of features via weighted sum, combined
with a translation via the added bias.
Given a dataset, our goal is to choose the weights w and the bias b such that on average, the predictions
made according to our model best fit the true prices observed in the data. Models whose
output prediction is determined by the affine transformation of input features are linear models,
where the affine transformation is specified by the chosen weights and bias.
In disciplines where it is common to focus on datasets with just a few features, explicitly expressing
models long-form like this is common. In machine learning, we usually work with highdimensional
datasets, so it is more convenient to employ linear algebra notation. When our inputs
consist of d features, we express our prediction ^y (in general the �hat� symbol denotes estimates)
as
^y = w1x1 + ::: + wdxd + b: (3.1.2)
Collecting all features into a vector x 2 Rd and all weights into a vector w 2 Rd, we can express
our model compactly using a dot product:
^y = w?x + b: (3.1.3)
In (3.1.3), the vector x corresponds to features of a single data example. We will often find it
convenient to refer to features of our entire dataset of n examples via the design matrix X 2 Rnd.
Here, X contains one row for every example and one column for every feature.
For a collection of features X, the predictions ^y 2 Rn can be expressed via the matrix-vector product:
^y = Xw + b; (3.1.4)
where broadcasting (see Section 2.1.3) is applied during the summation. Given features of a training
dataset X and corresponding (known) labels y, the goal of linear regression is to find the weight
vector w and the bias term b that given features of a new data example sampled from the same
distribution as X, the new example?s label will (in expectation) be predicted with the lowest error.
Even if we believe that the best model for predicting y given x is linear, we would not expect to
find a real-world dataset of n examples where y(i) exactly equals w?x(i) + b for all 1  i  n. For
example, whatever instruments we use to observe the features X and labels y might suffer small
90 Chapter 3. Linear Neural Networks
amount of measurement error. Thus, even when we are confident that the underlying relationship
is linear, we will incorporate a noise term to account for such errors.
Before we can go about searching for the best parameters (or model parameters) w and b, we will need
two more things: (i) a quality measure for some given model; and (ii) a procedure for updating the
model to improve its quality.
Loss Function
Before we start thinking about how to fit data with our model, we need to determine a measure of
fitness. The loss function quantifies the distance between the real and predicted value of the target.
The loss will usually be a non-negative number where smaller values are better and perfect predictions
incur a loss of 0. The most popular loss function in regression problems is the squared
error. When our prediction for an example i is ^y(i) and the corresponding true label is y(i), the
squared error is given by:
l(i)(w; b) =
1
2
(
^y(i) ?? y(i)
)2
: (3.1.5)
The constant 1
2 makes no real difference but will prove notationally convenient, canceling out
when we take the derivative of the loss. Since the training dataset is given to us, and thus out of
our control, the empirical error is only a function of the model parameters. To make things more
concrete, consider the example below where we plot a regression problem for a one-dimensional
case as shown in Fig. 3.1.1.
Fig. 3.1.1: Fit data with a linear model.
Note that large differences between estimates ^y(i) and observations y(i) lead to even larger contributions
to the loss, due to the quadratic dependence. To measure the quality of a model on the
entire dataset of n examples, we simply average (or equivalently, sum) the losses on the training
set.
L(w; b) =
1
n
?n
i=1
l(i)(w; b) =
1
n
?n
i=1
1
2
(
w?x(i) + b ?? y(i)
)2
: (3.1.6)
When training the model, we want to find parameters (w; b) that minimize the total loss across
all training examples:
w
; b

= argmin
w;b
L(w; b): (3.1.7)
3.1. Linear Regression 91
Analytic Solution
Linear regression happens to be an unusually simple optimization problem. Unlike most other
models that we will encounter in this book, linear regression can be solved analytically by applying
a simple formula. To start, we can subsume the bias b into the parameter w by appending a column
to the design matrix consisting of all ones. Then our prediction problem is to minimize ?y??Xw?2.
There is just one critical point on the loss surface and it corresponds to the minimum of the loss
over the entire domain. Taking the derivative of the loss with respect to w and setting it equal to
zero yields the analytic (closed-form) solution:
w
= (X?X)
??1X?y: (3.1.8)
While simple problems like linear regression may admit analytic solutions, you should not get
used to such good fortune. Although analytic solutions allow for nice mathematical analysis, the
requirement of an analytic solution is so restrictive that it would exclude all of deep learning.
Minibatch Stochastic Gradient Descent
Even in cases where we cannot solve the models analytically, it turns out that we can still train
models effectively in practice. Moreover, for many tasks, those difficult-to-optimize models turn
out to be so much better that figuring out how to train them ends up being well worth the trouble.
The key technique for optimizing nearly any deep learning model, and which we will call upon
throughout this book, consists of iteratively reducing the error by updating the parameters in the
direction that incrementally lowers the loss function. This algorithm is called gradient descent.
The most naive application of gradient descent consists of taking the derivative of the loss function,
which is an average of the losses computed on every single example in the dataset. In practice,
this can be extremely slow: we must pass over the entire dataset before making a single
update. Thus, we will often settle for sampling a random minibatch of examples every time we
need to compute the update, a variant called minibatch stochastic gradient descent.
In each iteration, we first randomly sample a minibatch B consisting of a fixed number of training
examples. We then compute the derivative (gradient) of the average loss on the minibatch with
regard to the model parameters. Finally, we multiply the gradient by a predetermined positive
value  and subtract the resulting term from the current parameter values.
We can express the update mathematically as follows (@ denotes the partial derivative):
(w; b)   (w; b) ?? 
jBj
?
i2B
@(w;b)l(i)(w; b): (3.1.9)
To summarize, steps of the algorithm are the following: (i) we initialize the values of the model
parameters, typically at random; (ii) we iteratively sample random minibatches from the data,
updating the parameters in the direction of the negative gradient. For quadratic losses and affine
transformations, we can write this out explicitly as follows:
w   w ?? 
jBj
?
i2B
@wl(i)(w; b) = w ?? 
jBj
?
i2B
x(i)
(
w?x(i) + b ?? y(i)
)
;
b   b ?? 
jBj
?
i2B
@bl(i)(w; b) = b ?? 
jBj
?
i2B
(
w?x(i) + b ?? y(i)
)
:
(3.1.10)
Note that w and x are vectors in (3.1.10). Here, the more elegant vector notation makes the math
much more readable than expressing things in terms of coefficients, say w1;w2; : : : ;wd. The set
92 Chapter 3. Linear Neural Networks
cardinality jBj represents the number of examples in each minibatch (the batch size) and  denotes
the learning rate. We emphasize that the values of the batch size and learning rate are manually
pre-specified and not typically learned through model training. These parameters that are tunable
but not updated in the training loop are called hyperparameters. Hyperparameter tuning is the
process by which hyperparameters are chosen, and typically requires that we adjust them based
on the results of the training loop as assessed on a separate validation dataset (or validation set).
After training for some predetermined number of iterations (or until some other stopping criteria
are met), we record the estimated model parameters, denoted ^ w;^ b. Note that even if our function
is truly linear and noiseless, these parameters will not be the exact minimizers of the loss because,
although the algorithm converges slowly towards the minimizers it cannot achieve it exactly in a
finite number of steps.
Linear regression happens to be a learning problem where there is only one minimum over the
entire domain. However, for more complicated models, like deep networks, the loss surfaces
contain many minima. Fortunately, for reasons that are not yet fully understood, deep learning
practitioners seldom struggle to find parameters that minimize the loss on training sets. The more
formidable task is to find parameters that will achieve low loss on data that we have not seen
before, a challenge called generalization. We return to these topics throughout the book.
Making Predictions with the Learned Model
Given the learned linear regression model ^ w?x+^b
, we can now estimate the price of a new house
(not contained in the training data) given its area x1 and age x2. Estimating targets given features
is commonly called prediction or inference.
We will try to stick with prediction because calling this step inference, despite emerging as standard
jargon in deep learning, is somewhat of a misnomer. In statistics, inference more often denotes
estimating parameters based on a dataset. This misuse of terminology is a common source of
confusion when deep learning practitioners talk to statisticians.
3.1.2 Vectorization for Speed
When training our models, we typically want to process whole minibatches of examples simultaneously.
Doing this efficiently requires that we vectorize the calculations and leverage fast linear
algebra libraries rather than writing costly for-loops in Python.
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np
import time
To illustrate why this matters so much, we can consider two methods for adding vectors. To start
we instantiate two 10000-dimensional vectors containing all ones. In one method we will loop
over the vectors with a Python for-loop. In the other method we will rely on a single call to +.
n = 10000
a = np.ones(n)
b = np.ones(n)
Since we will benchmark the running time frequently in this book, let us define a timer.
3.1. Linear Regression 93
class Timer: #@save
"""Record multiple running times."""
def __init__(self):
self.times = []
self.start()
def start(self):
"""Start the timer."""
self.tik = time.time()
def stop(self):
"""Stop the timer and record the time in a list."""
self.times.append(time.time() - self.tik)
return self.times[-1]
def avg(self):
"""Return the average time."""
return sum(self.times) / len(self.times)
def sum(self):
"""Return the sum of time."""
return sum(self.times)
def cumsum(self):
"""Return the accumulated time."""
return np.array(self.times).cumsum().tolist()
Now we can benchmark the workloads. First, we add them, one coordinate at a time, using a
for-loop.
c = np.zeros(n)
timer = Timer()
for i in range(n):
c[i] = a[i] + b[i]
f'{timer.stop():.5f} sec'
'4.22384 sec'
Alternatively, we rely on the reloaded + operator to compute the elementwise sum.
timer.start()
d = a + b
f'{timer.stop():.5f} sec'
'0.00023 sec'
You probably noticed that the second method is dramatically faster than the first. Vectorizing
code often yields order-of-magnitude speedups. Moreover, we push more of the mathematics to
the library and need not write as many calculations ourselves, reducing the potential for errors.
94 Chapter 3. Linear Neural Networks
3.1.3 The Normal Distribution and Squared Loss
While you can already get your hands dirty using only the information above, in the following we
can more formally motivate the square loss objective via assumptions about the distribution of
noise.
Linear regression was invented by Gauss in 1795, who also discovered the normal distribution
(also called the Gaussian). It turns out that the connection between the normal distribution and
linear regression runs deeper than common parentage. To refresh your memory, the probability
density of a normal distribution with mean  and variance 2 (standard deviation ) is given as
p(x) =
p 1
22
exp
(
?? 1
22 (x ?? )2
)
: (3.1.11)
Below we define a Python function to compute the normal distribution.
def normal(x, mu, sigma):
p = 1 / math.sqrt(2 * math.pi * sigma**2)
return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
We can now visualize the normal distributions.
# Use numpy again for visualization
x = np.arange(-7, 7, 0.01)
# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
ylabel='p(x)', figsize=(4.5, 2.5),
legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
As we can see, changing the mean corresponds to a shift along the x-axis, and increasing the
variance spreads the distribution out, lowering its peak.
One way to motivate linear regression with the mean squared error loss function (or simply square
loss) is to formally assume that observations arise from noisy observations, where the noise is
normally distributed as follows:
y = w?x + b + ? where ?  N(0; 2): (3.1.12)
3.1. Linear Regression 95
Thus, we can now write out the likelihood of seeing a particular y for a given x via
P(y j x) =
p 1
22
exp
(
?? 1
22 (y ?? w?x ?? b)2
)
: (3.1.13)
Now, according to the principle of maximum likelihood, the best values of parameters w and b are
those that maximize the likelihood of the entire dataset:
P(y j X) =
?n
i=1
p(y(i)jx(i)): (3.1.14)
Estimators chosen according to the principle of maximum likelihood are called maximum likelihood
estimators. While, maximizing the product of many exponential functions, might look difficult,
we can simplify things significantly, without changing the objective, by maximizing the log
of the likelihood instead. For historical reasons, optimizations are more often expressed as minimization
rather than maximization. So, without changing anything we can minimize the negative
log-likelihood ??log P(y j X). Working out the mathematics gives us:
??log P(y j X) =
?n
i=1
1
2
log(22) +
1
22
(
y(i) ?? w?x(i) ?? b
)2
: (3.1.15)
Now we just need one more assumption that  is some fixed constant. Thus we can ignore the first
term because it does not depend on w or b. Now the second term is identical to the squared error
loss introduced earlier, except for the multiplicative constant 1
2 . Fortunately, the solution does
not depend on . It follows that minimizing the mean squared error is equivalent to maximum
likelihood estimation of a linear model under the assumption of additive Gaussian noise.
3.1.4 From Linear Regression to Deep Networks
So far we only talked about linear models. While neural networks cover a much richer family of
models, we can begin thinking of the linear model as a neural network by expressing it in the
language of neural networks. To begin, let us start by rewriting things in a �layer� notation.
Neural Network Diagram
Deep learning practitioners like to draw diagrams to visualize what is happening in their models.
In Fig. 3.1.2, we depict our linear regression model as a neural network. Note that these diagrams
highlight the connectivity pattern such as how each input is connected to the output, but not the
values taken by the weights or biases.
Fig. 3.1.2: Linear regression is a single-layer neural network.
For the neural network shown in Fig. 3.1.2, the inputs are x1; : : : ; xd, so the number of inputs (or
feature dimensionality) in the input layer is d. The output of the network in Fig. 3.1.2 is o1, so the
96 Chapter 3. Linear Neural Networks
number of outputs in the output layer is 1. Note that the input values are all given and there is just
a single computed neuron. Focusing on where computation takes place, conventionally we do not
consider the input layer when counting layers. That is to say, the number of layers for the neural
network in Fig. 3.1.2 is 1. We can think of linear regression models as neural networks consisting
of just a single artificial neuron, or as single-layer neural networks.
Since for linear regression, every input is connected to every output (in this case there is only one
output), we can regard this transformation (the output layer in Fig. 3.1.2) as a fully-connected layer
or dense layer. We will talk a lot more about networks composed of such layers in the next chapter.
Biology
Since linear regression (invented in 1795) predates computational neuroscience, it might seem
anachronistic to describe linear regression as a neural network. To see why linear models were a
natural place to begin when the cyberneticists/neurophysiologists Warren McCulloch and Walter
Pitts began to develop models of artificial neurons, consider the cartoonish picture of a biological
neuron in Fig. 3.1.3, consisting of dendrites (input terminals), the nucleus (CPU), the axon (output
wire), and the axon terminals (output terminals), enabling connections to other neurons via
synapses.
Dendrite
Cell body
Node of
Ranvier
Axon Terminal
Schwann cell
Myelin sheath
Axon
Nucleus
Fig. 3.1.3: The real neuron.
Information xi arriving from other neurons (or environmental sensors such as the retina) is received
in the dendrites. In particular, that information is weighted by synaptic weights wi determining
the effect of the inputs (e.g., activation or inhibition via the product xiwi). The weighted inputs
arriving from multiple sources are aggregated in the nucleus as a weighted sum y =
?
i xiwi + b,
and this information is then sent for further processing in the axon y, typically after some nonlinear
processing via (y). From there it either reaches its destination (e.g., a muscle) or is fed into
another neuron via its dendrites.
Certainly, the high-level idea that many such units could be cobbled together with the right connectivity
and right learning algorithm, to produce far more interesting and complex behavior than
any one neuron alone could express owes to our study of real biological neural systems.
At the same time, most research in deep learning today draws little direct inspiration in neuroscience.
We invoke Stuart Russell and Peter Norvig who, in their classic AI text book Artificial Intelligence:
A Modern Approach (Russell & Norvig, 2016), pointed out that although airplanes might
have been inspired by birds, ornithology has not been the primary driver of aeronautics innovation
3.1. Linear Regression 97
for some centuries. Likewise, inspiration in deep learning these days comes in equal or greater
measure from mathematics, statistics, and computer science.
Summary
� Key ingredients in a machine learning model are training data, a loss function, an optimization
algorithm, and quite obviously, the model itself.
� Vectorizing makes everything better (mostly math) and faster (mostly code).
� Minimizing an objective function and performing maximum likelihood estimation can
mean the same thing.
� Linear regression models are neural networks, too.
Exercises
1. Assume that we have some data x1; : : : ; ? xn 2 R. Our goal is to find a constant b such that
i(xi ?? b)2 is minimized.
1. Find a analytic solution for the optimal value of b.
2. How does this problem and its solution relate to the normal distribution?
2. Derive the analytic solution to the optimization problem for linear regression with squared
error. To keep things simple, you can omit the bias b from the problem (we can do this in
principled fashion by adding one column to X consisting of all ones).
1. Write out the optimization problem in matrix and vector notation (treat all the data as
a single matrix, and all the target values as a single vector).
2. Compute the gradient of the loss with respect to w.
3. Find the analytic solution by setting the gradient equal to zero and solving the matrix
equation.
4. When might this be better than using stochastic gradient descent? When might this
method break?
3. Assume that the noise model governing the additive noise ? is the exponential distribution.
That is, p(?) = 1
2 exp(??j?j).
1. Write out the negative log-likelihood of the data under the model ??log P(y j X).
2. Can you find a closed form solution?
3. Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly
go wrong (hint: what happens near the stationary point as we keep on updating
the parameters)? Can you fix this?
Discussions51
51 https://discuss.d2l.ai/t/40
98 Chapter 3. Linear Neural Networks
3.2 Linear Regression Implementation from Scratch
Now that you understand the key ideas behind linear regression, we can begin to work through
a hands-on implementation in code. In this section, we will implement the entire method from
scratch, including the data pipeline, the model, the loss function, and the minibatch stochastic
gradient descent optimizer. While modern deep learning frameworks can automate nearly all of
this work, implementing things from scratch is the only way to make sure that you really know
what you are doing. Moreover, when it comes time to customize models, defining our own layers
or loss functions, understanding how things work under the hood will prove handy. In this section,
we will rely only on tensors and auto differentiation. Afterwards, we will introduce a more concise
implementation, taking advantage of bells and whistles of deep learning frameworks.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
import random
npx.set_np()
3.2.1 Generating the Dataset
To keep things simple, we will construct an artificial dataset according to a linear model with
additive noise. Our task will be to recover this model?s parameters using the finite set of examples
contained in our dataset. We will keep the data low-dimensional so we can visualize it easily. In
the following code snippet, we generate a dataset containing 1000 examples, each consisting of 2
features sampled from a standard normal distribution. Thus our synthetic dataset will be a matrix
X 2 R10002.
The true parameters generating our dataset will be w = [2;??3:4]? and b = 4:2, and our synthetic
labels will be assigned according to the following linear model with the noise term ?:
y = Xw + b + ?: (3.2.1)
You could think of ? as capturing potential measurement errors on the features and labels. We
will assume that the standard assumptions hold and thus that ? obeys a normal distribution with
mean of 0. To make our problem easy, we will set its standard deviation to 0.01. The following
code generates our synthetic dataset.
def synthetic_data(w, b, num_examples): #@save
"""Generate y = Xw + b + noise."""
X = np.random.normal(0, 1, (num_examples, len(w)))
y = np.dot(X, w) + b
y += np.random.normal(0, 0.01, y.shape)
return X, y.reshape((-1, 1))
true_w = np.array([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
Note that each row in features consists of a 2-dimensional data example and that each row in
labels consists of a 1-dimensional label value (a scalar).
3.2. Linear Regression Implementation from Scratch 99
print('features:', features[0],'\nlabel:', labels[0])
features: [2.2122064 1.1630787]
label: [4.662078]
By generating a scatter plot using the second feature features[:, 1] and labels, we can clearly
observe the linear correlation between the two.
d2l.set_figsize()
# The semicolon is for displaying the plot only
d2l.plt.scatter(d2l.numpy(features[:, 1]), d2l.numpy(labels), 1);
3.2.2 Reading the Dataset
Recall that training models consists of making multiple passes over the dataset, grabbing one
minibatch of examples at a time, and using them to update our model. Since this process is so
fundamental to training machine learning algorithms, it is worth defining a utility function to
shuffle the dataset and access it in minibatches.
In the following code, we define the data_iter function to demonstrate one possible implementation
of this functionality. The function takes a batch size, a matrix of features, and a vector of
labels, yielding minibatches of the size batch_size. Each minibatch consists of a tuple of features
and labels.
def data_iter(batch_size, features, labels):
num_examples = len(features)
indices = list(range(num_examples))
# The examples are read at random, in no particular order
random.shuffle(indices)
for i in range(0, num_examples, batch_size):
batch_indices = np.array(
indices[i: min(i + batch_size, num_examples)])
yield features[batch_indices], labels[batch_indices]
In general, note that we want to use reasonably sized minibatches to take advantage of the GPU
hardware, which excels at parallelizing operations. Because each example can be fed through our
models in parallel and the gradient of the loss function for each example can also be taken in
100 Chapter 3. Linear Neural Networks
parallel, GPUs allow us to process hundreds of examples in scarcely more time than it might take
to process just a single example.
To build some intuition, let us read and print the first small batch of data examples. The shape of
the features in each minibatch tells us both the minibatch size and the number of input features.
Likewise, our minibatch of labels will have a shape given by batch_size.
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
print(X, '\n', y)
break
[[-0.26001498 1.8115263 ]
[ 0.34593368 -0.7567092 ]
[-1.4900824 -1.0667098 ]
[-0.49231794 -1.9087739 ]
[-0.9669159 -0.8711447 ]
[-0.00572143 -0.31106037]
[-0.33182386 1.6597842 ]
[-0.18256646 0.8374752 ]
[-0.47508195 -0.24207895]
[ 0.77013063 -1.1259842 ]]
[[-2.4844196]
[ 7.478742 ]
[ 4.851502 ]
[ 9.706129 ]
[ 5.2282877]
[ 5.2416353]
[-2.114907 ]
[ 0.9842439]
[ 4.045882 ]
[ 9.558747 ]]
As we run the iteration, we obtain distinct minibatches successively until the entire dataset has
been exhausted (try this). While the iteration implemented above is good for didactic purposes,
it is inefficient in ways that might get us in trouble on real problems. For example, it requires that
we load all the data in memory and that we perform lots of random memory access. The built-in
iterators implemented in a deep learning framework are considerably more efficient and they can
deal with both data stored in files and data fed via data streams.
3.2.3 Initializing Model Parameters
Before we can begin optimizing our model?s parameters by minibatch stochastic gradient descent,
we need to have some parameters in the first place. In the following code, we initialize weights
by sampling random numbers from a normal distribution with mean 0 and a standard deviation
of 0.01, and setting the bias to 0.
w = np.random.normal(0, 0.01, (2, 1))
b = np.zeros(1)
w.attach_grad()
b.attach_grad()
3.2. Linear Regression Implementation from Scratch 101
After initializing our parameters, our next task is to update them until they fit our data sufficiently
well. Each update requires taking the gradient of our loss function with respect to the parameters.
Given this gradient, we can update each parameter in the direction that may reduce the loss.
Since nobody wants to compute gradients explicitly (this is tedious and error prone), we use automatic
differentiation, as introduced in Section 2.5, to compute the gradient.
3.2.4 Defining the Model
Next, we must define our model, relating its inputs and parameters to its outputs. Recall that to
calculate the output of the linear model, we simply take the matrix-vector dot product of the input
features X and the model weights w, and add the offset b to each example. Note that below Xw is
a vector and b is a scalar. Recall the broadcasting mechanism as described in Section 2.1.3. When
we add a vector and a scalar, the scalar is added to each component of the vector.
def linreg(X, w, b): #@save
"""The linear regression model."""
return np.dot(X, w) + b
3.2.5 Defining the Loss Function
Since updating our model requires taking the gradient of our loss function, we ought to define the
loss function first. Here we will use the squared loss function as described in Section 3.1. In the
implementation, we need to transform the true value y into the predicted value?s shape y_hat. The
result returned by the following function will also have the same shape as y_hat.
def squared_loss(y_hat, y): #@save
"""Squared loss."""
return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
3.2.6 Defining the Optimization Algorithm
As we discussed in Section 3.1, linear regression has a closed-form solution. However, this is not
a book about linear regression: it is a book about deep learning. Since none of the other models
that this book introduces can be solved analytically, we will take this opportunity to introduce your
first working example of minibatch stochastic gradient descent.
At each step, using one minibatch randomly drawn from our dataset, we will estimate the gradient
of the loss with respect to our parameters. Next, we will update our parameters in the direction
that may reduce the loss. The following code applies the minibatch stochastic gradient descent
update, given a set of parameters, a learning rate, and a batch size. The size of the update step is
determined by the learning rate lr. Because our loss is calculated as a sum over the minibatch of
examples, we normalize our step size by the batch size (batch_size), so that the magnitude of a
typical step size does not depend heavily on our choice of the batch size.
def sgd(params, lr, batch_size): #@save
"""Minibatch stochastic gradient descent."""
for param in params:
param[:] = param - lr * param.grad / batch_size
102 Chapter 3. Linear Neural Networks
3.2.7 Training
Now that we have all of the parts in place, we are ready to implement the main training loop. It
is crucial that you understand this code because you will see nearly identical training loops over
and over again throughout your career in deep learning.
In each iteration, we will grab a minibatch of training examples, and pass them through our model
to obtain a set of predictions. After calculating the loss, we initiate the backwards pass through
the network, storing the gradients with respect to each parameter. Finally, we will call the optimization
algorithm sgd to update the model parameters.
In summary, we will execute the following loop:
� Initialize parameters (w; b)
� Repeat until done
� Compute gradient g   @(w;b)
1
jBj
?
i2B l(x(i); y(i); w; b)
� Update parameters (w; b)   (w; b) ?? g
In each epoch, we will iterate through the entire dataset (using the data_iter function) once passing
through every example in the training dataset (assuming that the number of examples is divisible
by the batch size). The number of epochs num_epochs and the learning rate lr are both
hyperparameters, which we set here to 3 and 0.03, respectively. Unfortunately, setting hyperparameters
is tricky and requires some adjustment by trial and error. We elide these details for now
but revise them later in Chapter 11.
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss
for epoch in range(num_epochs):
for X, y in data_iter(batch_size, features, labels):
with autograd.record():
l = loss(net(X, w, b), y) # Minibatch loss in `X` and `y`
# Because `l` has a shape (`batch_size`, 1) and is not a scalar
# variable, the elements in `l` are added together to obtain a new
# variable, on which gradients with respect to [`w`, `b`] are computed
l.backward()
sgd([w, b], lr, batch_size) # Update parameters using their gradient
train_l = loss(net(features, w, b), labels)
print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
epoch 1, loss 0.025238
epoch 2, loss 0.000096
epoch 3, loss 0.000051
In this case, because we synthesized the dataset ourselves, we know precisely what the true parameters
are. Thus, we can evaluate our success in training by comparing the true parameters
with those that we learned through our training loop. Indeed they turn out to be very close to
each other.
3.2. Linear Regression Implementation from Scratch 103
print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')
print(f'error in estimating b: {true_b - b}')
error in estimating w: [ 0.00068665 -0.00013518]
error in estimating b: [0.00068951]
Note that we should not take it for granted that we are able to recover the parameters perfectly.
However, in machine learning, we are typically less concerned with recovering true underlying
parameters, and more concerned with parameters that lead to highly accurate prediction. Fortunately,
even on difficult optimization problems, stochastic gradient descent can often find remarkably
good solutions, owing partly to the fact that, for deep networks, there exist many configurations
of the parameters that lead to highly accurate prediction.
Summary
� We saw how a deep network can be implemented and optimized from scratch, using just
tensors and auto differentiation, without any need for defining layers or fancy optimizers.
� This section only scratches the surface of what is possible. In the following sections, we will
describe additional models based on the concepts that we have just introduced and learn
how to implement them more concisely.
Exercises
1. What would happen if we were to initialize the weights to zero. Would the algorithm still
work?
2. Assume that you are Georg Simon Ohm52 trying to come up with a model between voltage
and current. Can you use auto differentiation to learn the parameters of your model?
3. Can you use Planck?s Law53 to determine the temperature of an object using spectral energy
density?
4. What are the problems you might encounter if you wanted to compute the second derivatives?
How would you fix them?
5. Why is the reshape function needed in the squared_loss function?
6. Experiment using different learning rates to find out how fast the loss function value drops.
7. If the number of examples cannot be divided by the batch size, what happens to the
data_iter function?s behavior?
Discussions54
52 https://en.wikipedia.org/wiki/Georg_Ohm
53 https://en.wikipedia.org/wiki/Planck%27s_law
54 https://discuss.d2l.ai/t/42
104 Chapter 3. Linear Neural Networks
3.3 Concise Implementation of Linear Regression
Broad and intense interest in deep learning for the past several years has inspired companies,
academics, and hobbyists to develop a variety of mature open source frameworks for automating
the repetitive work of implementing gradient-based learning algorithms. In Section 3.2, we relied
only on (i) tensors for data storage and linear algebra; and (ii) auto differentiation for calculating
gradients. In practice, because data iterators, loss functions, optimizers, and neural network
layers are so common, modern libraries implement these components for us as well.
In this section, we will show you how to implement the linear regression model from Section 3.2
concisely by using high-level APIs of deep learning frameworks.
3.3.1 Generating the Dataset
To start, we will generate the same dataset as in Section 3.2.
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
npx.set_np()
true_w = np.array([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
3.3.2 Reading the Dataset
Rather than rolling our own iterator, we can call upon the existing API in a framework to read
data. We pass in features and labels as arguments and specify batch_size when instantiating
a data iterator object. Besides, the boolean value is_train indicates whether or not we want the
data iterator object to shuffle the data on each epoch (pass through the dataset).
def load_array(data_arrays, batch_size, is_train=True): #@save
"""Construct a Gluon data iterator."""
dataset = gluon.data.ArrayDataset(*data_arrays)
return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)
batch_size = 10
data_iter = load_array((features, labels), batch_size)
Now we can use data_iter in much the same way as we called the data_iter function in Section
3.2. To verify that it is working, we can read and print the first minibatch of examples. Comparing
with Section 3.2, here we use iter to construct a Python iterator and use next to obtain the first
item from the iterator.
next(iter(data_iter))
3.3. Concise Implementation of Linear Regression 105
[array([[ 0.5443711 , 0.7681698 ],
[-0.47063652, -0.59975994],
[-0.47508195, -0.24207895],
[ 0.76801354, -0.69288605],
[-2.7625482 , 1.7799325 ],
[-0.55819434, 0.33346984],
[-0.4121327 , -1.5466391 ],
[ 0.02849233, -0.71190894],
[-0.9567186 , -1.6827176 ],
[-0.51572496, 0.07394829]]),
array([[ 2.6835713],
[ 5.289975 ],
[ 4.045882 ],
[ 8.095631 ],
[-7.4035244],
[ 1.9458812],
[ 8.63532 ],
[ 6.669124 ],
[ 8.01163 ],
[ 2.9113574]])]
3.3.3 Defining the Model
When we implemented linear regression from scratch in Section 3.2, we defined our model parameters
explicitly and coded up the calculations to produce output using basic linear algebra
operations. You should know how to do this. But once your models get more complex, and once
you have to do this nearly every day, you will be glad for the assistance. The situation is similar
to coding up your own blog from scratch. Doing it once or twice is rewarding and instructive, but
you would be a lousy web developer if every time you needed a blog you spent a month reinventing
the wheel.
For standard operations, we can use a framework?s predefined layers, which allow us to focus especially
on the layers used to construct the model rather than having to focus on the implementation.
We will first define a model variable net, which will refer to an instance of the Sequential class.
The Sequential class defines a container for several layers that will be chained together. Given
input data, a Sequential instance passes it through the first layer, in turn passing the output as
the second layer?s input and so forth. In the following example, our model consists of only one
layer, so we do not really need Sequential. But since nearly all of our future models will involve
multiple layers, we will use it anyway just to familiarize you with the most standard workflow.
Recall the architecture of a single-layer network as shown in Fig. 3.1.2. The layer is said to be fullyconnected
because each of its inputs is connected to each of its outputs by means of a matrix-vector
multiplication.
In Gluon, the fully-connected layer is defined in the Dense class. Since we only want to generate a
single scalar output, we set that number to 1.
It is worth noting that, for convenience, Gluon does not require us to specify the input shape for
each layer. So here, we do not need to tell Gluon how many inputs go into this linear layer. When
we first try to pass data through our model, e.g., when we execute net(X) later, Gluon will automatically
infer the number of inputs to each layer. We will describe how this works in more detail
later.
106 Chapter 3. Linear Neural Networks
# `nn` is an abbreviation for neural networks
from mxnet.gluon import nn
net = nn.Sequential()
net.add(nn.Dense(1))
3.3.4 Initializing Model Parameters
Before using net, we need to initialize the model parameters, such as the weights and bias in the
linear regression model. Deep learning frameworks often have a predefined way to initialize the
parameters. Here we specify that each weight parameter should be randomly sampled from a normal
distribution with mean 0 and standard deviation 0.01. The bias parameter will be initialized
to zero.
We will import the initializer module from MXNet. This module provides various methods
for model parameter initialization. Gluon makes init available as a shortcut (abbreviation) to
access the initializer package. We only specify how to initialize the weight by calling init.
Normal(sigma=0.01). Bias parameters are initialized to zero by default.
from mxnet import init
net.initialize(init.Normal(sigma=0.01))
The code above may look straightforward but you should note that something strange is happening
here. We are initializing parameters for a network even though Gluon does not yet know how
many dimensions the input will have! It might be 2 as in our example or it might be 2000. Gluon
lets us get away with this because behind the scene, the initialization is actually deferred. The
real initialization will take place only when we for the first time attempt to pass data through the
network. Just be careful to remember that since the parameters have not been initialized yet, we
cannot access or manipulate them.
3.3.5 Defining the Loss Function
In Gluon, the loss module defines various loss functions. In this example, we will use the Gluon
implementation of squared loss (L2Loss).
loss = gluon.loss.L2Loss()
3.3.6 Defining the Optimization Algorithm
Minibatch stochastic gradient descent is a standard tool for optimizing neural networks and thus
Gluon supports it alongside a number of variations on this algorithm through its Trainer class.
When we instantiate Trainer, we will specify the parameters to optimize over (obtainable from
our model net via net.collect_params()), the optimization algorithm we wish to use (sgd), and
a dictionary of hyperparameters required by our optimization algorithm. Minibatch stochastic
gradient descent just requires that we set the value learning_rate, which is set to 0.03 here.
from mxnet import gluon
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})
3.3. Concise Implementation of Linear Regression 107
3.3.7 Training
You might have noticed that expressing our model through high-level APIs of a deep learning
framework requires comparatively few lines of code. We did not have to individually allocate
parameters, define our loss function, or implement minibatch stochastic gradient descent. Once
we start working with much more complex models, advantages of high-level APIs will grow considerably.
However, once we have all the basic pieces in place, the training loop itself is strikingly
similar to what we did when implementing everything from scratch.
To refresh your memory: for some number of epochs, we will make a complete pass over the
dataset (train_data), iteratively grabbing one minibatch of inputs and the corresponding groundtruth
labels. For each minibatch, we go through the following ritual:
� Generate predictions by calling net(X) and calculate the loss l (the forward propagation).
� Calculate gradients by running the backpropagation.
� Update the model parameters by invoking our optimizer.
For good measure, we compute the loss after each epoch and print it to monitor progress.
num_epochs = 3
for epoch in range(num_epochs):
for X, y in data_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(batch_size)
l = loss(net(features), labels)
print(f'epoch {epoch + 1}, loss {l.mean().asnumpy():f}')
epoch 1, loss 0.025011
epoch 2, loss 0.000087
epoch 3, loss 0.000051
Below, we compare the model parameters learned by training on finite data and the actual parameters
that generated our dataset. To access parameters, we first access the layer that we need from
net and then access that layer?s weights and bias. As in our from-scratch implementation, note
that our estimated parameters are close to their ground-truth counterparts.
w = net[0].weight.data()
print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')
b = net[0].bias.data()
print(f'error in estimating b: {true_b - b}')
error in estimating w: [4.3487549e-04 6.7949295e-05]
error in estimating b: [0.00079966]
108 Chapter 3. Linear Neural Networks
Summary
� Using Gluon, we can implement models much more concisely.
� In Gluon, the data module provides tools for data processing, the nn module defines a large
number of neural network layers, and the loss module defines many common loss functions.
� MXNet?s module initializer provides various methods for model parameter initialization.
� Dimensionality and storage are automatically inferred, but be careful not to attempt to access
parameters before they have been initialized.
Exercises
1. If we replace l = loss(output, y) with l = loss(output, y).mean(), we need to change
trainer.step(batch_size) to trainer.step(1) for the code to behave identically. Why?
2. Review the MXNet documentation to see what loss functions and initialization methods are
provided in the modules gluon.loss and init. Replace the loss by Huber?s loss.
3. How do you access the gradient of dense.weight?
Discussions55
3.4 Softmax Regression
In Section 3.1, we introduced linear regression, working through implementations from scratch
in Section 3.2 and again using high-level APIs of a deep learning framework in Section 3.3 to do
the heavy lifting.
Regression is the hammer we reach for when we want to answer how much? or how many? questions.
If you want to predict the number of dollars (price) at which a house will be sold, or the
number of wins a baseball team might have, or the number of days that a patient will remain
hospitalized before being discharged, then you are probably looking for a regression model.
In practice, we are more often interested in classification: asking not �how much� but �which one�:
� Does this email belong in the spam folder or the inbox?
� Is this customer more likely to sign up or not to sign up for a subscription service?
� Does this image depict a donkey, a dog, a cat, or a rooster?
� Which movie is Aston most likely to watch next?
Colloquially, machine learning practitioners overload the word classification to describe two subtly
different problems: (i) those where we are interested only in hard assignments of examples to
categories (classes); and (ii) those where we wish to make soft assignments, i.e., to assess the
probability that each category applies. The distinction tends to get blurred, in part, because often,
even when we only care about hard assignments, we still use models that make soft assignments.
55 https://discuss.d2l.ai/t/44
3.4. Softmax Regression 109
3.4.1 Classification Problem
To get our feet wet, let us start off with a simple image classification problem. Here, each input
consists of a 2  2 grayscale image. We can represent each pixel value with a single scalar, giving
us four features x1; x2; x3; x4. Further, let us assume that each image belongs to one among the
categories �cat�, �chicken�, and �dog�.
Next, we have to choose how to represent the labels. We have two obvious choices. Perhaps
the most natural impulse would be to choose y 2 f1; 2; 3g, where the integers represent
fdog; cat; chickeng respectively. This is a great way of storing such information on a computer.
If the categories had some natural ordering among them, say if we were trying to predict
fbaby; toddler; adolescent; young adult; adult; geriatricg, then it might even make sense to cast
this problem as regression and keep the labels in this format.
But general classification problems do not come with natural orderings among the classes. Fortunately,
statisticians long ago invented a simple way to represent categorical data: the one-hot
encoding. A one-hot encoding is a vector with as many components as we have categories. The
component corresponding to particular instance?s category is set to 1 and all other components
are set to 0. In our case, a label y would be a three-dimensional vector, with (1; 0; 0) corresponding
to �cat�, (0; 1; 0) to �chicken�, and (0; 0; 1) to �dog�:
y 2 f(1; 0; 0); (0; 1; 0); (0; 0; 1)g: (3.4.1)
3.4.2 Network Architecture
In order to estimate the conditional probabilities associated with all the possible classes, we need
a model with multiple outputs, one per class. To address classification with linear models, we will
need as many affine functions as we have outputs. Each output will correspond to its own affine
function. In our case, since we have 4 features and 3 possible output categories, we will need 12
scalars to represent the weights (w with subscripts), and 3 scalars to represent the biases (b with
subscripts). We compute these three logits, o1; o2, and o3, for each input:
o1 = x1w11 + x2w12 + x3w13 + x4w14 + b1;
o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2;
o3 = x1w31 + x2w32 + x3w33 + x4w34 + b3:
(3.4.2)
We can depict this calculation with the neural network diagram shown in Fig. 3.4.1. Just as in linear
regression, softmax regression is also a single-layer neural network. And since the calculation
of each output, o1; o2, and o3, depends on all inputs, x1, x2, x3, and x4, the output layer of softmax
regression can also be described as fully-connected layer.
Fig. 3.4.1: Softmax regression is a single-layer neural network.
To express the model more compactly, we can use linear algebra notation. In vector form, we
arrive at o = Wx + b, a form better suited both for mathematics, and for writing code. Note that
110 Chapter 3. Linear Neural Networks
we have gathered all of our weights into a 34 matrix and that for features of a given data example
x, our outputs are given by a matrix-vector product of our weights by our input features plus our
biases b.
3.4.3 Softmax Operation
The main approach that we are going to take here is to interpret the outputs of our model as probabilities.
We will optimize our parameters to produce probabilities that maximize the likelihood of
the observed data. Then, to generate predictions, we will set a threshold, for example, choosing
the label with the maximum predicted probabilities.
Put formally, we would like any output ^yj to be interpreted as the probability that a given item
belongs to class j. Then we can choose the class with the largest output value as our prediction
argmaxj yj . For example, if ^y1, ^y2, and ^y3 are 0.1, 0.8, and 0.1, respectively, then we predict category
2, which (in our example) represents �chicken�.
You might be tempted to suggest that we interpret the logits o directly as our outputs of interest.
However, there are some problems with directly interpreting the output of the linear layer as a
probability. On one hand, nothing constrains these numbers to sum to 1. On the other hand,
depending on the inputs, they can take negative values. These violate basic axioms of probability
presented in Section 2.6
To interpret our outputs as probabilities, we must guarantee that (even on new data), they will be
nonnegative and sum up to 1. Moreover, we need a training objective that encourages the model
to estimate faithfully probabilities. Of all instances when a classifier outputs 0.5, we hope that half
of those examples will actually belong to the predicted class. This is a property called calibration.
The softmax function, invented in 1959 by the social scientist R. Duncan Luce in the context of
choice models, does precisely this. To transform our logits such that they become nonnegative and
sum to 1, while requiring that the model remains differentiable, we first exponentiate each logit
(ensuring non-negativity) and then divide by their sum (ensuring that they sum to 1):
^y = softmax(o) where ^yj =
?exp(oj)
k exp(ok)
: (3.4.3)
It is easy to see ^y1+^y2+^y3 = 1 with 0  ^yj  1 for all j. Thus, ^y is a proper probability distribution
whose element values can be interpreted accordingly. Note that the softmax operation does not
change the ordering among the logits o, which are simply the pre-softmax values that determine
the probabilities assigned to each class. Therefore, during prediction we can still pick out the most
likely class by
argmax
j
^yj = argmax
j
oj : (3.4.4)
Although softmax is a nonlinear function, the outputs of softmax regression are still determined
by an affine transformation of input features; thus, softmax regression is a linear model.
3.4. Softmax Regression 111
3.4.4 Vectorization for Minibatches
To improve computational efficiency and take advantage of GPUs, we typically carry out vector
calculations for minibatches of data. Assume that we are given a minibatch X of examples with
feature dimensionality (number of inputs) d and batch size n. Moreover, assume that we have q
categories in the output. Then the minibatch features X are in Rnd, weights W 2 Rdq, and the
bias satisfies b 2 R1q.
O = XW + b;
^Y = softmax(O):
(3.4.5)
This accelerates the dominant operation into a matrix-matrix product XW vs. the matrix-vector
products we would be executing if we processed one example at a time. Since each row in X represents
a data example, the softmax operation itself can be computed rowwise: for each row of
O, exponentiate all entries and then normalize them by the sum. Triggering broadcasting during
the summation XW+b in (3.4.5), both the minibatch logits O and output probabilities ^Y are nq
matrices.
3.4.5 Loss Function
Next, we need a loss function to measure the quality of our predicted probabilities. We will rely
on maximum likelihood estimation, the very same concept that we encountered when providing a
probabilistic justification for the mean squared error objective in linear regression (Section 3.1.3).
Log-Likelihood
The softmax function gives us a vector ^y, which we can interpret as estimated conditional probabilities
of each class given any input x, e.g., ^y1 = P(y = cat j x). Suppose that the entire dataset
fX; Yg has n examples, where the example indexed by i consists of a feature vector x(i) and a onehot
label vector y(i). We can compare the estimates with reality by checking how probable the
actual classes are according to our model, given the features:
P(Y j X) =
?n
i=1
P(y(i) j x(i)): (3.4.6)
According to maximum likelihood estimation, we maximize P(Y j X), which is equivalent to minimizing
the negative log-likelihood:
??log P(Y j X) =
?n
i=1
??log P(y(i) j x(i)) =
?n
i=1
l(y(i); ^y(i)); (3.4.7)
where for any pair of label y and model prediction ^y over q classes, the loss function l is
l(y; ^y) = ??
?q
j=1
yj log ^yj : (3.4.8)
For reasons explained later on, the loss function in (3.4.8) is commonly called the cross-entropy loss.
Since y is a one-hot vector of length q, the sum over all its coordinates j vanishes for all but one
term. Since all ^yj are predicted probabilities, their logarithm is never larger than 0. Consequently,
the loss function cannot be minimized any further if we correctly predict the actual label with
112 Chapter 3. Linear Neural Networks
certainty, i.e., if the predicted probability P(y j x) = 1 for the actual label y. Note that this is
often impossible. For example, there might be label noise in the dataset (some examples may be
mislabeled). It may also not be possible when the input features are not sufficiently informative
to classify every example perfectly.
Softmax and Derivatives
Since the softmax and the corresponding loss are so common, it is worth understanding a bit
better how it is computed. Plugging (3.4.3) into the definition of the loss in (3.4.8) and using the
definition of the softmax we obtain:
l(y; ^y) = ??
?q
j=1
yj log
? exp(oj) q
k=1 exp(ok)
=
?q
j=1
yj log
?q
k=1
exp(ok) ??
?q
j=1
yjoj
= log
?q
k=1
exp(ok) ??
?q
j=1
yjoj :
(3.4.9)
To understand a bit better what is going on, consider the derivative with respect to any logit oj .
We get
@oj l(y; ^y) =
? exp(oj) q
k=1 exp(ok)
?? yj = softmax(o)j ?? yj : (3.4.10)
In other words, the derivative is the difference between the probability assigned by our model,
as expressed by the softmax operation, and what actually happened, as expressed by elements in
the one-hot label vector. In this sense, it is very similar to what we saw in regression, where the
gradient was the difference between the observation y and estimate ^y. This is not coincidence.
In any exponential family (see the online appendix on distributions56) model, the gradients of
the log-likelihood are given by precisely this term. This fact makes computing gradients easy in
practice.
Cross-Entropy Loss
Now consider the case where we observe not just a single outcome but an entire distribution over
outcomes. We can use the same representation as before for the label y. The only difference
is that rather than a vector containing only binary entries, say (0; 0; 1), we now have a generic
probability vector, say (0:1; 0:2; 0:7). The math that we used previously to define the loss l in
:eqref:eq_l_cross_entropy still works out fine, just that the interpretation is slightly more general.
It is the expected value of the loss for a distribution over labels. This loss is called the cross-entropy
loss and it is one of the most commonly used losses for classification problems. We can demystify
the name by introducing just the basics of information theory. If you wish to understand more
details of information theory, you may further refer to the online appendix on information theory57.
56 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html
57 https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html
3.4. Softmax Regression 113
3.4.6 Information Theory Basics
Information theory deals with the problem of encoding, decoding, transmitting, and manipulating
information (also known as data) in as concise form as possible.
Entropy
The central idea in information theory is to quantify the information content in data. This quantity
places a hard limit on our ability to compress the data. In information theory, this quantity is
called the entropy of a distribution p, and it is captured by the following equation:
H[p] =
?
j
??p(j) log p(j): (3.4.11)
One of the fundamental theorems of information theory states that in order to encode data drawn
randomly from the distribution p, we need at least H[p] �nats� to encode it. If you wonder what a
�nat� is, it is the equivalent of bit but when using a code with base e rather than one with base 2.
Thus, one nat is 1
log(2)
 1:44 bit.
Surprisal
You might be wondering what compression has to do with prediction. Imagine that we have a
stream of data that we want to compress. If it is always easy for us to predict the next token, then
this data is easy to compress! Take the extreme example where every token in the stream always
takes the same value. That is a very boring data stream! And not only it is boring, but it is also
easy to predict. Because they are always the same, we do not have to transmit any information to
communicate the contents of the stream. Easy to predict, easy to compress.
However if we cannot perfectly predict every event, then we might sometimes be surprised. Our
surprise is greater when we assigned an event lower probability. Claude Shannon settled on
log 1
P(j) = ??log P(j) to quantify one?s surprisal at observing an event j having assigned it a (subjective)
probability P(j). The entropy defined in (3.4.11) is then the expected surprisal when one
assigned the correct probabilities that truly match the data-generating process.
Cross-Entropy Revisited
So if entropy is level of surprise experienced by someone who knows the true probability, then
you might be wondering, what is cross-entropy? The cross-entropy from p to q, denoted H(p; q),
is the expected surprisal of an observer with subjective probabilities q upon seeing data that were
actually generated according to probabilities p. The lowest possible cross-entropy is achieved
when p = q. In this case, the cross-entropy from p to q is H(p; p) = H(p).
In short, we can think of the cross-entropy classification objective in two ways: (i) as maximizing
the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus the number of
bits) required to communicate the labels.
114 Chapter 3. Linear Neural Networks
3.4.7 Model Prediction and Evaluation
After training the softmax regression model, given any example features, we can predict the probability
of each output class. Normally, we use the class with the highest predicted probability as
the output class. The prediction is correct if it is consistent with the actual class (label). In the next
part of the experiment, we will use accuracy to evaluate the model?s performance. This is equal to
the ratio between the number of correct predictions and the total number of predictions.
Summary
� The softmax operation takes a vector and maps it into probabilities.
� Softmax regression applies to classification problems. It uses the probability distribution of
the output class in the softmax operation.
� Cross-entropy is a good measure of the difference between two probability distributions. It
measures the number of bits needed to encode the data given our model.
Exercises
1. We can explore the connection between exponential families and the softmax in some more
depth.
1. Compute the second derivative of the cross-entropy loss l(y; ^y) for the softmax.
2. Compute the variance of the distribution given by softmax(o) and show that it matches
the second derivative computed above.
2. Assume that we have three classes which occur with equal probability, i.e., the probability
vector is ( 1
3 ; 1
3 ; 1
3 ).
1. What is the problem if we try to design a binary code for it?
2. Can you design a better code? Hint: what happens if we try to encode two independent
observations? What if we encode n observations jointly?
3. Softmax is a misnomer for the mapping introduced above (but everyone in deep learning
uses it). The real softmax is defined as RealSoftMax(a; b) = log(exp(a) + exp(b)).
1. Prove that RealSoftMax(a; b) > max(a; b).
2. Prove that this holds for ??1RealSoftMax(a; b), provided that  > 0.
3. Show that for  ! 1we have ??1RealSoftMax(a; b) ! max(a; b).
4. What does the soft-min look like?
5. Extend this to more than two numbers.
Discussions58
58 https://discuss.d2l.ai/t/46
3.4. Softmax Regression 115
3.5 The Image Classification Dataset
One of the widely used dataset for image classification is the MNIST dataset (LeCun et al., 1998).
While it had a good run as a benchmark dataset, even simple models by today?s standards achieve
classification accuracy over 95%, making it unsuitable for distinguishing between stronger models
and weaker ones. Today, MNIST serves as more of sanity checks than as a benchmark. To up the
ante just a bit, we will focus our discussion in the coming sections on the qualitatively similar, but
comparatively complex Fashion-MNIST dataset (Xiao et al., 2017), which was released in 2017.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon
import sys
d2l.use_svg_display()
3.5.1 Reading the Dataset
We can download and read the Fashion-MNIST dataset into memory via the build-in functions in
the framework.
mnist_train = gluon.data.vision.FashionMNIST(train=True)
mnist_test = gluon.data.vision.FashionMNIST(train=False)
Fashion-MNIST consists of images from 10 categories, each represented by 6000 images in the
training dataset and by 1000 in the test dataset. A test dataset (or test set) is used for evaluating
model performance and not for training. Consequently the training set and the test set contain
60000 and 10000 images, respectively.
len(mnist_train), len(mnist_test)
(60000, 10000)
The height and width of each input image are both 28 pixels. Note that the dataset consists of
grayscale images, whose number of channels is 1. For brevity, throughout this book we store the
shape of any image with height h width w pixels as h  w or (h, w).
mnist_train[0][0].shape
(28, 28, 1)
The images in Fashion-MNIST are associated with the following categories: t-shirt, trousers,
pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. The following function converts
between numeric label indices and their names in text.
def get_fashion_mnist_labels(labels): #@save
"""Return text labels for the Fashion-MNIST dataset."""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
(continues on next page)
116 Chapter 3. Linear Neural Networks
(continued from previous page)
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]
We can now create a function to visualize these examples.
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save
"""Plot a list of images."""
figsize = (num_cols * scale, num_rows * scale)
_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
axes = axes.flatten()
for i, (ax, img) in enumerate(zip(axes, imgs)):
ax.imshow(d2l.numpy(img))
ax.axes.get_xaxis().set_visible(False)
ax.axes.get_yaxis().set_visible(False)
if titles:
ax.set_title(titles[i])
return axes
Here are the images and their corresponding labels (in text) for the first few examples in the training
dataset.
X, y = mnist_train[:18]
show_images(X.squeeze(axis=-1), 2, 9, titles=get_fashion_mnist_labels(y));
3.5.2 Reading a Minibatch
To make our life easier when reading from the training and test sets, we use the built-in data
iterator rather than creating one from scratch. Recall that at each iteration, a data loader reads a
minibatch of data with size batch_size each time. We also randomly shuffle the examples for the
training data iterator.
batch_size = 256
def get_dataloader_workers(): #@save
"""Use 4 processes to read the data except for Windows."""
return 0 if sys.platform.startswith('win') else 4
# `ToTensor` converts the image data from uint8 to 32-bit floating point. It
# divides all numbers by 255 so that all pixel values are between 0 and 1
transformer = gluon.data.vision.transforms.ToTensor()
(continues on next page)
3.5. The Image Classification Dataset 117
(continued from previous page)
train_iter = gluon.data.DataLoader(mnist_train.transform_first(transformer),
batch_size, shuffle=True,
num_workers=get_dataloader_workers())
Let us look at the time it takes to read the training data.
timer = d2l.Timer()
for X, y in train_iter:
continue
f'{timer.stop():.2f} sec'
'1.53 sec'
3.5.3 Putting All Things Together
Now we define the load_data_fashion_mnist function that obtains and reads the Fashion-MNIST
dataset. It returns the data iterators for both the training set and validation set. In addition, it
accepts an optional argument to resize images to another shape.
def load_data_fashion_mnist(batch_size, resize=None): #@save
"""Download the Fashion-MNIST dataset and then load it into memory."""
dataset = gluon.data.vision
trans = [dataset.transforms.ToTensor()]
if resize:
trans.insert(0, dataset.transforms.Resize(resize))
trans = dataset.transforms.Compose(trans)
mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)
mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)
return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,
num_workers=get_dataloader_workers()),
gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,
num_workers=get_dataloader_workers()))
Below we test the image resizing feature of the load_data_fashion_mnist function by specifying
the resize argument.
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
print(X.shape, X.dtype, y.shape, y.dtype)
break
(32, 1, 64, 64) <class 'numpy.float32'> (32,) <class 'numpy.int32'>
We are now ready to work with the Fashion-MNIST dataset in the sections that follow.
118 Chapter 3. Linear Neural Networks
Summary
� Fashion-MNIST is an apparel classification dataset consisting of images representing 10 categories.
We will use this dataset in subsequent sections and chapters to evaluate various
classification algorithms.
� We store the shape of any image with height h width w pixels as h  w or (h, w).
� Data iterators are a key component for efficient performance. Rely on well-implemented
data iterators that exploit high-performance computing to avoid slowing down your training
loop.
Exercises
1. Does reducing the batch_size (for instance, to 1) affect the reading performance?
2. The data iterator performance is important. Do you think the current implementation is fast
enough? Explore various options to improve it.
3. Check out the framework?s online API documentation. Which other datasets are available?
Discussions59
3.6 Implementation of Softmax Regression from Scratch
Just as we implemented linear regression from scratch, we believe that softmax regression is similarly
fundamental and you ought to know the gory details of how to implement it yourself. We
will work with the Fashion-MNIST dataset, just introduced in Section 3.5, setting up a data iterator
with batch size 256.
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon
from IPython import display
npx.set_np()
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
3.6.1 Initializing Model Parameters
As in our linear regression example, each example here will be represented by a fixed-length vector.
Each example in the raw dataset is a 2828 image. In this section, we will flatten each image,
treating them as vectors of length 784. In the future, we will talk about more sophisticated strategies
for exploiting the spatial structure in images, but for now we treat each pixel location as just
another feature.
Recall that in softmax regression, we have as many outputs as there are classes. Because our
dataset has 10 classes, our network will have an output dimension of 10. Consequently, our weights
will constitute a 784  10 matrix and the biases will constitute a 1  10 row vector. As with linear
59 https://discuss.d2l.ai/t/48
3.6. Implementation of Softmax Regression from Scratch 119
regression, we will initialize our weights W with Gaussian noise and our biases to take the initial
value 0.
num_inputs = 784
num_outputs = 10
W = np.random.normal(0, 0.01, (num_inputs, num_outputs))
b = np.zeros(num_outputs)
W.attach_grad()
b.attach_grad()
3.6.2 Defining the Softmax Operation
Before implementing the softmax regression model, let us briefly review how the sum operator
work along specific dimensions in a tensor, as discussed in Section 2.3.6 and Section 2.3.6. Given
a matrix X we can sum over all elements (by default) or only over elements in the same axis, i.e.,
the same column (axis 0) or the same row (axis 1). Note that if X is an tensor with shape (2, 3)
and we sum over the columns, the result will be a vector with shape (3,). When invoking the sum
operator, we can specify to keep the number of axes in the original tensor, rather than collapsing
out the dimension that we summed over. This will result in a two-dimensional tensor with shape
(1, 3).
X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
X.sum(0, keepdims=True), X.sum(1, keepdims=True)
(array([[5., 7., 9.]]),
array([[ 6.],
[15.]]))
We are now ready to implement the softmax operation. Recall that softmax consists of three steps:
i) we exponentiate each term (using exp); ii) we sum over each row (we have one row per example
in the batch) to get the normalization constant for each example; iii) we divide each row by its
normalization constant, ensuring that the result sums to 1. Before looking at the code, let us
recall how this looks expressed as an equation:
softmax(X)ij =
?exp(Xij)
k exp(Xik)
: (3.6.1)
The denominator, or normalization constant, is also sometimes called the partition function (and
its logarithm is called the log-partition function). The origins of that name are in statistical
physics60 where a related equation models the distribution over an ensemble of particles.
def softmax(X):
X_exp = np.exp(X)
partition = X_exp.sum(1, keepdims=True)
return X_exp / partition # The broadcasting mechanism is applied here
As you can see, for any random input, we turn each element into a non-negative number. Moreover,
each row sums up to 1, as is required for a probability.
60 https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)
120 Chapter 3. Linear Neural Networks
X = np.random.normal(0, 1, (2, 5))
X_prob = softmax(X)
X_prob, X_prob.sum(1)
(array([[0.22376052, 0.06659239, 0.06583703, 0.29964197, 0.3441681 ],
[0.63209665, 0.03179282, 0.194987 , 0.09209415, 0.04902935]]),
array([1. , 0.99999994]))
Note that while this looks correct mathematically, we were a bit sloppy in our implementation
because we failed to take precautions against numerical overflow or underflow due to large or
very small elements of the matrix.
3.6.3 Defining the Model
Now that we have defined the softmax operation, we can implement the softmax regression model.
The below code defines how the input is mapped to the output through the network. Note that we
flatten each original image in the batch into a vector using the reshape function before passing
the data through our model.
def net(X):
return softmax(np.dot(X.reshape((-1, W.shape[0])), W) + b)
3.6.4 Defining the Loss Function
Next, we need to implement the cross-entropy loss function, as introduced in Section 3.4. This
may be the most common loss function in all of deep learning because, at the moment, classification
problems far outnumber regression problems.
Recall that cross-entropy takes the negative log-likelihood of the predicted probability assigned
to the true label. Rather than iterating over the predictions with a Python for-loop (which tends
to be inefficient), we can pick all elements by a single operator. Below, we create a toy data y_hat
with 2 examples of predicted probabilities over 3 classes. Then we pick the probability of the first
class in the first example and the probability of the third class in the second example.
y = np.array([0, 2])
y_hat = np.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y]
array([0.1, 0.5])
Now we can implement the cross-entropy loss function efficiently with just one line of code.
def cross_entropy(y_hat, y):
return - np.log(y_hat[range(len(y_hat)), y])
cross_entropy(y_hat, y)
3.6. Implementation of Softmax Regression from Scratch 121
array([2.3025851, 0.6931472])
3.6.5 Classification Accuracy
Given the predicted probability distribution y_hat, we typically choose the class with the highest
predicted probability whenever we must output a hard prediction. Indeed, many applications
require that we make a choice. Gmail must categorize an email into �Primary�, �Social�, �Updates�,
or �Forums�. It might estimate probabilities internally, but at the end of the day it has to choose
one among the classes.
When predictions are consistent with the label class y, they are correct. The classification accuracy
is the fraction of all predictions that are correct. Although it can be difficult to optimize
accuracy directly (it is not differentiable), it is often the performance measure that we care most
about, and we will nearly always report it when training classifiers.
To compute accuracy we do the following. First, if y_hat is a matrix, we assume that the second
dimension stores prediction scores for each class. We use argmax to obtain the predicted class by
the index for the largest entry in each row. Then we compare the predicted class with the groundtruth
y elementwise. Since the equality operator == is sensitive to data types, we convert y_hat?s
data type to match that of y. The result is a tensor containing entries of 0 (false) and 1 (true).
Taking the sum yields the number of correct predictions.
def accuracy(y_hat, y): #@save
"""Compute the number of correct predictions."""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = y_hat.argmax(axis=1)
cmp = y_hat.astype(y.dtype) == y
return float(d2l.reduce_sum(cmp.astype(y.dtype)))
We will continue to use the variables y_hat and y defined before as the predicted probability distributions
and labels, respectively. We can see that the first example?s prediction class is 2 (the
largest element of the row is 0.6 with the index 2), which is inconsistent with the actual label, 0.
The second example?s prediction class is 2 (the largest element of the row is 0.5 with the index of
2), which is consistent with the actual label, 2. Therefore, the classification accuracy rate for these
two examples is 0.5.
accuracy(y_hat, y) / len(y)
0.5
Similarly, we can evaluate the accuracy for any model net on a dataset that is accessed via the data
iterator data_iter.
def evaluate_accuracy(net, data_iter): #@save
"""Compute the accuracy for a model on a dataset."""
metric = Accumulator(2) # No. of correct predictions, no. of predictions
for _, (X, y) in enumerate(data_iter):
metric.add(accuracy(net(X), y), y.size)
return metric[0] / metric[1]
122 Chapter 3. Linear Neural Networks
Here Accumulator is a utility class to accumulate sums over multiple variables. In the above evaluate_
accuracy function, we create 2 variables in the Accumulator instance for storing both the
number of correct predictions and the number of predictions, respectively. Both will be accumulated
over time as we iterate over the dataset.
class Accumulator: #@save
"""For accumulating sums over `n` variables."""
def __init__(self, n):
self.data = [0.0] * n
def add(self, *args):
self.data = [a + float(b) for a, b in zip(self.data, args)]
def reset(self):
self.data = [0.0] * len(self.data)
def __getitem__(self, idx):
return self.data[idx]
Because we initialized the net model with random weights, the accuracy of this model should be
close to random guessing, i.e., 0.1 for 10 classes.
evaluate_accuracy(net, test_iter)
0.0811
3.6.6 Training
The training loop for softmax regression should look strikingly familiar if you read through our
implementation of linear regression in Section 3.2. Here we refactor the implementation to make
it reusable. First, we define a function to train for one epoch. Note that updater is a general
function to update the model parameters, which accepts the batch size as an argument. It can be
either a wrapper of the d2l.sgd function or a framework?s built-in optimization function.
def train_epoch_ch3(net, train_iter, loss, updater): #@save
"""Train a model within one epoch (defined in Chapter 3)."""
# Sum of training loss, sum of training accuracy, no. of examples
metric = Accumulator(3)
if isinstance(updater, gluon.Trainer):
updater = updater.step
for X, y in train_iter:
# Compute gradients and update parameters
with autograd.record():
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
updater(X.shape[0])
metric.add(float(l.sum()), accuracy(y_hat, y), y.size)
# Return training loss and training accuracy
return metric[0] / metric[2], metric[1] / metric[2]
Before showing the implementation of the training function, we define a utility class that plot data
in animation. Again, it aims to simplify code in the rest of the book.
3.6. Implementation of Softmax Regression from Scratch 123
class Animator: #@save
"""For plotting data in animation."""
def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
figsize=(3.5, 2.5)):
# Incrementally plot multiple lines
if legend is None:
legend = []
d2l.use_svg_display()
self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
if nrows * ncols == 1:
self.axes = [self.axes, ]
# Use a lambda function to capture arguments
self.config_axes = lambda: d2l.set_axes(
self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
self.X, self.Y, self.fmts = None, None, fmts
def add(self, x, y):
# Add multiple data points into the figure
if not hasattr(y, "__len__"):
y = [y]
n = len(y)
if not hasattr(x, "__len__"):
x = [x] * n
if not self.X:
self.X = [[] for _ in range(n)]
if not self.Y:
self.Y = [[] for _ in range(n)]
for i, (a, b) in enumerate(zip(x, y)):
if a is not None and b is not None:
self.X[i].append(a)
self.Y[i].append(b)
self.axes[0].cla()
for x, y, fmt in zip(self.X, self.Y, self.fmts):
self.axes[0].plot(x, y, fmt)
self.config_axes()
display.display(self.fig)
display.clear_output(wait=True)
The following training function then trains a model net on a training dataset accessed via
train_iter for multiple epochs, which is specified by num_epochs. At the end of each epoch, the
model is evaluated on a testing dataset accessed via test_iter. We will leverage the Animator class
to visualize the training progress.
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save
"""Train a model (defined in Chapter 3)."""
animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
test_acc = evaluate_accuracy(net, test_iter)
animator.add(epoch + 1, train_metrics + (test_acc,))
train_loss, train_acc = train_metrics
assert train_loss < 0.5, train_loss
(continues on next page)
124 Chapter 3. Linear Neural Networks
(continued from previous page)
assert train_acc <= 1 and train_acc > 0.7, train_acc
assert test_acc <= 1 and test_acc > 0.7, test_acc
As an implementation from scratch, we use the minibatch stochastic gradient descent defined in
Section 3.2 to optimize the loss function of the model with a learning rate 0.1.
lr = 0.1
def updater(batch_size):
return d2l.sgd([W, b], lr, batch_size)
Now we train the model with 10 epochs. Note that both the number of epochs (num_epochs), and
learning rate (lr) are adjustable hyperparameters. By changing their values, we may be able to
increase the classification accuracy of the model.
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
3.6.7 Prediction
Now that training is complete, our model is ready to classify some images. Given a series of images,
we will compare their actual labels (first line of text output) and the predictions from the
model (second line of text output).
def predict_ch3(net, test_iter, n=6): #@save
"""Predict labels (defined in Chapter 3)."""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])
predict_ch3(net, test_iter)
3.6. Implementation of Softmax Regression from Scratch 125
Summary
� With softmax regression, we can train models for multiclass classification.
� The training loop of softmax regression is very similar to that in linear regression: retrieve
and read data, define models and loss functions, then train models using optimization algorithms.
As you will soon find out, most common deep learning models have similar training
procedures.
Exercises
1. In this section, we directly implemented the softmax function based on the mathematical
definition of the softmax operation. What problems might this cause? Hint: try to calculate
the size of exp(50).
2. The function cross_entropy in this section was implemented according to the definition of
the cross-entropy loss function. What could be the problem with this implementation? Hint:
consider the domain of the logarithm.
3. What solutions you can think of to fix the two problems above?
4. Is it always a good idea to return the most likely label? For example, would you do this for
medical diagnosis?
5. Assume that we want to use softmax regression to predict the next word based on some
features. What are some problems that might arise from a large vocabulary?
Discussions61
3.7 Concise Implementation of Softmax Regression
Just as high-level APIs of deep learning frameworks made it much easier to implement linear regression
in Section 3.3, we will find it similarly (or possibly more) convenient for implementing
classification models. Let us stick with the Fashion-MNIST dataset and keep the batch size at 256
as in Section 3.6.
from d2l import mxnet as d2l
from mxnet import gluon, init, npx
from mxnet.gluon import nn
npx.set_np()
61 https://discuss.d2l.ai/t/50
126 Chapter 3. Linear Neural Networks
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
3.7.1 Initializing Model Parameters
As mentioned in Section 3.4, the output layer of softmax regression is a fully-connected layer.
Therefore, to implement our model, we just need to add one fully-connected layer with 10 outputs
to our Sequential. Again, here, the Sequential is not really necessary, but we might as well form
the habit since it will be ubiquitous when implementing deep models. Again, we initialize the
weights at random with zero mean and standard deviation 0.01.
net = nn.Sequential()
net.add(nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))
3.7.2 Softmax Implementation Revisited
In the previous example of Section 3.6, we calculated our model?s output and then ran this output
through the cross-entropy loss. Mathematically, that is a perfectly reasonable thing to do. However,
from a computational perspective, exponentiation can be a source of numerical stability
issues.
Recall that the softmax function calculates ^yj =
?exp(oj )
k exp(ok) , where ^yj is the jth element of the
predicted probability distribution ^y and oj is the jth element of the logits o. If some of the ok
are very large (i.e., very positive), then exp(ok) might be larger than the largest number we can
have for certain data types (i.e., overflow). This would make the denominator (and/or numerator)
inf (infinity) and we wind up encountering either 0, inf, or nan (not a number) for ^yj . In these
situations we do not get a well-defined return value for cross entropy.
One trick to get around this is to first subtract max(ok) from all ok before proceeding with the
softmax calculation. You can verify that this shifting of each ok by constant factor does not change
the return value of softmax. After the subtraction and normalization step, it might be possible that
some oj have large negative values and thus that the corresponding exp(oj) will take values close
to zero. These might be rounded to zero due to finite precision (i.e., underflow), making ^yj zero and
giving us -inf for log(^yj). A few steps down the road in backpropagation, we might find ourselves
faced with a screenful of the dreaded nan results.
Fortunately, we are saved by the fact that even though we are computing exponential functions,
we ultimately intend to take their log (when calculating the cross-entropy loss). By combining
these two operators softmax and cross entropy together, we can escape the numerical stability
issues that might otherwise plague us during backpropagation. As shown in the equation below,
we avoid calculating exp(oj) and can use instead oj directly due to the canceling in log(exp()).
log (^yj) = log
(
?exp(oj)
k exp(ok)
)
= log (exp(oj)) ?? log
(
?
k
exp(ok)
)
= oj ?? log
(
?
k
exp(ok)
)
:
(3.7.1)
3.7. Concise Implementation of Softmax Regression 127
We will want to keep the conventional softmax function handy in case we ever want to evaluate
the output probabilities by our model. But instead of passing softmax probabilities into our new
loss function, we will just pass the logits and compute the softmax and its log all at once inside the
cross entropy loss function, which does smart things like the �LogSumExp trick�62.
loss = gluon.loss.SoftmaxCrossEntropyLoss()
3.7.3 Optimization Algorithm
Here, we use minibatch stochastic gradient descent with a learning rate of 0.1 as the optimization
algorithm. Note that this is the same as we applied in the linear regression example and it
illustrates the general applicability of the optimizers.
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})
3.7.4 Training
Next we call the training function defined in Section 3.6 to train the model.
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
As before, this algorithm converges to a solution that achieves a decent accuracy, albeit this time
with fewer lines of code than before.
62 https://en.wikipedia.org/wiki/LogSumExp
128 Chapter 3. Linear Neural Networks
Summary
� Using high-level APIs, we can implement softmax regression much more concisely.
� From a computational perspective, implementing softmax regression has intricacies. Note
that in many cases, a deep learning framework takes additional precautions beyond these
most well-known tricks to ensure numerical stability, saving us from even more pitfalls that
we would encounter if we tried to code all of our models from scratch in practice.
Exercises
1. Try adjusting the hyperparameters, such as the batch size, number of epochs, and learning
rate, to see what the results are.
2. Increase the numper of epochs for training. Why might the test accuracy decrease after a
while? How could we fix this?
Discussions63
63 https://discuss.d2l.ai/t/52
3.7. Concise Implementation of Softmax Regression 129
130 Chapter 3. Linear Neural Networks
4 | Multilayer Perceptrons
In this chapter, we will introduce your first truly deep network. The simplest deep networks are
called multilayer perceptrons, and they consist of multiple layers of neurons each fully connected
to those in the layer below (from which they receive input) and those above (which they, in turn,
influence). When we train high-capacity models we run the risk of overfitting. Thus, we will
need to provide your first rigorous introduction to the notions of overfitting, underfitting, and
model selection. To help you combat these problems, we will introduce regularization techniques
such as weight decay and dropout. We will also discuss issues relating to numerical stability and
parameter initialization that are key to successfully training deep networks. Throughout, we aim
to give you a firm grasp not just of the concepts but also of the practice of using deep networks.
At the end of this chapter, we apply what we have introduced so far to a real case: house price
prediction. We punt matters relating to the computational performance, scalability, and efficiency
of our models to subsequent chapters.
4.1 Multilayer Perceptrons
In Chapter 3, we introduced softmax regression (Section 3.4), implementing the algorithm from
scratch (Section 3.6) and using high-level APIs (Section 3.7), and training classifiers to recognize 10
categories of clothing from low-resolution images. Along the way, we learned how to wrangle data,
coerce our outputs into a valid probability distribution, apply an appropriate loss function, and
minimize it with respect to our model?s parameters. Now that we have mastered these mechanics
in the context of simple linear models, we can launch our exploration of deep neural networks,
the comparatively rich class of models with which this book is primarily concerned.
4.1.1 Hidden Layers
We have described the affine transformation in Section 3.1.1, which is a linear transformation
added by a bias. To begin, recall the model architecture corresponding to our softmax regression
example, illustrated in Fig. 3.4.1. This model mapped our inputs directly to our outputs via a
single affine transformation, followed by a softmax operation. If our labels truly were related to
our input data by an affine transformation, then this approach would be sufficient. But linearity
in affine transformations is a strong assumption.
131
Linear Models May Go Wrong
For example, linearity implies the weaker assumption of monotonicity: that any increase in our
feature must either always cause an increase in our model?s output (if the corresponding weight
is positive), or always cause a decrease in our model?s output (if the corresponding weight is negative).
Sometimes that makes sense. For example, if we were trying to predict whether an individual
will repay a loan, we might reasonably imagine that holding all else equal, an applicant
with a higher income would always be more likely to repay than one with a lower income. While
monotonic, this relationship likely is not linearly associated with the probability of repayment.
An increase in income from 0 to 50 thousand likely corresponds to a bigger increase in likelihood
of repayment than an increase from 1 million to 1.05 million. One way to handle this might be
to preprocess our data such that linearity becomes more plausible, say, by using the logarithm of
income as our feature.
Note that we can easily come up with examples that violate monotonicity. Say for example that we
want to predict probability of death based on body temperature. For individuals with a body temperature
above 37�C (98.6�F), higher temperatures indicate greater risk. However, for individuals
with body temperatures below 37� C, higher temperatures indicate lower risk! In this case too, we
might resolve the problem with some clever preprocessing. Namely, we might use the distance
from 37�C as our feature.
But what about classifying images of cats and dogs? Should increasing the intensity of the pixel
at location (13, 17) always increase (or always decrease) the likelihood that the image depicts a
dog? Reliance on a linear model corresponds to the implicit assumption that the only requirement
for differentiating cats vs. dogs is to assess the brightness of individual pixels. This approach is
doomed to fail in a world where inverting an image preserves the category.
And yet despite the apparent absurdity of linearity here, as compared with our previous examples,
it is less obvious that we could address the problem with a simple preprocessing fix. That
is because the significance of any pixel depends in complex ways on its context (the values of the
surrounding pixels). While there might exist a representation of our data that would take into
account the relevant interactions among our features, on top of which a linear model would be
suitable, we simply do not know how to calculate it by hand. With deep neural networks, we used
observational data to jointly learn both a representation via hidden layers and a linear predictor
that acts upon that representation.
Incorporating Hidden Layers
We can overcome these limitations of linear models and handle a more general class of functions
by incorporating one or more hidden layers. The easiest way to do this is to stack many fullyconnected
layers on top of each other. Each layer feeds into the layer above it, until we generate
outputs. We can think of the first L??1 layers as our representation and the final layer as our linear
predictor. This architecture is commonly called a multilayer perceptron, often abbreviated as MLP.
Below, we depict an MLP diagrammatically (Fig. 4.1.1).
132 Chapter 4. Multilayer Perceptrons
Fig. 4.1.1: An MLP with a hidden layer of 5 hidden units.
This MLP has 4 inputs, 3 outputs, and its hidden layer contains 5 hidden units. Since the input layer
does not involve any calculations, producing outputs with this network requires implementing the
computations for both the hidden and output layers; thus, the number of layers in this MLP is 2.
Note that these layers are both fully connected. Every input influences every neuron in the hidden
layer, and each of these in turn influences every neuron in the output layer.
From Linear to Nonlinear
As before, by the matrix X 2 Rnd, we denote a minibatch of n examples where each example has
d inputs (features). For a one-hidden-layer MLP whose hidden layer has h hidden units, denote
by H 2 Rnh the outputs of the hidden layer, which are hidden representations. In mathematics or
code, H is also known as a hidden-layer variable or a hidden variable. Since the hidden and output
layers are both fully connected, we have hidden-layer weights W(1) 2 Rdh and biases b(1) 2 R1h
and output-layer weights W(2) 2 Rhq and biases b(2) 2 R1q. Formally, we calculate the outputs
O 2 Rnq of the one-hidden-layer MLP as follows:
H = XW(1) + b(1);
O = HW(2) + b(2):
(4.1.1)
Note that after adding the hidden layer, our model now requires us to track and update additional
sets of parameters. So what have we gained in exchange? You might be surprised to find out that�
in the model defined above�we gain nothing for our troubles! The reason is plain. The hidden units
above are given by an affine function of the inputs, and the outputs (pre-softmax) are just an affine
function of the hidden units. An affine function of an affine function is itself an affine function.
Moreover, our linear model was already capable of representing any affine function.
We can view the equivalence formally by proving that for any values of the weights, we can just
collapse out the hidden layer, yielding an equivalent single-layer model with parameters W =
W(1)W(2) and b = b(1)W(2) + b(2):
O = (XW(1) + b(1))W(2) + b(2) = XW(1)W(2) + b(1)W(2) + b(2) = XW + b: (4.1.2)
In order to realize the potential of multilayer architectures, we need one more key ingredient:
a nonlinear activation function  to be applied to each hidden unit following the affine transformation.
The outputs of activation functions (e.g., ()) are called activations. In general, with
activation functions in place, it is no longer possible to collapse our MLP into a linear model:
H = (XW(1) + b(1));
O = HW(2) + b(2):
(4.1.3)
4.1. Multilayer Perceptrons 133
Since each row in X corresponds to an example in the minibatch, with some abuse of notation,
we define the nonlinearity  to apply to its inputs in a rowwise fashion, i.e., one example at a
time. Note that we used the notation for softmax in the same way to denote a rowwise operation
in Section 3.4.4. Often, as in this section, the activation functions that we apply to hidden layers
are not merely rowwise, but elementwise. That means that after computing the linear portion of
the layer, we can calculate each activation without looking at the values taken by the other hidden
units. This is true for most activation functions.
To build more general MLPs, we can continue stacking such hidden layers, e.g., H(1) = 1(XW(1)+
b(1)) and H(2) = 2(H(1)W(2) + b(2)), one atop another, yielding ever more expressive models.
Universal Approximators
MLPs can capture complex interactions among our inputs via their hidden neurons, which depend
on the values of each of the inputs. We can easily design hidden nodes to perform arbitrary computation,
for instance, basic logic operations on a pair of inputs. Moreover, for certain choices
of the activation function, it is widely known that MLPs are universal approximators. Even with
a single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set of
weights, we can model any function, though actually learning that function is the hard part. You
might think of your neural network as being a bit like the C programming language. The language,
like any other modern language, is capable of expressing any computable program. But actually
coming up with a program that meets your specifications is the hard part.
Moreover, just because a single-hidden-layer network can learn any function does not mean that
you should try to solve all of your problems with single-hidden-layer networks. In fact, we can
approximate many functions much more compactly by using deeper (vs. wider) networks. We
will touch upon more rigorous arguments in subsequent chapters.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
4.1.2 Activation Functions
Activation functions decide whether a neuron should be activated or not by calculating the
weighted sum and further adding bias with it. They are differentiable operators to transform
input signals to outputs, while most of them add non-linearity. Because activation functions are
fundamental to deep learning, let us briefly survey some common activation functions.
ReLU Function
The most popular choice, due to both simplicity of implementation and its good performance on a
variety of predictive tasks, is the rectified linear unit (ReLU). ReLU provides a very simple nonlinear
transformation. Given an element x, the function is defined as the maximum of that element and
0:
ReLU(x) = max(x; 0): (4.1.4)
134 Chapter 4. Multilayer Perceptrons
Informally, the ReLU function retains only positive elements and discards all negative elements
by setting the corresponding activations to 0. To gain some intuition, we can plot the function. As
you can see, the activation function is piecewise linear.
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
y = npx.relu(x)
d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))
When the input is negative, the derivative of the ReLU function is 0, and when the input is positive,
the derivative of the ReLU function is 1. Note that the ReLU function is not differentiable when the
input takes value precisely equal to 0. In these cases, we default to the left-hand-side derivative
and say that the derivative is 0 when the input is 0. We can get away with this because the input
may never actually be zero. There is an old adage that if subtle boundary conditions matter, we
are probably doing (real) mathematics, not engineering. That conventional wisdom may apply
here. We plot the derivative of the ReLU function plotted below.
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
The reason for using ReLU is that its derivatives are particularly well behaved: either they vanish
4.1. Multilayer Perceptrons 135
or they just let the argument through. This makes optimization better behaved and it mitigated
the well-documented problem of vanishing gradients that plagued previous versions of neural
networks (more on this later).
Note that there are many variants to the ReLU function, including the parameterized ReLU (pReLU)
function (He et al., 2015). This variation adds a linear term to ReLU, so some information still gets
through, even when the argument is negative:
pReLU(x) = max(0; x) +  min(0; x): (4.1.5)
Sigmoid Function
The sigmoid function transforms its inputs, for which values lie in the domain R, to outputs that lie
on the interval (0, 1). For that reason, the sigmoid is often called a squashing function: it squashes
any input in the range (-inf, inf) to some value in the range (0, 1):
sigmoid(x) =
1
1 + exp(??x)
: (4.1.6)
In the earliest neural networks, scientists were interested in modeling biological neurons which
either fire or do not fire. Thus the pioneers of this field, going all the way back to McCulloch and
Pitts, the inventors of the artificial neuron, focused on thresholding units. A thresholding activation
takes value 0 when its input is below some threshold and value 1 when the input exceeds the
threshold.
When attention shifted to gradient based learning, the sigmoid function was a natural choice because
it is a smooth, differentiable approximation to a thresholding unit. Sigmoids are still widely
used as activation functions on the output units, when we want to interpret the outputs as probabilities
for binary classification problems (you can think of the sigmoid as a special case of the
softmax). However, the sigmoid has mostly been replaced by the simpler and more easily trainable
ReLU for most use in hidden layers. In later chapters on recurrent neural networks, we will
describe architectures that leverage sigmoid units to control the flow of information across time.
Below, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid function
approaches a linear transformation.
with autograd.record():
y = npx.sigmoid(x)
d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))
136 Chapter 4. Multilayer Perceptrons
The derivative of the sigmoid function is given by the following equation:
d
dx
sigmoid(x) =
exp(??x)
(1 + exp(??x))2 = sigmoid(x) (1 ?? sigmoid(x)) : (4.1.7)
The derivative of the sigmoid function is plotted below. Note that when the input is 0, the derivative
of the sigmoid function reaches a maximum of 0.25. As the input diverges from 0 in either
direction, the derivative approaches 0.
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
Tanh Function
Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs, transforming
them into elements on the interval between -1 and 1:
tanh(x) =
1 ?? exp(??2x)
1 + exp(??2x)
: (4.1.8)
We plot the tanh function below. Note that as the input nears 0, the tanh function approaches a linear
transformation. Although the shape of the function is similar to that of the sigmoid function,
the tanh function exhibits point symmetry about the origin of the coordinate system.
with autograd.record():
y = np.tanh(x)
d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))
4.1. Multilayer Perceptrons 137
The derivative of the tanh function is:
d
dx
tanh(x) = 1 ?? tanh2(x): (4.1.9)
The derivative of tanh function is plotted below. As the input nears 0, the derivative of the tanh
function approaches a maximum of 1. And as we saw with the sigmoid function, as the input
moves away from 0 in either direction, the derivative of the tanh function approaches 0.
y.backward()
d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
In summary, we now know how to incorporate nonlinearities to build expressive multilayer neural
network architectures. As a side note, your knowledge already puts you in command of a similar
toolkit to a practitioner circa 1990. In some ways, you have an advantage over anyone working
in the 1990s, because you can leverage powerful open-source deep learning frameworks to build
models rapidly, using only a few lines of code. Previously, training these networks required researchers
to code up thousands of lines of C and Fortran.
138 Chapter 4. Multilayer Perceptrons
Summary
� MLP adds one or multiple fully-connected hidden layers between the output and input layers
and transforms the output of the hidden layer via an activation function.
� Commonly-used activation functions include the ReLU function, the sigmoid function, and
the tanh function.
Exercises
1. Compute the derivative of the pReLU activation function.
2. Show that an MLP using only ReLU (or pReLU) constructs a continuous piecewise linear
function.
3. Show that tanh(x) + 1 = 2 sigmoid(2x).
4. Assume that we have a nonlinearity that applies to one minibatch at a time. What kinds of
problems do you expect this to cause?
Discussions64
4.2 Implementation of Multilayer Perceptrons from Scratch
Now that we have characterized multilayer perceptrons (MLPs) mathematically, let us try to implement
one ourselves. To compare against our previous results achieved with softmax regression
(Section 3.6), we will continue to work with the Fashion-MNIST image classification dataset (Section
3.5).
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
npx.set_np()
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
4.2.1 Initializing Model Parameters
Recall that Fashion-MNIST contains 10 classes, and that each image consists of a 2828 = 784 grid
of grayscale pixel values. Again, we will disregard the spatial structure among the pixels for now,
so we can think of this as simply a classification dataset with 784 input features and 10 classes. To
begin, we will implement an MLP with one hidden layer and 256 hidden units. Note that we can
regard both of these quantities as hyperparameters. Typically, we choose layer widths in powers of
2, which tend to be computationally efficient because of how memory is allocated and addressed
in hardware.
Again, we will represent our parameters with several tensors. Note that for every layer, we must
keep track of one weight matrix and one bias vector. As always, we allocate memory for the gradients
of the loss with respect to these parameters.
64 https://discuss.d2l.ai/t/90
4.2. Implementation of Multilayer Perceptrons from Scratch 139
num_inputs, num_outputs, num_hiddens = 784, 10, 256
W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens))
b1 = np.zeros(num_hiddens)
W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs))
b2 = np.zeros(num_outputs)
params = [W1, b1, W2, b2]
for param in params:
param.attach_grad()
4.2.2 Activation Function
To make sure we know how everything works, we will implement the ReLU activation ourselves
using the maximum function rather than invoking the built-in relu function directly.
def relu(X):
return np.maximum(X, 0)
4.2.3 Model
Because we are disregarding spatial structure, we reshape each two-dimensional image into a flat
vector of length num_inputs. Finally, we implement our model with just a few lines of code.
def net(X):
X = X.reshape((-1, num_inputs))
H = relu(np.dot(X, W1) + b1)
return np.dot(H, W2) + b2
4.2.4 Loss Function
To ensure numerical stability, and because we already implemented the softmax function from
scratch (Section 3.6), we leverage the integrated function from high-level APIs for calculating the
softmax and cross-entropy loss. Recall our earlier discussion of these intricacies in Section 3.7.2.
We encourage the interested reader to examine the source code for the loss function to deepen
their knowledge of implementation details.
loss = gluon.loss.SoftmaxCrossEntropyLoss()
140 Chapter 4. Multilayer Perceptrons
4.2.5 Training
Fortunately, the training loop for MLPs is exactly the same as for softmax regression. Leveraging
the d2l package again, we call the train_ch3 function (see Section 3.6), setting the number of
epochs to 10 and the learning rate to 0.5.
num_epochs, lr = 10, 0.1
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,
lambda batch_size: d2l.sgd(params, lr, batch_size))
To evaluate the learned model, we apply it on some test data.
d2l.predict_ch3(net, test_iter)
Summary
� We saw that implementing a simple MLP is easy, even when done manually.
� However, with a large number of layers, implementing MLPs from scratch can still get messy
(e.g., naming and keeping track of our model?s parameters).
4.2. Implementation of Multilayer Perceptrons from Scratch 141
Exercises
1. Change the value of the hyperparameter num_hiddens and see how this hyperparameter influences
your results. Determine the best value of this hyperparameter, keeping all others
constant.
2. Try adding an additional hidden layer to see how it affects the results.
3. How does changing the learning rate alter your results? Fixing the model architecture and
other hyperparameters (including number of epochs), what learning rate gives you the best
results?
4. What is the best result you can get by optimizing over all the hyperparameters (learning rate,
number of epochs, number of hidden layers, number of hidden units per layer) jointly?
5. Describe why it is much more challenging to deal with multiple hyperparameters.
6. What is the smartest strategy you can think of for structuring a search over multiple hyperparameters?
Discussions65
4.3 Concise Implementation of Multilayer Perceptrons
As you might expect, by relying on the high-level APIs, we can implement MLPs even more concisely.
from d2l import mxnet as d2l
from mxnet import gluon, init, npx
from mxnet.gluon import nn
npx.set_np()
4.3.1 Model
As compared with our concise implementation of softmax regression implementation (Section
3.7), the only difference is that we add two fully-connected layers (previously, we added one). The
first is our hidden layer, which contains 256 hidden units and applies the ReLU activation function.
The second is our output layer.
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'),
nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))
The training loop is exactly the same as when we implemented softmax regression. This modularity
enables us to separate matters concerning the model architecture from orthogonal considerations.
65 https://discuss.d2l.ai/t/92
142 Chapter 4. Multilayer Perceptrons
batch_size, lr, num_epochs = 256, 0.1, 10
loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
Summary
� Using high-level APIs, we can implement MLPs much more concisely.
� For the same classification problem, the implementation of an MLP is the same as that of
softmax regression except for additional hidden layers with activation functions.
Exercises
1. Try adding different numbers of hidden layers (you may also modify the learning rate). What
setting works best?
2. Try out different activation functions. Which one works best?
3. Try different schemes for initializing the weights. What method works best?
Discussions66
66 https://discuss.d2l.ai/t/94
4.3. Concise Implementation of Multilayer Perceptrons 143
4.4 Model Selection, Underfitting, and Overfitting
As machine learning scientists, our goal is to discover patterns. But how can we be sure that we
have truly discovered a general pattern and not simply memorized our data? For example, imagine
that we wanted to hunt for patterns among genetic markers linking patients to their dementia
status, where the labels are drawn from the set fdementia; mild cognitive impairment; healthyg.
Because each person?s genes identify them uniquely (ignoring identical siblings), it is possible to
memorize the entire dataset.
We do not want our model to say �That�s Bob! I remember him! He has dementia!� The reason why
is simple. When we deploy the model in the future, we will encounter patients that the model has
never seen before. Our predictions will only be useful if our model has truly discovered a general
pattern.
To recapitulate more formally, our goal is to discover patterns that capture regularities in the underlying
population from which our training set was drawn. If we are successful in this endeavor,
then we could successfully assess risk even for individuals that we have never encountered before.
This problem�how to discover patterns that generalize�is the fundamental problem of machine
learning.
The danger is that when we train models, we access just a small sample of data. The largest public
image datasets contain roughly one million images. More often, we must learn from only thousands
or tens of thousands of data examples. In a large hospital system, we might access hundreds
of thousands of medical records. When working with finite samples, we run the risk that we might
discover apparent associations that turn out not to hold up when we collect more data.
The phenomenon of fitting our training data more closely than we fit the underlying distribution
is called overfitting, and the techniques used to combat overfitting are called regularization. In
the previous sections, you might have observed this effect while experimenting with the Fashion-
MNIST dataset. If you altered the model structure or the hyperparameters during the experiment,
you might have noticed that with enough neurons, layers, and training epochs, the model can
eventually reach perfect accuracy on the training set, even as the accuracy on test data deteriorates.
4.4.1 Training Error and Generalization Error
In order to discuss this phenomenon more formally, we need to differentiate between training
error and generalization error. The training error is the error of our model as calculated on the
training dataset, while generalization error is the expectation of our model?s error were we to apply
it to an infinite stream of additional data examples drawn from the same underlying data distribution
as our original sample.
Problematically, we can never calculate the generalization error exactly. That is because the
stream of infinite data is an imaginary object. In practice, we must estimate the generalization
error by applying our model to an independent test set constituted of a random selection of data
examples that were withheld from our training set.
The following three thought experiments will help illustrate this situation better. Consider a college
student trying to prepare for his final exam. A diligent student will strive to practice well and
test his abilities using exams from previous years. Nonetheless, doing well on past exams is no
guarantee that he will excel when it matters. For instance, the student might try to prepare by rote
learning the answers to the exam questions. This requires the student to memorize many things.
She might even remember the answers for past exams perfectly. Another student might prepare
144 Chapter 4. Multilayer Perceptrons
by trying to understand the reasons for giving certain answers. In most cases, the latter student
will do much better.
Likewise, consider a model that simply uses a lookup table to answer questions. If the set of allowable
inputs is discrete and reasonably small, then perhaps after viewing many training examples,
this approach would perform well. Still this model has no ability to do better than random guessing
when faced with examples that it has never seen before. In reality the input spaces are far too
large to memorize the answers corresponding to every conceivable input. For example, consider
the black and white 28  28 images. If each pixel can take one among 256 grayscale values, then
there are 256784 possible images. That means that there are far more low-resolution grayscale
thumbnail-sized images than there are atoms in the universe. Even if we could encounter such
data, we could never afford to store the lookup table.
Last, consider the problem of trying to classify the outcomes of coin tosses (class 0: heads, class
1: tails) based on some contextual features that might be available. Suppose that the coin is fair.
No matter what algorithm we come up with, the generalization error will always be 1
2 . However,
for most algorithms, we should expect our training error to be considerably lower, depending on
the luck of the draw, even if we did not have any features! Consider the dataset {0, 1, 1, 1, 0, 1}.
Our feature-less algorithm would have to fall back on always predicting the majority class, which
appears from our limited sample to be 1. In this case, the model that always predicts class 1 will
incur an error of 1
3 , considerably better than our generalization error. As we increase the amount
of data, the probability that the fraction of heads will deviate significantly from 1
2 diminishes, and
our training error would come to match the generalization error.
Statistical Learning Theory
Since generalization is the fundamental problem in machine learning, you might not be surprised
to learn that many mathematicians and theorists have dedicated their lives to developing formal
theories to describe this phenomenon. In their eponymous theorem67, Glivenko and Cantelli derived
the rate at which the training error converges to the generalization error. In a series of seminal
papers, Vapnik and Chervonenkis68 extended this theory to more general classes of functions.
This work laid the foundations of statistical learning theory.
In the standard supervised learning setting, which we have addressed up until now and will stick
with throughout most of this book, we assume that both the training data and the test data are
drawn independently from identical distributions. This is commonly called the i.i.d. assumption,
which means that the process that samples our data has no memory. In other words, the second
example drawn and the third drawn are no more correlated than the second and the two-millionth
sample drawn.
Being a good machine learning scientist requires thinking critically, and already you should be
poking holes in this assumption, coming up with common cases where the assumption fails. What
if we train a mortality risk predictor on data collected from patients at UCSF Medical Center, and
apply it on patients at Massachusetts General Hospital? These distributions are simply not identical.
Moreover, draws might be correlated in time. What if we are classifying the topics of Tweets?
The news cycle would create temporal dependencies in the topics being discussed, violating any
assumptions of independence.
Sometimes we can get away with minor violations of the i.i.d. assumption and our models will
continue to work remarkably well. After all, nearly every real-world application involves at least
67 https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem
68 https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory
4.4. Model Selection, Underfitting, and Overfitting 145
some minor violation of the i.i.d. assumption, and yet we have many useful tools for various applications
such as face recognition, speech recognition, and language translation.
Other violations are sure to cause trouble. Imagine, for example, if we try to train a face recognition
system by training it exclusively on university students and then want to deploy it as a tool
for monitoring geriatrics in a nursing home population. This is unlikely to work well since college
students tend to look considerably different from the elderly.
In subsequent chapters, we will discuss problems arising from violations of the i.i.d. assumption.
For now, even taking the i.i.d. assumption for granted, understanding generalization is a
formidable problem. Moreover, elucidating the precise theoretical foundations that might explain
why deep neural networks generalize as well as they do continues to vex the greatest minds
in learning theory.
When we train our models, we attempt to search for a function that fits the training data as well as
possible. If the function is so flexible that it can catch on to spurious patterns just as easily as to
true associations, then it might perform too well without producing a model that generalizes well
to unseen data. This is precisely what we want to avoid or at least control. Many of the techniques
in deep learning are heuristics and tricks aimed at guarding against overfitting.
Model Complexity
When we have simple models and abundant data, we expect the generalization error to resemble
the training error. When we work with more complex models and fewer examples, we expect the
training error to go down but the generalization gap to grow. What precisely constitutes model
complexity is a complex matter. Many factors govern whether a model will generalize well. For
example a model with more parameters might be considered more complex. A model whose
parameters can take a wider range of values might be more complex. Often with neural networks,
we think of a model that takes more training iterations as more complex, and one subject to early
stopping (fewer training iterations) as less complex.
It can be difficult to compare the complexity among members of substantially different model
classes (say, decision trees vs. neural networks). For now, a simple rule of thumb is quite useful:
a model that can readily explain arbitrary facts is what statisticians view as complex, whereas one
that has only a limited expressive power but still manages to explain the data well is probably
closer to the truth. In philosophy, this is closely related to Popper?s criterion of falsifiability of a
scientific theory: a theory is good if it fits data and if there are specific tests that can be used to
disprove it. This is important since all statistical estimation is post hoc, i.e., we estimate after we
observe the facts, hence vulnerable to the associated fallacy. For now, we will put the philosophy
aside and stick to more tangible issues.
In this section, to give you some intuition, we will focus on a few factors that tend to influence the
generalizability of a model class:
1. The number of tunable parameters. When the number of tunable parameters, sometimes
called the degrees of freedom, is large, models tend to be more susceptible to overfitting.
2. The values taken by the parameters. When weights can take a wider range of values, models
can be more susceptible to overfitting.
3. The number of training examples. It is trivially easy to overfit a dataset containing only
one or two examples even if your model is simple. But overfitting a dataset with millions of
examples requires an extremely flexible model.
146 Chapter 4. Multilayer Perceptrons
4.4.2 Model Selection
In machine learning, we usually select our final model after evaluating several candidate models.
This process is called model selection. Sometimes the models subject to comparison are fundamentally
different in nature (say, decision trees vs. linear models). At other times, we are comparing
members of the same class of models that have been trained with different hyperparameter settings.
With MLPs, for example, we may wish to compare models with different numbers of hidden layers,
different numbers of hidden units, and various choices of the activation functions applied to each
hidden layer. In order to determine the best among our candidate models, we will typically employ
a validation dataset.
Validation Dataset
In principle we should not touch our test set until after we have chosen all our hyperparameters.
Were we to use the test data in the model selection process, there is a risk that we might overfit
the test data. Then we would be in serious trouble. If we overfit our training data, there is always
the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever
know?
Thus, we should never rely on the test data for model selection. And yet we cannot rely solely on
the training data for model selection either because we cannot estimate the generalization error
on the very data that we use to train the model.
In practical applications, the picture gets muddier. While ideally we would only touch the test
data once, to assess the very best model or to compare a small number of models to each other,
real-world test data is seldom discarded after just one use. We can seldom afford a new test set for
each round of experiments.
The common practice to address this problem is to split our data three ways, incorporating a validation
dataset (or validation set) in addition to the training and test datasets. The result is a murky
practice where the boundaries between validation and test data are worryingly ambiguous. Unless
explicitly stated otherwise, in the experiments in this book we are really working with what
should rightly be called training data and validation data, with no true test sets. Therefore, the
accuracy reported in each experiment of the book is really the validation accuracy and not a true
test set accuracy.
K-Fold Cross-Validation
When training data is scarce, we might not even be able to afford to hold out enough data to constitute
a proper validation set. One popular solution to this problem is to employ K-fold crossvalidation.
Here, the original training data is split into K non-overlapping subsets. Then model
training and validation are executed K times, each time training on K ?? 1 subsets and validating
on a different subset (the one not used for training in that round). Finally, the training and
validation errors are estimated by averaging over the results from the K experiments.
4.4. Model Selection, Underfitting, and Overfitting 147
4.4.3 Underfitting or Overfitting?
When we compare the training and validation errors, we want to be mindful of two common situations.
First, we want to watch out for cases when our training error and validation error are both
substantial but there is a little gap between them. If the model is unable to reduce the training
error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the
pattern that we are trying to model. Moreover, since the generalization gap between our training
and validation errors is small, we have reason to believe that we could get away with a more
complex model. This phenomenon is known as underfitting.
On the other hand, as we discussed above, we want to watch out for the cases when our training
error is significantly lower than our validation error, indicating severe overfitting. Note that
overfitting is not always a bad thing. With deep learning especially, it is well known that the best
predictive models often perform far better on training data than on holdout data. Ultimately, we
usually care more about the validation error than about the gap between the training and validation
errors.
Whether we overfit or underfit can depend both on the complexity of our model and the size of
the available training datasets, two topics that we discuss below.
Model Complexity
To illustrate some classical intuition about overfitting and model complexity, we give an example
using polynomials. Given training data consisting of a single feature x and a corresponding realvalued
label y, we try to find the polynomial of degree d
^y =
?d
i=0
xiwi (4.4.1)
to estimate the labels y. This is just a linear regression problem where our features are given by
the powers of x, the model?s weights are given by wi, and the bias is given by w0 since x0 = 1 for all
x. Since this is just a linear regression problem, we can use the squared error as our loss function.
A higher-order polynomial function is more complex than a lower-order polynomial function,
since the higher-order polynomial has more parameters and the model function?s selection range
is wider. Fixing the training dataset, higher-order polynomial functions should always achieve
lower (at worst, equal) training error relative to lower degree polynomials. In fact, whenever the
data examples each have a distinct value of x, a polynomial function with degree equal to the
number of data examples can fit the training set perfectly. We visualize the relationship between
polynomial degree and underfitting vs. overfitting in Fig. 4.4.1.
148 Chapter 4. Multilayer Perceptrons
Fig. 4.4.1: Influence of model complexity on underfitting and overfitting
Dataset Size
The other big consideration to bear in mind is the dataset size. Fixing our model, the fewer samples
we have in the training dataset, the more likely (and more severely) we are to encounter overfitting.
As we increase the amount of training data, the generalization error typically decreases.
Moreover, in general, more data never hurt. For a fixed task and data distribution, there is typically
a relationship between model complexity and dataset size. Given more data, we might profitably
attempt to fit a more complex model. Absent sufficient data, simpler models may be more
difficult to beat. For many tasks, deep learning only outperforms linear models when many thousands
of training examples are available. In part, the current success of deep learning owes to
the current abundance of massive datasets due to Internet companies, cheap storage, connected
devices, and the broad digitization of the economy.
4.4.4 Polynomial Regression
We can now explore these concepts interactively by fitting polynomials to data.
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
from mxnet.gluon import nn
import math
npx.set_np()
4.4. Model Selection, Underfitting, and Overfitting 149
Generating the Dataset
First we need data. Given x, we will use the following cubic polynomial to generate the labels on
training and test data:
y = 5 + 1:2x ?? 3:4
x2
2!
+ 5:6
x3
3!
+ ? where ?  N(0; 0:12): (4.4.2)
The noise term ? obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. For
optimization, we typically want to avoid very large values of gradients or losses. This is why the
features are rescaled from xi to xi
i! . It allows us to avoid very large values for large exponents i. We
will synthesize 100 samples each for the training set and test set.
max_degree = 20 # Maximum degree of the polynomial
n_train, n_test = 100, 100 # Training and test dataset sizes
true_w = np.zeros(max_degree) # Allocate lots of empty space
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])
features = np.random.normal(size=(n_train + n_test, 1))
np.random.shuffle(features)
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))
for i in range(max_degree):
poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!
# Shape of `labels`: (`n_train` + `n_test`,)
labels = np.dot(poly_features, true_w)
labels += np.random.normal(scale=0.1, size=labels.shape)
Again, monomials stored in poly_features are rescaled by the gamma function, where ??(n) =
(n ?? 1)!. Take a look at the first 2 samples from the generated dataset. The value 1 is technically a
feature, namely the constant feature corresponding to the bias.
features[:2], poly_features[:2, :], labels[:2]
(array([[2.2122064],
[1.1630787]]),
array([[1.00000000e+00, 2.21220636e+00, 2.44692850e+00, 1.80437028e+00,
9.97909844e-01, 4.41516489e-01, 1.62787601e-01, 5.14456779e-02,
1.42260585e-02, 3.49677517e-03, 7.73558859e-04, 1.55570160e-04,
2.86794402e-05, 4.88037267e-06, 7.71170789e-07, 1.13732597e-07,
1.57249964e-08, 2.04629069e-09, 2.51489857e-10, 2.92814419e-11],
[1.00000000e+00, 1.16307867e+00, 6.76375985e-01, 2.62226164e-01,
7.62474164e-02, 1.77363474e-02, 3.43812793e-03, 5.71259065e-04,
8.30524004e-05, 1.07329415e-05, 1.24832559e-06, 1.31990987e-07,
1.27929916e-08, 1.14455811e-09, 9.50865081e-11, 7.37287228e-12,
5.35951925e-13, 3.66678970e-14, 2.36931378e-15, 1.45036750e-16]]),
array([9.629796, 5.51997 ]))
150 Chapter 4. Multilayer Perceptrons
Training and Testing the Model
Let us first implement a function to evaluate the loss on a given dataset.
def evaluate_loss(net, data_iter, loss): #@save
"""Evaluate the loss of a model on the given dataset."""
metric = d2l.Accumulator(2) # Sum of losses, no. of examples
for X, y in data_iter:
l = loss(net(X), y)
metric.add(d2l.reduce_sum(l), l.size)
return metric[0] / metric[1]
Now define the training function.
def train(train_features, test_features, train_labels, test_labels,
num_epochs=400):
loss = gluon.loss.L2Loss()
net = nn.Sequential()
# Switch off the bias since we already catered for it in the polynomial
# features
net.add(nn.Dense(1, use_bias=False))
net.initialize()
batch_size = min(10, train_labels.shape[0])
train_iter = d2l.load_array((train_features, train_labels), batch_size)
test_iter = d2l.load_array((test_features, test_labels), batch_size,
is_train=False)
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': 0.01})
animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
xlim=[1, num_epochs], ylim=[1e-3, 1e2],
legend=['train', 'test'])
for epoch in range(num_epochs):
d2l.train_epoch_ch3(net, train_iter, loss, trainer)
if epoch == 0 or (epoch + 1) % 20 == 0:
animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
evaluate_loss(net, test_iter, loss)))
print('weight:', net[0].weight.data().asnumpy())
Third-Order Polynomial Function Fitting (Normal)
We will begin by first using a third-order polynomial function, which is the same order as that
of the data generation function. The results show that this model?s training and test losses can
be both effectively reduced. The learned model parameters are also close to the true values w =
[5; 1:2;??3:4; 5:6].
# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the
# polynomial features
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
labels[:n_train], labels[n_train:])
weight: [[ 4.998482 1.2171441 -3.3890193 5.60067 ]]
4.4. Model Selection, Underfitting, and Overfitting 151
Linear Function Fitting (Underfitting)
Let us take another look at linear function fitting. After the decline in early epochs, it becomes
difficult to further decrease this model?s training loss. After the last epoch iteration has been
completed, the training loss is still high. When used to fit nonlinear patterns (like the third-order
polynomial function here) linear models are liable to underfit.
# Pick the first two dimensions, i.e., 1, x, from the polynomial features
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
labels[:n_train], labels[n_train:])
weight: [[2.6284926 4.6981072]]
152 Chapter 4. Multilayer Perceptrons
Higher-Order Polynomial Function Fitting (Overfitting)
Now let us try to train the model using a polynomial of too high degree. Here, there are insufficient
data to learn that the higher-degree coefficients should have values close to zero. As a result, our
overly-complex model is so susceptible that it is being influenced by noise in the training data.
Though the training loss can be effectively reduced, the test loss is still much higher. It shows that
the complex model overfits the data.
# Pick all the dimensions from the polynomial features
train(poly_features[:n_train, :], poly_features[n_train:, :],
labels[:n_train], labels[n_train:], num_epochs=1500)
weight: [[ 4.974571 1.3419539 -3.3172207 5.0192237 -0.15747286 1.4558134
0.17068975 0.21751356 0.06558682 -0.01592065 0.00982275 -0.05115314
-0.02413951 -0.01499531 -0.04940702 0.06389923 -0.04761839 -0.04380166
-0.05188227 0.05655775]]
In the subsequent sections, we will continue to discuss overfitting problems and methods for dealing
with them, such as weight decay and dropout.
Summary
� Since the generalization error cannot be estimated based on the training error, simply minimizing
the training error will not necessarily mean a reduction in the generalization error.
Machine learning models need to be careful to safeguard against overfitting so as to minimize
the generalization error.
� A validation set can be used for model selection, provided that it is not used too liberally.
� Underfitting means that a model is not able to reduce the training error. When training error
is much lower than validation error, there is overfitting.
� We should choose an appropriately complex model and avoid using insufficient training
samples.
4.4. Model Selection, Underfitting, and Overfitting 153
Exercises
1. Can you solve the polynomial regression problem exactly? Hint: use linear algebra.
2. Consider model selection for polynomials:
� Plot the training loss vs. model complexity (degree of the polynomial). What do you
observe? What degree of polynomial do you need to reduce the training loss to 0?
� Plot the test loss in this case.
� Generate the same plot as a function of the amount of data.
3. What happens if you drop the normalization (1/i!) of the polynomial features xi? Can you
fix this in some other way?
4. Can you ever expect to see zero generalization error?
Discussions69
4.5 Weight Decay
Now that we have characterized the problem of overfitting, we can introduce some standard techniques
for regularizing models. Recall that we can always mitigate overfitting by going out and
collecting more training data. That can be costly, time consuming, or entirely out of our control,
making it impossible in the short run. For now, we can assume that we already have as much
high-quality data as our resources permit and focus on regularization techniques.
Recall that in our polynomial regression example (Section 4.4) we could limit our model?s capacity
simply by tweaking the degree of the fitted polynomial. Indeed, limiting the number of features
is a popular technique to mitigate overfitting. However, simply tossing aside features can be too
blunt an instrument for the job. Sticking with the polynomial regression example, consider what
might happen with high-dimensional inputs. The natural extensions of polynomials to multivariate
data are called monomials, which are simply products of powers of variables. The degree of a
monomial is the sum of the powers. For example, x21x2, and x3x25
are both monomials of degree
3.
Note that the number of terms with degree d blows up rapidly as d grows larger. Given k variables,
the number of monomials of degree d (i.e., k multichoose d) is
(k??1+d
k??1
)
. Even small changes in
degree, say from 2 to 3, dramatically increase the complexity of our model. Thus we often need a
more fine-grained tool for adjusting function complexity.
4.5.1 Norms and Weight Decay
We have described both the L2 norm and the L1 norm, which are special cases of the more general
Lp norm in Section 2.3.10. Weight decay (commonly called L2 regularization), might be the most
widely-used technique for regularizing parametric machine learning models. The technique is
motivated by the basic intuition that among all functions f, the function f = 0 (assigning the
value 0 to all inputs) is in some sense the simplest, and that we can measure the complexity of a
function by its distance from zero. But how precisely should we measure the distance between
a function and zero? There is no single right answer. In fact, entire branches of mathematics,
69 https://discuss.d2l.ai/t/96
154 Chapter 4. Multilayer Perceptrons
including parts of functional analysis and the theory of Banach spaces, are devoted to answering
this issue.
One simple interpretation might be to measure the complexity of a linear function f(x) = w?x
by some norm of its weight vector, e.g., ?w?2. The most common method for ensuring a small
weight vector is to add its norm as a penalty term to the problem of minimizing the loss. Thus we
replace our original objective, minimizing the prediction loss on the training labels, with new objective,
minimizing the sum of the prediction loss and the penalty term. Now, if our weight vector grows
too large, our learning algorithm might focus on minimizing the weight norm ?w?2 vs. minimizing
the training error. That is exactly what we want. To illustrate things in code, let us revive our
previous example from Section 3.1 for linear regression. There, our loss was given by
L(w; b) =
1
n
?n
i=1
1
2
(
w?x(i) + b ?? y(i)
)2
: (4.5.1)
Recall that x(i) are the features, y(i) are labels for all data examples i, and (w; b) are the weight
and bias parameters, respectively. To penalize the size of the weight vector, we must somehow
add ?w?2 to the loss function, but how should the model trade off the standard loss for this new
additive penalty? In practice, we characterize this tradeoff via the regularization constant , a nonnegative
hyperparameter that we fit using validation data:
L(w; b) +

2
?w?2; (4.5.2)
For  = 0, we recover our original loss function. For  > 0, we restrict the size of ?w?. We divide
by 2 by convention: when we take the derivative of a quadratic function, the 2 and 1/2 cancel out,
ensuring that the expression for the update looks nice and simple. The astute reader might wonder
why we work with the squared norm and not the standard norm (i.e., the Euclidean distance). We
do this for computational convenience. By squaring the L2 norm, we remove the square root,
leaving the sum of squares of each component of the weight vector. This makes the derivative of
the penalty easy to compute: the sum of derivatives equals the derivative of the sum.
Moreover, you might ask why we work with the L2 norm in the first place and not, say, the L1
norm. In fact, other choices are valid and popular throughout statistics. While L2-regularized
linear models constitute the classic ridge regression algorithm, L1-regularized linear regression is
a similarly fundamental model in statistics, which is popularly known as lasso regression.
One reason to work with theL2 norm is that it places an outsize penalty on large components of the
weight vector. This biases our learning algorithm towards models that distribute weight evenly
across a larger number of features. In practice, this might make them more robust to measurement
error in a single variable. By contrast, L1 penalties lead to models that concentrate weights
on a small set of features by clearing the other weights to zero. This is called feature selection,
which may be desirable for other reasons.
Using the same notation in (3.1.10), the minibatch stochastic gradient descent updates for L2-
regularized regression follow:
w   (1 ?? ) w ?? 
jBj
?
i2B
x(i)
(
w?x(i) + b ?? y(i)
)
: (4.5.3)
As before, we update w based on the amount by which our estimate differs from the observation.
However, we also shrink the size of w towards zero. That is why the method is sometimes called
�weight decay�: given the penalty term alone, our optimization algorithm decays the weight at each
step of training. In contrast to feature selection, weight decay offers us a continuous mechanism
4.5. Weight Decay 155
for adjusting the complexity of a function. Smaller values of  correspond to less constrained w,
whereas larger values of  constrain w more considerably.
Whether we include a corresponding bias penalty b2 can vary across implementations, and may
vary across layers of a neural network. Often, we do not regularize the bias term of a network?s
output layer.
4.5.2 High-Dimensional Linear Regression
We can illustrate the benefits of weight decay through a simple synthetic example.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
First, we generate some data as before
y = 0:05 +
?d
i=1
0:01xi + ? where ?  N(0; 0:012): (4.5.4)
We choose our label to be a linear function of our inputs, corrupted by Gaussian noise with zero
mean and standard deviation 0.01. To make the effects of overfitting pronounced, we can increase
the dimensionality of our problem to d = 200 and work with a small training set containing only
20 examples.
n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5
true_w, true_b = np.ones((num_inputs, 1)) * 0.01, 0.05
train_data = d2l.synthetic_data(true_w, true_b, n_train)
train_iter = d2l.load_array(train_data, batch_size)
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
4.5.3 Implementation from Scratch
In the following, we will implement weight decay from scratch, simply by adding the squared L2
penalty to the original target function.
Initializing Model Parameters
First, we will define a function to randomly initialize our model parameters.
def init_params():
w = np.random.normal(scale=1, size=(num_inputs, 1))
b = np.zeros(1)
w.attach_grad()
b.attach_grad()
return [w, b]
156 Chapter 4. Multilayer Perceptrons
Defining L2 Norm Penalty
Perhaps the most convenient way to implement this penalty is to square all terms in place and
sum them up.
def l2_penalty(w):
return (w**2).sum() / 2
Defining the Training Loop
The following code fits a model on the training set and evaluates it on the test set. The linear
network and the squared loss have not changed since Chapter 3, so we will just import them via
d2l.linreg and d2l.squared_loss. The only change here is that our loss now includes the penalty
term.
def train(lambd):
w, b = init_params()
net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
num_epochs, lr = 100, 0.003
animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
xlim=[5, num_epochs], legend=['train', 'test'])
for epoch in range(num_epochs):
for X, y in train_iter:
with autograd.record():
# The L2 norm penalty term has been added, and broadcasting
# makes `l2_penalty(w)` a vector whose length is `batch_size`
l = loss(net(X), y) + lambd * l2_penalty(w)
l.backward()
d2l.sgd([w, b], lr, batch_size)
if (epoch + 1) % 5 == 0:
animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
d2l.evaluate_loss(net, test_iter, loss)))
print('L2 norm of w:', np.linalg.norm(w))
Training without Regularization
We now run this code with lambd = 0, disabling weight decay. Note that we overfit badly, decreasing
the training error but not the test error�a textook case of overfitting.
train(lambd=0)
L2 norm of w: 13.25939
4.5. Weight Decay 157
Using Weight Decay
Below, we run with substantial weight decay. Note that the training error increases but the test
error decreases. This is precisely the effect we expect from regularization.
train(lambd=3)
L2 norm of w: 0.38248405
4.5.4 Concise Implementation
Because weight decay is ubiquitous in neural network optimization, the deep learning framework
makes it especially convenient, integrating weight decay into the optimization algorithm itself for
easy use in combination with any loss function. Moreover, this integration serves a computational
benefit, allowing implementation tricks to add weight decay to the algorithm, without any additional
computational overhead. Since the weight decay portion of the update depends only on the
current value of each parameter, the optimizer must touch each parameter once anyway.
In the following code, we specify the weight decay hyperparameter directly through wd when instantiating
our Trainer. By default, Gluon decays both weights and biases simultaneously. Note
158 Chapter 4. Multilayer Perceptrons
that the hyperparameter wd will be multiplied by wd_mult when updating model parameters. Thus,
if we set wd_mult to zero, the bias parameter b will not decay.
def train_concise(wd):
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(init.Normal(sigma=1))
loss = gluon.loss.L2Loss()
num_epochs, lr = 100, 0.003
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': lr, 'wd': wd})
# The bias parameter has not decayed. Bias names generally end with "bias"
net.collect_params('.*bias').setattr('wd_mult', 0)
animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
xlim=[5, num_epochs], legend=['train', 'test'])
for epoch in range(num_epochs):
for X, y in train_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(batch_size)
if (epoch + 1) % 5 == 0:
animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
d2l.evaluate_loss(net, test_iter, loss)))
print('L2 norm of w:', np.linalg.norm(net[0].weight.data()))
The plots look identical to those when we implemented weight decay from scratch. However, they
run appreciably faster and are easier to implement, a benefit that will become more pronounced
for larger problems.
train_concise(0)
L2 norm of w: 15.014069
train_concise(3)
4.5. Weight Decay 159
L2 norm of w: 0.3399175
So far, we only touched upon one notion of what constitutes a simple linear function. Moreover,
what constitutes a simple nonlinear function can be an even more complex question. For instance,
reproducing kernel Hilbert space (RKHS)70 allows one to apply tools introduced for linear
functions in a nonlinear context. Unfortunately, RKHS-based algorithms tend to scale poorly
to large, high-dimensional data. In this book we will default to the simple heuristic of applying
weight decay on all layers of a deep network.
Summary
� Regularization is a common method for dealing with overfitting. It adds a penalty term to
the loss function on the training set to reduce the complexity of the learned model.
� One particular choice for keeping the model simple is weight decay using an L2 penalty. This
leads to weight decay in the update steps of the learning algorithm.
� The weight decay functionality is provided in optimizers from deep learning frameworks.
� Different sets of parameters can have different update behaviors within the same training
loop.
Exercises
1. Experiment with the value of  in the estimation problem in this section. Plot training and
test accuracy as a function of . What do you observe?
2. Use a validation set to find the optimal value of . Is it really the optimal value? Does this
matter?
3. What would the update equations look like if instead of ?w?2 we used
?
i
jwij as our penalty
of choice (L1 regularization)?
4. We know that ?w?2 = w?w. Can you find a similar equation for matrices (see the Frobenius
norm in Section 2.3.10)?
70 https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space
160 Chapter 4. Multilayer Perceptrons
5. Review the relationship between training error and generalization error. In addition to
weight decay, increased training, and the use of a model of suitable complexity, what other
ways can you think of to deal with overfitting?
6. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via
P(w j x) / P(x j w)P(w). How can you identify P(w) with regularization?
Discussions71
4.6 Dropout
In Section 4.5, we introduced the classical approach to regularizing statistical models by penalizing
the L2 norm of the weights. In probabilistic terms, we could justify this technique by arguing
that we have assumed a prior belief that weights take values from a Gaussian distribution
with mean zero. More intuitively, we might argue that we encouraged the model to spread out its
weights among many features rather than depending too much on a small number of potentially
spurious associations.
4.6.1 Overfitting Revisited
Faced with more features than examples, linear models tend to overfit. But given more examples
than features, we can generally count on linear models not to overfit. Unfortunately, the reliability
with which linear models generalize comes at a cost. Naively applied, linear models do not take
into account interactions among features. For every feature, a linear model must assign either a
positive or a negative weight, ignoring context.
In traditional texts, this fundamental tension between generalizability and flexibility is described
as the bias-variance tradeoff. Linear models have high bias: they can only represent a small class
of functions. However, these models have low variance: they give similar results across different
random samples of the data.
Deep neural networks inhabit the opposite end of the bias-variance spectrum. Unlike linear models,
neural networks are not confined to looking at each feature individually. They can learn interactions
among groups of features. For example, they might infer that �Nigeria� and �Western
Union� appearing together in an email indicates spam but that separately they do not.
Even when we have far more examples than features, deep neural networks are capable of overfitting.
In 2017, a group of researchers demonstrated the extreme flexibility of neural networks by
training deep nets on randomly-labeled images. Despite the absence of any true pattern linking
the inputs to the outputs, they found that the neural network optimized by stochastic gradient descent
could label every image in the training set perfectly. Consider what this means. If the labels
are assigned uniformly at random and there are 10 classes, then no classifier can do better than
10% accuracy on holdout data. The generalization gap here is a whopping 90%. If our models are
so expressive that they can overfit this badly, then when should we expect them not to overfit?
The mathematical foundations for the puzzling generalization properties of deep networks remain
open research questions, and we encourage the theoretically-oriented reader to dig deeper
into the topic. For now, we turn to the investigation of practical tools that tend to empirically
improve the generalization of deep nets.
71 https://discuss.d2l.ai/t/98
4.6. Dropout 161
4.6.2 Robustness through Perturbations
Let us think briefly about what we expect from a good predictive model. We want it to peform well
on unseen data. Classical generalization theory suggests that to close the gap between train and
test performance, we should aim for a simple model. Simplicity can come in the form of a small
number of dimensions. We explored this when discussing the monomial basis functions of linear
models in Section 4.4. Additionally, as we saw when discussing weight decay (L2 regularization)
in Section 4.5, the (inverse) norm of the parameters also represents a useful measure of simplicity.
Another useful notion of simplicity is smoothness, i.e., that the function should not be sensitive to
small changes to its inputs. For instance, when we classify images, we would expect that adding
some random noise to the pixels should be mostly harmless.
In 1995, Christopher Bishop formalized this idea when he proved that training with input noise is
equivalent to Tikhonov regularization (Bishop, 1995). This work drew a clear mathematical connection
between the requirement that a function be smooth (and thus simple), and the requirement
that it be resilient to perturbations in the input.
Then, in 2014, Srivastava et al. (Srivastava et al., 2014) developed a clever idea for how to apply
Bishop?s idea to the internal layers of a network, too. Namely, they proposed to inject noise into
each layer of the network before calculating the subsequent layer during training. They realized
that when training a deep network with many layers, injecting noise enforces smoothness just on
the input-output mapping.
Their idea, called dropout, involves injecting noise while computing each internal layer during
forward propagation, and it has become a standard technique for training neural networks. The
method is called dropout because we literally drop out some neurons during training. Throughout
training, on each iteration, standard dropout consists of zeroing out some fraction of the nodes in
each layer before calculating the subsequent layer.
To be clear, we are imposing our own narrative with the link to Bishop. The original paper on
dropout offers intuition through a surprising analogy to sexual reproduction. The authors argue
that neural network overfitting is characterized by a state in which each layer relies on a specifc
pattern of activations in the previous layer, calling this condition co-adaptation. Dropout, they
claim, breaks up co-adaptation just as sexual reproduction is argued to break up co-adapted genes.
The key challenge then is how to inject this noise. One idea is to inject the noise in an unbiased
manner so that the expected value of each layer�while fixing the others�equals to the value it
would have taken absent noise.
In Bishop?s work, he added Gaussian noise to the inputs to a linear model. At each training iteration,
he added noise sampled from a distribution with mean zero ?  N(0; 2) to the input x,
yielding a perturbed point x? = x + ?. In expectation, E[x?] = x.
In standard dropout regularization, one debiases each layer by normalizing by the fraction of
nodes that were retained (not dropped out). In other words, with dropout probability p, each intermediate
activation h is replaced by a random variable h? as follows:
h
?
=
{
0 with probability p
h
1??p otherwise
(4.6.1)
By design, the expectation remains unchanged, i.e., E[h?] = h.
162 Chapter 4. Multilayer Perceptrons
4.6.3 Dropout in Practice
Recall the MLP with a hidden layer and 5 hidden units in Fig. 4.1.1. When we apply dropout to
a hidden layer, zeroing out each hidden unit with probability p, the result can be viewed as a
network containing only a subset of the original neurons. In Fig. 4.6.1, h2 and h5 are removed.
Consequently, the calculation of the outputs no longer depends on h2 or h5 and their respective
gradient also vanishes when performing backpropagation. In this way, the calculation of the output
layer cannot be overly dependent on any one element of h1; : : : ; h5.
Fig. 4.6.1: MLP before and after dropout.
Typically, we disable dropout at test time. Given a trained model and a new example, we do not
drop out any nodes and thus do not need to normalize. However, there are some exceptions: some
researchers use dropout at test time as a heuristic for estimating the uncertainty of neural network
predictions: if the predictions agree across many different dropout masks, then we might say that
the network is more confident.
4.6.4 Implementation from Scratch
To implement the dropout function for a single layer, we must draw as many samples from a
Bernoulli (binary) random variable as our layer has dimensions, where the random variable takes
value 1 (keep) with probability 1 ?? p and 0 (drop) with probability p. One easy way to implement
this is to first draw samples from the uniform distribution U[0; 1]. Then we can keep those nodes
for which the corresponding sample is greater than p, dropping the rest.
In the following code, we implement a dropout_layer function that drops out the elements in the
tensor input X with probability dropout, rescaling the remainder as described above: dividing the
survivors by 1.0-dropout.
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
def dropout_layer(X, dropout):
assert 0 <= dropout <= 1
# In this case, all elements are dropped out
(continues on next page)
4.6. Dropout 163
(continued from previous page)
if dropout == 1:
return np.zeros_like(X)
# In this case, all elements are kept
if dropout == 0:
return X
mask = np.random.uniform(0, 1, X.shape) > dropout
return mask.astype(np.float32) * X / (1.0 - dropout)
We can test out the dropout_layer function on a few examples. In the following lines of code, we
pass our input X through the dropout operation, with probabilities 0, 0.5, and 1, respectively.
X = np.arange(16).reshape(2, 8)
print(dropout_layer(X, 0))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1))
[[ 0. 1. 2. 3. 4. 5. 6. 7.]
[ 8. 9. 10. 11. 12. 13. 14. 15.]]
[[ 0. 2. 4. 6. 8. 10. 12. 14.]
[ 0. 18. 20. 0. 0. 0. 28. 0.]]
[[0. 0. 0. 0. 0. 0. 0. 0.]
[0. 0. 0. 0. 0. 0. 0. 0.]]
Defining Model Parameters
Again, we work with the Fashion-MNIST dataset introduced in Section 3.5. We define an MLP with
two hidden layers containing 256 units each.
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens1))
b1 = np.zeros(num_hiddens1)
W2 = np.random.normal(scale=0.01, size=(num_hiddens1, num_hiddens2))
b2 = np.zeros(num_hiddens2)
W3 = np.random.normal(scale=0.01, size=(num_hiddens2, num_outputs))
b3 = np.zeros(num_outputs)
params = [W1, b1, W2, b2, W3, b3]
for param in params:
param.attach_grad()
164 Chapter 4. Multilayer Perceptrons
Defining the Model
The model below applies dropout to the output of each hidden layer (following the activation function).
We can set dropout probabilities for each layer separately. A common trend is to set a lower
dropout probability closer to the input layer. Below we set it to 0.2 and 0.5 for the first and second
hidden layers, respectively. We ensure that dropout is only active during training.
dropout1, dropout2 = 0.2, 0.5
def net(X):
X = X.reshape(-1, num_inputs)
H1 = npx.relu(np.dot(X, W1) + b1)
# Use dropout only when training the model
if autograd.is_training():
# Add a dropout layer after the first fully connected layer
H1 = dropout_layer(H1, dropout1)
H2 = npx.relu(np.dot(H1, W2) + b2)
if autograd.is_training():
# Add a dropout layer after the second fully connected layer
H2 = dropout_layer(H2, dropout2)
return np.dot(H2, W3) + b3
Training and Testing
This is similar to the training and testing of MLPs described previously.
num_epochs, lr, batch_size = 10, 0.5, 256
loss = gluon.loss.SoftmaxCrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,
lambda batch_size: d2l.sgd(params, lr, batch_size))
4.6. Dropout 165
4.6.5 Concise Implementation
With high-level APIs, all we need to do is add a Dropout layer after each fully-connected layer,
passing in the dropout probability as the only argument to its constructor. During training, the
Dropout layer will randomly drop out outputs of the previous layer (or equivalently, the inputs to
the subsequent layer) according to the specified dropout probability. When not in training mode,
the Dropout layer simply passes the data through during testing.
net = nn.Sequential()
net.add(nn.Dense(256, activation="relu"),
# Add a dropout layer after the first fully connected layer
nn.Dropout(dropout1),
nn.Dense(256, activation="relu"),
# Add a dropout layer after the second fully connected layer
nn.Dropout(dropout2),
nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))
Next, we train and test the model.
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
Summary
� Beyond controlling the number of dimensions and the size of the weight vector, dropout is
yet another tool to avoid overfitting. Often they are used jointly.
� Dropout replaces an activation h with a random variable with expected value h.
� Dropout is only used during training.
166 Chapter 4. Multilayer Perceptrons
Exercises
1. What happens if you change the dropout probabilities for the first and second layers? In
particular, what happens if you switch the ones for both layers? Design an experiment to
answer these questions, describe your results quantitatively, and summarize the qualitative
takeaways.
2. Increase the number of epochs and compare the results obtained when using dropout with
those when not using it.
3. What is the variance of the activations in each hidden layer when dropout is and is not applied?
Draw a plot to show how this quantity evolves over time for both models.
4. Why is dropout not typically used at test time?
5. Using the model in this section as an example, compare the effects of using dropout and
weight decay. What happens when dropout and weight decay are used at the same time?
Are the results additive? Are there diminished returns (or worse)? Do they cancel each other
out?
6. What happens if we apply dropout to the individual weights of the weight matrix rather than
the activations?
7. Invent another technique for injecting random noise at each layer that is different from the
standard dropout technique. Can you develop a method that outperforms dropout on the
Fashion-MNIST dataset (for a fixed architecture)?
Discussions72
4.7 Forward Propagation, Backward Propagation, and Computational
Graphs
So far, we have trained our models with minibatch stochastic gradient descent. However, when
we implemented the algorithm, we only worried about the calculations involved in forward propagation
through the model. When it came time to calculate the gradients, we just invoked the
backpropagation function provided by the deep learning framework.
The automatic calculation of gradients (automatic differentiation) profoundly simplifies the implementation
of deep learning algorithms. Before automatic differentiation, even small changes
to complicated models required recalculating complicated derivatives by hand. Surprisingly often,
academic papers had to allocate numerous pages to deriving update rules. While we must
continue to rely on automatic differentiation so we can focus on the interesting parts, you ought
to know how these gradients are calculated under the hood if you want to go beyond a shallow
understanding of deep learning.
In this section, we take a deep dive into the details of backward propagation (more commonly called
backpropagation). To convey some insight for both the techniques and their implementations, we
rely on some basic mathematics and computational graphs. To start, we focus our exposition on
a one-hidden-layer MLP with weight decay (L2 regularization).
72 https://discuss.d2l.ai/t/100
4.7. Forward Propagation, Backward Propagation, and Computational Graphs 167
4.7.1 Forward Propagation
Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables
(including outputs) for a neural network in order from the input layer to the output layer. We now
work step-by-step through the mechanics of a neural network with one hidden layer. This may
seem tedious but in the eternal words of funk virtuoso James Brown, you must �pay the cost to be
the boss�.
For the sake of simplicity, let us assume that the input example is x 2 Rd and that our hidden layer
does not include a bias term. Here the intermediate variable is:
z = W(1)x; (4.7.1)
where W(1) 2 Rhd is the weight parameter of the hidden layer. After running the intermediate
variable z 2 Rh through the activation function ? we obtain our hidden activation vector of length
h,
h = ?(z): (4.7.2)
The hidden variable h is also an intermediate variable. Assuming that the parameters of the output
layer only possess a weight of W(2) 2 Rqh, we can obtain an output layer variable with a vector
of length q:
o = W(2)h: (4.7.3)
Assuming that the loss function is l and the example label is y, we can then calculate the loss term
for a single data example,
L = l(o; y): (4.7.4)
According to the definition of L2 regularization, given the hyperparameter , the regularization
term is
s =

2
(
?W(1)?2
F + ?W(2)?2
F
)
; (4.7.5)
where the Frobenius norm of the matrix is simply the L2 norm applied after flattening the matrix
into a vector. Finally, the model?s regularized loss on a given data example is:
J = L + s: (4.7.6)
We refer to J as the objective function in the following discussion.
4.7.2 Computational Graph of Forward Propagation
Plotting computational graphs helps us visualize the dependencies of operators and variables
within the calculation. Fig. 4.7.1 contains the graph associated with the simple network described
above, where squares denote variables and circles denote operators. The lower-left corner signifies
the input and the upper-right corner is the output. Notice that the directions of the arrows
(which illustrate data flow) are primarily rightward and upward.
168 Chapter 4. Multilayer Perceptrons
Fig. 4.7.1: Computational graph of forward propagation.
4.7.3 Backpropagation
Backpropagation refers to the method of calculating the gradient of neural network parameters.
In short, the method traverses the network in reverse order, from the output to the input layer,
according to the chain rule from calculus. The algorithm stores any intermediate variables (partial
derivatives) required while calculating the gradient with respect to some parameters. Assume that
we have functions Y = f(X) and Z = g(Y), in which the input and the output X; Y; Z are tensors of
arbitrary shapes. By using the chain rule, we can compute the derivative of Z with respect to X via
@Z
@X = prod
(
@Z
@Y ;
@Y
@X
)
: (4.7.7)
Here we use the prod operator to multiply its arguments after the necessary operations, such as
transposition and swapping input positions, have been carried out. For vectors, this is straightforward:
it is simply matrix-matrix multiplication. For higher dimensional tensors, we use the
appropriate counterpart. The operator prod hides all the notation overhead.
Recall that the parameters of the simple network with one hidden layer, whose computational
graph is in Fig. 4.7.1, are W(1) and W(2). The objective of backpropagation is to calculate the gradients
@J/@W(1) and @J/@W(2). To accomplish this, we apply the chain rule and calculate, in turn,
the gradient of each intermediate variable and parameter. The order of calculations are reversed
relative to those performed in forward propagation, since we need to start with the outcome of the
computational graph and work our way towards the parameters. The first step is to calculate the
gradients of the objective function J = L+s with respect to the loss term L and the regularization
term s.
@J
@L
= 1 and @J
@s
= 1: (4.7.8)
Next, we compute the gradient of the objective function with respect to variable of the output layer
o according to the chain rule:
@J
@o = prod
(
@J
@L
;
@L
@o
)
=
@L
@o
2 Rq: (4.7.9)
Next, we calculate the gradients of the regularization term with respect to both parameters:
@s
@W(1)
= W(1) and @s
@W(2)
= W(2): (4.7.10)
Now we are able to calculate the gradient @J/@W(2) 2 Rqh of the model parameters closest to
the output layer. Using the chain rule yields:
@J
@W(2)
= prod
(
@J
@o ;
@o
@W(2)
)
+ prod
(
@J
@s
;
@s
@W(2)
)
=
@J
@o
h?
+ W(2): (4.7.11)
4.7. Forward Propagation, Backward Propagation, and Computational Graphs 169
To obtain the gradient with respect to W(1) we need to continue backpropagation along the output
layer to the hidden layer. The gradient with respect to the hidden layer?s outputs @J/@h 2 Rh is
given by
@J
@h = prod
(
@J
@o ;
@o
@h
)
= W(2)
? @J
@o : (4.7.12)
Since the activation function ? applies elementwise, calculating the gradient @J/@z 2 Rh of the
intermediate variable z requires that we use the elementwise multiplication operator, which we
denote by ?:
@J
@z = prod
(
@J
@h ;
@h
@z
)
=
@J
@h
? ?
?
(z) : (4.7.13)
Finally, we can obtain the gradient @J/@W(1) 2 Rhd of the model parameters closest to the input
layer. According to the chain rule, we get
@J
@W(1)
= prod
(
@J
@z ;
@z
@W(1)
)
+ prod
(
@J
@s
;
@s
@W(1)
)
=
@J
@z
x?
+ W(1): (4.7.14)
4.7.4 Training Neural Networks
When training neural networks, forward and backward propagation depend on each other. In
particular, for forward propagation, we traverse the computational graph in the direction of dependencies
and compute all the variables on its path. These are then used for backpropagation
where the compute order on the graph is reversed.
Take the aforementioned simple network as an example to illustrate. On one hand, computing
the regularization term (4.7.5) during forward propagation depends on the current values of
model parameters W(1) and W(2). They are given by the optimization algorithm according to backpropagation
in the latest iteration. On the other hand, the gradient calculation for the parameter
eq_backprop-J-h during backpropagation depends on the current value of the hidden variable h,
which is given by forward propagation.
Therefore when training neural networks, after model parameters are initialized, we alternate
forward propagation with backpropagation, updating model parameters using gradients given by
backpropagation. Note that backpropagation reuses the stored intermediate values from forward
propagation to avoid duplicate calculations. One of the consequences is that we need to retain
the intermediate values until backpropagation is complete. This is also one of the reasons why
training requires significantly more memory than plain prediction. Besides, the size of such intermediate
values is roughly proportional to the number of network layers and the batch size.
Thus, training deeper networks using larger batch sizes more easily leads to out of memory errors.
Summary
� Forward propagation sequentially calculates and stores intermediate variables within the
computational graph defined by the neural network. It proceeds from the input to the output
layer.
� Backpropagation sequentially calculates and stores the gradients of intermediate variables
and parameters within the neural network in the reversed order.
170 Chapter 4. Multilayer Perceptrons
� When training deep learning models, forward propagation and back propagation are interdependent.
� Training requires significantly more memory than prediction.
Exercises
1. Assume that the inputs X to some scalar function f are n  m matrices. What is the dimensionality
of the gradient of f with respect to X?
2. Add a bias to the hidden layer of the model described in this section (you do not need to
include bias in the regularization term).
1. Draw the corresponding computational graph.
2. Derive the forward and backward propagation equations.
3. Compute the memory footprint for training and prediction in the model described in this
section.
4. Assume that you want to compute second derivatives. What happens to the computational
graph? How long do you expect the calculation to take?
5. Assume that the computational graph is too large for your GPU.
1. Can you partition it over more than one GPU?
2. What are the advantages and disadvantages over training on a smaller minibatch?
Discussions73
4.8 Numerical Stability and Initialization
Thus far, every model that we have implemented required that we initialize its parameters according
to some pre-specified distribution. Until now, we took the initialization scheme for granted,
glossing over the details of how these choices are made. You might have even gotten the impression
that these choices are not especially important. To the contrary, the choice of initialization
scheme plays a significant role in neural network learning, and it can be crucial for maintaining
numerical stability. Moreover, these choices can be tied up in interesting ways with the choice of
the nonlinear activation function. Which function we choose and how we initialize parameters
can determine how quickly our optimization algorithm converges. Poor choices here can cause us
to encounter exploding or vanishing gradients while training. In this section, we delve into these
topics with greater detail and discuss some useful heuristics that you will find useful throughout
your career in deep learning.
73 https://discuss.d2l.ai/t/102
4.8. Numerical Stability and Initialization 171
4.8.1 Vanishing and Exploding Gradients
Consider a deep network with L layers, input x and output o. With each layer l defined by a transformation
fl parameterized by weights W(l), whose hidden variable is h(l) (let h(0) = x), our network
can be expressed as:
h(l) = fl(h(l??1)) and thus o = fL ? : : : ? f1(x): (4.8.1)
If all the hidden variables and the input are vectors, we can write the gradient of o with respect to
any set of parameters W(l) as follows:
@W(l) o = @h(L??1) h(L)
| {z }
M(L)def
=
 : : :  @h(l) h(l+1)
| {z }
M(l+1)def
=
@W(l) h(l)
| {z }
v(l)def
=
:
(4.8.2)
In other words, this gradient is the product of L??l matrices M(L)  : : :  M(l+1) and the gradient vector
v(l). Thus we are susceptible to the same problems of numerical underflow that often crop up
when multiplying together too many probabilities. When dealing with probabilities, a common
trick is to switch into log-space, i.e., shifting pressure from the mantissa to the exponent of the
numerical representation. Unfortunately, our problem above is more serious: initially the matrices
M(l) may have a wide variety of eigenvalues. They might be small or large, and their product
might be very large or very small.
The risks posed by unstable gradients go beyond numerical representation. Gradients of unpredictable
magnitude also threaten the stability of our optimization algorithms. We may be facing
parameter updates that are either (i) excessively large, destroying our model (the exploding
gradient problem); or (ii) excessively small (the vanishing gradient problem), rendering learning
impossible as parameters hardly move on each update.
Vanishing Gradients
One frequent culprit causing the vanishing gradient problem is the choice of the activation function
 that is appended following each layer?s linear operations. Historically, the sigmoid function
1/(1+exp(??x)) (introduced in :numref:sec_mlp) was popular because it resembles a thresholding
function. Since early artificial neural networks were inspired by biological neural networks, the
idea of neurons that fire either fully or not at all (like biological neurons) seemed appealing. Let
us take a closer look at the sigmoid to see why it can cause vanishing gradients.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
x = np.arange(-8.0, 8.0, 0.1)
x.attach_grad()
with autograd.record():
y = npx.sigmoid(x)
y.backward()
d2l.plot(x, [y, x.grad], legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
172 Chapter 4. Multilayer Perceptrons
As you can see, the sigmoid?s gradient vanishes both when its inputs are large and when they are
small. Moreover, when backpropagating through many layers, unless we are in the Goldilocks
zone, where the inputs to many of the sigmoids are close to zero, the gradients of the overall
product may vanish. When our network boasts many layers, unless we are careful, the gradient
will likely be cut off at some layer. Indeed, this problem used to plague deep network training.
Consequently, ReLUs, which are more stable (but less neurally plausible), have emerged as the
default choice for practitioners.
Exploding Gradients
The opposite problem, when gradients explode, can be similarly vexing. To illustrate this a bit
better, we draw 100 Gaussian random matrices and multiply them with some initial matrix. For
the scale that we picked (the choice of the variance 2 = 1), the matrix product explodes. When
this happens due to the initialization of a deep network, we have no chance of getting a gradient
descent optimizer to converge.
M = np.random.normal(size=(4, 4))
print('a single matrix', M)
for i in range(100):
M = np.dot(M, np.random.normal(size=(4, 4)))
print('after multiplying 100 matrices', M)
a single matrix [[ 2.2122064 1.1630787 0.7740038 0.4838046 ]
[ 1.0434405 0.29956347 1.1839255 0.15302546]
[ 1.8917114 -1.1688148 -1.2347414 1.5580711 ]
[-1.771029 -0.5459446 -0.45138445 -2.3556297 ]]
after multiplying 100 matrices [[ 3.4459714e+23 -7.8040680e+23 5.9973287e+23 4.5229990e+23]
[ 2.5275089e+23 -5.7240326e+23 4.3988473e+23 3.3174740e+23]
[ 1.3731286e+24 -3.1097155e+24 2.3897773e+24 1.8022959e+24]
[-4.4951040e+23 1.0180033e+24 -7.8232281e+23 -5.9000354e+23]]
4.8. Numerical Stability and Initialization 173
Breaking the Symmetry
Another problem in neural network design is the symmetry inherent in their parametrization.
Assume that we have a simple MLP with one hidden layer and two units. In this case, we could
permute the weights W(1) of the first layer and likewise permute the weights of the output layer
to obtain the same function. There is nothing special differentiating the first hidden unit vs. the
second hidden unit. In other words, we have permutation symmetry among the hidden units of
each layer.
This is more than just a theoretical nuisance. Consider the aforementioned one-hidden-layer MLP
with two hidden units. For illustration, suppose that the output layer transforms the two hidden
units into only one output unit. Imagine what would happen if we initialized all of the parameters
of the hidden layer as W(1) = c for some constant c. In this case, during forward propagation
either hidden unit takes the same inputs and parameters, producing the same activation, which
is fed to the output unit. During backpropagation, differentiating the output unit with respect to
parameters W(1) gives a gradient whose elements all take the same value. Thus, after gradientbased
iteration (e.g., minibatch stochastic gradient descent), all the elements of W(1) still take the
same value. Such iterations would never break the symmetry on its own and we might never be
able to realize the network?s expressive power. The hidden layer would behave as if it had only a
single unit. Note that while minibatch stochastic gradient descent would not break this symmetry,
dropout regularization would!
4.8.2 Parameter Initialization
One way of addressing�or at least mitigating�the issues raised above is through careful initialization.
Additional care during optimization and suitable regularization can further enhance stability.
Default Initialization
In the previous sections, e.g., in Section 3.3, we used a normal distribution to initialize the values
of our weights. If we do not specify the initialization method, the framework will use a default
random initialization method, which often works well in practice for moderate problem sizes.
Xavier Initialization
Let us look at the scale distribution of an output (e.g., a hidden variable) oi for some fullyconnected
layer without nonlinearities. With nin inputs xj and their associated weights wij for this
layer, an output is given by
oi =
?nin
j=1
wijxj : (4.8.3)
The weights wij are all drawn independently from the same distribution. Furthermore, let us
assume that this distribution has zero mean and variance 2. Note that this does not mean that
the distribution has to be Gaussian, just that the mean and variance need to exist. For now, let
us assume that the inputs to the layer xj also have zero mean and variance 
2 and that they are
174 Chapter 4. Multilayer Perceptrons
independent of wij and independent of each other. In this case, we can compute the mean and
variance of oi as follows:
E[oi] =
?nin
j=1
E[wijxj ]
=
?nin
j=1
E[wij ]E[xj ]
= 0;
Var[oi] = E[o2i
] ?? (E[oi])2
=
?nin
j=1
E[w2
ijx2j
] ?? 0
=
?nin
j=1
E[w2
ij ]E[x2j
]
= nin2
2:
(4.8.4)
One way to keep the variance fixed is to set nin2 = 1. Now consider backpropagation. There
we face a similar problem, albeit with gradients being propagated from the layers closer to the
output. Using the same reasoning as for forward propagation, we see that the gradients? variance
can blow up unless nout2 = 1, where nout is the number of outputs of this layer. This leaves us in
a dilemma: we cannot possibly satisfy both conditions simultaneously. Instead, we simply try to
satisfy:
1
2
(nin + nout)2 = 1 or equivalently  =
v
2
nin + nout
: (4.8.5)
This is the reasoning underlying the now-standard and practically beneficial Xavier initialization,
named after the first author of its creators (Glorot & Bengio, 2010). Typically, the Xavier initialization
samples weights from a Gaussian distribution with zero mean and variance 2 = 2
nin+nout . We
can also adapt Xavier?s intuition to choose the variance when sampling weights from a uniform
distribution. Note that the uniform distribution U(??a; a) has variance a2
3 . Plugging a2
3 into our
condition on 2 yields the suggestion to initialize according to
U
(
??
v
6
nin + nout
;
v
6
nin + nout
)
: (4.8.6)
Though the assumption for nonexistence of nonlinearities in the above mathematical reasoning
can be easily violated in neural networks, the Xavier initialization method turns out to work well
in practice.
Beyond
The reasoning above barely scratches the surface of modern approaches to parameter initialization.
A deep learning framework often implements over a dozen different heuristics. Moreover,
parameter initialization continues to be a hot area of fundamental research in deep learning.
Among these are heuristics specialized for tied (shared) parameters, super-resolution, sequence
models, and other situations. For instance, Xiao et al. demonstrated the possibility of training
10000-layer neural networks without architectural tricks by using a carefully-designed initialization
method (Xiao et al., 2018).
4.8. Numerical Stability and Initialization 175
If the topic interests you we suggest a deep dive into this module?s offerings, reading the papers
that proposed and analyzed each heuristic, and then exploring the latest publications on the topic.
Perhaps you will stumble across or even invent a clever idea and contribute an implementation to
deep learning frameworks.
Summary
� Vanishing and exploding gradients are common issues in deep networks. Great care in parameter
initialization is required to ensure that gradients and parameters remain well controlled.
� Initialization heuristics are needed to ensure that the initial gradients are neither too large
nor too small.
� ReLU activation functions mitigate the vanishing gradient problem. This can accelerate convergence.
� Random initialization is key to ensure that symmetry is broken before optimization.
� Xavier initialization suggests that, for each layer, variance of any output is not affected by
the number of inputs, and variance of any gradient is not affected by the number of outputs.
Exercises
1. Can you design other cases where a neural network might exhibit symmetry requiring breaking
besides the permutation symmetry in an MLP?s layers?
2. Can we initialize all weight parameters in linear regression or in softmax regression to the
same value?
3. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this
tell you about ensuring that gradients are well conditioned?
4. If we know that some terms diverge, can we fix this after the fact? Look at the paper on
layer-wise adaptive rate scaling for inspiration (You et al., 2017).
Discussions74
4.9 Environment and Distribution Shift
In the previous sections, we worked through a number of hands-on applications of machine learning,
fitting models to a variety of datasets. And yet, we never stopped to contemplate either where
data come from in the first place or what we plan to ultimately do with the outputs from our models.
Too often, machine learning developers in possession of data rush to develop models without
pausing to consider these fundamental issues.
Many failed machine learning deployments can be traced back to this pattern. Sometimes models
appear to perform marvelously as measured by test set accuracy but fail catastrophically in
deployment when the distribution of data suddenly shifts. More insidiously, sometimes the very
deployment of a model can be the catalyst that perturbs the data distribution. Say, for example,
that we trained a model to predict who will repay vs. default on a loan, finding that an applicant?s
74 https://discuss.d2l.ai/t/103
176 Chapter 4. Multilayer Perceptrons
choice of footwear was associated with the risk of default (Oxfords indicate repayment, sneakers
indicate default). We might be inclined to thereafter grant loans to all applicants wearing Oxfords
and to deny all applicants wearing sneakers.
In this case, our ill-considered leap from pattern recognition to decision-making and our failure
to critically consider the environment might have disastrous consequences. For starters, as soon
as we began making decisions based on footwear, customers would catch on and change their
behavior. Before long, all applicants would be wearing Oxfords, without any coinciding improvement
in credit-worthiness. Take a minute to digest this because similar issues abound in many
applications of machine learning: by introducing our model-based decisions to the environment,
we might break the model.
While we cannot possibly give these topics a complete treatment in one section, we aim here to
expose some common concerns, and to stimulate the critical thinking required to detect these
situations early, mitigate damage, and use machine learning responsibly. Some of the solutions
are simple (ask for the �right� data), some are technically difficult (implement a reinforcement
learning system), and others require that we step outside the realm of statistical prediction altogether
and grapple with difficult philosophical questions concerning the ethical application of
algorithms.
4.9.1 Types of Distribution Shift
To begin, we stick with the passive prediction setting considering the various ways that data distributions
might shift and what might be done to salvage model performance. In one classic setup,
we assume that our training data were sampled from some distribution pS(x; y) but that our test
data will consist of unlabeled examples drawn from some different distribution pT (x; y). Already,
we must confront a sobering reality. Absent any assumptions on how pS and pT relate to each
other, learning a robust classifier is impossible.
Consider a binary classification problem, where we wish to distinguish between dogs and cats.
If the distribution can shift in arbitrary ways, then our setup permits the pathological case in
which the distribution over inputs remains constant: pS(x) = pT (x), but the labels are all flipped:
pS(yjx) = 1 ?? pT (yjx). In other words, if God can suddenly decide that in the future all �cats�
are now dogs and what we previously called �dogs� are now cats�without any change in the distribution
of inputs p(x), then we cannot possibly distinguish this setting from one in which the
distribution did not change at all.
Fortunately, under some restricted assumptions on the ways our data might change in the future,
principled algorithms can detect shift and sometimes even adapt on the fly, improving on the
accuracy of the original classifier.
Covariate Shift
Among categories of distribution shift, covariate shift may be the most widely studied. Here, we
assume that while the distribution of inputs may change over time, the labeling function, i.e., the
conditional distribution P(y j x) does not change. Statisticians call this covariate shift because
the problem arises due to a shift in the distribution of the covariates (features). While we can
sometimes reason about distribution shift without invoking causality, we note that covariate shift
is the natural assumption to invoke in settings where we believe that x causes y.
Consider the challenge of distinguishing cats and dogs. Our training data might consist of images
of the kind in Fig. 4.9.1.
4.9. Environment and Distribution Shift 177
Fig. 4.9.1: Training data for distinguishing cats and dogs.
At test time we are asked to classify the images in Fig. 4.9.2.
Fig. 4.9.2: Test data for distinguishing cats and dogs.
The training set consists of photos, while the test set contains only cartoons. Training on a dataset
with substantially different characteristics from the test set can spell trouble absent a coherent
plan for how to adapt to the new domain.
Label Shift
Label shift describes the converse problem. Here, we assume that the label marginal P(y) can
change but the class-conditional distribution P(x j y) remains fixed across domains. Label shift is
a reasonable assumption to make when we believe that y causes x. For example, we may want to
predict diagnoses given their symptoms (or other manifestations), even as the relative prevalence
of diagnoses are changing over time. Label shift is the appropriate assumption here because diseases
cause symptoms. In some degenerate cases the label shift and covariate shift assumptions
can hold simultaneously. For example, when the label is deterministic, the covariate shift assumption
will be satisfied, even when y causes x. Interestingly, in these cases, it is often advantageous
to work with methods that flow from the label shift assumption. That is because these methods
tend to involve manipulating objects that look like labels (often low-dimensional), as opposed to
objects that look like inputs, which tend to be high-dimensional in deep learning.
178 Chapter 4. Multilayer Perceptrons
Concept Shift
We may also encounter the related problem of concept shift, which arises when the very definitions
of labels can change. This sounds weird�a cat is a cat, no? However, other categories are subject to
changes in usage over time. Diagnostic criteria for mental illness, what passes for fashionable, and
job titles, are all subject to considerable amounts of concept shift. It turns out that if we navigate
around the United States, shifting the source of our data by geography, we will find considerable
concept shift regarding the distribution of names for soft drinks as shown in Fig. 4.9.3.
Fig. 4.9.3: Concept shift on soft drink names in the United States.
If we were to build a machine translation system, the distribution P(y j x) might be different depending
on our location. This problem can be tricky to spot. We might hope to exploit knowledge
that shift only takes place gradually either in a temporal or geographic sense.
4.9.2 Examples of Distribution Shift
Before delving into formalism and algorithms, we can discuss some concrete situations where
covariate or concept shift might not be obvious.
Medical Diagnostics
Imagine that you want to design an algorithm to detect cancer. You collect data from healthy and
sick people and you train your algorithm. It works fine, giving you high accuracy and you conclude
that you are ready for a successful career in medical diagnostics. Not so fast.
The distributions that gave rise to the training data and those you will encounter in the wild might
differ considerably. This happened to an unfortunate startup that some of us (authors) worked
with years ago. They were developing a blood test for a disease that predominantly affects older
men and hoped to study it using blood samples that they had collected from patients. However, it
is considerably more difficult to obtain blood samples from healthy men than sick patients already
in the system. To compensate, the startup solicited blood donations from students on a university
campus to serve as healthy controls in developing their test. Then they asked whether we could
help them to build a classifier for detecting the disease.
4.9. Environment and Distribution Shift 179
As we explained to them, it would indeed be easy to distinguish between the healthy and sick
cohorts with near-perfect accuracy. However, that is because the test subjects differed in age,
hormone levels, physical activity, diet, alcohol consumption, and many more factors unrelated
to the disease. This was unlikely to be the case with real patients. Due to their sampling procedure,
we could expect to encounter extreme covariate shift. Moreover, this case was unlikely to
be correctable via conventional methods. In short, they wasted a significant sum of money.
Self-Driving Cars
Say a company wanted to leverage machine learning for developing self-driving cars. One key
component here is a roadside detector. Since real annotated data are expensive to get, they had
the (smart and questionable) idea to use synthetic data from a game rendering engine as additional
training data. This worked really well on �test data� drawn from the rendering engine. Alas, inside
a real car it was a disaster. As it turned out, the roadside had been rendered with a very simplistic
texture. More importantly, all the roadside had been rendered with the same texture and the
roadside detector learned about this �feature� very quickly.
A similar thing happened to the US Army when they first tried to detect tanks in the forest. They
took aerial photographs of the forest without tanks, then drove the tanks into the forest and took
another set of pictures. The classifier appeared to work perfectly. Unfortunately, it had merely
learned how to distinguish trees with shadows from trees without shadows�the first set of pictures
was taken in the early morning, the second set at noon.
Nonstationary Distributions
A much more subtle situation arises when the distribution changes slowly (also known as nonstationary
distribution) and the model is not updated adequately. Below are some typical cases.
� We train a computational advertising model and then fail to update it frequently (e.g., we
forget to incorporate that an obscure new device called an iPad was just launched).
� We build a spam filter. It works well at detecting all spam that we have seen so far. But then
the spammers wisen up and craft new messages that look unlike anything we have seen
before.
� We build a product recommendation system. It works throughout the winter but then continues
to recommend Santa hats long after Christmas.
More Anecdotes
� We build a face detector. It works well on all benchmarks. Unfortunately it fails on test
data�the offending examples are close-ups where the face fills the entire image (no such
data were in the training set).
� We build a Web search engine for the US market and want to deploy it in the UK.
� We train an image classifier by compiling a large dataset where each among a large set of
classes is equally represented in the dataset, say 1000 categories, represented by 1000 images
each. Then we deploy the system in the real world, where the actual label distribution of
photographs is decidedly non-uniform.
180 Chapter 4. Multilayer Perceptrons
4.9.3 Correction of Distribution Shift
As we have discussed, there are many cases where training and test distributions P(x; y) are different.
In some cases, we get lucky and the models work despite covariate, label, or concept shift.
In other cases, we can do better by employing principled strategies to cope with the shift. The remainder
of this section grows considerably more technical. The impatient reader could continue
on to the next section as this material is not prerequisite to subsequent concepts.
Empirical Risk and True Risk
Let us first reflect about what exactly is happening during model training: we iterate over features
and associated labels of training data f(x1; y1); : : : ; (xn; yn)g and update the parameters of a model
f after every minibatch. For simplicity we do not consider regularization, so we largely minimize
the loss on the training:
minimize
f
1
n
?n
i=1
l(f(xi); yi); (4.9.1)
where l is the loss function measuring �how bad� the prediction f(xi) is given the associated label
yi. Statisticians call the term in (4.9.1) empirical risk. Empirical risk is an average loss over
the training data to approximate the true risk, which is the expectation of the loss over the entire
population of data drawn from their true distribution p(x; y):
Ep(x;y)[l(f(x); y)] =
? ?
l(f(x); y)p(x; y) dxdy: (4.9.2)
However, in practice we typically cannot obtain the entire population of data. Thus, empirical risk
minimization, which is minimizing empirical risk in (4.9.1), is a practical strategy for machine
learning, with the hope to approximate minimizing true risk.
Covariate Shift Correction
Assume that we want to estimate some dependency P(y j x) for which we have labeled data (xi; yi).
Unfortunately, the observations xi are drawn from some source distribution q(x) rather than the
target distribution p(x). Fortunately, the dependency assumption means that the conditional distribution
does not change: p(y j x) = q(y j x). If the source distribution q(x) is �wrong�, we can
correct for that by using the following simple identity in true risk:
? ?
l(f(x); y)p(y j x)p(x) dxdy =
? ?
l(f(x); y)q(y j x)q(x)
p(x)
q(x)
dxdy: (4.9.3)
In other words, we need to reweigh each data example by the ratio of the probability that it would
have been drawn from the correct distribution to that from the wrong one:
i
def
=
p(xi)
q(xi)
: (4.9.4)
Plugging in the weight i for each data example (xi; yi) we can train our model using weighted
empirical risk minimization:
minimize
f
1
n
?n
i=1
il(f(xi); yi): (4.9.5)
4.9. Environment and Distribution Shift 181
Alas, we do not know that ratio, so before we can do anything useful we need to estimate it. Many
methods are available, including some fancy operator-theoretic approaches that attempt to recalibrate
the expectation operator directly using a minimum-norm or a maximum entropy principle.
Note that for any such approach, we need samples drawn from both distributions�the �true� p,
e.g., by access to test data, and the one used for generating the training set q (the latter is trivially
available). Note however, that we only need features x  p(x); we do not need to access labels
y  p(y).
In this case, there exists a very effective approach that will give almost as good results as the original:
logistic regression, which is a special case of softmax regression for binary classification.
This is all that is needed to compute estimated probability ratios. We learn a classifier to distinguish
between data drawn from p(x) and data drawn from q(x). If it is impossible to distinguish
between the two distributions then it means that the associated instances are equally likely to
come from either one of the two distributions. On the other hand, any instances that can be well
discriminated should be significantly overweighted or underweighted accordingly. For simplicity
?s sake assume that we have an equal number of instances from both distributions p(x) and q(x),
respectively. Now denote by z labels that are 1 for data drawn from p and ??1 for data drawn from
q. Then the probability in a mixed dataset is given by
P(z = 1 j x) =
p(x)
p(x) + q(x)
and hence P(z = 1 j x)
P(z = ??1 j x)
=
p(x)
q(x)
: (4.9.6)
Thus, if we use a logistic regression approach, where P(z = 1 j x) = 1
1+exp(??h(x)) (h is a parameterized
function), it follows that
i =
1/(1 + exp(??h(xi)))
exp(??h(xi))/(1 + exp(??h(xi)))
= exp(h(xi)): (4.9.7)
As a result, we need to solve two problems: first one to distinguish between data drawn from both
distributions, and then a weighted empirical risk minimization problem in (4.9.5) where we weigh
terms by i.
Now we are ready to describe a correction algorithm. Suppose that we have a training set
f(x1; y1); : : : ; (xn; yn)g and an unlabeled test set fu1; : : : ; umg. For covariate shift, we assume that
xi for all 1  i  n are drawn from some source distribution and ui for all 1  i  m are drawn
from the target distribution. Here is a prototypical algorithm for correcting covariate shift:
1. Generate a binary-classification training set: f(x1;??1); : : : ; (xn;??1); (u1; 1); : : : ; (um; 1)g.
2. Train a binary classifier using logistic regression to get function h.
3. Weigh training data using i = exp(h(xi)) or better i = min(exp(h(xi)); c) for some constant
c.
4. Use weights i for training on f(x1; y1); : : : ; (xn; yn)g in (4.9.5).
Note that the above algorithm relies on a crucial assumption. For this scheme to work, we need
that each data example in the target (e.g., test time) distribution had nonzero probability of occurring
at training time. If we find a point where p(x) > 0 but q(x) = 0, then the corresponding
importance weight should be infinity.
182 Chapter 4. Multilayer Perceptrons
Label Shift Correction
Assume that we are dealing with a classification task with k categories. Using the same notation in
Section 4.9.3, q and p are the source distribution (e.g., training time) and target distribution (e.g.,
test time), respectively. Assume that the distribution of labels shifts over time: q(y) ?= p(y), but
the class-conditional distribution stays the same: q(x j y) = p(x j y). If the source distribution
q(y) is �wrong�, we can correct for that according to the following identity in true risk as defined
in (4.9.2):
? ?
l(f(x); y)p(x j y)p(y) dxdy =
? ?
l(f(x); y)q(x j y)q(y)
p(y)
q(y)
dxdy: (4.9.8)
Here, our importance weights will correspond to the label likelihood ratios
i
def
=
p(yi)
q(yi)
: (4.9.9)
One nice thing about label shift is that if we have a reasonably good model on the source distribution,
then we can get consistent estimates of these weights without ever having to deal with the
ambient dimension. In deep learning, the inputs tend to be high-dimensional objects like images,
while the labels are often simpler objects like categories.
To estimate the target label distribution, we first take our reasonably good off-the-shelf classifier
(typically trained on the training data) and compute its confusion matrix using the validation set
(also from the training distribution). The confusion matrix, C, is simply a k  k matrix, where
each column corresponds to the label category (ground truth) and each row corresponds to our
model?s predicted category. Each cell?s value cij is the fraction of total predictions on the validation
set where the true label was j and our model predicted i.
Now, we cannot calculate the confusion matrix on the target data directly, because we do not get
to see the labels for the examples that we see in the wild, unless we invest in a complex real-time
annotation pipeline. What we can do, however, is average all of our models predictions at test
time together, yielding the mean model outputs (^y) 2 Rk, whose ith element (^yi) is the fraction
of total predictions on the test set where our model predicted i.
It turns out that under some mild conditions�if our classifier was reasonably accurate in the first
place, and if the target data contain only categories that we have seen before, and if the label shift
assumption holds in the first place (the strongest assumption here), then we can estimate the test
set label distribution by solving a simple linear system
Cp(y) = (^y); (4.9.10)
because as an estimate
?k
j=1 cijp(yj) = (^yi) holds for all 1  i  k, where p(yj) is the jth element
of the k-dimensional label distribution vector p(y). If our classifier is sufficiently accurate to begin
with, then the confusion matrix C will be invertible, and we get a solution p(y) = C??1(^y).
Because we observe the labels on the source data, it is easy to estimate the distribution q(y). Then
for any training example i with label yi, we can take the ratio of our estimated p(yi)/q(yi) to calculate
the weight i, and plug this into weighted empirical risk minimization in (4.9.5).
4.9. Environment and Distribution Shift 183
Concept Shift Correction
Concept shift is much harder to fix in a principled manner. For instance, in a situation where
suddenly the problem changes from distinguishing cats from dogs to one of distinguishing white
from black animals, it will be unreasonable to assume that we can do much better than just collecting
new labels and training from scratch. Fortunately, in practice, such extreme shifts are
rare. Instead, what usually happens is that the task keeps on changing slowly. To make things
more concrete, here are some examples:
� In computational advertising, new products are launched, old products become less popular.
This means that the distribution over ads and their popularity changes gradually and any
click-through rate predictor needs to change gradually with it.
� Traffic camera lenses degrade gradually due to environmental wear, affecting image quality
progressively.
� News content changes gradually (i.e., most of the news remains unchanged but new stories
appear).
In such cases, we can use the same approach that we used for training networks to make them
adapt to the change in the data. In other words, we use the existing network weights and simply
perform a few update steps with the new data rather than training from scratch.
4.9.4 A Taxonomy of Learning Problems
Armed with knowledge about how to deal with changes in distributions, we can now consider
some other aspects of machine learning problem formulation.
Batch Learning
In batch learning, we have access to training features and labels f(x1; y1); : : : ; (xn; yn)g, which we
use to train a model f(x). Later on, we deploy this model to score new data (x; y) drawn from the
same distribution. This is the default assumption for any of the problems that we discuss here. For
instance, we might train a cat detector based on lots of pictures of cats and dogs. Once we trained
it, we ship it as part of a smart catdoor computer vision system that lets only cats in. This is then
installed in a customer?s home and is never updated again (barring extreme circumstances).
Online Learning
Now imagine that the data (xi; yi) arrives one sample at a time. More specifically, assume that
we first observe xi, then we need to come up with an estimate f(xi) and only once we have done
this, we observe yi and with it, we receive a reward or incur a loss, given our decision. Many
real problems fall into this category. For example, we need to predict tomorrow?s stock price, this
allows us to trade based on that estimate and at the end of the day we find out whether our estimate
allowed us to make a profit. In other words, in online learning, we have the following cycle where
we are continuously improving our model given new observations.
model ft ??! data xt ??! estimate ft(xt) ??! observation yt ??! loss l(yt; ft(xt)) ??! model ft+1
(4.9.11)
184 Chapter 4. Multilayer Perceptrons
Bandits
Bandits are a special case of the problem above. While in most learning problems we have a continuously
parametrized function f where we want to learn its parameters (e.g., a deep network),
in a bandit problem we only have a finite number of arms that we can pull, i.e., a finite number
of actions that we can take. It is not very surprising that for this simpler problem stronger theoretical
guarantees in terms of optimality can be obtained. We list it mainly since this problem is
often (confusingly) treated as if it were a distinct learning setting.
Control
In many cases the environment remembers what we did. Not necessarily in an adversarial manner
but it will just remember and the response will depend on what happened before. For instance, a
coffee boiler controller will observe different temperatures depending on whether it was heating
the boiler previously. PID (proportional-integral-derivative) controller algorithms are a popular
choice there. Likewise, a user?s behavior on a news site will depend on what we showed him
previously (e.g., he will read most news only once). Many such algorithms form a model of the
environment in which they act such as to make their decisions appear less random. Recently,
control theory (e.g., PID variants) has also been used to automatically tune hyperparameters to
achive better disentangling and reconstruction quality, and improve the diversity of generated
text and the reconstruction quality of generated images (Shao et al., 2020).
Reinforcement Learning
In the more general case of an environment with memory, we may encounter situations where
the environment is trying to cooperate with us (cooperative games, in particular for non-zero-sum
games), or others where the environment will try to win. Chess, Go, Backgammon, or StarCraft
are some of the cases in reinforcement learning. Likewise, we might want to build a good controller
for autonomous cars. The other cars are likely to respond to the autonomous car?s driving style in
nontrivial ways, e.g., trying to avoid it, trying to cause an accident, and trying to cooperate with
it.
Considering the Environment
One key distinction between the different situations above is that the same strategy that might have
worked throughout in the case of a stationary environment, might not work throughout when the
environment can adapt. For instance, an arbitrage opportunity discovered by a trader is likely to
disappear once he starts exploiting it. The speed and manner at which the environment changes
determines to a large extent the type of algorithms that we can bring to bear. For instance, if we
know that things may only change slowly, we can force any estimate to change only slowly, too. If
we know that the environment might change instantaneously, but only very infrequently, we can
make allowances for that. These types of knowledge are crucial for the aspiring data scientist to
deal with concept shift, i.e., when the problem that he is trying to solve changes over time.
4.9. Environment and Distribution Shift 185
4.9.5 Fairness, Accountability, and Transparency in Machine Learning
Finally, it is important to remember that when you deploy machine learning systems you are not
merely optimizing a predictive model�you are typically providing a tool that will be used to (partially
or fully) automate decisions. These technical systems can impact the lives of individuals
subject to the resulting decisions. The leap from considering predictions to decisions raises not
only new technical questions, but also a slew of ethical questions that must be carefully considered.
If we are deploying a medical diagnostic system, we need to know for which populations
it may work and which it may not. Overlooking foreseeable risks to the welfare of a subpopulation
could cause us to administer inferior care. Moreover, once we contemplate decision-making
systems, we must step back and reconsider how we evaluate our technology. Among other consequences
of this change of scope, we will find that accuracy is seldom the right measure. For
instance, when translating predictions into actions, we will often want to take into account the
potential cost sensitivity of erring in various ways. If one way of misclassifying an image could
be perceived as a racial sleight of hand, while misclassification to a different category would be
harmless, then we might want to adjust our thresholds accordingly, accounting for societal values
in designing the decision-making protocol. We also want to be careful about how prediction
systems can lead to feedback loops. For example, consider predictive policing systems, which allocate
patrol officers to areas with high forecasted crime. It is easy to see how a worrying pattern
can emerge:
1. Neighborhoods with more crime get more patrols.
2. Consequently, more crimes are discovered in these neighborhoods, entering the training
data available for future iterations.
3. Exposed to more positives, the model predicts yet more crime in these neighborhoods.
4. In the next iteration, the updated model targets the same neighborhood even more heavily
leading to yet more crimes discovered, etc.
Often, the various mechanisms by which a model?s predictions become coupled to its training data
are unaccounted for in the modeling process. This can lead to what researchers call runaway feedback
loops. Additionally, we want to be careful about whether we are addressing the right problem
in the first place. Predictive algorithms now play an outsize role in mediating the dissemination of
information. Should the news that an individual encounters be determined by the set of Facebook
pages they have Liked? These are just a few among the many pressing ethical dilemmas that you
might encounter in a career in machine learning.
Summary
� In many cases training and test sets do not come from the same distribution. This is called
distribution shift.
� True risk is the expectation of the loss over the entire population of data drawn from their
true distribution. However, this entire population is usually unavailable. Empirical risk is
an average loss over the training data to approximate the true risk. In practice, we perform
empirical risk minimization.
� Under the corresponding assumptions, covariate and label shift can be detected and corrected
for at test time. Failure to account for this bias can become problematic at test time.
� In some cases, the environment may remember automated actions and respond in surprising
ways. We must account for this possibility when building models and continue to mon-
186 Chapter 4. Multilayer Perceptrons
itor live systems, open to the possibility that our models and the environment will become
entangled in unanticipated ways.
Exercises
1. What could happen when we change the behavior of a search engine? What might the users
do? What about the advertisers?
2. Implement a covariate shift detector. Hint: build a classifier.
3. Implement a covariate shift corrector.
4. Besides distribution shift, what else could affect how empirical risk approximates true risk?
Discussions75
4.10 Predicting House Prices on Kaggle
Now that we have introduced some basic tools for building and training deep networks and regularizing
them with techniques including weight decay and dropout, we are ready to put all this
knowledge into practice by participating in a Kaggle competition. The house price prediction
competition is a great place to start. The data are fairly generic and do not exhibit exotic structure
that might require specialized models (as audio or video might). This dataset, collected by Bart de
Cock in 2011 (DeCock, 2011), covers house prices in Ames, IA from the period of 2006�2010. It is
considerably larger than the famous Boston housing dataset76 of Harrison and Rubinfeld (1978),
boasting both more examples and more features.
In this section, we will walk you through details of data preprocessing, model design, and hyperparameter
selection. We hope that through a hands-on approach, you will gain some intuitions
that will guide you in your career as a data scientist.
4.10.1 Downloading and Caching Datasets
Throughout the book, we will train and test models on various downloaded datasets. Here, we
implement several utility functions to facilitate data downloading. First, we maintain a dictionary
DATA_HUB that maps a string (the name of the dataset) to a tuple containing both the URL to locate
the dataset and the SHA-1 key that verifies the integrity of the file. All such datasets are hosted at
the site whose address is DATA_URL.
import os
import requests
import zipfile
import tarfile
import hashlib
DATA_HUB = dict() #@save
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/' #@save
75 https://discuss.d2l.ai/t/105
76 https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names
4.10. Predicting House Prices on Kaggle 187
The following download function downloads a dataset, caching it in a local directory (../data by
default) and returns the name of the downloaded file. If a file corresponding to this dataset already
exists in the cache directory and its SHA-1 matches the one stored in DATA_HUB, our code will use
the cached file to avoid clogging up your internet with redundant downloads.
def download(name, cache_dir=os.path.join('..', 'data')): #@save
"""Download a file inserted into DATA_HUB, return the local filename."""
assert name in DATA_HUB, f"{name} does not exist in {DATA_HUB}."
url, sha1_hash = DATA_HUB[name]
d2l.mkdir_if_not_exist(cache_dir)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname # Hit cache
print(f'Downloading {fname} from {url}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname
We also implement two additional utility functions: one is to download and extract a zip or tar
file and the other to download all the datasets used in this book from DATA_HUB into the cache
directory.
def download_extract(name, folder=None): #@save
"""Download and extract a zip/tar file."""
fname = download(name)
base_dir = os.path.dirname(fname)
data_dir, ext = os.path.splitext(fname)
if ext == '.zip':
fp = zipfile.ZipFile(fname, 'r')
elif ext in ('.tar', '.gz'):
fp = tarfile.open(fname, 'r')
else:
assert False, 'Only zip/tar files can be extracted.'
fp.extractall(base_dir)
return os.path.join(base_dir, folder) if folder else data_dir
def download_all(): #@save
"""Download all files in the DATA_HUB."""
for name in DATA_HUB:
download(name)
188 Chapter 4. Multilayer Perceptrons
4.10.2 Kaggle
Kaggle77 is a popular platform that hosts machine learning competitions. Each competition centers
on a dataset and many are sponsored by stakeholders who offer prizes to the winning solutions.
The platform helps users to interact via forums and shared code, fostering both collaboration
and competition. While leaderboard chasing often spirals out of control, with researchers
focusing myopically on preprocessing steps rather than asking fundamental questions, there is
also tremendous value in the objectivity of a platform that facilitates direct quantitative comparisons
among competing approaches as well as code sharing so that everyone can learn what did
and did not work. If you want to participate in a Kaggle competition, you will first need to register
for an account (see Fig. 4.10.1).
Fig. 4.10.1: The Kaggle website.
On the house price prediction competition page, as illustrated in Fig. 4.10.2, you can find the
dataset (under the �Data� tab), submit predictions, and see your ranking, The URL is right here:
https://www.kaggle.com/c/house-prices-advanced-regression-techniques
Fig. 4.10.2: The house price prediction competition page.
77 https://www.kaggle.com
4.10. Predicting House Prices on Kaggle 189
4.10.3 Accessing and Reading the Dataset
Note that the competition data is separated into training and test sets. Each record includes the
property value of the house and attributes such as street type, year of construction, roof type,
basement condition, etc. The features consist of various data types. For example, the year of
construction is represented by an integer, the roof type by discrete categorical assignments, and
other features by floating point numbers. And here is where reality complicates things: for some
examples, some data are altogether missing with the missing value marked simply as �na�. The
price of each house is included for the training set only (it is a competition after all). We will want
to partition the training set to create a validation set, but we only get to evaluate our models on
the official test set after uploading predictions to Kaggle. The �Data� tab on the competition tab in
Fig. 4.10.2 has links to download the data.
To get started, we will read in and process the data using pandas, which we have introduced in
Section 2.2. So, you will want to make sure that you have pandas installed before proceeding further.
Fortunately, if you are reading in Jupyter, we can install pandas without even leaving the
notebook.
# If pandas is not installed, please uncomment the following line:
# !pip install pandas
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, autograd, init, np, npx
from mxnet.gluon import nn
import pandas as pd
npx.set_np()
For convenience, we can download and cache the Kaggle housing dataset using the script we defined
above.
DATA_HUB['kaggle_house_train'] = ( #@save
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')
DATA_HUB['kaggle_house_test'] = ( #@save
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')
We use pandas to load the two csv files containing training and test data respectively.
train_data = pd.read_csv(download('kaggle_house_train'))
test_data = pd.read_csv(download('kaggle_house_test'))
Downloading ../data/kaggle_house_pred_train.csv from http://d2l-data.s3-accelerate.amazonaws.
,!com/kaggle_house_pred_train.csv...
Downloading ../data/kaggle_house_pred_test.csv from http://d2l-data.s3-accelerate.amazonaws.
,!com/kaggle_house_pred_test.csv...
The training dataset includes 1460 examples, 80 features, and 1 label, while the test data contains
1459 examples and 80 features.
190 Chapter 4. Multilayer Perceptrons
print(train_data.shape)
print(test_data.shape)
(1460, 81)
(1459, 80)
Let us take a look at the first four and last two features as well as the label (SalePrice) from the first
four examples.
print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])
Id MSSubClass MSZoning LotFrontage SaleType SaleCondition SalePrice
0 1 60 RL 65.0 WD Normal 208500
1 2 20 RL 80.0 WD Normal 181500
2 3 60 RL 68.0 WD Normal 223500
3 4 70 RL 60.0 WD Abnorml 140000
We can see that in each example, the first feature is the ID. This helps the model identify each
training example. While this is convenient, it does not carry any information for prediction purposes.
Hence, we remove it from the dataset before feeding the data into the model.
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))
4.10.4 Data Preprocessing
As stated above, we have a wide variety of data types. We will need to preprocess the data before we
can start modeling. Let us start with the numerical features. First, we apply a heuristic, replacing
all missing values by the corresponding feature?s mean. Then, to put all features on a common
scale, we standardize the data by rescaling features to zero mean and unit variance:
x   x ?? 

: (4.10.1)
To verify that this indeed transforms our feature (variable) such that it has zero mean and unit
variance, note that E[x??
 ] = ??
 = 0 and that E[(x??)2] = (2+2)??22+2 = 2. Intuitively,
we standardize the data for two reasons. First, it proves convenient for optimization. Second,
because we do not know a priori which features will be relevant, we do not want to penalize coefficients
assigned to one feature more than on any other.
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
lambda x: (x - x.mean()) / (x.std()))
# After standardizing the data all means vanish, hence we can set missing
# values to 0
all_features[numeric_features] = all_features[numeric_features].fillna(0)
Next we deal with discrete values. This includes features such as �MSZoning�. We replace them by
a one-hot encoding in the same way that we previously transformed multiclass labels into vectors
(see Section 3.4.1). For instance, �MSZoning� assumes the values �RL� and �RM�. Dropping the
�MSZoning� feature, two new indicator features �MSZoning_RL� and �MSZoning_RM� are created
4.10. Predicting House Prices on Kaggle 191
with values being either 0 or 1. According to one-hot encoding, if the original value of �MSZoning�
is �RL�, then �MSZoning_RL� is 1 and �MSZoning_RM� is 0. The pandas package does this
automatically for us.
# `Dummy_na=True` considers "na" (missing value) as a valid feature value, and
# creates an indicator feature for it
all_features = pd.get_dummies(all_features, dummy_na=True)
all_features.shape
(2919, 331)
You can see that this conversion increases the number of features from 79 to 331. Finally, via the
values attribute, we can extract the NumPy format from the pandas format and convert it into the
tensor representation for training.
n_train = train_data.shape[0]
train_features = np.array(all_features[:n_train].values, dtype=np.float32)
test_features = np.array(all_features[n_train:].values, dtype=np.float32)
train_labels = np.array(
train_data.SalePrice.values.reshape(-1, 1), dtype=np.float32)
4.10.5 Training
To get started we train a linear model with squared loss. Not surprisingly, our linear model will
not lead to a competition-winning submission but it provides a sanity check to see whether there
is meaningful information in the data. If we cannot do better than random guessing here, then
there might be a good chance that we have a data processing bug. And if things work, the linear
model will serve as a baseline giving us some intuition about how close the simple model gets
to the best reported models, giving us a sense of how much gain we should expect from fancier
models.
loss = gluon.loss.L2Loss()
def get_net():
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize()
return net
With house prices, as with stock prices, we care about relative quantities more than absolute quantities.
Thus we tend to care more about the relative error y??^y
y than about the absolute error y ?? ^y.
For instance, if our prediction is off by USD 100,000 when estimating the price of a house in Rural
Ohio, where the value of a typical house is 125,000 USD, then we are probably doing a horrible job.
On the other hand, if we err by this amount in Los Altos Hills, California, this might represent a
stunningly accurate prediction (there, the median house price exceeds 4 million USD).
One way to address this problem is to measure the discrepancy in the logarithm of the price estimates.
In fact, this is also the official error measure used by the competition to evaluate the quality
of submissions. After all, a small value  for j log y ?? log ^yj   translates into e??  ^y
y
 e. This
leads to the following root-mean-squared-error between the logarithm of the predicted price and
192 Chapter 4. Multilayer Perceptrons
the logarithm of the label price:
vuut
1
n
?n
i=1
(log yi ?? log ^yi)2: (4.10.2)
def log_rmse(net, features, labels):
# To further stabilize the value when the logarithm is taken, set the
# value less than 1 as 1
clipped_preds = np.clip(net(features), 1, float('inf'))
return np.sqrt(2 * loss(np.log(clipped_preds), np.log(labels)).mean())
Unlike in previous sections, our training functions will rely on the Adam optimizer (we will describe
it in greater detail later). The main appeal of this optimizer is that, despite doing no better
(and sometimes worse) given unlimited resources for hyperparameter optimization, people tend
to find that it is significantly less sensitive to the initial learning rate.
def train(net, train_features, train_labels, test_features, test_labels,
num_epochs, learning_rate, weight_decay, batch_size):
train_ls, test_ls = [], []
train_iter = d2l.load_array((train_features, train_labels), batch_size)
# The Adam optimization algorithm is used here
trainer = gluon.Trainer(net.collect_params(), 'adam', {
'learning_rate': learning_rate, 'wd': weight_decay})
for epoch in range(num_epochs):
for X, y in train_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(batch_size)
train_ls.append(log_rmse(net, train_features, train_labels))
if test_labels is not None:
test_ls.append(log_rmse(net, test_features, test_labels))
return train_ls, test_ls
4.10.6 K-Fold Cross-Validation
You might recall that we introducedK-fold cross-validation in the section where we discussed how
to deal with model selection (Section 4.4). We will put this to good use to select the model design
and to adjust the hyperparameters. We first need a function that returns the ith fold of the data in
a K-fold cross-validation procedure. It proceeds by slicing out the ith segment as validation data
and returning the rest as training data. Note that this is not the most efficient way of handling data
and we would definitely do something much smarter if our dataset was considerably larger. But
this added complexity might obfuscate our code unnecessarily so we can safely omit it here owing
to the simplicity of our problem.
def get_k_fold_data(k, i, X, y):
assert k > 1
fold_size = X.shape[0] // k
X_train, y_train = None, None
for j in range(k):
idx = slice(j * fold_size, (j + 1) * fold_size)
(continues on next page)
4.10. Predicting House Prices on Kaggle 193
(continued from previous page)
X_part, y_part = X[idx, :], y[idx]
if j == i:
X_valid, y_valid = X_part, y_part
elif X_train is None:
X_train, y_train = X_part, y_part
else:
X_train = np.concatenate([X_train, X_part], 0)
y_train = np.concatenate([y_train, y_part], 0)
return X_train, y_train, X_valid, y_valid
The training and verification error averages are returned when we train K times in the K-fold
cross-validation.
def k_fold(k, X_train, y_train, num_epochs,
learning_rate, weight_decay, batch_size):
train_l_sum, valid_l_sum = 0, 0
for i in range(k):
data = get_k_fold_data(k, i, X_train, y_train)
net = get_net()
train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
weight_decay, batch_size)
train_l_sum += train_ls[-1]
valid_l_sum += valid_ls[-1]
if i == 0:
d2l.plot(list(range(1, num_epochs+1)), [train_ls, valid_ls],
xlabel='epoch', ylabel='rmse',
legend=['train', 'valid'], yscale='log')
print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '
f'valid log rmse {float(valid_ls[-1]):f}')
return train_l_sum / k, valid_l_sum / k
4.10.7 Model Selection
In this example, we pick an untuned set of hyperparameters and leave it up to the reader to improve
the model. Finding a good choice can take time, depending on how many variables one
optimizes over. With a large enough dataset, and the normal sorts of hyperparameters, K-fold
cross-validation tends to be reasonably resilient against multiple testing. However, if we try an
unreasonably large number of options we might just get lucky and find that our validation performance
is no longer representative of the true error.
k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
weight_decay, batch_size)
print(f'{k}-fold validation: avg train log rmse: {float(train_l):f}, '
f'avg valid log rmse: {float(valid_l):f}')
fold 1, train log rmse 0.169762, valid log rmse 0.156920
fold 2, train log rmse 0.162264, valid log rmse 0.189161
fold 3, train log rmse 0.163649, valid log rmse 0.167805
fold 4, train log rmse 0.167752, valid log rmse 0.154732
(continues on next page)
194 Chapter 4. Multilayer Perceptrons
(continued from previous page)
fold 5, train log rmse 0.162732, valid log rmse 0.182790
5-fold validation: avg train log rmse: 0.165232, avg valid log rmse: 0.170282
Notice that sometimes the number of training errors for a set of hyperparameters can be very
low, even as the number of errors on K-fold cross-validation is considerably higher. This indicates
that we are overfitting. Throughout training you will want to monitor both numbers. Less
overfitting might indicate that our data can support a more powerful model. Massive overfitting
might suggest that we can gain by incorporating regularization techniques.
4.10.8 Submitting Predictions on Kaggle
Now that we know what a good choice of hyperparameters should be, we might as well use all the
data to train on it (rather than just 1??1/K of the data that are used in the cross-validation slices).
The model that we obtain in this way can then be applied to the test set. Saving the predictions in
a csv file will simplify uploading the results to Kaggle.
def train_and_pred(train_features, test_feature, train_labels, test_data,
num_epochs, lr, weight_decay, batch_size):
net = get_net()
train_ls, _ = train(net, train_features, train_labels, None, None,
num_epochs, lr, weight_decay, batch_size)
d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
ylabel='log rmse', yscale='log')
print(f'train log rmse {float(train_ls[-1]):f}')
# Apply the network to the test set
preds = d2l.numpy(net(test_features))
# Reformat it to export to Kaggle
test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
submission.to_csv('submission.csv', index=False)
One nice sanity check is to see whether the predictions on the test set resemble those of theK-fold
cross-validation process. If they do, it is time to upload them to Kaggle. The following code will
generate a file called submission.csv.
4.10. Predicting House Prices on Kaggle 195
train_and_pred(train_features, test_features, train_labels, test_data,
num_epochs, lr, weight_decay, batch_size)
train log rmse 0.162304
Next, as demonstrated in Fig. 4.10.3, we can submit our predictions on Kaggle and see how they
compare with the actual house prices (labels) on the test set. The steps are quite simple:
� Log in to the Kaggle website and visit the house price prediction competition page.
� Click the �Submit Predictions� or �Late Submission� button (as of this writing, the button is
located on the right).
� Click the �Upload Submission File� button in the dashed box at the bottom of the page and
select the prediction file you wish to upload.
� Click the �Make Submission� button at the bottom of the page to view your results.
Fig. 4.10.3: Submitting data to Kaggle
196 Chapter 4. Multilayer Perceptrons
Summary
� Real data often contain a mix of different data types and need to be preprocessed.
� Rescaling real-valued data to zero mean and unit variance is a good default. So is replacing
missing values with their mean.
� Transforming categorical features into indicator features allows us to treat them like one-hot
vectors.
� We can use K-fold cross-validation to select the model and adjust the hyperparameters.
� Logarithms are useful for relative errors.
Exercises
1. Submit your predictions for this section to Kaggle. How good are your predictions?
2. Can you improve your model by minimizing the logarithm of prices directly? What happens
if you try to predict the logarithm of the price rather than the price?
3. Is it always a good idea to replace missing values by their mean? Hint: can you construct a
situation where the values are not missing at random?
4. Improve the score on Kaggle by tuning the hyperparameters through K-fold crossvalidation.
5. Improve the score by improving the model (e.g., layers, weight decay, and dropout).
6. What happens if we do not standardize the continuous numerical features like what we have
done in this section?
Discussions78
78 https://discuss.d2l.ai/t/106
4.10. Predicting House Prices on Kaggle 197
198 Chapter 4. Multilayer Perceptrons
5 | Deep Learning Computation
Alongside giant datasets and powerful hardware, great software tools have played an indispensable
role in the rapid progress of deep learning. Starting with the pathbreaking Theano library
released in 2007, flexible open-source tools have enabled researchers to rapidly prototype models,
avoiding repetitive work when recycling standard components while still maintaining the ability
to make low-level modifications. Over time, deep learning?s libraries have evolved to offer increasingly
coarse abstractions. Just as semiconductor designers went from specifying transistors
to logical circuits to writing code, neural networks researchers have moved from thinking about
the behavior of individual artificial neurons to conceiving of networks in terms of whole layers,
and now often design architectures with far coarser blocks in mind.
So far, we have introduced some basic machine learning concepts, ramping up to fully-functional
deep learning models. In the last chapter, we implemented each component of an MLP from
scratch and even showed how to leverage high-level APIs to roll out the same models effortlessly.
To get you that far that fast, we called upon the libraries, but skipped over more advanced details
about how they work. In this chapter, we will peel back the curtain, digging deeper into the key
components of deep learning computation, namely model construction, parameter access and
initialization, designing custom layers and blocks, reading and writing models to disk, and leveraging
GPUs to achieve dramatic speedups. These insights will move you from end user to power
user, giving you the tools needed to reap the benefits of a mature deep learning library while retaining
the flexibility to implement more complex models, including those you invent yourself!
While this chapter does not introduce any new models or datasets, the advanced modeling chapters
that follow rely heavily on these techniques.
5.1 Layers and Blocks
When we first introduced neural networks, we focused on linear models with a single output.
Here, the entire model consists of just a single neuron. Note that a single neuron (i) takes some
set of inputs; (ii) generates a corresponding scalar output; and (iii) has a set of associated parameters
that can be updated to optimize some objective function of interest. Then, once we started
thinking about networks with multiple outputs, we leveraged vectorized arithmetic to characterize
an entire layer of neurons. Just like individual neurons, layers (i) take a set of inputs, (ii) generate
corresponding outputs, and (iii) are described by a set of tunable parameters. When we worked
through softmax regression, a single layer was itself the model. However, even when we subsequently
introduced MLPs, we could still think of the model as retaining this same basic structure.
Interestingly, for MLPs, both the entire model and its constituent layers share this structure. The
entire model takes in raw inputs (the features), generates outputs (the predictions), and possesses
parameters (the combined parameters from all constituent layers). Likewise, each individual layer
ingests inputs (supplied by the previous layer) generates outputs (the inputs to the subsequent
199
layer), and possesses a set of tunable parameters that are updated according to the signal that
flows backwards from the subsequent layer.
While you might think that neurons, layers, and models give us enough abstractions to go about
our business, it turns out that we often find it convenient to speak about components that are
larger than an individual layer but smaller than the entire model. For example, the ResNet-152
architecture, which is wildly popular in computer vision, possesses hundreds of layers. These
layers consist of repeating patterns of groups of layers. Implementing such a network one layer at
a time can grow tedious. This concern is not just hypothetical�such design patterns are common
in practice. The ResNet architecture mentioned above won the 2015 ImageNet and COCO computer
vision competitions for both recognition and detection (He et al., 2016a) and remains a go-to
architecture for many vision tasks. Similar architectures in which layers are arranged in various
repeating patterns are now ubiquitous in other domains, including natural language processing
and speech.
To implement these complex networks, we introduce the concept of a neural network block. A
block could describe a single layer, a component consisting of multiple layers, or the entire model
itself! One benefit of working with the block abstraction is that they can be combined into larger
artifacts, often recursively. This is illustrated in Fig. 5.1.1. By defining code to generate blocks
of arbitrary complexity on demand, we can write surprisingly compact code and still implement
complex neural networks.
Fig. 5.1.1: Multiple layers are combined into blocks, forming repeating patterns of larger models.
From a programing standpoint, a block is represented by a class. Any subclass of it must define a
forward propagation function that transforms its input into output and must store any necessary
parameters. Note that some blocks do not require any parameters at all. Finally a block must possess
a backpropagation function, for purposes of calculating gradients. Fortunately, due to some
behind-the-scenes magic supplied by the auto differentiation (introduced in Section 2.5) when
defining our own block, we only need to worry about parameters and the forward propagation
function.
To begin, we revisit the code that we used to implement MLPs (Section 4.3). The following code
generates a network with one fully-connected hidden layer with 256 units and ReLU activation,
followed by a fully-connected output layer with 10 units (no activation function).
200 Chapter 5. Deep Learning Computation
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
X = np.random.uniform(size=(2, 20))
net(X)
array([[ 0.06240272, -0.03268593, 0.02582653, 0.02254182, -0.03728798,
-0.04253786, 0.00540613, -0.01364186, -0.09915452, -0.02272738],
[ 0.02816677, -0.03341204, 0.03565666, 0.02506382, -0.04136416,
-0.04941845, 0.01738528, 0.01081961, -0.09932579, -0.01176298]])
In this example, we constructed our model by instantiating an nn.Sequential, assigning the returned
object to the net variable. Next, we repeatedly call its add function, appending layers in
the order that they should be executed. In short, nn.Sequential defines a special kind of Block,
the class that presents a block in Gluon. It maintains an ordered list of constituent Blocks. The
add function simply facilitates the addition of each successive Block to the list. Note that each
layer is an instance of the Dense class which is itself a subclass of Block. The forward propagation
(forward) function is also remarkably simple: it chains each Block in the list together, passing the
output of each as the input to the next. Note that until now, we have been invoking our models via
the construction net(X) to obtain their outputs. This is actually just shorthand for net.forward(X),
a slick Python trick achieved via the Block class?s __call__ function.
5.1.1 A Custom Block
Perhaps the easiest way to develop intuition about how a block works is to implement one ourselves.
Before we implement our own custom block, we briefly summarize the basic functionality
that each block must provide:
1. Ingest input data as arguments to its forward propagation function.
2. Generate an output by having the forward propagation function return a value. Note that
the output may have a different shape from the input. For example, the first fully-connected
layer in our model above ingests an input of arbitrary dimension but returns an output of
dimension 256.
3. Calculate the gradient of its output with respect to its input, which can be accessed via its
backpropagation function. Typically this happens automatically.
4. Store and provide access to those parameters necessary to execute the forward propagation
computation.
5. Initialize model parameters as needed.
In the following snippet, we code up a block from scratch corresponding to an MLP with one hidden
layer with 256 hidden units, and a 10-dimensional output layer. Note that the MLP class below
inherits the class that represents a block. We will heavily rely on the parent class?s functions, supplying
only our own constructor (the __init__ function in Python) and the forward propagation
function.
5.1. Layers and Blocks 201
class MLP(nn.Block):
# Declare a layer with model parameters. Here, we declare two
# fully-connected layers
def __init__(self, **kwargs):
# Call the constructor of the `MLP` parent class `Block` to perform
# the necessary initialization. In this way, other function arguments
# can also be specified during class instantiation, such as the model
# parameters, `params` (to be described later)
super().__init__(**kwargs)
self.hidden = nn.Dense(256, activation='relu') # Hidden layer
self.out = nn.Dense(10) # Output layer
# Define the forward propagation of the model, that is, how to return the
# required model output based on the input `X`
def forward(self, X):
return self.out(self.hidden(X))
Let us first focus on the forward propagation function. Note that it takes X as the input, calculates
the hidden representation with the activation function applied, and outputs its logits. In this MLP
implementation, both layers are instance variables. To see why this is reasonable, imagine instantiating
two MLPs, net1 and net2, and training them on different data. Naturally, we would expect
them to represent two different learned models.
We instantiate the MLP?s layers in the constructor and subsequently invoke these layers on each
call to the forward propagation function. Note a few key details. First, our customized __init__
function invokes the parent class?s __init__ function via super().__init__() sparing us the
pain of restating boilerplate code applicable to most blocks. We then instantiate our two fullyconnected
layers, assigning them to self.hidden and self.out. Note that unless we implement a
new operator, we need not worry about the backpropagation function or parameter initialization.
The system will generate these functions automatically. Let us try this out.
net = MLP()
net.initialize()
net(X)
array([[-0.03989594, -0.1041471 , 0.06799038, 0.05245074, 0.02526059,
-0.00640342, 0.04182098, -0.01665319, -0.02067346, -0.07863817],
[-0.03612847, -0.07210436, 0.09159479, 0.07890771, 0.02494172,
-0.01028665, 0.01732428, -0.02843242, 0.03772651, -0.06671704]])
A key virtue of the block abstraction is its versatility. We can subclass a block to create layers (such
as the fully-connected layer class), entire models (such as the MLP class above), or various components
of intermediate complexity. We exploit this versatility throughout the following chapters,
such as when addressing convolutional neural networks.
202 Chapter 5. Deep Learning Computation
5.1.2 The Sequential Block
We can now take a closer look at how the Sequential class works. Recall that Sequential was
designed to daisy-chain other blocks together. To build our own simplified MySequential, we just
need to define two key function: 1. A function to append blocks one by one to a list. 2. A forward
propagation function to pass an input through the chain of blocks, in the same order as they were
appended.
The following MySequential class delivers the same functionality of the default Sequential class.
class MySequential(nn.Block):
def add(self, block):
# Here, `block` is an instance of a `Block` subclass, and we assume
# that it has a unique name. We save it in the member variable
# `_children` of the `Block` class, and its type is OrderedDict. When
# the `MySequential` instance calls the `initialize` function, the
# system automatically initializes all members of `_children`
self._children[block.name] = block
def forward(self, X):
# OrderedDict guarantees that members will be traversed in the order
# they were added
for block in self._children.values():
X = block(X)
return X
The add function adds a single block to the ordered dictionary _children. You might wonder why
every Gluon Block possesses a _children attribute and why we used it rather than just define a
Python list ourselves. In short the chief advantage of _children is that during our block?s parameter
initialization, Gluon knows to look inside the _children dictionary to find sub-blocks whose
parameters also need to be initialized.
When our MySequential?s forward propagation function is invoked, each added block is executed
in the order in which they were added. We can now reimplement an MLP using our MySequential
class.
net = MySequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
net.initialize()
net(X)
array([[-0.07645682, -0.01130233, 0.04952145, -0.04651389, -0.04131573,
-0.05884133, -0.0621381 , 0.01311472, -0.01379425, -0.02514282],
[-0.05124625, 0.00711231, -0.00155935, -0.07555379, -0.06675334,
-0.01762914, 0.00589084, 0.01447191, -0.04330775, 0.03317726]])
Note that this use of MySequential is identical to the code we previously wrote for the Sequential
class (as described in Section 4.3).
5.1. Layers and Blocks 203
5.1.3 Executing Code in the Forward Propagation Function
The Sequential class makes model construction easy, allowing us to assemble new architectures
without having to define our own class. However, not all architectures are simple daisy chains.
When greater flexibility is required, we will want to define our own blocks. For example, we
might want to execute Python?s control flow within the forward propagation function. Moreover,
we might want to perform arbitrary mathematical operations, not simply relying on predefined
neural network layers.
You might have noticed that until now, all of the operations in our networks have acted upon our
network?s activations and its parameters. Sometimes, however, we might want to incorporate
terms that are neither the result of previous layers nor updatable parameters. We call these constant
parameters. Say for example that we want a layer that calculates the function f(x; w) = cw?x,
where x is the input, w is our parameter, and c is some specified constant that is not updated during
optimization. So we implement a FixedHiddenMLP class as follows.
class FixedHiddenMLP(nn.Block):
def __init__(self, **kwargs):
super().__init__(**kwargs)
# Random weight parameters created with the `get_constant` function
# are not updated during training (i.e., constant parameters)
self.rand_weight = self.params.get_constant(
'rand_weight', np.random.uniform(size=(20, 20)))
self.dense = nn.Dense(20, activation='relu')
def forward(self, X):
X = self.dense(X)
# Use the created constant parameters, as well as the `relu` and `dot`
# functions
X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)
# Reuse the fully-connected layer. This is equivalent to sharing
# parameters with two fully-connected layers
X = self.dense(X)
# Control flow
while np.abs(X).sum() > 1:
X /= 2
return X.sum()
In this FixedHiddenMLP model, we implement a hidden layer whose weights (self.rand_weight)
are initialized randomly at instantiation and are thereafter constant. This weight is not a model
parameter and thus it is never updated by backpropagation. The network then passes the output
of this �fixed� layer through a fully-connected layer.
Note that before returning the output, our model did something unusual. We ran a while-loop,
testing on the condition its L1 norm is larger than 1, and dividing our output vector by 2 until it
satisfied the condition. Finally, we returned the sum of the entries in X. To our knowledge, no
standard neural network performs this operation. Note that this particular operation may not be
useful in any real-world task. Our point is only to show you how to integrate arbitrary code into
the flow of your neural network computations.
net = FixedHiddenMLP()
net.initialize()
net(X)
204 Chapter 5. Deep Learning Computation
array(0.52637565)
We can mix and match various ways of assembling blocks together. In the following example, we
nest blocks in some creative ways.
class NestMLP(nn.Block):
def __init__(self, **kwargs):
super().__init__(**kwargs)
self.net = nn.Sequential()
self.net.add(nn.Dense(64, activation='relu'),
nn.Dense(32, activation='relu'))
self.dense = nn.Dense(16, activation='relu')
def forward(self, X):
return self.dense(self.net(X))
chimera = nn.Sequential()
chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())
chimera.initialize()
chimera(X)
array(0.97720534)
5.1.4 Compilation
The avid reader might start to worry about the efficiency of some of these operations. After all, we
have lots of dictionary lookups, code execution, and lots of other Pythonic things taking place in
what is supposed to be a high-performance deep learning library. The problems of Python?s global
interpreter lock79 are well known. In the context of deep learning, we worry that our extremely
fast GPU(s) might have to wait until a puny CPU runs Python code before it gets another job to run.
The best way to speed up Python is by avoiding it altogether.
One way that Gluon does this is by allowing for hybridization, which will be described later. Here,
the Python interpreter executes a block the first time it is invoked. The Gluon runtime records
what is happening and the next time around it short-circuits calls to Python. This can accelerate
things considerably in some cases but care needs to be taken when control flow (as above) leads
down different branches on different passes through the net. We recommend that the interested
reader checks out the hybridization section (Section 12.1) to learn about compilation after finishing
the current chapter.
79 https://wiki.python.org/moin/GlobalInterpreterLock
5.1. Layers and Blocks 205
Summary
� Layers are blocks.
� Many layers can comprise a block.
� Many blocks can comprise a block.
� A block can contain code.
� Blocks take care of lots of housekeeping, including parameter initialization and backpropagation.
� Sequential concatenations of layers and blocks are handled by the Sequential block.
Exercises
1. What kinds of problems will occur if you change MySequential to store blocks in a Python
list?
2. Implement a block that takes two blocks as an argument, say net1 and net2 and returns
the concatenated output of both networks in the forward propagation. This is also called a
parallel block.
3. Assume that you want to concatenate multiple instances of the same network. Implement
a factory function that generates multiple instances of the same block and build a larger
network from it.
Discussions80
5.2 Parameter Management
Once we have chosen an architecture and set our hyperparameters, we proceed to the training
loop, where our goal is to find parameter values that minimize our loss function. After training, we
will need these parameters in order to make future predictions. Additionally, we will sometimes
wish to extract the parameters either to reuse them in some other context, to save our model
to disk so that it may be executed in other software, or for examination in the hope of gaining
scientific understanding.
Most of the time, we will be able to ignore the nitty-gritty details of how parameters are declared
and manipulated, relying on deep learning frameworks to do the heavy lifting. However, when we
move away from stacked architectures with standard layers, we will sometimes need to get into
the weeds of declaring and manipulating parameters. In this section, we cover the following:
� Accessing parameters for debugging, diagnostics, and visualizations.
� Parameter initialization.
� Sharing parameters across different model components.
We start by focusing on an MLP with one hidden layer.
80 https://discuss.d2l.ai/t/54
206 Chapter 5. Deep Learning Computation
from mxnet import init, np, npx
from mxnet.gluon import nn
npx.set_np()
net = nn.Sequential()
net.add(nn.Dense(8, activation='relu'))
net.add(nn.Dense(1))
net.initialize() # Use the default initialization method
X = np.random.uniform(size=(2, 4))
net(X) # Forward computation
array([[0.0054572 ],
[0.00488594]])
5.2.1 Parameter Access
Let us start with how to access parameters from the models that you already know. When a model
is defined via the Sequential class, we can first access any layer by indexing into the model as
though it were a list. Each layer?s parameters are conveniently located in its attribute. We can
inspect the parameters of the second fully-connected layer as follows.
print(net[1].params)
dense1_ (
Parameter dense1_weight (shape=(1, 8), dtype=float32)
Parameter dense1_bias (shape=(1,), dtype=float32)
)
The output tells us a few important things. First, this fully-connected layer contains two parameters,
corresponding to that layer?s weights and biases, respectively. Both are stored as single precision
floats (float32). Note that the names of the parameters allow us to uniquely identify each
layer?s parameters, even in a network containing hundreds of layers.
Targeted Parameters
Note that each parameter is represented as an instance of the parameter class. To do anything
useful with the parameters, we first need to access the underlying numerical values. There are
several ways to do this. Some are simpler while others are more general. The following code
extracts the bias from the second neural network layer, which returns a parameter class instance,
and further accesses that parameter?s value.
print(type(net[1].bias))
print(net[1].bias)
print(net[1].bias.data())
<class 'mxnet.gluon.parameter.Parameter'>
Parameter dense1_bias (shape=(1,), dtype=float32)
[0.]
5.2. Parameter Management 207
Parameters are complex objects, containing values, gradients, and additional information. That?s
why we need to request the value explicitly.
In addition to the value, each parameter also allows us to access the gradient. Because we have
not invoked backpropagation for this network yet, it is in its initial state.
net[1].weight.grad()
array([[0., 0., 0., 0., 0., 0., 0., 0.]])
All Parameters at Once
When we need to perform operations on all parameters, accessing them one-by-one can grow
tedious. The situation can grow especially unwieldy when we work with more complex blocks
(e.g., nested blocks), since we would need to recurse through the entire tree to extract each subblock
?s parameters. Below we demonstrate accessing the parameters of the first fully-connected
layer vs. accessing all layers.
print(net[0].collect_params())
print(net.collect_params())
dense0_ (
Parameter dense0_weight (shape=(8, 4), dtype=float32)
Parameter dense0_bias (shape=(8,), dtype=float32)
)
sequential0_ (
Parameter dense0_weight (shape=(8, 4), dtype=float32)
Parameter dense0_bias (shape=(8,), dtype=float32)
Parameter dense1_weight (shape=(1, 8), dtype=float32)
Parameter dense1_bias (shape=(1,), dtype=float32)
)
This provides us with another way of accessing the parameters of the network as follows.
net.collect_params()['dense1_bias'].data()
array([0.])
Collecting Parameters from Nested Blocks
Let us see how the parameter naming conventions work if we nest multiple blocks inside each
other. For that we first define a function that produces blocks (a block factory, so to speak) and
then combine these inside yet larger blocks.
def block1():
net = nn.Sequential()
net.add(nn.Dense(32, activation='relu'))
net.add(nn.Dense(16, activation='relu'))
(continues on next page)
208 Chapter 5. Deep Learning Computation
(continued from previous page)
return net
def block2():
net = nn.Sequential()
for _ in range(4):
# Nested here
net.add(block1())
return net
rgnet = nn.Sequential()
rgnet.add(block2())
rgnet.add(nn.Dense(10))
rgnet.initialize()
rgnet(X)
array([[-6.3465841e-09, -1.1096747e-09, 6.4161774e-09, 6.6354131e-09,
-1.1265502e-09, 1.3285141e-10, 9.3619361e-09, 3.2229091e-09,
5.9429857e-09, 8.8181418e-09],
[-8.6219423e-09, -7.5150813e-10, 8.3133243e-09, 8.9321119e-09,
-1.6739999e-09, 3.2406069e-10, 1.2115976e-08, 4.4926454e-09,
8.0741742e-09, 1.2075874e-08]])
Now that we have designed the network, let us see how it is organized.
print(rgnet.collect_params)
print(rgnet.collect_params())
<bound method Block.collect_params of Sequential(
(0): Sequential(
(0): Sequential(
(0): Dense(4 -> 32, Activation(relu))
(1): Dense(32 -> 16, Activation(relu))
)
(1): Sequential(
(0): Dense(16 -> 32, Activation(relu))
(1): Dense(32 -> 16, Activation(relu))
)
(2): Sequential(
(0): Dense(16 -> 32, Activation(relu))
(1): Dense(32 -> 16, Activation(relu))
)
(3): Sequential(
(0): Dense(16 -> 32, Activation(relu))
(1): Dense(32 -> 16, Activation(relu))
)
)
(1): Dense(16 -> 10, linear)
)>
sequential1_ (
Parameter dense2_weight (shape=(32, 4), dtype=float32)
Parameter dense2_bias (shape=(32,), dtype=float32)
Parameter dense3_weight (shape=(16, 32), dtype=float32)
Parameter dense3_bias (shape=(16,), dtype=float32)
(continues on next page)
5.2. Parameter Management 209
(continued from previous page)
Parameter dense4_weight (shape=(32, 16), dtype=float32)
Parameter dense4_bias (shape=(32,), dtype=float32)
Parameter dense5_weight (shape=(16, 32), dtype=float32)
Parameter dense5_bias (shape=(16,), dtype=float32)
Parameter dense6_weight (shape=(32, 16), dtype=float32)
Parameter dense6_bias (shape=(32,), dtype=float32)
Parameter dense7_weight (shape=(16, 32), dtype=float32)
Parameter dense7_bias (shape=(16,), dtype=float32)
Parameter dense8_weight (shape=(32, 16), dtype=float32)
Parameter dense8_bias (shape=(32,), dtype=float32)
Parameter dense9_weight (shape=(16, 32), dtype=float32)
Parameter dense9_bias (shape=(16,), dtype=float32)
Parameter dense10_weight (shape=(10, 16), dtype=float32)
Parameter dense10_bias (shape=(10,), dtype=float32)
)
Since the layers are hierarchically nested, we can also access them as though indexing through
nested lists. For instance, we can access the first major block, within it the second sub-block, and
within that the bias of the first layer, with as follows.
rgnet[0][1][0].bias.data()
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
5.2.2 Parameter Initialization
Now that we know how to access the parameters, let us look at how to initialize them properly.
We discussed the need for proper initialization in Section 4.8. The deep learning framework provides
default random initializations to its layers. However, we often want to initialize our weights
according to various other protocols. The framework provides most commonly used protocols,
and also allows to create a custom initializer.
By default, MXNet initializes weight parameters by randomly drawing from a uniform distribution
U(??0:07; 0:07), clearing bias parameters to zero. MXNet?s init module provides a variety of preset
initialization methods.
Built-in Initialization
Let us begin by calling on built-in initializers. The code below initializes all weight parameters as
Gaussian random variables with standard deviation 0.01, while bias parameters cleared to zero.
# Here `force_reinit` ensures that parameters are freshly initialized even if
# they were already initialized previously
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
net[0].weight.data()[0]
array([-0.00324057, -0.00895028, -0.00698632, 0.01030831])
210 Chapter 5. Deep Learning Computation
We can also initialize all the parameters to a given constant value (say, 1).
net.initialize(init=init.Constant(1), force_reinit=True)
net[0].weight.data()[0]
array([1., 1., 1., 1.])
We can also apply different initializers for certain blocks. For example, below we initialize the
first layer with the Xavier initializer and initialize the second layer to a constant value of 42.
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)
net[1].initialize(init=init.Constant(42), force_reinit=True)
print(net[0].weight.data()[0])
print(net[1].weight.data())
[-0.17594433 0.02314097 -0.1992535 0.09509248]
[[42. 42. 42. 42. 42. 42. 42. 42.]]
Custom Initialization
Sometimes, the initialization methods we need are not provided by the deep learning framework.
In the example below, we define an initializer for any weight parameter w using the following
strange distribution:
w 
8><
>:
U(5; 10) with probability 1
4
0 with probability 1
2
U(??10;??5) with probability 1
4
(5.2.1)
Here we define a subclass of the Initializer class. Usually, we only need to implement the
_init_weight function which takes a tensor argument (data) and assigns to it the desired initialized
values.
class MyInit(init.Initializer):
def _init_weight(self, name, data):
print('Init', name, data.shape)
data[:] = np.random.uniform(-10, 10, data.shape)
data *= np.abs(data) >= 5
net.initialize(MyInit(), force_reinit=True)
net[0].weight.data()[:2]
Init dense0_weight (8, 4)
Init dense1_weight (1, 8)
array([[ 0. , -0. , -0. , 8.522827 ],
[ 0. , -8.828651 , -0. , -5.6012006]])
Note that we always have the option of setting parameters directly.
5.2. Parameter Management 211
net[0].weight.data()[:] += 1
net[0].weight.data()[0, 0] = 42
net[0].weight.data()[0]
array([42. , 1. , 1. , 9.522827])
A note for advanced users: if you want to adjust parameters within an autograd scope, you need
to use set_data to avoid confusing the automatic differentiation mechanics.
5.2.3 Tied Parameters
Often, we want to share parameters across multiple layers. Let us see how to do this elegantly.
In the following we allocate a dense layer and then use its parameters specifically to set those of
another layer.
net = nn.Sequential()
# We need to give the shared layer a name so that we can refer to its
# parameters
shared = nn.Dense(8, activation='relu')
net.add(nn.Dense(8, activation='relu'),
shared,
nn.Dense(8, activation='relu', params=shared.params),
nn.Dense(10))
net.initialize()
X = np.random.uniform(size=(2, 20))
net(X)
# Check whether the parameters are the same
print(net[1].weight.data()[0] == net[2].weight.data()[0])
net[1].weight.data()[0, 0] = 100
# Make sure that they are actually the same object rather than just having the
# same value
print(net[1].weight.data()[0] == net[2].weight.data()[0])
[ True True True True True True True True]
[ True True True True True True True True]
This example shows that the parameters of the second and third layer are tied. They are not just
equal, they are represented by the same exact tensor. Thus, if we change one of the parameters,
the other one changes, too. You might wonder, when parameters are tied what happens to the
gradients? Since the model parameters contain gradients, the gradients of the second hidden
layer and the third hidden layer are added together during backpropagation.
212 Chapter 5. Deep Learning Computation
Summary
� We have several ways to access, initialize, and tie model parameters.
� We can use custom initialization.
Exercises
1. Use the FancyMLP model defined in Section 5.1 and access the parameters of the various layers.
2. Look at the initialization module document to explore different initializers.
3. Construct an MLP containing a shared parameter layer and train it. During the training
process, observe the model parameters and gradients of each layer.
4. Why is sharing parameters a good idea?
Discussions81
5.3 Deferred Initialization
So far, it might seem that we got away with being sloppy in setting up our networks. Specifically,
we did the following unintuitive things, which might not seem like they should work:
� We defined the network architectures without specifying the input dimensionality.
� We added layers without specifying the output dimension of the previous layer.
� We even �initialized� these parameters before providing enough information to determine
how many parameters our models should contain.
You might be surprised that our code runs at all. After all, there is no way the deep learning
framework could tell what the input dimensionality of a network would be. The trick here is that
the framework defers initialization, waiting until the first time we pass data through the model, to
infer the sizes of each layer on the fly.
Later on, when working with convolutional neural networks, this technique will become even
more convenient since the input dimensionality (i.e., the resolution of an image) will affect the
dimensionality of each subsequent layer. Hence, the ability to set parameters without the need
to know, at the time of writing the code, what the dimensionality is can greatly simplify the task
of specifying and subsequently modifying our models. Next, we go deeper into the mechanics of
initialization.
81 https://discuss.d2l.ai/t/56
5.3. Deferred Initialization 213
5.3.1 Instantiating a Network
To begin, let us instantiate an MLP.
from mxnet import init, np, npx
from mxnet.gluon import nn
npx.set_np()
def get_net():
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'))
net.add(nn.Dense(10))
return net
net = get_net()
At this point, the network cannot possibly know the dimensions of the input layer?s weights because
the input dimension remains unknown. Consequently the framework has not yet initialized
any parameters. We confirm by attempting to access the parameters below.
print(net.collect_params)
print(net.collect_params())
<bound method Block.collect_params of Sequential(
(0): Dense(-1 -> 256, Activation(relu))
(1): Dense(-1 -> 10, linear)
)>
sequential0_ (
Parameter dense0_weight (shape=(256, -1), dtype=float32)
Parameter dense0_bias (shape=(256,), dtype=float32)
Parameter dense1_weight (shape=(10, -1), dtype=float32)
Parameter dense1_bias (shape=(10,), dtype=float32)
)
Note that while the parameter objects exist, the input dimension to each layer is listed as -1. MXNet
uses the special value -1 to indicate that the parameter dimension remains unknown. At this point,
attempts to access net[0].weight.data() would trigger a runtime error stating that the network
must be initialized before the parameters can be accessed. Now let us see what happens when we
attempt to initialize parameters via the initialize function.
net.initialize()
net.collect_params()
sequential0_ (
Parameter dense0_weight (shape=(256, -1), dtype=float32)
Parameter dense0_bias (shape=(256,), dtype=float32)
Parameter dense1_weight (shape=(10, -1), dtype=float32)
Parameter dense1_bias (shape=(10,), dtype=float32)
)
As we can see, nothing has changed. When input dimensions are unknown, calls to initialize do
not truly initialize the parameters. Instead, this call registers to MXNet that we wish (and optionally,
according to which distribution) to initialize the parameters.
214 Chapter 5. Deep Learning Computation
Next let us pass data through the network to make the framework finally initialize parameters.
X = np.random.uniform(size=(2, 20))
net(X)
net.collect_params()
sequential0_ (
Parameter dense0_weight (shape=(256, 20), dtype=float32)
Parameter dense0_bias (shape=(256,), dtype=float32)
Parameter dense1_weight (shape=(10, 256), dtype=float32)
Parameter dense1_bias (shape=(10,), dtype=float32)
)
As soon as we know the input dimensionality, 20, the framework can identify the shape of the
first layer?s weight matrix by plugging in the value of 20. Having recognized the first layer?s shape,
the framework proceeds to the second layer, and so on through the computational graph until all
shapes are known. Note that in this case, only the first layer requires deferred initialization, but
the framework initializes sequentially. Once all parameter shapes are known, the framework can
finally initialize the parameters.
Summary
� Deferred initialization can be convenient, allowing the framework to infer parameter shapes
automatically, making it easy to modify architectures and eliminating one common source
of errors.
� We can pass data through the model to make the framework finally initialize parameters.
Exercises
1. What happens if you specify the input dimensions to the first layer but not to subsequent
layers? Do you get immediate initialization?
2. What happens if you specify mismatching dimensions?
3. What would you need to do if you have input of varying dimensionality? Hint: look at the
parameter tying.
Discussions82
82 https://discuss.d2l.ai/t/280
5.3. Deferred Initialization 215
5.4 Custom Layers
One factor behind deep learning?s success is the availability of a wide range of layers that can be
composed in creative ways to design architectures suitable for a wide variety of tasks. For instance,
researchers have invented layers specifically for handling images, text, looping over sequential
data, and performing dynamic programming. Sooner or later, you will encounter or invent a layer
that does not exist yet in the deep learning framework. In these cases, you must build a custom
layer. In this section, we show you how.
5.4.1 Layers without Parameters
To start, we construct a custom layer that does not have any parameters of its own. This should
look familiar if you recall our introduction to block in Section 5.1. The following CenteredLayer
class simply subtracts the mean from its input. To build it, we simply need to inherit from the
base layer class and implement the forward propagation function.
from mxnet import gluon, np, npx
from mxnet.gluon import nn
npx.set_np()
class CenteredLayer(nn.Block):
def __init__(self, **kwargs):
super().__init__(**kwargs)
def forward(self, X):
return X - X.mean()
Let us verify that our layer works as intended by feeding some data through it.
layer = CenteredLayer()
layer(np.array([1, 2, 3, 4, 5]))
array([-2., -1., 0., 1., 2.])
We can now incorporate our layer as a component in constructing more complex models.
net = nn.Sequential()
net.add(nn.Dense(128), CenteredLayer())
net.initialize()
As an extra sanity check, we can send random data through the network and check that the mean
is in fact 0. Because we are dealing with floating point numbers, we may still see a very small
nonzero number due to quantization.
Y = net(np.random.uniform(size=(4, 8)))
Y.mean()
array(3.783498e-10)
216 Chapter 5. Deep Learning Computation
5.4.2 Layers with Parameters
Now that we know how to define simple layers, let us move on to defining layers with parameters
that can be adjusted through training. We can use built-in functions to create parameters, which
provide some basic housekeeping functionality. In particular, they govern access, initialization,
sharing, saving, and loading model parameters. This way, among other benefits, we will not need
to write custom serialization routines for every custom layer.
Now let us implement our own version of the fully-connected layer. Recall that this layer requires
two parameters, one to represent the weight and the other for the bias. In this implementation,
we bake in the ReLU activation as a default. This layer requires to input arguments: in_units and
units, which denote the number of inputs and outputs, respectively.
class MyDense(nn.Block):
def __init__(self, units, in_units, **kwargs):
super().__init__(**kwargs)
self.weight = self.params.get('weight', shape=(in_units, units))
self.bias = self.params.get('bias', shape=(units,))
def forward(self, x):
linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(
ctx=x.ctx)
return npx.relu(linear)
Next, we instantiate the MyDense class and access its model parameters.
dense = MyDense(units=3, in_units=5)
dense.params
mydense0_ (
Parameter mydense0_weight (shape=(5, 3), dtype=<class 'numpy.float32'>)
Parameter mydense0_bias (shape=(3,), dtype=<class 'numpy.float32'>)
)
We can directly carry out forward propagation calculations using custom layers.
dense.initialize()
dense(np.random.uniform(size=(2, 5)))
array([[0. , 0.01633355, 0. ],
[0. , 0.01581812, 0. ]])
We can also construct models using custom layers. Once we have that we can use it just like the
built-in fully-connected layer.
net = nn.Sequential()
net.add(MyDense(8, in_units=64),
MyDense(1, in_units=8))
net.initialize()
net(np.random.uniform(size=(2, 64)))
5.4. Custom Layers 217
array([[0.06508517],
[0.0615553 ]])
Summary
� We can design custom layers via the basic layer class. This allows us to define flexible new
layers that behave differently from any existing layers in the library.
� Once defined, custom layers can be invoked in arbitrary contexts and architectures.
� Layers can have local parameters, which can be created through built-in functions.
Exercises
1. Design a layer that takes an input ? and computes a tensor reduction, i.e., it returns yk =
i;j Wijkxixj .
2. Design a layer that returns the leading half of the Fourier coefficients of the data.
Discussions83
5.5 File I/O
So far we discussed how to process data and how to build, train, and test deep learning models.
However, at some point, we will hopefully be happy enough with the learned models that we will
want to save the results for later use in various contexts (perhaps even to make predictions in deployment).
Additionally, when running a long training process, the best practice is to periodically
save intermediate results (checkpointing) to ensure that we do not lose several days worth of computation
if we trip over the power cord of our server. Thus it is time to learn how to load and store
both individual weight vectors and entire models. This section addresses both issues.
5.5.1 Loading and Saving Tensors
For individual tensors, we can directly invoke the load and save functions to read and write them
respectively. Both functions require that we supply a name, and save requires as input the variable
to be saved.
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
x = np.arange(4)
npx.save('x-file', x)
We can now read the data from the stored file back into memory.
83 https://discuss.d2l.ai/t/58
218 Chapter 5. Deep Learning Computation
x2 = npx.load('x-file')
x2
[array([0., 1., 2., 3.])]
We can store a list of tensors and read them back into memory.
y = np.zeros(4)
npx.save('x-files', [x, y])
x2, y2 = npx.load('x-files')
(x2, y2)
(array([0., 1., 2., 3.]), array([0., 0., 0., 0.]))
We can even write and read a dictionary that maps from strings to tensors. This is convenient
when we want to read or write all the weights in a model.
mydict = {'x': x, 'y': y}
npx.save('mydict', mydict)
mydict2 = npx.load('mydict')
mydict2
{'x': array([0., 1., 2., 3.]), 'y': array([0., 0., 0., 0.])}
5.5.2 Loading and Saving Model Parameters
Saving individual weight vectors (or other tensors) is useful, but it gets very tedious if we want
to save (and later load) an entire model. After all, we might have hundreds of parameter groups
sprinkled throughout. For this reason the deep learning framework provides built-in functionalities
to load and save entire networks. An important detail to note is that this saves model parameters
and not the entire model. For example, if we have a 3-layer MLP, we need to specify the
architecture separately. The reason for this is that the models themselves can contain arbitrary
code, hence they cannot be serialized as naturally. Thus, in order to reinstate a model, we need
to generate the architecture in code and then load the parameters from disk. Let us start with our
familiar MLP.
class MLP(nn.Block):
def __init__(self, **kwargs):
super(MLP, self).__init__(**kwargs)
self.hidden = nn.Dense(256, activation='relu')
self.output = nn.Dense(10)
def forward(self, x):
return self.output(self.hidden(x))
net = MLP()
net.initialize()
X = np.random.uniform(size=(2, 20))
Y = net(X)
5.5. File I/O 219
Next, we store the parameters of the model as a file with the name �mlp.params�.
net.save_parameters('mlp.params')
To recover the model, we instantiate a clone of the original MLP model. Instead of randomly
initializing the model parameters, we read the parameters stored in the file directly.
clone = MLP()
clone.load_parameters('mlp.params')
Since both instances have the same model parameters, the computational result of the same input
X should be the same. Let us verify this.
Y_clone = clone(X)
Y_clone == Y
array([[ True, True, True, True, True, True, True, True, True,
True],
[ True, True, True, True, True, True, True, True, True,
True]])
Summary
� The save and load functions can be used to perform file I/O for tensor objects.
� We can save and load the entire sets of parameters for a network via a parameter dictionary.
� Saving the architecture has to be done in code rather than in parameters.
Exercises
1. Even if there is no need to deploy trained models to a different device, what are the practical
benefits of storing model parameters?
2. Assume that we want to reuse only parts of a network to be incorporated into a network
of a different architecture. How would you go about using, say the first two layers from a
previous network in a new network?
3. How would you go about saving the network architecture and parameters? What restrictions
would you impose on the architecture?
Discussions84
84 https://discuss.d2l.ai/t/60
220 Chapter 5. Deep Learning Computation
5.6 GPUs
In Table 1.5.1, we discussed the rapid growth of computation over the past two decades. In a
nutshell, GPU performance has increased by a factor of 1000 every decade since 2000. This offers
great opportunities but it also suggests a significant need to provide such performance.
In this section, we begin to discuss how to harness this computational performance for your research.
First by using single GPUs and at a later point, how to use multiple GPUs and multiple
servers (with multiple GPUs).
Specifically, we will discuss how to use a single NVIDIA GPU for calculations. First, make sure
you have at least one NVIDIA GPU installed. Then, download the NVIDIA driver and CUDA85 and
follow the prompts to set the appropriate path. Once these preparations are complete, the nvidiasmi
command can be used to view the graphics card information.
!nvidia-smi
Sun Aug 9 22:24:36 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67 Driver Version: 418.67 CUDA Version: 10.1 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+======================|
| 0 Tesla V100-SXM2... Off | 00000000:00:1B.0 Off | 0 |
| N/A 45C P0 51W / 300W | 0MiB / 16130MiB | 0% Default |
+-------------------------------+----------------------+----------------------+
| 1 Tesla V100-SXM2... Off | 00000000:00:1C.0 Off | 0 |
| N/A 39C P0 50W / 300W | 0MiB / 16130MiB | 0% Default |
+-------------------------------+----------------------+----------------------+
| 2 Tesla V100-SXM2... Off | 00000000:00:1D.0 Off | 0 |
| N/A 76C P0 189W / 300W | 2436MiB / 16130MiB | 73% Default |
+-------------------------------+----------------------+----------------------+
| 3 Tesla V100-SXM2... Off | 00000000:00:1E.0 Off | 0 |
| N/A 50C P0 42W / 300W | 11MiB / 16130MiB | 0% Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 2 47745 C ...iconda3/envs/d2l-en-master-1/bin/python 2425MiB |
+-----------------------------------------------------------------------------+
You might have noticed that a MXNet tensor looks almost identical to a NumPy ndarray. But there
are a few crucial differences. One of the key features that distinguishes MXNet from NumPy is its
support for diverse hardware devices.
In MXNet, every array has a context. So far, by default, all variables and associated computation
have been assigned to the CPU. Typically, other contexts might be various GPUs. Things can get
even hairier when we deploy jobs across multiple servers. By assigning arrays to contexts intelligently,
we can minimize the time spent transferring data between devices. For example, when
85 https://developer.nvidia.com/cuda-downloads
5.6. GPUs 221
training neural networks on a server with a GPU, we typically prefer for the model?s parameters
to live on the GPU.
Next, we need to confirm that the GPU version of MXNet is installed. If a CPU version of MXNet
is already installed, we need to uninstall it first. For example, use the pip uninstall mxnet command,
then install the corresponding MXNet version according to your CUDA version. Assuming
you have CUDA 10.0 installed, you can install the MXNet version that supports CUDA 10.0 via pip
install mxnet-cu100.
To run the programs in this section, you need at least two GPUs. Note that this might be extravagant
for most desktop computers but it is easily available in the cloud, e.g., by using the AWS EC2 multi-
GPU instances. Almost all other sections do not require multiple GPUs. Instead, this is simply to
illustrate how data flow between different devices.
5.6.1 Computing Devices
We can specify devices, such as CPUs and GPUs, for storage and calculation. By default, tensors
are created in the main memory and then use the CPU to calculate it.
In MXNet, the CPU and GPU can be indicated by cpu() and gpu(). It should be noted that cpu() (or
any integer in the parentheses) means all physical CPUs and memory. This means that MXNet?s
calculations will try to use all CPU cores. However, gpu() only represents one card and the corresponding
memory. If there are multiple GPUs, we use gpu(i) to represent the ith GPU (i starts
from 0). Also, gpu(0) and gpu() are equivalent.
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
npx.cpu(), npx.gpu(), npx.gpu(1)
(cpu(0), gpu(0), gpu(1))
We can query the number of available GPUs.
npx.num_gpus()
2
Now we define two convenient functions that allow us to run code even if the requested GPUs do
not exist.
def try_gpu(i=0): #@save
"""Return gpu(i) if exists, otherwise return cpu()."""
return npx.gpu(i) if npx.num_gpus() >= i + 1 else npx.cpu()
def try_all_gpus(): #@save
"""Return all available GPUs, or [cpu()] if no GPU exists."""
devices = [npx.gpu(i) for i in range(npx.num_gpus())]
return devices if devices else [npx.cpu()]
try_gpu(), try_gpu(10), try_all_gpus()
222 Chapter 5. Deep Learning Computation
(gpu(0), cpu(0), [gpu(0), gpu(1)])
5.6.2 Tensors and GPUs
By default, tensors are created on the CPU. We can query the device where the tensor is located.
x = np.array([1, 2, 3])
x.ctx
cpu(0)
It is important to note that whenever we want to operate on multiple terms, they need to be on the
same device. For instance, if we sum two tensors, we need to make sure that both arguments live
on the same device�otherwise the framework would not know where to store the result or even
how to decide where to perform the computation.
Storage on the GPU
There are several ways to store a tensor on the GPU. For example, we can specify a storage device
when creating a tensor. Next, we create the tensor variable X on the first gpu. The tensor created
on a GPU only consumes the memory of this GPU. We can use the nvidia-smi command to view
GPU memory usage. In general, we need to make sure that we do not create data that exceed the
GPU memory limit.
X = np.ones((2, 3), ctx=try_gpu())
X
array([[1., 1., 1.],
[1., 1., 1.]], ctx=gpu(0))
Assuming that you have at least two GPUs, the following code will create a random tensor on the
second GPU.
Y = np.random.uniform(size=(2, 3), ctx=try_gpu(1))
Y
array([[0.67478997, 0.07540122, 0.9956977 ],
[0.09488854, 0.415456 , 0.11231736]], ctx=gpu(1))
5.6. GPUs 223
Copying
If we want to compute X + Y, we need to decide where to perform this operation. For instance,
as shown in Fig. 5.6.1, we can transfer X to the second GPU and perform the operation there. Do
not simply add X and Y, since this will result in an exception. The runtime engine would not know
what to do: it cannot find data on the same device and it fails. Since Y lives on the second GPU, we
need to move X there before we can add the two.
Fig. 5.6.1: Copy data to perform an operation on the same device.
Z = X.copyto(try_gpu(1))
print(X)
print(Z)
[[1. 1. 1.]
[1. 1. 1.]] @gpu(0)
[[1. 1. 1.]
[1. 1. 1.]] @gpu(1)
Now that the data are on the same GPU (both Z and Y are), we can add them up.
Y + Z
array([[1.6747899, 1.0754012, 1.9956977],
[1.0948886, 1.415456 , 1.1123173]], ctx=gpu(1))
Imagine that your variable Z already lives on your second GPU. What happens if we still call Z.
copyto(gpu(1))? It will make a copy and allocate new memory, even though that variable already
lives on the desired device. There are times where, depending on the environment our code is
running in, two variables may already live on the same device. So we want to make a copy only
if the variables currently live in different devices. In these cases, we can call as_in_ctx. If the
variable already live in the specified device then this is a no-op. Unless you specifically want to
make a copy, as_in_ctx is the method of choice.
Z.as_in_ctx(try_gpu(1)) is Z
True
224 Chapter 5. Deep Learning Computation
Side Notes
People use GPUs to do machine learning because they expect them to be fast. But transferring
variables between devices is slow. So we want you to be 100% certain that you want to do something
slow before we let you do it. If the deep learning framework just did the copy automatically
without crashing then you might not realize that you had written some slow code.
Also, transferring data between devices (CPU, GPUs, and other machines) is something that is
much slower than computation. It also makes parallelization a lot more difficult, since we have to
wait for data to be sent (or rather to be received) before we can proceed with more operations. This
is why copy operations should be taken with great care. As a rule of thumb, many small operations
are much worse than one big operation. Moreover, several operations at a time are much better
than many single operations interspersed in the code unless you know what you are doing. This
is the case since such operations can block if one device has to wait for the other before it can do
something else. It is a bit like ordering your coffee in a queue rather than pre-ordering it by phone
and finding out that it is ready when you are.
Last, when we print tensors or convert tensors to the NumPy format, if the data is not in the main
memory, the framework will copy it to the main memory first, resulting in additional transmission
overhead. Even worse, it is now subject to the dreaded global interpreter lock that makes
everything wait for Python to complete.
5.6.3 Neural Networks and GPUs
Similarly, a neural network model can specify devices. The following code puts the model parameters
on the GPU.
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(ctx=try_gpu())
We will see many more examples of how to run models on GPUs in the following chapters, simply
since they will become somewhat more computationally intensive.
When the input is a tensor on the GPU, the model will calculate the result on the same GPU.
net(X)
array([[0.04995865],
[0.04995865]], ctx=gpu(0))
Let us confirm that the model parameters are stored on the same GPU.
net[0].weight.data().ctx
gpu(0)
In short, as long as all data and parameters are on the same device, we can learn models efficiently.
In the following chapters we will see several such examples.
5.6. GPUs 225
Summary
� We can specify devices for storage and calculation, such as the CPU or GPU. By default, data
are created in the main memory and then use the CPU for calculations.
� The deep learning framework requires all input data for calculation to be on the same device,
be it CPU or the same GPU.
� You can lose significant performance by moving data without care. A typical mistake is as
follows: computing the loss for every minibatch on the GPU and reporting it back to the user
on the command line (or logging it in a NumPy ndarray) will trigger a global interpreter lock
which stalls all GPUs. It is much better to allocate memory for logging inside the GPU and
only move larger logs.
Exercises
1. Try a larger computation task, such as the multiplication of large matrices, and see the difference
in speed between the CPU and GPU. What about a task with a small amount of calculations?
2. How should we read and write model parameters on the GPU?
3. Measure the time it takes to compute 1000 matrix-matrix multiplications of 100100 matrices
and log the Frobenius norm of the output matrix one result at a time vs. keeping a log on
the GPU and transferring only the final result.
4. Measure how much time it takes to perform two matrix-matrix multiplications on two GPUs
at the same time vs. in sequence on one GPU. Hint: you should see almost linear scaling.
Discussions86
86 https://discuss.d2l.ai/t/62
226 Chapter 5. Deep Learning Computation
6 | Convolutional Neural Networks
In earlier chapters, we came up against image data, for which each example consists of a twodimensional
grid of pixels. Depending on whether we are handling black-and-white or color images,
each pixel location might be associated with either one or multiple numerical values, respectively.
Until now, our way of dealing with this rich structure was deeply unsatisfying. We simply
discarded each image?s spatial structure by flattening them into one-dimensional vectors, feeding
them through a fully-connected MLP. Because these networks are invariant to the order of the features,
we could get similar results regardless of whether we preserve an order corresponding to
the spatial structure of the pixels or if we permute the columns of our design matrix before fitting
the MLP?s parameters. Preferably, we would leverage our prior knowledge that nearby pixels are
typically related to each other, to build efficient models for learning from image data.
This chapter introduces convolutional neural networks (CNNs), a powerful family of neural networks
that are designed for precisely this purpose. CNN-based architectures are now ubiquitous in the
field of computer vision, and have become so dominant that hardly anyone today would develop a
commercial application or enter a competition related to image recognition, object detection, or
semantic segmentation, without building off of this approach.
Modern CNNs, as they are called colloquially owe their design to inspirations from biology, group
theory, and a healthy dose of experimental tinkering. In addition to their sample efficiency in
achieving accurate models, CNNs tend to be computationally efficient, both because they require
fewer parameters than fully-connected architectures and because convolutions are easy to parallelize
across GPU cores. Consequently, practitioners often apply CNNs whenever possible, and
increasingly they have emerged as credible competitors even on tasks with a one-dimensional sequence
structure, such as audio, text, and time series analysis, where recurrent neural networks
are conventionally used. Some clever adaptations of CNNs have also brought them to bear on
graph-structured data and in recommender systems.
First, we will walk through the basic operations that comprise the backbone of all convolutional
networks. These include the convolutional layers themselves, nitty-gritty details including
padding and stride, the pooling layers used to aggregate information across adjacent spatial
regions, the use of multiple channels at each layer, and a careful discussion of the structure of
modern architectures. We will conclude the chapter with a full working example of LeNet, the
first convolutional network successfully deployed, long before the rise of modern deep learning.
In the next chapter, we will dive into full implementations of some popular and comparatively
recent CNN architectures whose designs represent most of the techniques commonly used by
modern practitioners.
227
6.1 From Fully-Connected Layers to Convolutions
To this day, the models that we have discussed so far remain appropriate options when we are
dealing with tabular data. By tabular, we mean that the data consist of rows corresponding to
examples and columns corresponding to features. With tabular data, we might anticipate that
the patterns we seek could involve interactions among the features, but we do not assume any
structure a priori concerning how the features interact.
Sometimes, we truly lack knowledge to guide the construction of craftier architectures. In these
cases, an MLP may be the best that we can do. However, for high-dimensional perceptual data,
such structure-less networks can grow unwieldy.
For instance, let us return to our running example of distinguishing cats from dogs. Say that we do
a thorough job in data collection, collecting an annotated dataset of one-megapixel photographs.
This means that each input to the network has one million dimensions. Even an aggressive reduction
to one thousand hidden dimensions would require a fully-connected layer characterized by
106  103 = 109 parameters. Unless we have lots of GPUs, a talent for distributed optimization,
and an extraordinary amount of patience, learning the parameters of this network may turn out
to be infeasible.
A careful reader might object to this argument on the basis that one megapixel resolution may not
be necessary. However, while we might be able to get away with one hundred thousand pixels,
our hidden layer of size 1000 grossly underestimates the number of hidden units that it takes to
learn good representations of images, so a practical system will still require billions of parameters.
Moreover, learning a classifier by fitting so many parameters might require collecting an
enormous dataset. And yet today both humans and computers are able to distinguish cats from
dogs quite well, seemingly contradicting these intuitions. That is because images exhibit rich
structure that can be exploited by humans and machine learning models alike. Convolutional
neural networks (CNNs) are one creative way that machine learning has embraced for exploiting
some of the known structure in natural images.
6.1.1 Invariance
Imagine that you want to detect an object in an image. It seems reasonable that whatever method
we use to recognize objects should not be overly concerned with the precise location of the object
in the image. Ideally, our system should exploit this knowledge. Pigs usually do not fly and
planes usually do not swim. Nonetheless, we should still recognize a pig were one to appear at the
top of the image. We can draw some inspiration here from the children?s game �Where?s Waldo�
(depicted in Fig. 6.1.1). The game consists of a number of chaotic scenes bursting with activities.
Waldo shows up somewhere in each, typically lurking in some unlikely location. The reader?s goal
is to locate him. Despite his characteristic outfit, this can be surprisingly difficult, due to the large
number of distractions. However, what Waldo looks like does not depend upon where Waldo is located.
We could sweep the image with a Waldo detector that could assign a score to each patch,
indicating the likelihood that the patch contains Waldo. CNNs systematize this idea of spatial invariance,
exploiting it to learn useful representations with fewer parameters.
228 Chapter 6. Convolutional Neural Networks
Fig. 6.1.1: An image of the �Where?s Waldo� game.
We can now make these intuitions more concrete by enumerating a few desiderata to guide our
design of a neural network architecture suitable for computer vision:
1. In the earliest layers, our network should respond similarly to the same patch, regardless of
where it appears in the image. This principle is called translation invariance.
2. The earliest layers of the network should focus on local regions, without regard for the contents
of the image in distant regions. This is the locality principle. Eventually, these local
representations can be aggregated to make predictions at the whole image level.
Let us see how this translates into mathematics.
6.1.2 Constraining the MLP
To start off, we can consider an MLP with two-dimensional images X as inputs and their immediate
hidden representations H similarly represented as matrices in mathematics and as twodimensional
tensors in code, where both X and H have the same shape. Let that sink in. We now
conceive of not only the inputs but also the hidden representations as possessing spatial structure.
Let [X]i;j and [H]i;j denote the pixel at location (i, j) in the input image and hidden representation,
respectively. Consequently, to have each of the hidden units receive input from each of the input
pixels, we would switch from using weight matrices (as we did previously in MLPs) to representing
our parameters as fourth-order weight tensors W. Suppose that U contains biases, we could
formally express the fully-connected layer as
[H]i;j = [U]i;j +
?
k
?
l
[W]i;j;k;l[X]k;l
= [U]i;j +
?
a
?
b
[V]i;j;a;b[X]i+a;j+b:
; (6.1.1)
where the switch from W to V is entirely cosmetic for now since there is a one-to-one correspondence
between coefficients in both fourth-order tensors. We simply re-index the subscripts (k; l)
such that k = i + a and l = j + b. In other words, we set [V]i;j;a;b = [W]i;j;i+a;j+b. The indices a and
6.1. From Fully-Connected Layers to Convolutions 229
b run over both positive and negative offsets, covering the entire image. For any given location (i,
j) in the hidden representation [H]i;j , we compute its value by summing over pixels in x, centered
around (i; j) and weighted by [V]i;j;a;b.
Translation Invariance
Now let us invoke the first principle established above: translation invariance. This implies that
a shift in the input X should simply lead to a shift in the hidden representation H. This is only
possible if V and U do not actually depend on (i; j), i.e., we have [V]i;j;a;b = [V]a;b and U is a constant,
say u. As a result, we can simplify the definition for H:
[H]i;j = u +
?
a
?
b
[V]a;b[X]i+a;j+b: (6.1.2)
This is a convolution! We are effectively weighting pixels at (i + a; j + b) in the vicinity of location
(i; j) with coefficients [V]a;b to obtain the value [H]i;j . Note that [V]a;b needs many fewer coefficients
than [V]i;j;a;b since it no longer depends on the location within the image. We have made significant
progress!
Locality
Now let us invoke the second principle: locality. As motivated above, we believe that we should
not have to look very far away from location (i; j) in order to glean relevant information to assess
what is going on at [H]i;j . This means that outside some range jaj > ? or jbj > ?, we should set
[V]a;b = 0. Equivalently, we can rewrite [H]i;j as
[H]i;j = u +
??
a=???
??
b=???
[V]a;b[X]i+a;j+b: (6.1.3)
Note that (6.1.3), in a nutshell, is a convolutional layer. Convolutional neural networks (CNNs) are
a special family of neural networks that contain convolutional layers. In the deep learning research
community, V is referred to as a convolution kernel, a filter, or simply the layer?s weights that
are often learnable parameters. When the local region is small, the difference as compared with
a fully-connected network can be dramatic. While previously, we might have required billions
of parameters to represent just a single layer in an image-processing network, we now typically
need just a few hundred, without altering the dimensionality of either the inputs or the hidden
representations. The price paid for this drastic reduction in parameters is that our features are
now translation invariant and that our layer can only incorporate local information, when determining
the value of each hidden activation. All learning depends on imposing inductive bias.
When that bias agrees with reality, we get sample-efficient models that generalize well to unseen
data. But of course, if those biases do not agree with reality, e.g., if images turned out not to be
translation invariant, our models might struggle even to fit our training data.
230 Chapter 6. Convolutional Neural Networks
6.1.3 Convolutions
Before going further, we should briefly review why the above operation is called a convolution. In
mathematics, the convolution between two functions, say f; g : Rd ! R is defined as
(f  g)(x) =
?
f(z)g(x ?? z)dz: (6.1.4)
That is, we measure the overlap between f and g when one function is �flipped� and shifted by x.
Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors from
the set of square summable infinite dimensional vectors with index running over Z we obtain the
following definition:
(f  g)(i) =
?
a
f(a)g(i ?? a): (6.1.5)
For two-dimensional tensors, we have a corresponding sum with indices (a; b) for f and (i??a; j??b)
for g, respectively:
(f  g)(i; j) =
?
a
?
b
f(a; b)g(i ?? a; j ?? b): (6.1.6)
This looks similar to (6.1.3), with one major difference. Rather than using (i+a; j+b), we are using
the difference instead. Note, though, that this distinction is mostly cosmetic since we can always
match the notation between (6.1.3) and (6.1.6). Our original definition in (6.1.3) more properly
describes a cross-correlation. We will come back to this in the following section.
6.1.4 �Where�s Waldo� Revisited
Returning to our Waldo detector, let us see what this looks like. The convolutional layer picks
windows of a given size and weighs intensities according to the filter V, as demonstrated in Fig.
6.1.2. We might aim to learn a model so that wherever the �waldoness� is highest, we should find
a peak in the hidden layer representations.
Fig. 6.1.2: Detect Waldo.
6.1. From Fully-Connected Layers to Convolutions 231
Channels
There is just one problem with this approach. So far, we blissfully ignored that images consist
of 3 channels: red, green, and blue. In reality, images are not two-dimensional objects but rather
third-order tensors, characterized by a height, width, and channel, e.g., with shape 102410243
pixels. While the first two of these axes concern spatial relationships, the third can be regarded
as assigning a multidimensional representation to each pixel location.
We thus index X as [X]i;j;k. The convolutional filter has to adapt accordingly. Instead of [V]a;b, we
now have [V]a;b;c.
Moreover, just as our input consists of a third-order tensor, it turns out to be a good idea to similarly
formulate our hidden representations as third-order tensors H. In other words, rather than just
having a single hidden representation corresponding to each spatial location, we want an entire
vector of hidden representations corresponding to each spatial location. We could think of the
hidden representations as comprising a number of two-dimensional grids stacked on top of each
other. As in the inputs, these are sometimes called channels. They are also sometimes called feature
maps, as each provides a spatialized set of learned features to the subsequent layer. Intuitively,
you might imagine that at lower layers that are closer to inputs, some channels could become
specialized to recognize edges while others could recognize textures.
To support multiple channels in both inputs (X) and hidden representations (H), we can add a
fourth coordinate to V: [V]a;b;c;d. Putting everything together we have:
[H]i;j;d =
??
a=???
??
b=???
?
c
[V]a;b;c;d[X]i+a;j+b;c; (6.1.7)
where d indexes the output channels in the hidden representations H. The subsequent convolutional
layer will go on to take a third-order tensor, H, as the input. Being more general, (6.1.7) is
the definition of a convolutional layer for multiple channels, where V is a kernel or filter of the
layer.
There are still many operations that we need to address. For instance, we need to figure out how to
combine all the hidden representations to a single output, e.g., whether there is a Waldo anywhere
in the image. We also need to decide how to compute things efficiently, how to combine multiple
layers, appropriate activation functions, and how to make reasonable design choices to yield
networks that are effective in practice. We turn to these issues in the remainder of the chapter.
Summary
� Translation invariance in images implies that all patches of an image will be treated in the
same manner.
� Locality means that only a small neighborhood of pixels will be used to compute the corresponding
hidden representations.
� In image processing, convolutional layers typically require many fewer parameters than
fully-connected layers.
� CNNS are a special family of neural networks that contain convolutional layers.
� Channels on input and output allow our model to capture multiple aspects of an image at
each spatial location.
232 Chapter 6. Convolutional Neural Networks
Exercises
1. Assume that the size of the convolution kernel is ? = 0. Show that in this case the convolution
kernel implements an MLP independently for each set of channels.
2. Why might translation invariance not be a good idea after all?
3. What problems must we deal with when deciding how to treat hidden representations corresponding
to pixel locations at the boundary of an image?
4. Describe an analogous convolutional layer for audio.
5. Do you think that convolutional layers might also be applicable for text data? Why or why
not?
6. Prove that f  g = g  f.
Discussions87
6.2 Convolutions for Images
Now that we understand how convolutional layers work in theory, we are ready to see how they
work in practice. Building on our motivation of convolutional neural networks as efficient architectures
for exploring structure in image data, we stick with images as our running example.
6.2.1 The Cross-Correlation Operation
Recall that strictly speaking, convolutional layers are a misnomer, since the operations they express
are more accurately described as cross-correlations. Based on our descriptions of convolutional
layers in Section 6.1, in such a layer, an input tensor and a kernel tensor are combined to
produce an output tensor through a cross-correlation operation.
Let us ignore channels for now and see how this works with two-dimensional data and hidden
representations. In Fig. 6.2.1, the input is a two-dimensional tensor with a height of 3 and width
of 3. We mark the shape of the tensor as 3  3 or (3, 3). The height and width of the kernel are
both 2. The shape of the kernel window (or convolution window) is given by the height and width of
the kernel (here it is 2  2).
Fig. 6.2.1: Two-dimensional cross-correlation operation. The shaded portions are the first output
element as well as the input and kernel tensor elements used for the output computation: 00+
1  1 + 3  2 + 4  3 = 19.
In the two-dimensional cross-correlation operation, we begin with the convolution window positioned
at the top-left corner of the input tensor and slide it across the input tensor, both from left
87 https://discuss.d2l.ai/t/64
6.2. Convolutions for Images 233
to right and top to bottom. When the convolution window slides to a certain position, the input
subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting
tensor is summed up yielding a single scalar value. This result gives the value of the output
tensor at the corresponding location. Here, the output tensor has a height of 2 and width of 2 and
the four elements are derived from the two-dimensional cross-correlation operation:
0  0 + 1  1 + 3  2 + 4  3 = 19;
1  0 + 2  1 + 4  2 + 5  3 = 25;
3  0 + 4  1 + 6  2 + 7  3 = 37;
4  0 + 5  1 + 7  2 + 8  3 = 43:
(6.2.1)
Note that along each axis, the output size is slightly smaller than the input size. Because the kernel
has width and height greater than one, we can only properly compute the cross-correlation for
locations where the kernel fits wholly within the image, the output size is given by the input size
nh  nw minus the size of the convolution kernel kh  kw via
(nh ?? kh + 1)  (nw ?? kw + 1): (6.2.2)
This is the case since we need enough space to �shift� the convolution kernel across the image.
Later we will see how to keep the size unchanged by padding the image with zeros around its
boundary so that there is enough space to shift the kernel. Next, we implement this process in
the corr2d function, which accepts an input tensor X and a kernel tensor K and returns an output
tensor Y.
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
from mxnet.gluon import nn
npx.set_np()
def corr2d(X, K): #@save
"""Compute 2D cross-correlation."""
h, w = K.shape
Y = np.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
for i in range(Y.shape[0]):
for j in range(Y.shape[1]):
Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))
return Y
We can construct the input tensor X and the kernel tensor K from Fig. 6.2.1 to validate the output
of the above implementation of the two-dimensional cross-correlation operation.
X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
K = np.array([[0.0, 1.0], [2.0, 3.0]])
corr2d(X, K)
array([[19., 25.],
[37., 43.]])
234 Chapter 6. Convolutional Neural Networks
6.2.2 Convolutional Layers
A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an
output. The two parameters of a convolutional layer are the kernel and the scalar bias. When
training models based on convolutional layers, we typically initialize the kernels randomly, just
as we would with a fully-connected layer.
We are now ready to implement a two-dimensional convolutional layer based on the corr2d function
defined above. In the __init__ constructor function, we declare weight and bias as the two
model parameters. The forward propagation function calls the corr2d function and adds the bias.
class Conv2D(nn.Block):
def __init__(self, kernel_size, **kwargs):
super().__init__(**kwargs)
self.weight = self.params.get('weight', shape=kernel_size)
self.bias = self.params.get('bias', shape=(1,))
def forward(self, x):
return corr2d(x, self.weight.data()) + self.bias.data()
In hw convolution or a hw convolution kernel, the height and width of the convolution kernel
are h and w, respectively. We also refer to a convolutional layer with a h  w convolution kernel
simply as a h  w convolutional layer.
6.2.3 Object Edge Detection in Images
Let us take a moment to parse a simple application of a convolutional layer: detecting the edge of
an object in an image by finding the location of the pixel change. First, we construct an �image�
of 6  8 pixels. The middle four columns are black (0) and the rest are white (1).
X = np.ones((6, 8))
X[:, 2:6] = 0
X
array([[1., 1., 0., 0., 0., 0., 1., 1.],
[1., 1., 0., 0., 0., 0., 1., 1.],
[1., 1., 0., 0., 0., 0., 1., 1.],
[1., 1., 0., 0., 0., 0., 1., 1.],
[1., 1., 0., 0., 0., 0., 1., 1.],
[1., 1., 0., 0., 0., 0., 1., 1.]])
Next, we construct a kernel K with a height of 1 and a width of 2. When we perform the crosscorrelation
operation with the input, if the horizontally adjacent elements are the same, the output
is 0. Otherwise, the output is non-zero.
K = np.array([[1.0, -1.0]])
We are ready to perform the cross-correlation operation with arguments X (our input) and K (our
kernel). As you can see, we detect 1 for the edge from white to black and -1 for the edge from black
to white. All other outputs take value 0.
6.2. Convolutions for Images 235
Y = corr2d(X, K)
Y
array([[ 0., 1., 0., 0., 0., -1., 0.],
[ 0., 1., 0., 0., 0., -1., 0.],
[ 0., 1., 0., 0., 0., -1., 0.],
[ 0., 1., 0., 0., 0., -1., 0.],
[ 0., 1., 0., 0., 0., -1., 0.],
[ 0., 1., 0., 0., 0., -1., 0.]])
We can now apply the kernel to the transposed image. As expected, it vanishes. The kernel K only
detects vertical edges.
corr2d(X.T, K)
array([[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.]])
6.2.4 Learning a Kernel
Designing an edge detector by finite differences [1, -1] is neat if we know this is precisely what
we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions,
it might be impossible to specify precisely what each filter should be doing manually.
Now let us see whether we can learn the kernel that generated Y from X by looking at the input�
output pairs only. We first construct a convolutional layer and initialize its kernel as a random
tensor. Next, in each iteration, we will use the squared error to compare Y with the output of
the convolutional layer. We can then calculate the gradient to update the kernel. For the sake of
simplicity, in the following we use the built-in class for two-dimensional convolutional layers and
ignore the bias.
# Construct a two-dimensional convolutional layer with 1 output channel and a
# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here
conv2d = nn.Conv2D(1, kernel_size=(1, 2), use_bias=False)
conv2d.initialize()
# The two-dimensional convolutional layer uses four-dimensional input and
# output in the format of (example, channel, height, width), where the batch
# size (number of examples in the batch) and the number of channels are both 1
X = X.reshape(1, 1, 6, 8)
Y = Y.reshape(1, 1, 6, 7)
for i in range(10):
with autograd.record():
Y_hat = conv2d(X)
(continues on next page)
236 Chapter 6. Convolutional Neural Networks
(continued from previous page)
l = (Y_hat - Y) ** 2
l.backward()
# Update the kernel
conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()
if (i + 1) % 2 == 0:
print(f'batch {i+1}, loss {float(l.sum()):.3f}')
batch 2, loss 4.949
batch 4, loss 0.831
batch 6, loss 0.140
batch 8, loss 0.024
batch 10, loss 0.004
Note that the error has dropped to a small value after 10 iterations. Now we will take a look at the
kernel tensor we learned.
d2l.reshape(conv2d.weight.data(), (1, 2))
array([[ 0.9895 , -0.9873705]])
Indeed, the learned kernel tensor is remarkably close to the kernel tensor K we defined earlier.
6.2.5 Cross-Correlation and Convolution
Recall our observation from Section 6.1 of the correspondence between the cross-correlation and
convolution operations. Here let us continue to consider two-dimensional convolutional layers.
What if such layers perform strict convolution operations as defined in (6.1.6) instead of crosscorrelations?
In order to obtain the output of the strict convolution operation, we only need to flip
the two-dimensional kernel tensor both horizontally and vertically, and then perform the crosscorrelation
operation with the input tensor.
It is noteworthy that since kernels are learned from data in deep learning, the outputs of convolutional
layers remain unaffected no matter such layers perform either the strict convolution
operations or the cross-correlation operations.
To illustrate this, suppose that a convolutional layer performs cross-correlation and learns the kernel
in Fig. 6.2.1, which is denoted as the matrix K here. Assuming that other conditions remain
unchanged, when this layer performs strict convolution instead, the learned kernel K? will be the
same as K after K? is flipped both horizontally and vertically. That is to say, when the convolutional
layer performs strict convolution for the input in Fig. 6.2.1 and K?, the same output in Fig.
6.2.1 (cross-correlation of the input and K) will be obtained.
In keeping with standard terminology with deep learning literature, we will continue to refer to the
cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different.
Besides, we use the term element to refer to an entry (or component) of any tensor representing a
layer representation or a convolution kernel.
6.2. Convolutions for Images 237
6.2.6 Feature Map and Receptive Field
As described in Section 6.1.4, the convolutional layer output in Fig. 6.2.1 is sometimes called a feature
map, as it can be regarded as the learned representations (features) in the spatial dimensions
(e.g., width and height) to the subsequent layer. In CNNs, for any element x of some layer, its receptive
field refers to all the elements (from all the previous layers) that may affect the calculation
of x during the forward propagation. Note that the receptive field may be larger than the actual
size of the input.
Let us continue to use Fig. 6.2.1 to explain the receptive field. Given the 2  2 convolution kernel,
the receptive field of the shaded output element (of value 19) is the four elements in the shaded
portion of the input. Now let us denote the 2  2 output as Y and consider a deeper CNN with an
additional 22 convolutional layer that takes Y as its input, outputting a single element z. In this
case, the receptive field of z on Y includes all the four elements of Y, while the receptive field on
the input includes all the nine input elements. Thus, when any element in a feature map needs a
larger receptive field to detect input features over a broader area, we can build a deeper network.
Summary
� The core computation of a two-dimensional convolutional layer is a two-dimensional crosscorrelation
operation. In its simplest form, this performs a cross-correlation operation on
the two-dimensional input data and the kernel, and then adds a bias.
� We can design a kernel to detect edges in images.
� We can learn the kernel?s parameters from data.
� With kernels learned from data, the outputs of convolutional layers remain unaffected
regardless of such layers? performed operations (either strict convolution or crosscorrelation).
� When any element in a feature map needs a larger receptive field to detect broader features
on the input, a deeper network can be considered.
Exercises
1. Construct an image X with diagonal edges.
1. What happens if you apply the kernel K in this section to it?
2. What happens if you transpose X?
3. What happens if you transpose K?
2. When you try to automatically find the gradient for the Conv2D class we created, what kind
of error message do you see?
3. How do you represent a cross-correlation operation as a matrix multiplication by changing
the input and kernel tensors?
4. Design some kernels manually.
1. What is the form of a kernel for the second derivative?
2. What is the kernel for an integral?
238 Chapter 6. Convolutional Neural Networks
3. What is the minimum size of a kernel to obtain a derivative of degree d?
Discussions88
6.3 Padding and Stride
In the previous example of Fig. 6.2.1, our input had both a height and width of 3 and our convolution
kernel had both a height and width of 2, yielding an output representation with dimension
22. As we generalized in :numref:sec_conv_layer, assuming that the input shape is nhnw and
the convolution kernel shape is khkw, then the output shape will be (nh??kh+1)(nw??kw+1).
Therefore, the output shape of the convolutional layer is determined by the shape of the input and
the shape of the convolution kernel.
In several cases, we incorporate techniques, including padding and strided convolutions, that affect
the size of the output. As motivation, note that since kernels generally have width and height
greater than 1, after applying many successive convolutions, we tend to wind up with outputs that
are considerably smaller than our input. If we start with a 240240 pixel image, 10 layers of 55
convolutions reduce the image to 200  200 pixels, slicing off 30% of the image and with it obliterating
any interesting information on the boundaries of the original image. Padding is the most
popular tool for handling this issue.
In other cases, we may want to reduce the dimensionality drastically, e.g., if we find the original
input resolution to be unwieldy. Strided convolutions are a popular technique that can help in these
instances.
6.3.1 Padding
As described above, one tricky issue when applying convolutional layers is that we tend to lose
pixels on the perimeter of our image. Since we typically use small kernels, for any given convolution,
we might only lose a few pixels, but this can add up as we apply many successive convolutional
layers. One straightforward solution to this problem is to add extra pixels of filler around
the boundary of our input image, thus increasing the effective size of the image. Typically, we
set the values of the extra pixels to zero. In Fig. 6.3.1, we pad a 3  3 input, increasing its size to
55. The corresponding output then increases to a 44 matrix. The shaded portions are the first
output element as well as the input and kernel tensor elements used for the output computation:
0  0 + 0  1 + 0  2 + 0  3 = 0.
Fig. 6.3.1: Two-dimensional cross-correlation with padding.
88 https://discuss.d2l.ai/t/65
6.3. Padding and Stride 239
In general, if we add a total of ph rows of padding (roughly half on top and half on bottom) and
a total of pw columns of padding (roughly half on the left and half on the right), the output shape
will be
(nh ?? kh + ph + 1)  (nw ?? kw + pw + 1): (6.3.1)
This means that the height and width of the output will increase by ph and pw, respectively.
In many cases, we will want to set ph = kh ?? 1 and pw = kw ?? 1 to give the input and output the
same height and width. This will make it easier to predict the output shape of each layer when
constructing the network. Assuming that kh is odd here, we will pad ph/2 rows on both sides of
the height. If kh is even, one possibility is to pad ?ph/2? rows on the top of the input and ?ph/2?
rows on the bottom. We will pad both sides of the width in the same way.
CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7.
Choosing odd kernel sizes has the benefit that we can preserve the spatial dimensionality while
padding with the same number of rows on top and bottom, and the same number of columns on
left and right.
Moreover, this practice of using odd kernels and padding to precisely preserve dimensionality
offers a clerical benefit. For any two-dimensional tensor X, when the kernel?s size is odd and the
number of padding rows and columns on all sides are the same, producing an output with the same
height and width as the input, we know that the output Y[i, j] is calculated by cross-correlation
of the input and convolution kernel with the window centered on X[i, j].
In the following example, we create a two-dimensional convolutional layer with a height and width
of 3 and apply 1 pixel of padding on all sides. Given an input with a height and width of 8, we find
that the height and width of the output is also 8.
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
# For convenience, we define a function to calculate the convolutional layer.
# This function initializes the convolutional layer weights and performs
# corresponding dimensionality elevations and reductions on the input and
# output
def comp_conv2d(conv2d, X):
conv2d.initialize()
# Here (1, 1) indicates that the batch size and the number of channels
# are both 1
X = X.reshape((1, 1) + X.shape)
Y = conv2d(X)
# Exclude the first two dimensions that do not interest us: examples and
# channels
return Y.reshape(Y.shape[2:])
# Note that here 1 row or column is padded on either side, so a total of 2
# rows or columns are added
conv2d = nn.Conv2D(1, kernel_size=3, padding=1)
X = np.random.uniform(size=(8, 8))
comp_conv2d(conv2d, X).shape
(8, 8)
240 Chapter 6. Convolutional Neural Networks
When the height and width of the convolution kernel are different, we can make the output and
input have the same height and width by setting different padding numbers for height and width.
# Here, we use a convolution kernel with a height of 5 and a width of 3. The
# padding numbers on either side of the height and width are 2 and 1,
# respectively
conv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
(8, 8)
6.3.2 Stride
When computing the cross-correlation, we start with the convolution window at the top-left corner
of the input tensor, and then slide it over all locations both down and to the right. In previous
examples, we default to sliding one element at a time. However, sometimes, either for computational
efficiency or because we wish to downsample, we move our window more than one element
at a time, skipping the intermediate locations.
We refer to the number of rows and columns traversed per slide as the stride. So far, we have used
strides of 1, both for height and width. Sometimes, we may want to use a larger stride. Fig. 6.3.2
shows a two-dimensional cross-correlation operation with a stride of 3 vertically and 2 horizontally.
The shaded portions are the output elements as well as the input and kernel tensor elements
used for the output computation: 00+01+12+23 = 8, 00+61+02+03 = 6.
We can see that when the second element of the first column is outputted, the convolution window
slides down three rows. The convolution window slides two columns to the right when the
second element of the first row is outputted. When the convolution window continues to slide
two columns to the right on the input, there is no output because the input element cannot fill the
window (unless we add another column of padding).
Fig. 6.3.2: Cross-correlation with strides of 3 and 2 for height and width, respectively.
In general, when the stride for the height is sh and the stride for the width is sw, the output shape
is
?(nh ?? kh + ph + sh)/sh?  ?(nw ?? kw + pw + sw)/sw?: (6.3.2)
If we set ph = kh ?? 1 and pw = kw ?? 1, then the output shape will be simplified to ?(nh + sh ??
1)/sh?  ?(nw + sw ?? 1)/sw?. Going a step further, if the input height and width are divisible by
the strides on the height and width, then the output shape will be (nh/sh)  (nw/sw).
Below, we set the strides on both the height and width to 2, thus halving the input height and width.
6.3. Padding and Stride 241
conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)
comp_conv2d(conv2d, X).shape
(4, 4)
Next, we will look at a slightly more complicated example.
conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))
comp_conv2d(conv2d, X).shape
(2, 2)
For the sake of brevity, when the padding number on both sides of the input height and width are
ph and pw respectively, we call the padding (ph; pw). Specifically, when ph = pw = p, the padding is
p. When the strides on the height and width are sh and sw, respectively, we call the stride (sh; sw).
Specifically, when sh = sw = s, the stride is s. By default, the padding is 0 and the stride is 1.
In practice, we rarely use inhomogeneous strides or padding, i.e., we usually have ph = pw and
sh = sw.
Summary
� Padding can increase the height and width of the output. This is often used to give the output
the same height and width as the input.
� The stride can reduce the resolution of the output, for example reducing the height and width
of the output to only 1/n of the height and width of the input (n is an integer greater than 1).
� Padding and stride can be used to adjust the dimensionality of the data effectively.
Exercises
1. For the last example in this section, use mathematics to calculate the output shape to see if
it is consistent with the experimental result.
2. Try other padding and stride combinations on the experiments in this section.
3. For audio signals, what does a stride of 2 correspond to?
4. What are the computational benefits of a stride larger than 1?
Discussions89
89 https://discuss.d2l.ai/t/67
242 Chapter 6. Convolutional Neural Networks
6.4 Multiple Input and Multiple Output Channels
While we have described the multiple channels that comprise each image (e.g., color images have
the standard RGB channels to indicate the amount of red, green and blue) and convolutional layers
for multiple channels in Section 6.1.4, until now, we simplified all of our numerical examples by
working with just a single input and a single output channel. This has allowed us to think of our
inputs, convolution kernels, and outputs each as two-dimensional tensors.
When we add channels into the mix, our inputs and hidden representations both become threedimensional
tensors. For example, each RGB input image has shape 3hw. We refer to this axis,
with a size of 3, as the channel dimension. In this section, we will take a deeper look at convolution
kernels with multiple input and multiple output channels.
6.4.1 Multiple Input Channels
When the input data contain multiple channels, we need to construct a convolution kernel with
the same number of input channels as the input data, so that it can perform cross-correlation with
the input data. Assuming that the number of channels for the input data is ci, the number of input
channels of the convolution kernel also needs to be ci. If our convolution kernel?s window shape
is kh  kw, then when ci = 1, we can think of our convolution kernel as just a two-dimensional
tensor of shape kh  kw.
However, when ci > 1, we need a kernel that contains a tensor of shape kh  kw for every input
channel. Concatenating these ci tensors together yields a convolution kernel of shape cikhkw.
Since the input and convolution kernel each have ci channels, we can perform a cross-correlation
operation on the two-dimensional tensor of the input and the two-dimensional tensor of the convolution
kernel for each channel, adding the ci results together (summing over the channels) to
yield a two-dimensional tensor. This is the result of a two-dimensional cross-correlation between
a multi-channel input and a multi-input-channel convolution kernel.
In Fig. 6.4.1, we demonstrate an example of a two-dimensional cross-correlation with two input
channels. The shaded portions are the first output element as well as the input and kernel tensor
elements used for the output computation: (11+22+43+54)+(00+11+32+43) = 56.
Fig. 6.4.1: Cross-correlation computation with 2 input channels.
To make sure we really understand what is going on here, we can implement cross-correlation
operations with multiple input channels ourselves. Notice that all we are doing is performing one
cross-correlation operation per channel and then adding up the results.
6.4. Multiple Input and Multiple Output Channels 243
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
def corr2d_multi_in(X, K):
# First, iterate through the 0th dimension (channel dimension) of `X` and
# `K`. Then, add them together
return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
We can construct the input tensor X and the kernel tensor K corresponding to the values in Fig.
6.4.1 to validate the output of the cross-correlation operation.
X = np.array([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = np.array([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])
corr2d_multi_in(X, K)
array([[ 56., 72.],
[104., 120.]])
6.4.2 Multiple Output Channels
Regardless of the number of input channels, so far we always ended up with one output channel.
However, as we discussed in Section 6.1.4, it turns out to be essential to have multiple channels
at each layer. In the most popular neural network architectures, we actually increase the channel
dimension as we go higher up in the neural network, typically downsampling to trade off spatial
resolution for greater channel depth. Intuitively, you could think of each channel as responding to
some different set of features. Reality is a bit more complicated than the most naive interpretations
of this intuition since representations are not learned independent but are rather optimized
to be jointly useful. So it may not be that a single channel learns an edge detector but rather that
some direction in channel space corresponds to detecting edges.
Denote by ci and co the number of input and output channels, respectively, and let kh and kw
be the height and width of the kernel. To get an output with multiple channels, we can create a
kernel tensor of shape ci  kh  kw for every output channel. We concatenate them on the output
channel dimension, so that the shape of the convolution kernel is co  ci  kh  kw. In crosscorrelation
operations, the result on each output channel is calculated from the convolution kernel
corresponding to that output channel and takes input from all channels in the input tensor.
We implement a cross-correlation function to calculate the output of multiple channels as shown
below.
def corr2d_multi_in_out(X, K):
# Iterate through the 0th dimension of `K`, and each time, perform
# cross-correlation operations with input `X`. All of the results are
# stacked together
return np.stack([corr2d_multi_in(X, k) for k in K], 0)
We construct a convolution kernel with 3 output channels by concatenating the kernel tensor K
with K+1 (plus one for each element in K) and K+2.
244 Chapter 6. Convolutional Neural Networks
K = np.stack((K, K + 1, K + 2), 0)
K.shape
(3, 2, 2, 2)
Below, we perform cross-correlation operations on the input tensor X with the kernel tensor K.
Now the output contains 3 channels. The result of the first channel is consistent with the result of
the previous input tensor X and the multi-input channel, single-output channel kernel.
corr2d_multi_in_out(X, K)
array([[[ 56., 72.],
[104., 120.]],
[[ 76., 100.],
[148., 172.]],
[[ 96., 128.],
[192., 224.]]])
6.4.3 1  1 Convolutional Layer
At first, a 1  1 convolution, i.e., kh = kw = 1, does not seem to make much sense. After all, a
convolution correlates adjacent pixels. A 11 convolution obviously does not. Nonetheless, they
are popular operations that are sometimes included in the designs of complex deep networks. Let
us see in some detail what it actually does.
Because the minimum window is used, the 1  1 convolution loses the ability of larger convolutional
layers to recognize patterns consisting of interactions among adjacent elements in the
height and width dimensions. The only computation of the 11 convolution occurs on the channel
dimension.
Fig. 6.4.2 shows the cross-correlation computation using the 11 convolution kernel with 3 input
channels and 2 output channels. Note that the inputs and outputs have the same height and width.
Each element in the output is derived from a linear combination of elements at the same position in
the input image. You could think of the 11 convolutional layer as constituting a fully-connected
layer applied at every single pixel location to transform the ci corresponding input values into co
output values. Because this is still a convolutional layer, the weights are tied across pixel location.
Thus the 1  1 convolutional layer requires co  ci weights (plus the bias).
Fig. 6.4.2: The cross-correlation computation uses the 11 convolution kernel with 3 input channels
and 2 output channels. The input and output have the same height and width.
6.4. Multiple Input and Multiple Output Channels 245
Let us check whether this works in practice: we implement a 1  1 convolution using a fullyconnected
layer. The only thing is that we need to make some adjustments to the data shape
before and after the matrix multiplication.
def corr2d_multi_in_out_1x1(X, K):
c_i, h, w = X.shape
c_o = K.shape[0]
X = X.reshape((c_i, h * w))
K = K.reshape((c_o, c_i))
Y = np.dot(K, X) # Matrix multiplication in the fully-connected layer
return Y.reshape((c_o, h, w))
When performing 1  1 convolution, the above function is equivalent to the previously implemented
cross-correlation function corr2d_multi_in_out. Let us check this with some sample
data.
X = np.random.normal(0, 1, (3, 3, 3))
K = np.random.normal(0, 1, (2, 3, 1, 1))
Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
assert float(d2l.reduce_sum(np.abs(Y1 - Y2))) < 1e-6
Summary
� Multiple channels can be used to extend the model parameters of the convolutional layer.
� The 1  1 convolutional layer is equivalent to the fully-connected layer, when applied on a
per pixel basis.
� The 1  1 convolutional layer is typically used to adjust the number of channels between
network layers and to control model complexity.
Exercises
1. Assume that we have two convolution kernels of size k1 and k2, respectively (with no nonlinearity
in between).
1. Prove that the result of the operation can be expressed by a single convolution.
2. What is the dimensionality of the equivalent single convolution?
3. Is the converse true?
2. Assume an input of shape ci  h  w and a convolution kernel of shape co  ci  kh  kw,
padding of (ph; pw), and stride of (sh; sw).
1. What is the computational cost (multiplications and additions) for the forward propagation?
2. What is the memory footprint?
3. What is the memory footprint for the backward computation?
4. What is the computational cost for the backpropagation?
246 Chapter 6. Convolutional Neural Networks
3. By what factor does the number of calculations increase if we double the number of input
channels ci and the number of output channels co? What happens if we double the padding?
4. If the height and width of a convolution kernel is kh = kw = 1, what is the computational
complexity of the forward propagation?
5. Are the variables Y1 and Y2 in the last example of this section exactly the same? Why?
6. How would you implement convolutions using matrix multiplication when the convolution
window is not 1  1?
Discussions90
6.5 Pooling
Often, as we process images, we want to gradually reduce the spatial resolution of our hidden
representations, aggregating information so that the higher up we go in the network, the larger
the receptive field (in the input) to which each hidden node is sensitive.
Often our ultimate task asks some global question about the image, e.g., does it contain a cat? So
typically the units of our final layer should be sensitive to the entire input. By gradually aggregating
information, yielding coarser and coarser maps, we accomplish this goal of ultimately learning
a global representation, while keeping all of the advantages of convolutional layers at the intermediate
layers of processing.
Moreover, when detecting lower-level features, such as edges (as discussed in Section 6.2), we
often want our representations to be somewhat invariant to translation. For instance, if we take
the image X with a sharp delineation between black and white and shift the whole image by one
pixel to the right, i.e., Z[i, j] = X[i, j + 1], then the output for the new image Z might be vastly
different. The edge will have shifted by one pixel. In reality, objects hardly ever occur exactly at
the same place. In fact, even with a tripod and a stationary object, vibration of the camera due to
the movement of the shutter might shift everything by a pixel or so (high-end cameras are loaded
with special features to address this problem).
This section introduces pooling layers, which serve the dual purposes of mitigating the sensitivity
of convolutional layers to location and of spatially downsampling representations.
6.5.1 Maximum Pooling and Average Pooling
Like convolutional layers, pooling operators consist of a fixed-shape window that is slid over all
regions in the input according to its stride, computing a single output for each location traversed
by the fixed-shape window (sometimes known as the pooling window). However, unlike the crosscorrelation
computation of the inputs and kernels in the convolutional layer, the pooling layer
contains no parameters (there is no kernel). Instead, pooling operators are deterministic, typically
calculating either the maximum or the average value of the elements in the pooling window. These
operations are called maximum pooling (max pooling for short) and average pooling, respectively.
In both cases, as with the cross-correlation operator, we can think of the pooling window as starting
from the top left of the input tensor and sliding across the input tensor from left to right and
90 https://discuss.d2l.ai/t/69
6.5. Pooling 247
top to bottom. At each location that the pooling window hits, it computes the maximum or average
value of the input subtensor in the window, depending on whether max or average pooling is
employed.
Fig. 6.5.1: Maximum pooling with a pooling window shape of 2  2. The shaded portions are
the first output element as well as the input tensor elements used for the output computation:
max(0; 1; 3; 4) = 4.
The output tensor in Fig. 6.5.1 has a height of 2 and a width of 2. The four elements are derived
from the maximum value in each pooling window:
max(0; 1; 3; 4) = 4;
max(1; 2; 4; 5) = 5;
max(3; 4; 6; 7) = 7;
max(4; 5; 7; 8) = 8:
(6.5.1)
A pooling layer with a pooling window shape of p  q is called a p  q pooling layer. The pooling
operation is called p  q pooling.
Let us return to the object edge detection example mentioned at the beginning of this section.
Now we will use the output of the convolutional layer as the input for 22 maximum pooling. Set
the convolutional layer input as X and the pooling layer output as Y. Whether or not the values of
X[i, j] and X[i, j + 1] are different, or X[i, j + 1] and X[i, j + 2] are different, the pooling
layer always outputs Y[i, j] = 1. That is to say, using the 2  2 maximum pooling layer, we can
still detect if the pattern recognized by the convolutional layer moves no more than one element
in height or width.
In the code below, we implement the forward propagation of the pooling layer in the pool2d function.
This function is similar to the corr2d function in Section 6.2. However, here we have no
kernel, computing the output as either the maximum or the average of each region in the input.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
def pool2d(X, pool_size, mode='max'):
p_h, p_w = pool_size
Y = np.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
for i in range(Y.shape[0]):
for j in range(Y.shape[1]):
if mode == 'max':
Y[i, j] = X[i: i + p_h, j: j + p_w].max()
elif mode == 'avg':
Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
return Y
248 Chapter 6. Convolutional Neural Networks
We can construct the input tensor X in Fig. 6.5.1 to validate the output of the two-dimensional
maximum pooling layer.
X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
pool2d(X, (2, 2))
array([[4., 5.],
[7., 8.]])
Also, we experiment with the average pooling layer.
pool2d(X, (2, 2), 'avg')
array([[2., 3.],
[5., 6.]])
6.5.2 Padding and Stride
As with convolutional layers, pooling layers can also change the output shape. And as before, we
can alter the operation to achieve a desired output shape by padding the input and adjusting the
stride. We can demonstrate the use of padding and strides in pooling layers via the built-in twodimensional
maximum pooling layer from the deep learning framework. We first construct an
input tensor X whose shape has four dimensions, where the number of examples and number of
channels are both 1.
X = d2l.reshape(np.arange(16, dtype=np.float32), (1, 1, 4, 4))
X
array([[[[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.]]]])
By default, the stride and the pooling window in the instance from the framework?s built-in class
have the same shape. Below, we use a pooling window of shape (3, 3), so we get a stride shape
of (3, 3) by default.
pool2d = nn.MaxPool2D(3)
# Because there are no model parameters in the pooling layer, we do not need
# to call the parameter initialization function
pool2d(X)
array([[[[10.]]]])
The stride and padding can be manually specified.
pool2d = nn.MaxPool2D(3, padding=1, strides=2)
pool2d(X)
6.5. Pooling 249
array([[[[ 5., 7.],
[13., 15.]]]])
Of course, we can specify an arbitrary rectangular pooling window and specify the padding and
stride for height and width, respectively.
pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))
pool2d(X)
array([[[[ 0., 3.],
[ 8., 11.],
[12., 15.]]]])
6.5.3 Multiple Channels
When processing multi-channel input data, the pooling layer pools each input channel separately,
rather than summing the inputs up over channels as in a convolutional layer. This means that the
number of output channels for the pooling layer is the same as the number of input channels.
Below, we will concatenate tensors X and X + 1 on the channel dimension to construct an input
with 2 channels.
X = np.concatenate((X, X + 1), 1)
X
array([[[[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.]],
[[ 1., 2., 3., 4.],
[ 5., 6., 7., 8.],
[ 9., 10., 11., 12.],
[13., 14., 15., 16.]]]])
As we can see, the number of output channels is still 2 after pooling.
pool2d = nn.MaxPool2D(3, padding=1, strides=2)
pool2d(X)
array([[[[ 5., 7.],
[13., 15.]],
[[ 6., 8.],
[14., 16.]]]])
250 Chapter 6. Convolutional Neural Networks
Summary
� Taking the input elements in the pooling window, the maximum pooling operation assigns
the maximum value as the output and the average pooling operation assigns the average
value as the output.
� One of the major benefits of a pooling layer is to alleviate the excessive sensitivity of the
convolutional layer to location.
� We can specify the padding and stride for the pooling layer.
� Maximum pooling, combined with a stride larger than 1 can be used to reduce the spatial
dimensions (e.g., width and height).
� The pooling layer?s number of output channels is the same as the number of input channels.
Exercises
1. Can you implement average pooling as a special case of a convolution layer? If so, do it.
2. Can you implement max pooling as a special case of a convolution layer? If so, do it.
3. What is the computational cost of the pooling layer? Assume that the input to the pooling
layer is of size chw, the pooling window has a shape of phpw with a padding of (ph; pw)
and a stride of (sh; sw).
4. Why do you expect maximum pooling and average pooling to work differently?
5. Do we need a separate minimum pooling layer? Can you replace it with another operation?
6. Is there another operation between average and maximum pooling that you could consider
(hint: recall the softmax)? Why might it not be so popular?
Discussions91
6.6 Convolutional Neural Networks (LeNet)
We now have all the ingredients required to assemble a fully-functional CNN. In our earlier encounter
with image data, we applied a softmax regression model (Section 3.6) and an MLP model
(Section 4.2) to pictures of clothing in the Fashion-MNIST dataset. To make such data amenable
to softmax regression and MLPs, we first flattened each image from a 28  28 matrix into a fixedlength
784-dimensional vector, and thereafter processed them with fully-connected layers. Now
that we have a handle on convolutional layers, we can retain the spatial structure in our images. As
an additional benefit of replacing fully-connected layers with convolutional layers, we will enjoy
more parsimonious models that require far fewer parameters.
In this section, we will introduce LeNet, among the first published CNNs to capture wide attention
for its performance on computer vision tasks. The model was introduced by (and named for) Yann
LeCun, then a researcher at AT&T Bell Labs, for the purpose of recognizing handwritten digits
in images (LeCun et al., 1998). This work represented the culmination of a decade of research
developing the technology. In 1989, LeCun published the first study to successfully train CNNs via
backpropagation.
91 https://discuss.d2l.ai/t/71
6.6. Convolutional Neural Networks (LeNet) 251
At the time LeNet achieved outstanding results matching the performance of support vector machines,
then a dominant approach in supervised learning. LeNet was eventually adapted to recognize
digits for processing deposits in ATM machines. To this day, some ATMs still run the code
that Yann and his colleague Leon Bottou wrote in the 1990s!
6.6.1 LeNet
At a high level, LeNet (LeNet-5) consists of two parts: (i) a convolutional encoder consisting of
two convolutional layers; and (ii) a dense block consisting of three fully-connected layers; The
architecture is summarized in Fig. 6.6.1.
Fig. 6.6.1: Data flow in LeNet. The input is a handwritten digit, the output a probability over 10
possible outcomes.
The basic units in each convolutional block are a convolutional layer, a sigmoid activation function,
and a subsequent average pooling operation. Note that while ReLUs and max-pooling work
better, these discoveries had not yet been made in the 1990s. Each convolutional layer uses a 55
kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number
of two-dimensional feature maps, typically increasing the number of channels. The first convolutional
layer has 6 output channels, while the second has 16. Each 22 pooling operation (stride 2)
reduces dimensionality by a factor of 4 via spatial downsampling. The convolutional block emits
an output with shape given by (batch size, number of channel, height, width).
In order to pass output from the convolutional block to the dense block, we must flatten each
example in the minibatch. In other words, we take this four-dimensional input and transform
it into the two-dimensional input expected by fully-connected layers: as a reminder, the twodimensional
representation that we desire has uses the first dimension to index examples in the
minibatch and the second to give the flat vector representation of each example. LeNet?s dense
block has three fully-connected layers, with 120, 84, and 10 outputs, respectively. Because we
are still performing classification, the 10-dimensional output layer corresponds to the number of
possible output classes.
While getting to the point where you truly understand what is going on inside LeNet may have
taken a bit of work, hopefully the following code snippet will convince you that implementing such
models with modern deep learning frameworks is remarkably simple. We need only to instantiate
a Sequential block and chain together the appropriate layers.
252 Chapter 6. Convolutional Neural Networks
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
net = nn.Sequential()
net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='sigmoid'),
nn.AvgPool2D(pool_size=2, strides=2),
nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
nn.AvgPool2D(pool_size=2, strides=2),
# `Dense` will transform an input of the shape (batch size, number of
# channels, height, width) into an input of the shape (batch size,
# number of channels * height * width) automatically by default
nn.Dense(120, activation='sigmoid'),
nn.Dense(84, activation='sigmoid'),
nn.Dense(10))
We took a small liberty with the original model, removing the Gaussian activation in the final
layer. Other than that, this network matches the original LeNet-5 architecture.
By passing a single-channel (black and white) 2828 image through the network and printing the
output shape at each layer, we can inspect the model to make sure that its operations line up with
what we expect from Fig. 6.6.2.
Fig. 6.6.2: Compressed notation for LeNet-5.
X = np.random.uniform(size=(1, 1, 28, 28))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)
6.6. Convolutional Neural Networks (LeNet) 253
conv0 output shape: (1, 6, 28, 28)
pool0 output shape: (1, 6, 14, 14)
conv1 output shape: (1, 16, 10, 10)
pool1 output shape: (1, 16, 5, 5)
dense0 output shape: (1, 120)
dense1 output shape: (1, 84)
dense2 output shape: (1, 10)
Note that the height and width of the representation at each layer throughout the convolutional
block is reduced (compared with the previous layer). The first convolutional layer uses 2 pixels of
padding to compensate for the reduction in height and width that would otherwise result from using
a 55 kernel. In contrast, the second convolutional layer forgoes padding, and thus the height
and width are both reduced by 4 pixels. As we go up the stack of layers, the number of channels
increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and 16 after
the second convolutional layer. However, each pooling layer halves the height and width. Finally,
each fully-connected layer reduces dimensionality, finally emitting an output whose dimension
matches the number of classes.
6.6.2 Training
Now that we have implemented the model, let us run an experiment to see how LeNet fares on
Fashion-MNIST.
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
While CNNs have fewer parameters, they can still be more expensive to compute than similarly
deep MLPs because each parameter participates in many more multiplications. If you have access
to a GPU, this might be a good time to put it into action to speed up training.
For evaluation, we need to make a slight modification to the evaluate_accuracy function that we
described in Section 3.6. Since the full dataset is in the main memory, we need to copy it to the
GPU memory before the model uses GPU to compute with the dataset.
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
"""Compute the accuracy for a model on a dataset using a GPU."""
if not device: # Query the first device where the first parameter is on
device = list(net.collect_params().values())[0].list_ctx()[0]
# No. of correct predictions, no. of predictions
metric = d2l.Accumulator(2)
for X, y in data_iter:
X, y = X.as_in_ctx(device), y.as_in_ctx(device)
metric.add(d2l.accuracy(net(X), y), y.size)
return metric[0]/metric[1]
We also need to update our training function to deal with GPUs. Unlike the train_epoch_ch3 defined
in Section 3.6, we now need to move each minibatch of data to our designated device (hopefully,
the GPU) prior to making the forward and backward propagations.
The training function train_ch6 is also similar to train_ch3 defined in Section 3.6. Since we will
be implementing networks with many layers going forward, we will rely primarily on high-level
APIs. The following training function assumes a model created from high-level APIs as input and
254 Chapter 6. Convolutional Neural Networks
is optimized accordingly. We initialize the model parameters on the device indicated by the device
argument, using Xavier initialization as introduced in Section 4.8.2. Just as with MLPs, our loss
function is cross-entropy, and we minimize it via minibatch stochastic gradient descent. Since
each epoch takes tens of seconds to run, we visualize the training loss more frequently.
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr,
device=d2l.try_gpu()):
"""Train a model with a GPU (defined in Chapter 6)."""
net.initialize(force_reinit=True, ctx=device, init=init.Xavier())
loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(),
'sgd', {'learning_rate': lr})
animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
legend=['train loss', 'train acc', 'test acc'])
timer = d2l.Timer()
for epoch in range(num_epochs):
# Sum of training loss, sum of training accuracy, no. of examples
metric = d2l.Accumulator(3)
for i, (X, y) in enumerate(train_iter):
timer.start()
# Here is the major difference compared with `d2l.train_epoch_ch3`
X, y = X.as_in_ctx(device), y.as_in_ctx(device)
with autograd.record():
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
trainer.step(X.shape[0])
metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])
timer.stop()
train_loss = metric[0] / metric[2]
train_acc = metric[1] / metric[2]
if (i + 1) % 50 == 0:
animator.add(epoch + i / len(train_iter),
(train_loss, train_acc, None))
test_acc = evaluate_accuracy_gpu(net, test_iter)
animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {train_loss:.3f}, train acc {train_acc:.3f}, '
f'test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(device)}')
Now let us train and evaluate the LeNet-5 model.
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.471, train acc 0.822, test acc 0.824
49576.0 examples/sec on gpu(0)
6.6. Convolutional Neural Networks (LeNet) 255
Summary
� A CNN is a network that employs convolutional layers.
� In a CNN, we interleave convolutions, nonlinearities, and (often) pooling operations.
� In a CNN, convolutional layers are typically arranged so that they gradually decrease the
spatial resolution of the representations, while increasing the number of channels.
� In traditional CNNs, the representations encoded by the convolutional blocks are processed
by one or more fully-connected layers prior to emitting output.
� LeNet was arguably the first successful deployment of such a network.
Exercises
1. Replace the average pooling with max pooling. What happens?
2. Try to construct a more complex network based on LeNet to improve its accuracy.
1. Adjust the convolution window size.
2. Adjust the number of output channels.
3. Adjust the activation function (e.g., ReLU).
4. Adjust the number of convolution layers.
5. Adjust the number of fully connected layers.
6. Adjust the learning rates and other training details (e.g., initialization and number of
epochs.)
3. Try out the improved network on the original MNIST dataset.
4. Display the activations of the first and second layer of LeNet for different inputs (e.g.,
sweaters and coats).
Discussions92
92 https://discuss.d2l.ai/t/73
256 Chapter 6. Convolutional Neural Networks
7 | Modern Convolutional Neural Networks
Now that we understand the basics of wiring together CNNs, we will take you through a tour of
modern CNN architectures. In this chapter, each section corresponds to a significant CNN architecture
that was at some point (or currently) the base model upon which many research projects
and deployed systems were built. Each of these networks was briefly a dominant architecture and
many were winners or runners-up in the ImageNet competition, which has served as a barometer
of progress on supervised learning in computer vision since 2010.
These models include AlexNet, the first large-scale network deployed to beat conventional computer
vision methods on a large-scale vision challenge; the VGG network, which makes use of a
number of repeating blocks of elements; the network in network (NiN) which convolves whole
neural networks patch-wise over inputs; GoogLeNet, which uses networks with parallel concatenations;
residual networks (ResNet), which remain the most popular off-the-shelf architecture in
computer vision; and densely connected networks (DenseNet), which are expensive to compute
but have set some recent benchmarks.
While the idea of deep neural networks is quite simple (stack together a bunch of layers), performance
can vary wildly across architectures and hyperparameter choices. The neural networks
described in this chapter are the product of intuition, a few mathematical insights, and a whole
lot of trial and error. We present these models in chronological order, partly to convey a sense
of the history so that you can form your own intuitions about where the field is heading and perhaps
develop your own architectures. For instance, batch normalization and residual connections
described in this chapter have offered two popular ideas for training and designing deep models.
7.1 Deep Convolutional Neural Networks (AlexNet)
Although CNNs were well known in the computer vision and machine learning communities following
the introduction of LeNet, they did not immediately dominate the field. Although LeNet
achieved good results on early small datasets, the performance and feasibility of training CNNs on
larger, more realistic datasets had yet to be established. In fact, for much of the intervening time
between the early 1990s and the watershed results of 2012, neural networks were often surpassed
by other machine learning methods, such as support vector machines.
For computer vision, this comparison is perhaps not fair. That is although the inputs to convolutional
networks consist of raw or lightly-processed (e.g., by centering) pixel values, practitioners
would never feed raw pixels into traditional models. Instead, typical computer vision pipelines
consisted of manually engineering feature extraction pipelines. Rather than learn the features, the
features were crafted. Most of the progress came from having more clever ideas for features, and
the learning algorithm was often relegated to an afterthought.
257
Although some neural network accelerators were available in the 1990s, they were not yet sufficiently
powerful to make deep multichannel, multilayer CNNs with a large number of parameters.
Moreover, datasets were still relatively small. Added to these obstacles, key tricks for training
neural networks including parameter initialization heuristics, clever variants of stochastic gradient
descent, non-squashing activation functions, and effective regularization techniques were
still missing.
Thus, rather than training end-to-end (pixel to classification) systems, classical pipelines looked
more like this:
1. Obtain an interesting dataset. In early days, these datasets required expensive sensors (at
the time, 1 megapixel images were state-of-the-art).
2. Preprocess the dataset with hand-crafted features based on some knowledge of optics, geometry,
other analytic tools, and occasionally on the serendipitous discoveries of lucky graduate
students.
3. Feed the data through a standard set of feature extractors such as the SIFT (scale-invariant
feature transform) (Lowe, 2004), the SURF (speeded up robust features) (Bay et al., 2006), or
any number of other hand-tuned pipelines.
4. Dump the resulting representations into your favorite classifier, likely a linear model or kernel
method, to train a classifier.
If you spoke to machine learning researchers, they believed that machine learning was both important
and beautiful. Elegant theories proved the properties of various classifiers. The field of
machine learning was thriving, rigorous, and eminently useful. However, if you spoke to a computer
vision researcher, you would hear a very different story. The dirty truth of image recognition,
they would tell you, is that features, not learning algorithms, drove progress. Computer vision
researchers justifiably believed that a slightly bigger or cleaner dataset or a slightly improved
feature-extraction pipeline mattered far more to the final accuracy than any learning algorithm.
7.1.1 Learning Representations
Another way to cast the state of affairs is that the most important part of the pipeline was the representation.
And up until 2012 the representation was calculated mechanically. In fact, engineering
a new set of feature functions, improving results, and writing up the method was a prominent
genre of paper. SIFT (Lowe, 2004), SURF (Bay et al., 2006), HOG (histograms of oriented gradient)
(Dalal & Triggs, 2005), bags of visual words93 and similar feature extractors ruled the roost.
Another group of researchers, including Yann LeCun, Geoff Hinton, Yoshua Bengio, Andrew Ng,
Shun-ichi Amari, and Juergen Schmidhuber, had different plans. They believed that features
themselves ought to be learned. Moreover, they believed that to be reasonably complex, the features
ought to be hierarchically composed with multiple jointly learned layers, each with learnable
parameters. In the case of an image, the lowest layers might come to detect edges, colors, and
textures. Indeed, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton proposed a new variant of a
CNN, AlexNet, that achieved excellent performance in the 2012 ImageNet challenge. AlexNet was
named after Alex Krizhevsky, the first author of the breakthrough ImageNet classification paper
(Krizhevsky et al., 2012).
Interestingly in the lowest layers of the network, the model learned feature extractors that resembled
some traditional filters. Fig. 7.1.1 is reproduced from the AlexNet paper (Krizhevsky et al.,
2012) and describes lower-level image descriptors.
93 https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision
258 Chapter 7. Modern Convolutional Neural Networks
Fig. 7.1.1: Image filters learned by the first layer of AlexNet.
Higher layers in the network might build upon these representations to represent larger structures,
like eyes, noses, blades of grass, and so on. Even higher layers might represent whole objects
like people, airplanes, dogs, or frisbees. Ultimately, the final hidden state learns a compact
representation of the image that summarizes its contents such that data belonging to different
categories be separated easily.
While the ultimate breakthrough for many-layered CNNs came in 2012, a core group of researchers
had dedicated themselves to this idea, attempting to learn hierarchical representations of visual
data for many years. The ultimate breakthrough in 2012 can be attributed to two key factors.
Missing Ingredient: Data
Deep models with many layers require large amounts of data in order to enter the regime where
they significantly outperform traditional methods based on convex optimizations (e.g., linear and
kernel methods). However, given the limited storage capacity of computers, the relative expense
of sensors, and the comparatively tighter research budgets in the 1990s, most research relied on
tiny datasets. Numerous papers addressed the UCI collection of datasets, many of which contained
only hundreds or (a few) thousands of images captured in unnatural settings with low resolution.
In 2009, the ImageNet dataset was released, challenging researchers to learn models from 1 million
examples, 1000 each from 1000 distinct categories of objects. The researchers, led by Fei-Fei
Li, who introduced this dataset leveraged Google Image Search to prefilter large candidate sets for
each category and employed the Amazon Mechanical Turk crowdsourcing pipeline to confirm for
each image whether it belonged to the associated category. This scale was unprecedented. The
associated competition, dubbed the ImageNet Challenge pushed computer vision and machine
learning research forward, challenging researchers to identify which models performed best at a
greater scale than academics had previously considered.
7.1. Deep Convolutional Neural Networks (AlexNet) 259
Missing Ingredient: Hardware
Deep learning models are voracious consumers of compute cycles. Training can take hundreds
of epochs, and each iteration requires passing data through many layers of computationallyexpensive
linear algebra operations. This is one of the main reasons why in the 1990s and early
2000s, simple algorithms based on the more-efficiently optimized convex objectives were preferred.
Graphical processing units (GPUs) proved to be a game changer in making deep learning feasible.
These chips had long been developed for accelerating graphics processing to benefit computer
games. In particular, they were optimized for high throughput 44 matrix-vector products, which
are needed for many computer graphics tasks. Fortunately, this math is strikingly similar to that
required to calculate convolutional layers. Around that time, NVIDIA and ATI had begun optimizing
GPUs for general computing operations, going as far as to market them as general-purpose GPUs
(GPGPU).
To provide some intuition, consider the cores of a modern microprocessor (CPU). Each of the
cores is fairly powerful running at a high clock frequency and sporting large caches (up to several
megabytes of L3). Each core is well-suited to executing a wide range of instructions, with branch
predictors, a deep pipeline, and other bells and whistles that enable it to run a large variety of
programs. This apparent strength, however, is also its Achilles heel: general-purpose cores are
very expensive to build. They require lots of chip area, a sophisticated support structure (memory
interfaces, caching logic between cores, high-speed interconnects, and so on), and they are comparatively
bad at any single task. Modern laptops have up to 4 cores, and even high-end servers
rarely exceed 64 cores, simply because it is not cost effective.
By comparison, GPUs consist of 100  1000 small processing elements (the details differ somewhat
between NVIDIA, ATI, ARM and other chip vendors), often grouped into larger groups
(NVIDIA calls them warps). While each core is relatively weak, sometimes even running at sub-
1GHz clock frequency, it is the total number of such cores that makes GPUs orders of magnitude
faster than CPUs. For instance, NVIDIA?s recent Volta generation offers up to 120 TFlops per chip
for specialized instructions (and up to 24 TFlops for more general-purpose ones), while floating
point performance of CPUs has not exceeded 1 TFlop to date. The reason for why this is possible is
actually quite simple: first, power consumption tends to grow quadratically with clock frequency.
Hence, for the power budget of a CPU core that runs 4 times faster (a typical number), you can use
16 GPU cores at 1/4 the speed, which yields 16  1/4 = 4 times the performance. Furthermore,
GPU cores are much simpler (in fact, for a long time they were not even able to execute generalpurpose
code), which makes them more energy efficient. Last, many operations in deep learning
require high memory bandwidth. Again, GPUs shine here with buses that are at least 10 times as
wide as many CPUs.
Back to 2012. A major breakthrough came when Alex Krizhevsky and Ilya Sutskever implemented
a deep CNN that could run on GPU hardware. They realized that the computational bottlenecks
in CNNs, convolutions and matrix multiplications, are all operations that could be parallelized in
hardware. Using two NVIDIA GTX 580s with 3GB of memory, they implemented fast convolutions.
The code cuda-convnet94 was good enough that for several years it was the industry standard and
powered the first couple years of the deep learning boom.
94 https://code.google.com/archive/p/cuda-convnet/
260 Chapter 7. Modern Convolutional Neural Networks
7.1.2 AlexNet
AlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recognition Challenge
2012 by a phenomenally large margin. This network showed, for the first time, that the
features obtained by learning can transcend manually-designed features, breaking the previous
paradigm in computer vision.
The architectures of AlexNet and LeNet are very similar, as Fig. 7.1.2 illustrates. Note that we
provide a slightly streamlined version of AlexNet removing some of the design quirks that were
needed in 2012 to make the model fit on two small GPUs.
Fig. 7.1.2: From LeNet (left) to AlexNet (right).
The design philosophies of AlexNet and LeNet are very similar, but there are also significant differences.
First, AlexNet is much deeper than the comparatively small LeNet5. AlexNet consists of
eight layers: five convolutional layers, two fully-connected hidden layers, and one fully-connected
output layer. Second, AlexNet used the ReLU instead of the sigmoid as its activation function. Let
us delve into the details below.
7.1. Deep Convolutional Neural Networks (AlexNet) 261
Architecture
In AlexNet?s first layer, the convolution window shape is 11  11. Since most images in ImageNet
are more than ten times higher and wider than the MNIST images, objects in ImageNet data tend
to occupy more pixels. Consequently, a larger convolution window is needed to capture the object.
The convolution window shape in the second layer is reduced to 55, followed by 33. In addition,
after the first, second, and fifth convolutional layers, the network adds maximum pooling layers
with a window shape of 33 and a stride of 2. Moreover, AlexNet has ten times more convolution
channels than LeNet.
After the last convolutional layer there are two fully-connected layers with 4096 outputs. These
two huge fully-connected layers produce model parameters of nearly 1 GB. Due to the limited
memory in early GPUs, the original AlexNet used a dual data stream design, so that each of their
two GPUs could be responsible for storing and computing only its half of the model. Fortunately,
GPU memory is comparatively abundant now, so we rarely need to break up models across GPUs
these days (our version of the AlexNet model deviates from the original paper in this aspect).
Activation Functions
Besides, AlexNet changed the sigmoid activation function to a simpler ReLU activation function.
On one hand, the computation of the ReLU activation function is simpler. For example, it does
not have the exponentiation operation found in the sigmoid activation function. On the other
hand, the ReLU activation function makes model training easier when using different parameter
initialization methods. This is because, when the output of the sigmoid activation function is very
close to 0 or 1, the gradient of these regions is almost 0, so that backpropagation cannot continue
to update some of the model parameters. In contrast, the gradient of the ReLU activation function
in the positive interval is always 1. Therefore, if the model parameters are not properly initialized,
the sigmoid function may obtain a gradient of almost 0 in the positive interval, so that the model
cannot be effectively trained.
Capacity Control and Preprocessing
AlexNet controls the model complexity of the fully-connected layer by dropout (Section 4.6), while
LeNet only uses weight decay. To augment the data even further, the training loop of AlexNet
added a great deal of image augmentation, such as flipping, clipping, and color changes. This
makes the model more robust and the larger sample size effectively reduces overfitting. We will
discuss data augmentation in greater detail in Section 13.1.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
net = nn.Sequential()
# Here, we use a larger 11 x 11 window to capture objects. At the same time,
# we use a stride of 4 to greatly reduce the height and width of the output.
# Here, the number of output channels is much larger than that in LeNet
net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
nn.MaxPool2D(pool_size=3, strides=2),
# Make the convolution window smaller, set padding to 2 for consistent
(continues on next page)
262 Chapter 7. Modern Convolutional Neural Networks
(continued from previous page)
# height and width across the input and output, and increase the
# number of output channels
nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
nn.MaxPool2D(pool_size=3, strides=2),
# Use three successive convolutional layers and a smaller convolution
# window. Except for the final convolutional layer, the number of
# output channels is further increased. Pooling layers are not used to
# reduce the height and width of input after the first two
# convolutional layers
nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
nn.MaxPool2D(pool_size=3, strides=2),
# Here, the number of outputs of the fully-connected layer is several
# times larger than that in LeNet. Use the dropout layer to mitigate
# overfitting
nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
# Output layer. Since we are using Fashion-MNIST, the number of
# classes is 10, instead of 1000 as in the paper
nn.Dense(10))
We construct a single-channel data example with both height and width of 224 to observe the
output shape of each layer. It matches the AlexNet architecture in Fig. 7.1.2.
X = np.random.uniform(size=(1, 1, 224, 224))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)
conv0 output shape: (1, 96, 54, 54)
pool0 output shape: (1, 96, 26, 26)
conv1 output shape: (1, 256, 26, 26)
pool1 output shape: (1, 256, 12, 12)
conv2 output shape: (1, 384, 12, 12)
conv3 output shape: (1, 384, 12, 12)
conv4 output shape: (1, 256, 12, 12)
pool2 output shape: (1, 256, 5, 5)
dense0 output shape: (1, 4096)
dropout0 output shape: (1, 4096)
dense1 output shape: (1, 4096)
dropout1 output shape: (1, 4096)
dense2 output shape: (1, 10)
7.1. Deep Convolutional Neural Networks (AlexNet) 263
7.1.3 Reading the Dataset
Although AlexNet is trained on ImageNet in the paper, we use Fashion-MNIST here since training
an ImageNet model to convergence could take hours or days even on a modern GPU. One of the
problems with applying AlexNet directly on Fashion-MNIST is that its images have lower resolution
(2828 pixels) than ImageNet images. To make things work, we upsample them to 224224
(generally not a smart practice, but we do it here to be faithful to the AlexNet architecture). We
perform this resizing with the resize argument in the d2l.load_data_fashion_mnist function.
batch_size = 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
7.1.4 Training
Now, we can start training AlexNet. Compared with LeNet in Section 6.6, the main change here is
the use of a smaller learning rate and much slower training due to the deeper and wider network,
the higher image resolution, and the more costly convolutions.
lr, num_epochs = 0.01, 10
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.333, train acc 0.879, test acc 0.893
4200.6 examples/sec on gpu(0)
Summary
� AlexNet has a similar structure to that of LeNet, but uses more convolutional layers and a
larger parameter space to fit the large-scale ImageNet dataset.
� Today AlexNet has been surpassed by much more effective architectures but it is a key step
from shallow to deep networks that are used nowadays.
� Although it seems that there are only a few more lines in AlexNet?s implementation than in
LeNet, it took the academic community many years to embrace this conceptual change and
264 Chapter 7. Modern Convolutional Neural Networks
take advantage of its excellent experimental results. This was also due to the lack of efficient
computational tools.
� Dropout, ReLU, and preprocessing were the other key steps in achieving excellent performance
in computer vision tasks.
Exercises
1. Try increasing the number of epochs. Compared with LeNet, how are the results different?
Why?
2. AlexNet may be too complex for the Fashion-MNIST dataset.
1. Try simplifying the model to make the training faster, while ensuring that the accuracy
does not drop significantly.
2. Design a better model that works directly on 28  28 images.
3. Modify the batch size, and observe the changes in accuracy and GPU memory.
4. Analyze computational performance of AlexNet.
1. What is the dominant part for the memory footprint of AlexNet?
2. What is the dominant part for computation in AlexNet?
3. How about memory bandwidth when computing the results?
5. Apply dropout and ReLU to LeNet-5. Does it improve? How about preprocessing?
Discussions95
7.2 Networks Using Blocks (VGG)
While AlexNet offered empirical evidence that deep CNNs can achieve good results, it did not
provide a general template to guide subsequent researchers in designing new networks. In the
following sections, we will introduce several heuristic concepts commonly used to design deep
networks.
Progress in this field mirrors that in chip design where engineers went from placing transistors to
logical elements to logic blocks. Similarly, the design of neural network architectures had grown
progressively more abstract, with researchers moving from thinking in terms of individual neurons
to whole layers, and now to blocks, repeating patterns of layers.
The idea of using blocks first emerged from the Visual Geometry Group96 (VGG) at Oxford University,
in their eponymously-named VGG network. It is easy to implement these repeated structures
in code with any modern deep learning framework by using loops and subroutines.
95 https://discuss.d2l.ai/t/75
96 http://www.robots.ox.ac.uk/~vgg/
7.2. Networks Using Blocks (VGG) 265
7.2.1 VGG Blocks
The basic building block of classic CNNs is a sequence of the following: (i) a convolutional layer
with padding to maintain the resolution, (ii) a nonlinearity such as a ReLU, (iii) a pooling layer such
as a max pooling layer. One VGG block consists of a sequence of convolutional layers, followed by
a max pooling layer for spatial downsampling. In the original VGG paper (Simonyan & Zisserman,
2014), the authors employed convolutions with 33 kernels with padding of 1 (keeping height and
width) and 22 max pooling with stride of 2 (halving the resolution after each block). In the code
below, we define a function called vgg_block to implement one VGG block. The function takes
two arguments corresponding to the number of convolutional layers num_convs and the number
of output channels num_channels.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
def vgg_block(num_convs, num_channels):
blk = nn.Sequential()
for _ in range(num_convs):
blk.add(nn.Conv2D(num_channels, kernel_size=3,
padding=1, activation='relu'))
blk.add(nn.MaxPool2D(pool_size=2, strides=2))
return blk
7.2.2 VGG Network
Like AlexNet and LeNet, the VGG Network can be partitioned into two parts: the first consisting
mostly of convolutional and pooling layers and the second consisting of fully-connected layers.
This is depicted in Fig. 7.2.1.
266 Chapter 7. Modern Convolutional Neural Networks
Fig. 7.2.1: From AlexNet to VGG that is designed from building blocks.
The convolutional part of the network connects several VGG blocks from Fig. 7.2.1 (also defined
in the vgg_block function) in succession. The following variable conv_arch consists of a list of
tuples (one per block), where each contains two values: the number of convolutional layers and
the number of output channels, which are precisely the arguments required to call the vgg_block
function. The fully-connected part of the VGG network is identical to that covered in AlexNet.
The original VGG network had 5 convolutional blocks, among which the first two have one convolutional
layer each and the latter three contain two convolutional layers each. The first block has
64 output channels and each subsequent block doubles the number of output channels, until that
number reaches 512. Since this network uses 8 convolutional layers and 3 fully-connected layers,
it is often called VGG-11.
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
The following code implements VGG-11. This is a simple matter of executing a for-loop over
conv_arch.
def vgg(conv_arch):
net = nn.Sequential()
# The convolutional part
for (num_convs, num_channels) in conv_arch:
net.add(vgg_block(num_convs, num_channels))
# The fully-connected part
net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
nn.Dense(10))
return net
net = vgg(conv_arch)
7.2. Networks Using Blocks (VGG) 267
Next, we will construct a single-channel data example with a height and width of 224 to observe
the output shape of each layer.
net.initialize()
X = np.random.uniform(size=(1, 1, 224, 224))
for blk in net:
X = blk(X)
print(blk.name, 'output shape:\t', X.shape)
sequential1 output shape: (1, 64, 112, 112)
sequential2 output shape: (1, 128, 56, 56)
sequential3 output shape: (1, 256, 28, 28)
sequential4 output shape: (1, 512, 14, 14)
sequential5 output shape: (1, 512, 7, 7)
dense0 output shape: (1, 4096)
dropout0 output shape: (1, 4096)
dense1 output shape: (1, 4096)
dropout1 output shape: (1, 4096)
dense2 output shape: (1, 10)
As you can see, we halve height and width at each block, finally reaching a height and width of 7
before flattening the representations for processing by the fully-connected part of the network.
7.2.3 Training
Since VGG-11 is more computationally-heavy than AlexNet we construct a network with a smaller
number of channels. This is more than sufficient for training on Fashion-MNIST.
ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)
Apart from using a slightly larger learning rate, the model training process is similar to that of
AlexNet in Section 7.1.
lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.175, train acc 0.935, test acc 0.923
1814.6 examples/sec on gpu(0)
268 Chapter 7. Modern Convolutional Neural Networks
Summary
� VGG-11 constructs a network using reusable convolutional blocks. Different VGG models can
be defined by the differences in the number of convolutional layers and output channels in
each block.
� The use of blocks leads to very compact representations of the network definition. It allows
for efficient design of complex networks.
� In their VGG paper, Simonyan and Ziserman experimented with various architectures. In
particular, they found that several layers of deep and narrow convolutions (i.e., 3  3) were
more effective than fewer layers of wider convolutions.
Exercises
1. When printing out the dimensions of the layers we only saw 8 results rather than 11. Where
did the remaining 3 layer information go?
2. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs
more GPU memory. Analyze the reasons for this.
3. Try changing the height and width of the images in Fashion-MNIST from 224 to 96. What
influence does this have on the experiments?
4. Refer to Table 1 in the VGG paper (Simonyan & Zisserman, 2014) to construct other common
models, such as VGG-16 or VGG-19.
Discussions97
97 https://discuss.d2l.ai/t/77
7.2. Networks Using Blocks (VGG) 269
7.3 Network in Network (NiN)
LeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting spatial
structure via a sequence of convolution and pooling layers and then post-process the representations
via fully-connected layers. The improvements upon LeNet by AlexNet and VGG mainly lie in
how these later networks widen and deepen these two modules. Alternatively, one could imagine
using fully-connected layers earlier in the process. However, a careless use of dense layers might
give up the spatial structure of the representation entirely, network in network (NiN) blocks offer
an alternative. They were proposed based on a very simple insight: to use an MLP on the channels
for each pixel separately (Lin et al., 2013).
7.3.1 NiN Blocks
Recall that the inputs and outputs of convolutional layers consist of four-dimensional tensors with
axes corresponding to the example, channel, height, and width. Also recall that the inputs and
outputs of fully-connected layers are typically two-dimensional tensors corresponding to the example
and feature. The idea behind NiN is to apply a fully-connected layer at each pixel location
(for each height and width). If we tie the weights across each spatial location, we could think of
this as a 11 convolutional layer (as described in Section 6.4) or as a fully-connected layer acting
independently on each pixel location. Another way to view this is to think of each element in the
spatial dimension (height and width) as equivalent to an example and a channel as equivalent to
a feature.
Fig. 7.3.1 illustrates the main structural differences between VGG and NiN, and their blocks. The
NiN block consists of one convolutional layer followed by two 1  1 convolutional layers that act
as per-pixel fully-connected layers with ReLU activations. The convolution window shape of the
first layer is typically set by the user. The subsequent window shapes are fixed to 1  1.
270 Chapter 7. Modern Convolutional Neural Networks
Fig. 7.3.1: Comparing architectures of VGG and NiN, and their blocks.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
def nin_block(num_channels, kernel_size, strides, padding):
blk = nn.Sequential()
blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,
activation='relu'),
nn.Conv2D(num_channels, kernel_size=1, activation='relu'),
nn.Conv2D(num_channels, kernel_size=1, activation='relu'))
return blk
7.3. Network in Network (NiN) 271
7.3.2 NiN Model
The original NiN network was proposed shortly after AlexNet and clearly draws some inspiration.
NiN uses convolutional layers with window shapes of 1111, 55, and 33, and the corresponding
numbers of output channels are the same as in AlexNet. Each NiN block is followed by a maximum
pooling layer with a stride of 2 and a window shape of 3  3.
One significant difference between NiN and AlexNet is that NiN avoids fully-connected layers altogether.
Instead, NiN uses an NiN block with a number of output channels equal to the number
of label classes, followed by a global average pooling layer, yielding a vector of logits. One advantage
of NiN?s design is that it significantly reduces the number of required model parameters.
However, in practice, this design sometimes requires increased model training time.
net = nn.Sequential()
net.add(nin_block(96, kernel_size=11, strides=4, padding=0),
nn.MaxPool2D(pool_size=3, strides=2),
nin_block(256, kernel_size=5, strides=1, padding=2),
nn.MaxPool2D(pool_size=3, strides=2),
nin_block(384, kernel_size=3, strides=1, padding=1),
nn.MaxPool2D(pool_size=3, strides=2),
nn.Dropout(0.5),
# There are 10 label classes
nin_block(10, kernel_size=3, strides=1, padding=1),
# The global average pooling layer automatically sets the window shape
# to the height and width of the input
nn.GlobalAvgPool2D(),
# Transform the four-dimensional output into two-dimensional output
# with a shape of (batch size, 10)
nn.Flatten())
We create a data example to see the output shape of each block.
X = np.random.uniform(size=(1, 1, 224, 224))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)
sequential1 output shape: (1, 96, 54, 54)
pool0 output shape: (1, 96, 26, 26)
sequential2 output shape: (1, 256, 26, 26)
pool1 output shape: (1, 256, 12, 12)
sequential3 output shape: (1, 384, 12, 12)
pool2 output shape: (1, 384, 5, 5)
dropout0 output shape: (1, 384, 5, 5)
sequential4 output shape: (1, 10, 5, 5)
pool3 output shape: (1, 10, 1, 1)
flatten0 output shape: (1, 10)
272 Chapter 7. Modern Convolutional Neural Networks
7.3.3 Training
As before we use Fashion-MNIST to train the model. NiN?s training is similar to that for AlexNet
and VGG.
lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.616, train acc 0.760, test acc 0.773
3009.7 examples/sec on gpu(0)
Summary
� NiN uses blocks consisting of a convolutional layer and multiple 1  1 convolutional layers.
This can be used within the convolutional stack to allow for more per-pixel nonlinearity.
� NiN removes the fully-connected layers and replaces them with global average pooling (i.e.,
summing over all locations) after reducing the number of channels to the desired number
of outputs (e.g., 10 for Fashion-MNIST).
� Removing the fully-connected layers reduces overfitting. NiN has dramatically fewer parameters.
� The NiN design influenced many subsequent CNN designs.
Exercises
1. Tune the hyperparameters to improve the classification accuracy.
2. Why are there two 1  1 convolutional layers in the NiN block? Remove one of them, and
then observe and analyze the experimental phenomena.
3. Calculate the resource usage for NiN.
1. What is the number of parameters?
2. What is the amount of computation?
7.3. Network in Network (NiN) 273
3. What is the amount of memory needed during training?
4. What is the amount of memory needed during prediction?
4. What are possible problems with reducing the 384  5  5 representation to a 10  5  5
representation in one step?
Discussions98
7.4 Networks with Parallel Concatenations (GoogLeNet)
In 2014, GoogLeNet won the ImageNet Challenge, proposing a structure that combined the
strengths of NiN and paradigms of repeated blocks (Szegedy et al., 2015). One focus of the paper
was to address the question of which sized convolution kernels are best. After all, previous popular
networks employed choices as small as 1  1 and as large as 11  11. One insight in this paper
was that sometimes it can be advantageous to employ a combination of variously-sized kernels. In
this section, we will introduce GoogLeNet, presenting a slightly simplified version of the original
model: we omit a few ad-hoc features that were added to stabilize training but are unnecessary
now with better training algorithms available.
7.4.1 Inception Blocks
The basic convolutional block in GoogLeNet is called an Inception block, likely named due to a quote
from the movie Inception (�We Need To Go Deeper�), which launched a viral meme.
Fig. 7.4.1: Structure of the Inception block.
As depicted in Fig. 7.4.1, the inception block consists of four parallel paths. The first three paths
use convolutional layers with window sizes of 1  1, 3  3, and 5  5 to extract information from
different spatial sizes. The middle two paths perform a 1  1 convolution on the input to reduce
the number of channels, reducing the model?s complexity. The fourth path uses a 33 maximum
pooling layer, followed by a 1  1 convolutional layer to change the number of channels. The
four paths all use appropriate padding to give the input and output the same height and width.
Finally, the outputs along each path are concatenated along the channel dimension and comprise
the block?s output. The commonly-tuned hyperparameters of the Inception block are the number
of output channels per layer.
98 https://discuss.d2l.ai/t/79
274 Chapter 7. Modern Convolutional Neural Networks
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
class Inception(nn.Block):
# `c1`--`c4` are the number of output channels for each path
def __init__(self, c1, c2, c3, c4, **kwargs):
super(Inception, self).__init__(**kwargs)
# Path 1 is a single 1 x 1 convolutional layer
self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')
# Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3
# convolutional layer
self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')
self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,
activation='relu')
# Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5
# convolutional layer
self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')
self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,
activation='relu')
# Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1
# convolutional layer
self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)
self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')
def forward(self, x):
p1 = self.p1_1(x)
p2 = self.p2_2(self.p2_1(x))
p3 = self.p3_2(self.p3_1(x))
p4 = self.p4_2(self.p4_1(x))
# Concatenate the outputs on the channel dimension
return np.concatenate((p1, p2, p3, p4), axis=1)
To gain some intuition for why this network works so well, consider the combination of the filters.
They explore the image in varying ranges. This means that details at different extents can be
recognized efficiently by different filters. At the same time, we can allocate different amounts of
parameters for different ranges (e.g., more for short range but not ignore the long range entirely).
7.4.2 GoogLeNet Model
As shown in Fig. 7.4.2, GoogLeNet uses a stack of a total of 9 inception blocks and global average
pooling to generate its estimates. Maximum pooling between inception blocks reduces the
dimensionality. The first module is similar to AlexNet and LeNet. The stack of blocks is inherited
from VGG and the global average pooling avoids a stack of fully-connected layers at the end.
7.4. Networks with Parallel Concatenations (GoogLeNet) 275
Fig. 7.4.2: The GoogLeNet architecture.
We can now implement GoogLeNet piece by piece. The first module uses a 64-channel 7  7 convolutional
layer.
b1 = nn.Sequential()
b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))
The second module uses two convolutional layers: first, a 64-channel 1  1 convolutional layer,
then a 3  3 convolutional layer that triples the number of channels. This corresponds to the
second path in the Inception block.
b2 = nn.Sequential()
b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),
nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))
The third module connects two complete Inception blocks in series. The number of output channels
of the first Inception block is 64 + 128 + 32 + 32 = 256, and the number-of-output-channel
ratio among the four paths is 64 : 128 : 32 : 32 = 2 : 4 : 1 : 1. The second and third paths first
reduce the number of input channels to 96/192 = 1/2 and 16/192 = 1/12, respectively, and then
connect the second convolutional layer. The number of output channels of the second Inception
block is increased to 128 + 192 + 96 + 64 = 480, and the number-of-output-channel ratio among
the four paths is 128 : 192 : 96 : 64 = 4 : 6 : 3 : 2. The second and third paths first reduce the
number of input channels to 128/256 = 1/2 and 32/256 = 1/8, respectively.
276 Chapter 7. Modern Convolutional Neural Networks
b3 = nn.Sequential()
b3.add(Inception(64, (96, 128), (16, 32), 32),
Inception(128, (128, 192), (32, 96), 64),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))
The fourth module is more complicated. It connects five Inception blocks in series, and they have
192+208+48+64 = 512, 160+224+64+64 = 512, 128+256+64+64 = 512, 112+288+64+64 = 528,
and 256+320+128+128 = 832 output channels, respectively. The number of channels assigned to
these paths is similar to that in the third module: the second path with the 33 convolutional layer
outputs the largest number of channels, followed by the first path with only the 11 convolutional
layer, the third path with the 55 convolutional layer, and the fourth path with the 33 maximum
pooling layer. The second and third paths will first reduce the number of channels according to
the ratio. These ratios are slightly different in different Inception blocks.
b4 = nn.Sequential()
b4.add(Inception(192, (96, 208), (16, 48), 64),
Inception(160, (112, 224), (24, 64), 64),
Inception(128, (128, 256), (24, 64), 64),
Inception(112, (144, 288), (32, 64), 64),
Inception(256, (160, 320), (32, 128), 128),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))
The fifth module has two Inception blocks with 256+320+128+128 = 832 and 384+384+128+
128 = 1024 output channels. The number of channels assigned to each path is the same as that in
the third and fourth modules, but differs in specific values. It should be noted that the fifth block is
followed by the output layer. This block uses the global average pooling layer to change the height
and width of each channel to 1, just as in NiN. Finally, we turn the output into a two-dimensional
array followed by a fully-connected layer whose number of outputs is the number of label classes.
b5 = nn.Sequential()
b5.add(Inception(256, (160, 320), (32, 128), 128),
Inception(384, (192, 384), (48, 128), 128),
nn.GlobalAvgPool2D())
net = nn.Sequential()
net.add(b1, b2, b3, b4, b5, nn.Dense(10))
The GoogLeNet model is computationally complex, so it is not as easy to modify the number of
channels as in VGG. To have a reasonable training time on Fashion-MNIST, we reduce the input
height and width from 224 to 96. This simplifies the computation. The changes in the shape of the
output between the various modules are demonstrated below.
X = np.random.uniform(size=(1, 1, 96, 96))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)
sequential0 output shape: (1, 64, 24, 24)
sequential1 output shape: (1, 192, 12, 12)
sequential2 output shape: (1, 480, 6, 6)
sequential3 output shape: (1, 832, 3, 3)
(continues on next page)
7.4. Networks with Parallel Concatenations (GoogLeNet) 277
(continued from previous page)
sequential4 output shape: (1, 1024, 1, 1)
dense0 output shape: (1, 10)
7.4.3 Training
As before, we train our model using the Fashion-MNIST dataset. We transform it to 96  96 pixel
resolution before invoking the training procedure.
lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.255, train acc 0.903, test acc 0.894
2272.2 examples/sec on gpu(0)
Summary
� The Inception block is equivalent to a subnetwork with four paths. It extracts information
in parallel through convolutional layers of different window shapes and maximum pooling
layers. 1  1 convolutions reduce channel dimensionality on a per-pixel level. Maximum
pooling reduces the resolution.
� GoogLeNet connects multiple well-designed Inception blocks with other layers in series.
The ratio of the number of channels assigned in the Inception block is obtained through a
large number of experiments on the ImageNet dataset.
� GoogLeNet, as well as its succeeding versions, was one of the most efficient models on ImageNet,
providing similar test accuracy with lower computational complexity.
278 Chapter 7. Modern Convolutional Neural Networks
Exercises
1. There are several iterations of GoogLeNet. Try to implement and run them. Some of them
include the following:
� Add a batch normalization layer (Ioffe & Szegedy, 2015), as described later in Section
7.5.
� Make adjustments to the Inception block (Szegedy et al., 2016).
� Use label smoothing for model regularization (Szegedy et al., 2016).
� Include it in the residual connection (Szegedy et al., 2017), as described later in Section
7.6.
2. What is the minimum image size for GoogLeNet to work?
3. Compare the model parameter sizes of AlexNet, VGG, and NiN with GoogLeNet. How do the
latter two network architectures significantly reduce the model parameter size?
4. Why do we need a long range convolution initially?
Discussions99
7.5 Batch Normalization
Training deep neural networks is difficult. And getting them to converge in a reasonable amount
of time can be tricky. In this section, we describe batch normalization, a popular and effective
technique that consistently accelerates the convergence of deep networks (Ioffe & Szegedy, 2015).
Together with residual blocks�covered later in Section 7.6�batch normalization has made it possible
for practitioners to routinely train networks with over 100 layers.
7.5.1 Training Deep Networks
To motivate batch normalization, let us review a few practical challenges that arise when training
machine learning models and neural networks in particular.
First, choices regarding data preprocessing often make an enormous difference in the final results.
Recall our application of MLPs to predicting house prices (Section 4.10). Our first step when
working with real data was to standardize our input features to each have a mean of zero and variance
of one. Intuitively, this standardization plays nicely with our optimizers because it puts the
parameters a priori at a similar scale.
Second, for a typical MLP or CNN, as we train, the variables (e.g., affine transformation outputs in
MLP) in intermediate layers may take values with widely varying magnitudes: both along the layers
from the input to the output, across units in the same layer, and over time due to our updates to
the model parameters. The inventors of batch normalization postulated informally that this drift
in the distribution of such variables could hamper the convergence of the network. Intuitively,
we might conjecture that if one layer has variable values that are 100 times that of another layer,
this might necessitate compensatory adjustments in the learning rates.
Third, deeper networks are complex and easily capable of overfitting. This means that regularization
becomes more critical.
99 https://discuss.d2l.ai/t/81
7.5. Batch Normalization 279
Batch normalization is applied to individual layers (optionally, to all of them) and works as follows:
In each training iteration, we first normalize the inputs (of batch normalization) by subtracting
their mean and dividing by their standard deviation, where both are estimated based on the statistics
of the current minibatch. Next, we apply a scale coefficient and a scale offset. It is precisely
due to this normalization based on batch statistics that batch normalization derives its name.
Note that if we tried to apply batch normalization with minibatches of size 1, we would not be able
to learn anything. That is because after subtracting the means, each hidden unit would take value
0! As you might guess, since we are devoting a whole section to batch normalization, with large
enough minibatches, the approach proves effective and stable. One takeaway here is that when
applying batch normalization, the choice of batch size may be even more significant than without
batch normalization.
Formally, denoting by x 2 B an input to batch normalization (BN) that is from a minibatch B,
batch normalization transforms x according to the following expression:
BN(x) = 
 ? x ?? ^B
^B
+ : (7.5.1)
In (7.5.1), ^B is the sample mean and ^B is the sample standard deviation of the minibatch B.
After applying standardization, the resulting minibatch has zero mean and unit variance. Because
the choice of unit variance (vs. some other magic number) is an arbitrary choice, we commonly
include element-wise scale parameter 
 and shift parameter  that have the same shape as x. Note
that 
 and  are parameters that need to be learned jointly with the other model parameters.
Consequently, the variable magnitudes for intermediate layers cannot diverge during training because
batch normalization actively centers and rescales them back to a given mean and size (via
^B and ^B). One piece of practitioner?s intuition or wisdom is that batch normalization seems to
allow for more aggressive learning rates.
Formally, we calculate ^B and ^B in (7.5.1) as follows:
^B =
1
jBj
?
x2B
x;
^2
B =
1
jBj
?
x2B
(x ?? ^B)2 + ?:
(7.5.2)
Note that we add a small constant ? > 0 to the variance estimate to ensure that we never attempt
division by zero, even in cases where the empirical variance estimate might vanish. The estimates
^B and ^B counteract the scaling issue by using noisy estimates of mean and variance. You might
think that this noisiness should be a problem. As it turns out, this is actually beneficial.
This turns out to be a recurring theme in deep learning. For reasons that are not yet wellcharacterized
theoretically, various sources of noise in optimization often lead to faster training
and less overfitting: this variation appears to act as a form of regularization. In some preliminary
research, (Teye et al., 2018) and (Luo et al., 2018) relate the properties of batch normalization to
Bayesian priors and penalties respectively. In particular, this sheds some light on the puzzle of
why batch normalization works best for moderate minibatches sizes in the 50  100 range.
Fixing a trained model, you might think that we would prefer using the entire dataset to estimate
the mean and variance. Once training is complete, why would we want the same image to be classified
differently, depending on the batch in which it happens to reside? During training, such
exact calculation is infeasible because the intermediate variables for all data examples change every
time we update our model. However, once the model is trained, we can calculate the means
and variances of each layer?s variables based on the entire dataset. Indeed this is standard practice
280 Chapter 7. Modern Convolutional Neural Networks
for models employing batch normalization and thus batch normalization layers function differently
in training mode (normalizing by minibatch statistics) and in prediction mode (normalizing by
dataset statistics).
We are now ready to take a look at how batch normalization works in practice.
7.5.2 Batch Normalization Layers
Batch normalization implementations for fully-connected layers and convolutional layers are
slightly different. We discuss both cases below. Recall that one key differences between batch
normalization and other layers is that because batch normalization operates on a full minibatch
at a time, we cannot just ignore the batch dimension as we did before when introducing other
layers.
Fully-Connected Layers
When applying batch normalization to fully-connected layers, the original paper inserts batch
normalization after the affine transformation and before the nonlinear activation function (later
applications may insert batch normalization right after activation functions) (Ioffe & Szegedy,
2015). Denoting the input to the fully-connected layer by x, the affine transformation by Wx + b
(with the weight parameter W and the bias parameter b), and the activation function by ?, we
can express the computation of a batch-normalization-enabled, fully-connected layer output h as
follows:
h = ?(BN(Wx + b)): (7.5.3)
Recall that mean and variance are computed on the same minibatch on which the transformation
is applied.
Convolutional Layers
Similarly, with convolutional layers, we can apply batch normalization after the convolution and
before the nonlinear activation function. When the convolution has multiple output channels, we
need to carry out batch normalization for each of the outputs of these channels, and each channel
has its own scale and shift parameters, both of which are scalars. Assume that our minibatches
contain m examples and that for each channel, the output of the convolution has height p and
width q. For convolutional layers, we carry out each batch normalization over thempq elements
per output channel simultaneously. Thus, we collect the values over all spatial locations when
computing the mean and variance and consequently apply the same mean and variance within a
given channel to normalize the value at each spatial location.
7.5. Batch Normalization 281
Batch Normalization During Prediction
As we mentioned earlier, batch normalization typically behaves differently in training mode and
prediction mode. First, the noise in the sample mean and the sample variance arising from estimating
each on minibatches are no longer desirable once we have trained the model. Second,
we might not have the luxury of computing per-batch normalization statistics. For example, we
might need to apply our model to make one prediction at a time.
Typically, after training, we use the entire dataset to compute stable estimates of the variable
statistics and then fix them at prediction time. Consequently, batch normalization behaves differently
during training and at test time. Recall that dropout also exhibits this characteristic.
7.5.3 Implementation from Scratch
Below, we implement a batch normalization layer with tensors from scratch.
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, init
from mxnet.gluon import nn
npx.set_np()
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
# Use `autograd` to determine whether the current mode is training mode or
# prediction mode
if not autograd.is_training():
# If it is prediction mode, directly use the mean and variance
# obtained by moving average
X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)
else:
assert len(X.shape) in (2, 4)
if len(X.shape) == 2:
# When using a fully-connected layer, calculate the mean and
# variance on the feature dimension
mean = X.mean(axis=0)
var = ((X - mean) ** 2).mean(axis=0)
else:
# When using a two-dimensional convolutional layer, calculate the
# mean and variance on the channel dimension (axis=1). Here we
# need to maintain the shape of `X`, so that the broadcasting
# operation can be carried out later
mean = X.mean(axis=(0, 2, 3), keepdims=True)
var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)
# In training mode, the current mean and variance are used for the
# standardization
X_hat = (X - mean) / np.sqrt(var + eps)
# Update the mean and variance using moving average
moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
moving_var = momentum * moving_var + (1.0 - momentum) * var
Y = gamma * X_hat + beta # Scale and shift
return Y, moving_mean, moving_var
We can now create a proper BatchNorm layer. Our layer will maintain proper parameters for scale
gamma and shift beta, both of which will be updated in the course of training. Additionally, our
layer will maintain moving averages of the means and variances for subsequent use during model
prediction.
282 Chapter 7. Modern Convolutional Neural Networks
Putting aside the algorithmic details, note the design pattern underlying our implementation of
the layer. Typically, we define the mathematics in a separate function, say batch_norm. We then
integrate this functionality into a custom layer, whose code mostly addresses bookkeeping matters,
such as moving data to the right device context, allocating and initializing any required variables,
keeping track of moving averages (here for mean and variance), and so on. This pattern
enables a clean separation of mathematics from boilerplate code. Also note that for the sake of
convenience we did not worry about automatically inferring the input shape here, thus we need
to specify the number of features throughout. Do not worry, the high-level batch normalization
APIs in the deep learning framework will care of this for us and we will demonstrate that later.
class BatchNorm(nn.Block):
# `num_features`: the number of outputs for a fully-connected layer
# or the number of output channels for a convolutional layer. `num_dims`:
# 2 for a fully-connected layer and 4 for a convolutional layer
def __init__(self, num_features, num_dims, **kwargs):
super().__init__(**kwargs)
if num_dims == 2:
shape = (1, num_features)
else:
shape = (1, num_features, 1, 1)
# The scale parameter and the shift parameter (model parameters) are
# initialized to 1 and 0, respectively
self.gamma = self.params.get('gamma', shape=shape, init=init.One())
self.beta = self.params.get('beta', shape=shape, init=init.Zero())
# The variables that are not model parameters are initialized to 0
self.moving_mean = np.zeros(shape)
self.moving_var = np.zeros(shape)
def forward(self, X):
# If `X` is not on the main memory, copy `moving_mean` and
# `moving_var` to the device where `X` is located
if self.moving_mean.ctx != X.ctx:
self.moving_mean = self.moving_mean.copyto(X.ctx)
self.moving_var = self.moving_var.copyto(X.ctx)
# Save the updated `moving_mean` and `moving_var`
Y, self.moving_mean, self.moving_var = batch_norm(
X, self.gamma.data(), self.beta.data(), self.moving_mean,
self.moving_var, eps=1e-12, momentum=0.9)
return Y
7.5.4 Applying Batch Normalization in LeNet
To see how to apply BatchNorm in context, below we apply it to a traditional LeNet model (Section
6.6). Recall that batch normalization is applied after the convolutional layers or fully-connected
layers but before the corresponding activation functions.
net = nn.Sequential()
net.add(nn.Conv2D(6, kernel_size=5),
BatchNorm(6, num_dims=4),
nn.Activation('sigmoid'),
nn.MaxPool2D(pool_size=2, strides=2),
nn.Conv2D(16, kernel_size=5),
BatchNorm(16, num_dims=4),
(continues on next page)
7.5. Batch Normalization 283
(continued from previous page)
nn.Activation('sigmoid'),
nn.MaxPool2D(pool_size=2, strides=2),
nn.Dense(120),
BatchNorm(120, num_dims=2),
nn.Activation('sigmoid'),
nn.Dense(84),
BatchNorm(84, num_dims=2),
nn.Activation('sigmoid'),
nn.Dense(10))
As before, we will train our network on the Fashion-MNIST dataset. This code is virtually identical
to that when we first trained LeNet (Section 6.6). The main difference is the considerably larger
learning rate.
lr, num_epochs, batch_size = 1.0, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.241, train acc 0.912, test acc 0.821
21051.7 examples/sec on gpu(0)
Let us have a look at the scale parameter gamma and the shift parameter beta learned from the first
batch normalization layer.
net[1].gamma.data().reshape(-1,), net[1].beta.data().reshape(-1,)
(array([1.956041 , 0.8407571, 3.1203268, 1.492053 , 1.2769177, 2.3011174], ctx=gpu(0)),
array([ 1.1319363 , 0.36316666, -3.2172477 , -0.7223122 , -0.57278776,
-0.21445163], ctx=gpu(0)))
284 Chapter 7. Modern Convolutional Neural Networks
7.5.5 Concise Implementation
Compared with the BatchNorm class, which we just defined ourselves, we can use the BatchNorm
class defined in high-level APIs from the deep learning framework directly. The code looks virtually
identical to the application our implementation above.
net = nn.Sequential()
net.add(nn.Conv2D(6, kernel_size=5),
nn.BatchNorm(),
nn.Activation('sigmoid'),
nn.MaxPool2D(pool_size=2, strides=2),
nn.Conv2D(16, kernel_size=5),
nn.BatchNorm(),
nn.Activation('sigmoid'),
nn.MaxPool2D(pool_size=2, strides=2),
nn.Dense(120),
nn.BatchNorm(),
nn.Activation('sigmoid'),
nn.Dense(84),
nn.BatchNorm(),
nn.Activation('sigmoid'),
nn.Dense(10))
Below, we use the same hyperparameters to train our model. Note that as usual, the high-level API
variant runs much faster because its code has been compiled to C++ or CUDA while our custom
implementation must be interpreted by Python.
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.252, train acc 0.907, test acc 0.825
40203.7 examples/sec on gpu(0)
7.5. Batch Normalization 285
7.5.6 Controversy
Intuitively, batch normalization is thought to make the optimization landscape smoother. However,
we must be careful to distinguish between speculative intuitions and true explanations for
the phenomena that we observe when training deep models. Recall that we do not even know why
simpler deep neural networks (MLPs and conventional CNNs) generalize well in the first place.
Even with dropout and weight decay, they remain so flexible that their ability to generalize to
unseen data cannot be explained via conventional learning-theoretic generalization guarantees.
In the original paper proposing batch normalization, the authors, in addition to introducing a
powerful and useful tool, offered an explanation for why it works: by reducing internal covariate
shift. Presumably by internal covariate shift the authors meant something like the intuition
expressed above�the notion that the distribution of variable values changes over the course of
training. However, there were two problems with this explanation: i) This drift is very different
from covariate shift, rendering the name a misnomer. ii) The explanation offers an under-specified
intuition but leaves the question of why precisely this technique works an open question wanting for
a rigorous explanation. Throughout this book, we aim to convey the intuitions that practitioners
use to guide their development of deep neural networks. However, we believe that it is important
to separate these guiding intuitions from established scientific fact. Eventually, when you master
this material and start writing your own research papers you will want to be clear to delineate
between technical claims and hunches.
Following the success of batch normalization, its explanation in terms of internal covariate shift
has repeatedly surfaced in debates in the technical literature and broader discourse about how to
present machine learning research. In a memorable speech given while accepting a Test of Time
Award at the 2017 NeurIPS conference, Ali Rahimi used internal covariate shift as a focal point in
an argument likening the modern practice of deep learning to alchemy. Subsequently, the example
was revisited in detail in a position paper outlining troubling trends in machine learning
(Lipton & Steinhardt, 2018). Other authors have proposed alternative explanations for the success
of batch normalization, some claiming that batch normalization?s success comes despite exhibiting
behavior that is in some ways opposite to those claimed in the original paper (Santurkar et al.,
2018).
We note that the internal covariate shift is no more worthy of criticism than any of thousands of
similarly vague claims made every year in the technical machine learning literature. Likely, its
resonance as a focal point of these debates owes to its broad recognizability to the target audience.
Batch normalization has proven an indispensable method, applied in nearly all deployed image
classifiers, earning the paper that introduced the technique tens of thousands of citations.
Summary
� During model training, batch normalization continuously adjusts the intermediate output
of the neural network by utilizing the mean and standard deviation of the minibatch, so that
the values of the intermediate output in each layer throughout the neural network are more
stable.
� The batch normalization methods for fully-connected layers and convolutional layers are
slightly different.
� Like a dropout layer, batch normalization layers have different computation results in training
mode and prediction mode.
286 Chapter 7. Modern Convolutional Neural Networks
� Batch normalization has many beneficial side effects, primarily that of regularization. On
the other hand, the original motivation of reducing internal covariate shift seems not to be
a valid explanation.
Exercises
1. Can we remove the bias parameter from the fully-connected layer or the convolutional layer
before the batch normalization? Why?
2. Compare the learning rates for LeNet with and without batch normalization.
1. Plot the increase in training and test accuracy.
2. How large can you make the learning rate?
3. Do we need batch normalization in every layer? Experiment with it?
4. Can you replace dropout by batch normalization? How does the behavior change?
5. Fix the parameters beta and gamma, and observe and analyze the results.
6. Review the online documentation for BatchNorm from the high-level APIs to see the other
applications for batch normalization.
7. Research ideas: think of other normalization transforms that you can apply? Can you apply
the probability integral transform? How about a full rank covariance estimate?
Discussions100
7.6 Residual Networks (ResNet)
As we design increasingly deeper networks it becomes imperative to understand how adding layers
can increase the complexity and expressiveness of the network. Even more important is the
ability to design networks where adding layers makes networks strictly more expressive rather
than just different. To make some progress we need a bit of mathematics.
7.6.1 Function Classes
Consider F, the class of functions that a specific network architecture (together with learning
rates and other hyperparameter settings) can reach. That is, for all f 2 F there exists some set of
parameters (e.g., weights and biases) that can be obtained through training on a suitable dataset.
Let us assume that f is the �truth� function that we really would like to find. If it is in F, we are in
good shape but typically we will not be quite so lucky. Instead, we will try to find some fF
which
is our best bet within F. For instance, given a dataset with features X and labels y, we might try
finding it by solving the following optimization problem:
f
F
:= argmin
f
L(X; y; f) subject to f 2 F: (7.6.1)
It is only reasonable to assume that if we design a different and more powerful architecture F? we
should arrive at a better outcome. In other words, we would expect that fF
? is �better� than fF
.
However, if F ? F? there is no guarantee that this should even happen. In fact, fF
? might well
100 https://discuss.d2l.ai/t/83
7.6. Residual Networks (ResNet) 287
be worse. As illustrated by Fig. 7.6.1, for non-nested function classes, a larger function class does
not always move closer to the �truth� function f. For instance, on the left of Fig. 7.6.1, though
F3 is closer to f than F1, F6 moves away and there is no guarantee that further increasing the
complexity can reduce the distance from f. With nested function classes where F1  : : :  F6
on the right of Fig. 7.6.1, we can avoid the aforementioned issue from the non-nested function
classes.
Fig. 7.6.1: For non-nested function classes, a larger (indicated by area) function class does not
guarantee to get closer to the �truth� function (f). This does not happen in nested function
classes.
Thus, only if larger function classes contain the smaller ones are we guaranteed that increasing
them strictly increases the expressive power of the network. For deep neural networks, if we can
train the newly-added layer into an identity function f(x) = x, the new model will be as effective
as the original model. As the new model may get a better solution to fit the training dataset, the
added layer might make it easier to reduce training errors.
This is the question that He et al. considered when working on very deep computer vision models
(He et al., 2016a). At the heart of their proposed residual network (ResNet) is the idea that every
additional layer should more easily contain the identity function as one of its elements. These
considerations are rather profound but they led to a surprisingly simple solution, a residual block.
With it, ResNet won the ImageNet Large Scale Visual Recognition Challenge in 2015. The design
had a profound influence on how to build deep neural networks.
7.6.2 Residual Blocks
Let us focus on a local part of a neural network, as depicted in Fig. 7.6.2. Denote the input by x.
We assume that the desired underlying mapping we want to obtain by learning is f(x), to be used
as the input to the activation function on the top. On the left of Fig. 7.6.2, the portion within the
dotted-line box must directly learn the mapping f(x). On the right, the portion within the dottedline
box needs to learn the residual mapping f(x) ?? x, which is how the residual block derives its
name. If the identity mapping f(x) = x is the desired underlying mapping, the residual mapping
is easier to learn: we only need to push the weights and biases of the upper weight layer (e.g.,
fully-connected layer and convolutional layer) within the dotted-line box to zero. The right figure
in Fig. 7.6.2 illustrates the residual block of ResNet, where the solid line carrying the layer input x to
288 Chapter 7. Modern Convolutional Neural Networks
the addition operator is called a residual connection (or shortcut connection). With residual blocks,
inputs can forward propagate faster through the residual connections across layers.
Fig. 7.6.2: A regular block (left) and a residual block (right).
ResNet follows VGG?s full 3  3 convolutional layer design. The residual block has two 3  3 convolutional
layers with the same number of output channels. Each convolutional layer is followed
by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution
operations and add the input directly before the final ReLU activation function. This kind of
design requires that the output of the two convolutional layers has to be of the same shape as the
input, so that they can be added together. If we want to change the number of channels, we need
to introduce an additional 1  1 convolutional layer to transform the input into the desired shape
for the addition operation. Let us have a look at the code below.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
class Residual(nn.Block): #@save
def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
super().__init__(**kwargs)
self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
strides=strides)
self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
if use_1x1conv:
self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
strides=strides)
else:
self.conv3 = None
self.bn1 = nn.BatchNorm()
self.bn2 = nn.BatchNorm()
def forward(self, X):
Y = npx.relu(self.bn1(self.conv1(X)))
(continues on next page)
7.6. Residual Networks (ResNet) 289
(continued from previous page)
Y = self.bn2(self.conv2(Y))
if self.conv3:
X = self.conv3(X)
return npx.relu(Y + X)
This code generates two types of networks: one where we add the input to the output before applying
the ReLU nonlinearity whenever use_1x1conv=False, and one where we adjust channels
and resolution by means of a 1  1 convolution before adding. Fig. 7.6.3 illustrates this:
Fig. 7.6.3: ResNet block with and without 1  1 convolution.
Now let us look at a situation where the input and output are of the same shape.
blk = Residual(3)
blk.initialize()
X = np.random.uniform(size=(4, 3, 6, 6))
blk(X).shape
(4, 3, 6, 6)
We also have the option to halve the output height and width while increasing the number of
output channels.
blk = Residual(6, use_1x1conv=True, strides=2)
blk.initialize()
blk(X).shape
(4, 6, 3, 3)
290 Chapter 7. Modern Convolutional Neural Networks
7.6.3 ResNet Model
The first two layers of ResNet are the same as those of the GoogLeNet we described before: the
77 convolutional layer with 64 output channels and a stride of 2 is followed by the 33 maximum
pooling layer with a stride of 2. The difference is the batch normalization layer added after each
convolutional layer in ResNet.
net = nn.Sequential()
net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
nn.BatchNorm(), nn.Activation('relu'),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))
GoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four modules
made up of residual blocks, each of which uses several residual blocks with the same number
of output channels. The number of channels in the first module is the same as the number of
input channels. Since a maximum pooling layer with a stride of 2 has already been used, it is not
necessary to reduce the height and width. In the first residual block for each of the subsequent
modules, the number of channels is doubled compared with that of the previous module, and the
height and width are halved.
Now, we implement this module. Note that special processing has been performed on the first
module.
def resnet_block(num_channels, num_residuals, first_block=False):
blk = nn.Sequential()
for i in range(num_residuals):
if i == 0 and not first_block:
blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
else:
blk.add(Residual(num_channels))
return blk
Then, we add all the modules to ResNet. Here, two residual blocks are used for each module.
net.add(resnet_block(64, 2, first_block=True),
resnet_block(128, 2),
resnet_block(256, 2),
resnet_block(512, 2))
Finally, just like GoogLeNet, we add a global average pooling layer, followed by the fully-connected
layer output.
net.add(nn.GlobalAvgPool2D(), nn.Dense(10))
There are 4 convolutional layers in each module (excluding the 11 convolutional layer). Together
with the first 7  7 convolutional layer and the final fully-connected layer, there are 18 layers in
total. Therefore, this model is commonly known as ResNet-18. By configuring different numbers
of channels and residual blocks in the module, we can create different ResNet models, such as
the deeper 152-layer ResNet-152. Although the main architecture of ResNet is similar to that of
GoogLeNet, ResNet?s structure is simpler and easier to modify. All these factors have resulted in
the rapid and widespread use of ResNet. Fig. 7.6.4 depicts the full ResNet-18.
7.6. Residual Networks (ResNet) 291
Fig. 7.6.4: The ResNet-18 architecture.
Before training ResNet, let us observe how the input shape changes across different modules in
ResNet. As in all the previous architectures, the resolution decreases while the number of channels
increases up until the point where a global average pooling layer aggregates all features.
X = np.random.uniform(size=(1, 1, 224, 224))
net.initialize()
for layer in net:
X = layer(X)
print(layer.name, 'output shape:\t', X.shape)
292 Chapter 7. Modern Convolutional Neural Networks
conv5 output shape: (1, 64, 112, 112)
batchnorm4 output shape: (1, 64, 112, 112)
relu0 output shape: (1, 64, 112, 112)
pool0 output shape: (1, 64, 56, 56)
sequential1 output shape: (1, 64, 56, 56)
sequential2 output shape: (1, 128, 28, 28)
sequential3 output shape: (1, 256, 14, 14)
sequential4 output shape: (1, 512, 7, 7)
pool1 output shape: (1, 512, 1, 1)
dense0 output shape: (1, 10)
7.6.4 Training
We train ResNet on the Fashion-MNIST dataset, just like before.
lr, num_epochs, batch_size = 0.05, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
loss 0.010, train acc 0.998, test acc 0.907
4886.6 examples/sec on gpu(0)
Summary
� Nested function classes are desirable. Learning an additional layer in deep neural networks
as an identity function (though this is an extreme case) should be made easy.
� The residual mapping can learn the identity function more easily, such as pushing parameters
in the weight layer to zero.
� We can train an effective deep neural network by having residual blocks. Inputs can forward
propagate faster through the residual connections across layers.
� ResNet had a major influence on the design of subsequent deep neural networks, both for
convolutional and sequential nature.
7.6. Residual Networks (ResNet) 293
Exercises
1. What are the major differences between the Inception block in Fig. 7.4.1 and the residual
block? After removing some paths in the Inception block, how are they related to each other?
2. Refer to Table 1 in the ResNet paper (He et al., 2016a) to implement different variants.
3. For deeper networks, ResNet introduces a �bottleneck� architecture to reduce model complexity.
Try to implement it.
4. In subsequent versions of ResNet, the authors changed the �convolution, batch normalization,
and activation� structure to the �batch normalization, activation, and convolution�
structure. Make this improvement yourself. See Figure 1 in (He et al., 2016b) for details.
5. Why cannot we just increase the complexity of functions without bound, even if the function
classes are nested?
Discussions101
7.7 Densely Connected Networks (DenseNet)
ResNet significantly changed the view of how to parametrize the functions in deep networks.
DenseNet (dense convolutional network) is to some extent the logical extension of this (Huang
et al., 2017). To understand how to arrive at it, let us take a small detour to mathematics.
7.7.1 From ResNet to DenseNet
Recall the Taylor expansion for functions. For the point x = 0 it can be written as
f(x) = f(0) + f
?
(0)x +
f??(0)
2!
x2 +
f???(0)
3!
x3 + : : : : (7.7.1)
The key point is that it decomposes a function into increasingly higher order terms. In a similar
vein, ResNet decomposes functions into
f(x) = x + g(x): (7.7.2)
That is, ResNet decomposes f into a simple linear term and a more complex nonlinear one. What
if we want to capture (not necessarily add) information beyond two terms? One solution was
DenseNet (Huang et al., 2017).
Fig. 7.7.1: The main difference between ResNet (left) and DenseNet (right) in cross-layer connections:
use of addition and use of concatenation.
101 https://discuss.d2l.ai/t/85
294 Chapter 7. Modern Convolutional Neural Networks
As shown in Fig. 7.7.1, the key difference between ResNet and DenseNet is that in the latter case
outputs are concatenated (denoted by [; ]) rather than added. As a result, we perform a mapping
from x to its values after applying an increasingly complex sequence of functions:
x ! [x; f1(x); f2([x; f1(x)]); f3([x; f1(x); f2([x; f1(x)])]); : : :] : (7.7.3)
In the end, all these functions are combined in MLP to reduce the number of features again. In
terms of implementation this is quite simple: rather than adding terms, we concatenate them.
The name DenseNet arises from the fact that the dependency graph between variables becomes
quite dense. The last layer of such a chain is densely connected to all previous layers. The dense
connections are shown in Fig. 7.7.2.
Fig. 7.7.2: Dense connections in DenseNet.
The main components that compose a DenseNet are dense blocks and transition layers. The former
define how the inputs and outputs are concatenated, while the latter control the number of
channels so that it is not too large.
7.7.2 Dense Blocks
DenseNet uses the modified �batch normalization, activation, and convolution� structure of
ResNet (see the exercise in Section 7.6). First, we implement this convolution block structure.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
def conv_block(num_channels):
blk = nn.Sequential()
blk.add(nn.BatchNorm(),
nn.Activation('relu'),
nn.Conv2D(num_channels, kernel_size=3, padding=1))
return blk
A dense block consists of multiple convolution blocks, each using the same number of output channels.
In the forward propagation, however, we concatenate the input and output of each convolution
block on the channel dimension.
class DenseBlock(nn.Block):
def __init__(self, num_convs, num_channels, **kwargs):
super().__init__(**kwargs)
self.net = nn.Sequential()
(continues on next page)
7.7. Densely Connected Networks (DenseNet) 295
(continued from previous page)
for _ in range(num_convs):
self.net.add(conv_block(num_channels))
def forward(self, X):
for blk in self.net:
Y = blk(X)
# Concatenate the input and output of each block on the channel
# dimension
X = np.concatenate((X, Y), axis=1)
return X
In the following example, we define a DenseBlock instance with 2 convolution blocks of 10 output
channels. When using an input with 3 channels, we will get an output with 3+210 = 23 channels.
The number of convolution block channels controls the growth in the number of output channels
relative to the number of input channels. This is also referred to as the growth rate.
blk = DenseBlock(2, 10)
blk.initialize()
X = np.random.uniform(size=(4, 3, 8, 8))
Y = blk(X)
Y.shape
(4, 23, 8, 8)
7.7.3 Transition Layers
Since each dense block will increase the number of channels, adding too many of them will lead
to an excessively complex model. A transition layer is used to control the complexity of the model.
It reduces the number of channels by using the 1  1 convolutional layer and halves the height
and width of the average pooling layer with a stride of 2, further reducing the complexity of the
model.
def transition_block(num_channels):
blk = nn.Sequential()
blk.add(nn.BatchNorm(), nn.Activation('relu'),
nn.Conv2D(num_channels, kernel_size=1),
nn.AvgPool2D(pool_size=2, strides=2))
return blk
Apply a transition layer with 10 channels to the output of the dense block in the previous example.
This reduces the number of output channels to 10, and halves the height and width.
blk = transition_block(10)
blk.initialize()
blk(Y).shape
(4, 10, 4, 4)
296 Chapter 7. Modern Convolutional Neural Networks
7.7.4 DenseNet Model
Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer
and maximum pooling layer as in ResNet.
net = nn.Sequential()
net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
nn.BatchNorm(), nn.Activation('relu'),
nn.MaxPool2D(pool_size=3, strides=2, padding=1))
Then, similar to the four modules made up of residual blocks that ResNet uses, DenseNet uses
four dense blocks. Similar to ResNet, we can set the number of convolutional layers used in each
dense block. Here, we set it to 4, consistent with the ResNet-18 model in Section 7.6. Furthermore,
we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block
to 32, so 128 channels will be added to each dense block.
In ResNet, the height and width are reduced between each module by a residual block with a
stride of 2. Here, we use the transition layer to halve the height and width and halve the number
of channels.
# `num_channels`: the current number of channels
num_channels, growth_rate = 64, 32
num_convs_in_dense_blocks = [4, 4, 4, 4]
for i, num_convs in enumerate(num_convs_in_dense_blocks):
net.add(DenseBlock(num_convs, growth_rate))
# This is the number of output channels in the previous dense block
num_channels += num_convs * growth_rate
# A transition layer that halves the number of channels is added between
# the dense blocks
if i != len(num_convs_in_dense_blocks) - 1:
num_channels //= 2
net.add(transition_block(num_channels))
Similar to ResNet, a global pooling layer and a fully-connected layer are connected at the end to
produce the output.
net.add(nn.BatchNorm(),
nn.Activation('relu'),
nn.GlobalAvgPool2D(),
nn.Dense(10))
7.7.5 Training
Since we are using a deeper network here, in this section, we will reduce the input height and
width from 224 to 96 to simplify the computation.
lr, num_epochs, batch_size = 0.1, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)
7.7. Densely Connected Networks (DenseNet) 297
loss 0.144, train acc 0.947, test acc 0.881
5505.3 examples/sec on gpu(0)
Summary
� In terms of cross-layer connections, unlike ResNet, where inputs and outputs are added together,
DenseNet concatenates inputs and outputs on the channel dimension.
� The main components that compose DenseNet are dense blocks and transition layers.
� We need to keep the dimensionality under control when composing the network by adding
transition layers that shrink the number of channels again.
Exercises
1. Why do we use average pooling rather than maximum pooling in the transition layer?
2. One of the advantages mentioned in the DenseNet paper is that its model parameters are
smaller than those of ResNet. Why is this the case?
3. One problem for which DenseNet has been criticized is its high memory consumption.
1. Is this really the case? Try to change the input shape to 224  224 to see the actual GPU
memory consumption.
2. Can you think of an alternative means of reducing the memory consumption? How
would you need to change the framework?
4. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper
(Huang et al., 2017).
5. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price
prediction task in Section 4.10.
Discussions102
102 https://discuss.d2l.ai/t/87
298 Chapter 7. Modern Convolutional Neural Networks
8 | Recurrent Neural Networks
So far we encountered two types of data: generic vectors and images. For the latter we designed
specialized layers to take advantage of the regularity properties in them. In other words, if we were
to permute the pixels in an image, it would be much more difficult to reason about its content of
something that would look much like the background of a test pattern in the times of analog TV.
Most importantly, so far we tacitly assumed that our data is generated i.i.d., i.e., independently
and identically distributed, all drawn from some distribution. Unfortunately, this is not true for
most data. For instance, the words in this paragraph are written in sequence, and it would be
quite difficult to decipher its meaning if they were permuted randomly. Likewise, image frames
in a video, the audio signal in a conversation, or the browsing behavior on a website, all follow
sequential order. It is thus only reasonable to assume that specialized models for such data will
do better at describing it and at solving estimation problems.
Another issue arises from the fact that we might not only receive a sequence as an input but rather
might be expected to continue the sequence. For instance, the task could be to continue the series
2, 4, 6, 8, 10, � This is quite common in time series analysis, to predict the stock market, the fever
curve of a patient or the acceleration needed for a race car. Again we want to have models that
can handle such data.
In short, while convolutional neural networks can efficiently process spatial information, recurrent
neural networks are designed to better handle sequential information. These networks introduce
state variables to store past information, together with the current inputs, to determine
the current outputs.
Many of the examples for using recurrent networks are based on text data. Hence, we will emphasize
language models in this chapter. After a more formal review of sequence data we discuss
basic concepts of a language model and use this discussion as the inspiration for the design of recurrent
neural networks. Next, we describe the gradient calculation method in recurrent neural
networks to explore problems that may be encountered in recurrent neural network training.
8.1 Sequence Models
Imagine that you are watching movies on Netflix. As a good Netflix user, you decide to rate each
of the movies religiously. After all, a good movie is a good movie, and you want to watch more of
them, right? As it turns out, things are not quite so simple. People?s opinions on movies can change
quite significantly over time. In fact, psychologists even have names for some of the effects:
� There is anchoring103, based on someone else?s opinion. For instance after the Oscar awards,
ratings for the corresponding movie go up, even though it is still the same movie. This effect
103 https://en.wikipedia.org/wiki/Anchoring
299
persists for a few months until the award is forgotten. (Wu et al., 2017) showed that the effect
lifts rating by over half a point.
� There is the Hedonic adaptation104, where humans quickly adapt to accept an improved (or
a bad) situation as the new normal. For instance, after watching many good movies, the
expectations that the next movie is equally good or better are high, hence even an average
movie might be considered a bad movie after many great ones.
� There is seasonality. Very few viewers like to watch a Santa Claus movie in August.
� In some cases movies become unpopular due to the misbehaviors of directors or actors in
the production.
� Some movies become cult movies, because they were almost comically bad. Plan 9 from
Outer Space and Troll 2 achieved a high degree of notoriety for this reason.
In short, ratings are anything but stationary. Using temporal dynamics helped (Koren, 2009) to
recommend movies more accurately. But it is not just about movies.
� Many users have highly particular behavior when it comes to the time when they open apps.
For instance, social media apps are much more popular after school with students. Stock
market trading apps are more commonly used when the markets are open.
� It is much harder to predict tomorrow?s stock prices than to fill in the blanks for a stock price
we missed yesterday, even though both are just a matter of estimating one number. After
all, foresight is so much harder than hindsight. In statistics, the former (predicting beyond
the known observations) is called extrapolation whereas the latter (estimating between the
existing observations) is called interpolation.
� Music, speech, text, movies, steps, etc. are all sequential in nature. If we were to permute
them they would make little sense. The headline dog bites man is much less surprising than
man bites dog, even though the words are identical.
� Earthquakes are strongly correlated, i.e., after a massive earthquake there are very likely
several smaller aftershocks, much more so than without the strong quake. In fact, earthquakes
are spatiotemporally correlated, i.e., the aftershocks typically occur within a short
time span and in close proximity.
� Humans interact with each other in a sequential nature, as can be seen in Twitter fights,
dance patterns and debates.
8.1.1 Statistical Tools
In short, we need statistical tools and new deep neural networks architectures to deal with sequence
data. To keep things simple, we use the stock price illustrated in Fig. 8.1.1 as an example.
104 https://en.wikipedia.org/wiki/Hedonic_treadmill
300 Chapter 8. Recurrent Neural Networks
Fig. 8.1.1: FTSE 100 index over 30 years
Let us denote the prices by xt  0, i.e., at time t 2 N we observe price xt. For a trader to do well
in the stock market on day t he should want to predict xt via
xt  p(xt j xt??1; : : : ; x1): (8.1.1)
Autoregressive Models
In order to achieve this, our trader could use a regressor such as the one we trained in Section
3.3. There is just a major problem: the number of inputs, xt??1; : : : ; x1 varies, depending on t.
That is, the number increases with the amount of data that we encounter, and we will need an
approximation to make this computationally tractable. Much of what follows in this chapter will
revolve around how to estimate p(xt j xt??1; : : : ; x1) efficiently. In a nutshell it boils down to two
strategies:
1. Assume that the potentially rather long sequence xt??1; : : : ; x1 is not really necessary. In this
case we might content ourselves with some timespan  and only use xt??1; : : : ; xt?? observations.
The immediate benefit is that now the number of arguments is always the same, at
least for t >  . This allows us to train a deep network as indicated above. Such models will
be called autoregressive models, as they quite literally perform regression on themselves.
2. Another strategy, shown in Fig. 8.1.2, is to try and keep some summary ht of the past observations,
at the same time update ht in addition to the prediction ^xt. This leads to models that
estimate xt with ^xt = p(xt j xt??1; ht) and moreover updates of the form ht = g(ht??1; xt??1).
Since ht is never observed, these models are also called latent autoregressive models. LSTMs
and GRUs are examples of this.
Fig. 8.1.2: A latent autoregressive model.
8.1. Sequence Models 301
Both cases raise the obvious question of how to generate training data. One typically uses historical
observations to predict the next observation given the ones up to right now. Obviously we do
not expect time to stand still. However, a common assumption is that while the specific values of
xt might change, at least the dynamics of the time series itself will not. This is reasonable, since
novel dynamics are just that, novel and thus not predictable using data that we have so far. Statisticians
call dynamics that do not change stationary. Regardless of what we do, we will thus get an
estimate of the entire time series via
p(x1; : : : ; xT ) =
?T
t=1
p(xt j xt??1; : : : ; x1): (8.1.2)
Note that the above considerations still hold if we deal with discrete objects, such as words, rather
than numbers. The only difference is that in such a situation we need to use a classifier rather
than a regressor to estimate p(xt j xt??1; : : : ; x1).
Markov Model
Recall the approximation that in an autoregressive model we use only (xt??1; : : : ; xt?? ) instead of
(xt??1; : : : ; x1) to estimate xt. Whenever this approximation is accurate we say that the sequence
satisfies a Markov condition. In particular, if  = 1, we have a first order Markov model and p(x) is
given by
p(x1; : : : ; xT ) =
?T
t=1
p(xt j xt??1): (8.1.3)
Such models are particularly nice whenever xt assumes only a discrete value, since in this case
dynamic programming can be used to compute values along the chain exactly. For instance, we
can compute p(xt+1 j xt??1) efficiently using the fact that we only need to take into account a very
short history of past observations:
p(xt+1 j xt??1) =
?
xt
p(xt+1 j xt)p(xt j xt??1): (8.1.4)
Going into details of dynamic programming is beyond the scope of this section, but we will introduce
it in Section 9.4. Control and reinforcement learning algorithms use such tools extensively.
Causality
In principle, there is nothing wrong with unfolding p(x1; : : : ; xT ) in reverse order. After all, by
conditioning we can always write it via
p(x1; : : : ; xT ) =
?1
t=T
p(xt j xt+1; : : : ; xT ): (8.1.5)
In fact, if we have a Markov model, we can obtain a reverse conditional probability distribution,
too. In many cases, however, there exists a natural direction for the data, namely going forward
in time. It is clear that future events cannot influence the past. Hence, if we change xt, we may be
able to influence what happens for xt+1 going forward but not the converse. That is, if we change
xt, the distribution over past events will not change. Consequently, it ought to be easier to explain
p(xt+1 j xt) rather than p(xt j xt+1). For instance, (Hoyer et al., 2009) show that in some cases we
can find xt+1 = f(xt) + ? for some additive noise, whereas the converse is not true. This is great
news, since it is typically the forward direction that we are interested in estimating. For more on
this topic see e.g., the book by (Peters et al., 2017a). We are barely scratching the surface of it.
302 Chapter 8. Recurrent Neural Networks
8.1.2 A Toy Example
After so much theory, let us try this out in practice. Let us begin by generating some data. To keep
things simple we generate our time series by using a sine function with some additive noise.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx, gluon, init
from mxnet.gluon import nn
npx.set_np()
T = 1000 # Generate a total of 1000 points
time = np.arange(0, T, dtype=np.float32)
x = np.sin(0.01 * time) + np.random.normal(0, 0.2, (T,))
d2l.plot(time, [x])
Next we need to turn this time series into features and labels that the network can train on. Based
on the embedding dimension  we map the data into pairs yt = xt and zt = (xt??1; : : : ; xt?? ). The
astute reader might have noticed that this gives us  fewer data examples, since we do not have
sufficient history for the first  of them. A simple fix, in particular if the time series is long is to
discard those few terms. Alternatively we could pad the time series with zeros. The code below
is essentially identical to the training code in previous sections. We kept the architecture fairly
simple.
tau = 4
features = np.zeros((T-tau, tau))
for i in range(tau):
features[:, i] = x[i: T-tau+i]
labels = d2l.reshape(x[tau:], (-1, 1))
batch_size, n_train = 16, 600
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
batch_size, is_train=True)
A few layers of a fully connected network, ReLU activation andL2 loss. Since much of the modeling
is identical to the previous sections when we built regression estimators, we will not delve into
much detail.
8.1. Sequence Models 303
# Vanilla MLP architecture
def get_net():
net = nn.Sequential()
net.add(nn.Dense(10, activation='relu'),
nn.Dense(1))
net.initialize(init.Xavier())
return net
# Least mean squares loss
loss = gluon.loss.L2Loss()
Now we are ready to train.
def train_net(net, train_iter, loss, epochs, lr):
trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': lr})
for epoch in range(1, epochs + 1):
for X, y in train_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(batch_size)
print(f'epoch {epoch}, '
f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')
net = get_net()
train_net(net, train_iter, loss, 10, 0.01)
epoch 1, loss: 0.039526
epoch 2, loss: 0.032300
epoch 3, loss: 0.028369
epoch 4, loss: 0.027100
epoch 5, loss: 0.026571
epoch 6, loss: 0.026366
epoch 7, loss: 0.025704
epoch 8, loss: 0.025523
epoch 9, loss: 0.026450
epoch 10, loss: 0.026086
8.1.3 Predictions
Since training loss is small, we would expect our model to work well. Let us see what this means
in practice. The first thing to check is how well the model is able to predict what happens in the
next timestep.
estimates = net(features)
d2l.plot([time, time[tau:]], [d2l.numpy(x), d2l.numpy(estimates)],
legend=['data', 'estimate'])
304 Chapter 8. Recurrent Neural Networks
This looks nice, just as we expected it. Even beyond 600 observations the estimates still look rather
trustworthy. There is just one little problem to this: if we observe data only until timestep 600, we
cannot hope to receive the ground truth for all future predictions. Instead, we need to work our
way forward one step at a time:
x601 = f(x600; : : : ; x597);
x602 = f(x601; : : : ; x598);
x603 = f(x602; : : : ; x599):
(8.1.6)
In other words, we will have to use our own predictions to make future predictions. Let us see
how well this goes.
predictions = np.zeros(T)
predictions[:n_train] = x[:n_train]
for i in range(n_train, T):
predictions[i] = net(
predictions[(i-tau):i].reshape(1, -1)).reshape(1)
d2l.plot([time, time[tau:], time[n_train:]],
[d2l.numpy(x), d2l.numpy(estimates),
d2l.numpy(predictions[n_train:])],
legend=['data', 'estimate', 'multistep'], figsize=(4.5, 2.5))
As the above example shows, this is a spectacular failure. The estimates decay to a constant pretty
quickly after a few prediction steps. Why did the algorithm work so poorly? This is ultimately due
8.1. Sequence Models 305
to the fact that the errors build up. Let us say that after step 1 we have some error ?1 = ?. Now
the input for step 2 is perturbed by ?1, hence we suffer some error in the order of ?2 = ? + L?1,
and so on. The error can diverge rather rapidly from the true observations. This is a common
phenomenon. For instance, weather forecasts for the next 24 hours tend to be pretty accurate but
beyond that the accuracy declines rapidly. We will discuss methods for improving this throughout
this chapter and beyond.
Let us verify this observation by computing the k-step predictions on the entire sequence.
k = 33 # Look up to k - tau steps ahead
features = np.zeros((k, T-k))
for i in range(tau): # Copy the first tau features from x
features[i] = x[i:T-k+i]
for i in range(tau, k): # Predict the (i-tau)-th step
features[i] = net(features[(i-tau):i].T).T
steps = (4, 8, 16, 32)
d2l.plot([time[i:T-k+i] for i in steps],
[d2l.numpy(features[i]) for i in steps],
legend=[f'step {i}' for i in steps], figsize=(4.5, 2.5))
This clearly illustrates how the quality of the estimates changes as we try to predict further into the
future. While the 8-step predictions are still pretty good, anything beyond that is pretty useless.
Summary
� Sequence models require specialized statistical tools for estimation. Two popular choices
are autoregressive models and latent-variable autoregressive models.
� As we predict further in time, the errors accumulate and the quality of the estimates degrades,
often dramatically.
� There is quite a difference in difficulty between interpolation and extrapolation. Consequently,
if you have a time series, always respect the temporal order of the data when training,
i.e., never train on future data.
� For causal models (e.g., time going forward), estimating the forward direction is typically a
lot easier than the reverse direction.
306 Chapter 8. Recurrent Neural Networks
Exercises
1. Improve the above model.
� Incorporate more than the past 4 observations? How many do you really need?
� How many would you need if there was no noise? Hint: you can write sin and cos as a
differential equation.
� Can you incorporate older features while keeping the total number of features constant?
Does this improve accuracy? Why?
� Change the neural network architecture and see what happens.
2. An investor wants to find a good security to buy. He looks at past returns to decide which
one is likely to do well. What could possibly go wrong with this strategy?
3. Does causality also apply to text? To which extent?
4. Give an example for when a latent autoregressive model might be needed to capture the
dynamic of the data.
Discussions105
8.2 Text Preprocessing
Text is an important example of sequence data. An article can be simply viewed as a sequence of
words, or a sequence of characters. Given text data is a major data format besides images we are
using in this book, this section will dedicate to explain the common preprocessing steps for text
data. Such preprocessing often consists of four steps:
1. Load text as strings into memory.
2. Split strings into tokens, where a token could be a word or a character.
3. Build a vocabulary for these tokens to map them into numerical indices.
4. Map all the tokens in data into indices for ease of feeding into models.
import collections
from d2l import mxnet as d2l
import re
8.2.1 Reading the Dataset
To get started we load text from H. G. Wells? Time Machine106. This is a fairly small corpus of just
over 30; 000 words, but for the purpose of what we want to illustrate this is just fine. More realistic
document collections contain many billions of words. The following function reads the dataset
into a list of sentences, each sentence is a string. Here we ignore punctuation and capitalization.
105 https://discuss.d2l.ai/t/113
106 http://www.gutenberg.org/ebooks/35
8.2. Text Preprocessing 307
#@save
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
'090b5e7e70c295757f55df93cb0a180b9691891a')
def read_time_machine(): #@save
"""Load the time machine book into a list of sentences."""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line.strip().lower())
for line in lines]
lines = read_time_machine()
f'# sentences {len(lines)}'
'# sentences 3221'
8.2.2 Tokenization
For each sentence, we split it into a list of tokens. A token is a data example the model will train
and predict. The following function supports splitting a sentence into words or characters, and
returns a list of split strings.
def tokenize(lines, token='word'): #@save
"""Split sentences into word or char tokens."""
if token == 'word':
return [line.split(' ') for line in lines]
elif token == 'char':
return [list(line) for line in lines]
else:
print('ERROR: unknown token type '+token)
tokens = tokenize(lines)
tokens[0:2]
[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], ['']]
8.2.3 Vocabulary
The string type of the token is inconvenient to be used by models, which take numerical inputs.
Now let us build a dictionary, often called vocabulary as well, to map string tokens into numerical
indices starting from 0. To do so, we first count the unique tokens in all documents, called corpus,
and then assign a numerical index to each unique token according to its frequency. Rarely
appeared tokens are often removed to reduce the complexity. A token does not exist in corpus
or has been removed is mapped into a special unknown (�<unk>�) token. We optionally add a list
of reserved tokens, such as �<pad>� a token for padding, �<bos>� to present the beginning for a
sentence, and �<eos>� for the ending of a sentence.
class Vocab: #@save
def __init__(self, tokens, min_freq=0, reserved_tokens=None):
(continues on next page)
308 Chapter 8. Recurrent Neural Networks
(continued from previous page)
if reserved_tokens is None:
reserved_tokens = []
# Sort according to frequencies
counter = count_corpus(tokens)
self.token_freqs = sorted(counter.items(), key=lambda x: x[0])
self.token_freqs.sort(key=lambda x: x[1], reverse=True)
self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens
uniq_tokens += [token for token, freq in self.token_freqs
if freq >= min_freq and token not in uniq_tokens]
self.idx_to_token, self.token_to_idx = [], dict()
for token in uniq_tokens:
self.idx_to_token.append(token)
self.token_to_idx[token] = len(self.idx_to_token) - 1
def __len__(self):
return len(self.idx_to_token)
def __getitem__(self, tokens):
if not isinstance(tokens, (list, tuple)):
return self.token_to_idx.get(tokens, self.unk)
return [self.__getitem__(token) for token in tokens]
def to_tokens(self, indices):
if not isinstance(indices, (list, tuple)):
return self.idx_to_token[indices]
return [self.idx_to_token[index] for index in indices]
def count_corpus(sentences): #@save
# Flatten a list of token lists into a list of tokens
tokens = [tk for line in sentences for tk in line]
return collections.Counter(tokens)
We construct a vocabulary with the time machine dataset as the corpus, and then print the map
between a few tokens and their indices.
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[0:10])
[('<unk>', 0), ('the', 1), ('', 2), ('i', 3), ('and', 4), ('of', 5), ('a', 6), ('to', 7), (
,!'was', 8), ('in', 9)]
After that, we can convert each sentence into a list of numerical indices. To illustrate in detail, we
print two sentences with their corresponding indices.
for i in range(8, 10):
print('words:', tokens[i])
print('indices:', vocab[tokens[i]])
words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to',
,!'speak', 'of', 'him', '']
indices: [1, 20, 72, 17, 38, 12, 120, 43, 706, 7, 660, 5, 112, 2]
words: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes',
,!'shone', 'and']
(continues on next page)
8.2. Text Preprocessing 309
(continued from previous page)
indices: [8, 1654, 6, 3864, 634, 7, 131, 26, 344, 127, 484, 4]
8.2.4 Putting All Things Together
Using the above functions, we package everything into the load_corpus_time_machine function,
which returns corpus, a list of token indices, and vocab, the vocabulary of the time machine corpus.
The modification we did here is that corpus is a single list, not a list of token lists, since we
do not keep the sequence information in the following models. Besides, we use character tokens
to simplify the training in later sections.
def load_corpus_time_machine(max_tokens=-1): #@save
lines = read_time_machine()
tokens = tokenize(lines, 'char')
vocab = Vocab(tokens)
corpus = [vocab[tk] for line in tokens for tk in line]
if max_tokens > 0:
corpus = corpus[:max_tokens]
return corpus, vocab
corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
(171489, 28)
Summary
� We preprocessed the documents by tokenizing them into words or characters and then mapping
into indices.
Exercises
1. Tokenization is a key preprocessing step. It varies for different languages. Try to find another
3 commonly used methods to tokenize sentences.
Discussions107
8.3 Language Models and the Dataset
In Section 8.2, we see how to map text data into tokens, and these tokens can be viewed as a time
series of discrete observations. Assuming the tokens in a text of length T are in turn x1; x2; : : : ; xT ,
then, in the discrete time series, xt(1  t  T) can be considered as the output or label of timestep
t. Given such a sequence, the goal of a language model is to estimate the probability
p(x1; x2; : : : ; xT ): (8.3.1)
107 https://discuss.d2l.ai/t/115
310 Chapter 8. Recurrent Neural Networks
Language models are incredibly useful. For instance, an ideal language model would be able
to generate natural text just on its own, simply by drawing one word at a time wt  p(wt j
wt??1; : : : ;w1). Quite unlike the monkey using a typewriter, all text emerging from such a model
would pass as natural language, e.g., English text. Furthermore, it would be sufficient for generating
a meaningful dialog, simply by conditioning the text on previous dialog fragments. Clearly
we are still very far from designing such a system, since it would need to understand the text rather
than just generate grammatically sensible content.
Nonetheless language models are of great service even in their limited form. For instance, the
phrases �to recognize speech� and �to wreck a nice beach� sound very similar. This can cause ambiguity
in speech recognition, ambiguity that is easily resolved through a language model which
rejects the second translation as outlandish. Likewise, in a document summarization algorithm
it is worth while knowing that �dog bites man� is much more frequent than �man bites dog�, or
that �I want to eat grandma� is a rather disturbing statement, whereas �I want to eat, grandma� is
much more benign.
8.3.1 Estimating a Language Model
The obvious question is how we should model a document, or even a sequence of words. We can
take recourse to the analysis we applied to sequence models in the previous section. Let us start
by applying basic probability rules:
p(w1;w2; : : : ;wT ) = p(w1)
?T
t=2
p(wt j w1; : : : ;wt??1): (8.3.2)
For example, the probability of a text sequence containing four tokens consisting of words and
punctuation would be given as:
p(Statistics; is; fun; :) = p(Statistics)p(is j Statistics)p(fun j Statistics; is)p(: j Statistics; is; fun):
(8.3.3)
In order to compute the language model, we need to calculate the probability of words and the
conditional probability of a word given the previous few words, i.e., language model parameters.
Here, we assume that the training dataset is a large text corpus, such as all Wikipedia entries,
Project Gutenberg108, or all text posted online on the web. The probability of words can be calculated
from the relative word frequency of a given word in the training dataset.
For example, p(Statistics) can be calculated as the probability of any sentence starting with the
word �statistics�. A slightly less accurate approach would be to count all occurrences of the word
�statistics� and divide it by the total number of words in the corpus. This works fairly well, particularly
for frequent words. Moving on, we could attempt to estimate
^p(is j Statistics) =
n(Statistics is)
n(Statistics)
: (8.3.4)
Here n(w) and n(w;w?) are the number of occurrences of singletons and pairs of words respectively.
Unfortunately, estimating the probability of a word pair is somewhat more difficult, since
the occurrences of �Statistics is� are a lot less frequent. In particular, for some unusual word
combinations it may be tricky to find enough occurrences to get accurate estimates. Things take
a turn for the worse for 3-word combinations and beyond. There will be many plausible 3-word
combinations that we likely will not see in our dataset. Unless we provide some solution to give
108 https://en.wikipedia.org/wiki/Project_Gutenberg
8.3. Language Models and the Dataset 311
such word combinations nonzero weight, we will not be able to use these as a language model. If
the dataset is small or if the words are very rare, we might not find even a single one of them.
A common strategy is to perform some form of Laplace smoothing. We already encountered this
in our discussion of naive Bayes in Section 18.9 where the solution was to add a small constant to
all counts. This helps with singletons, e.g., via
^p(w) =
n(w) + ?1/m
n + ?1
;
^p(w
? j w) =
n(w;w?) + ?2 ^p(w?)
n(w) + ?2
;
^p(w
?? j w
?
;w) =
n(w;w?;w??) + ?3 ^p(w?;w??)
n(w;w?) + ?3
:
(8.3.5)
Here the coefficients ?i > 0 determine how much we use the estimate for a shorter sequence as
a fill-in for longer ones. Moreover, m is the total number of words we encounter. The above is
a rather primitive variant of what Kneser-Ney smoothing and Bayesian nonparametrics can accomplish.
See e.g., (Wood et al., 2011) for more detail of how to accomplish this. Unfortunately,
models like this get unwieldy rather quickly for the following reasons. First, we need to store all
counts. Second, this entirely ignores the meaning of the words. For instance, �cat� and �feline�
should occur in related contexts. It is quite difficult to adjust such models to additional contexts,
whereas, deep learning based language models are well suited to take this into account. Last, long
word sequences are almost certain to be novel, hence a model that simply counts the frequency
of previously seen word sequences is bound to perform poorly there.
8.3.2 Markov Models and n-grams
Before we discuss solutions involving deep learning, we need some more terminology and concepts.
Recall our discussion of Markov Models in the previous section. Let us apply this to language
modeling. A distribution over sequences satisfies the Markov property of first order if
p(wt+1 j wt; : : : ;w1) = p(wt+1 j wt). Higher orders correspond to longer dependencies. This
leads to a number of approximations that we could apply to model a sequence:
p(w1;w2;w3;w4) = p(w1)p(w2)p(w3)p(w4);
p(w1;w2;w3;w4) = p(w1)p(w2 j w1)p(w3 j w2)p(w4 j w3);
p(w1;w2;w3;w4) = p(w1)p(w2 j w1)p(w3 j w1;w2)p(w4 j w2;w3):
(8.3.6)
The probability formulae that involve one, two, and three variables are typically referred to as
unigram, bigram, and trigram models respectively. In the following, we will learn how to design
better models.
8.3.3 Natural Language Statistics
Let us see how this works on real data. We construct a vocabulary based on the time machine data
similar to Section 8.2 and print the top 10 most frequent words.
from d2l import mxnet as d2l
from mxnet import np, npx
import random
npx.set_np()
312 Chapter 8. Recurrent Neural Networks
tokens = d2l.tokenize(d2l.read_time_machine())
vocab = d2l.Vocab(tokens)
vocab.token_freqs[:10]
[('the', 2261),
('', 1282),
('i', 1267),
('and', 1245),
('of', 1155),
('a', 816),
('to', 695),
('was', 552),
('in', 541),
('that', 443)]
As we can see, the most popular words are actually quite boring to look at. They are often referred
to as stop words109 and thus filtered out. That said, they still carry meaning and we will use
them nonetheless. However, one thing that is quite clear is that the word frequency decays rather
rapidly. The 10th most frequent word is less than 1/5 as common as the most popular one. To get
a better idea we plot the graph of the word frequency.
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
xscale='log', yscale='log')
We are on to something quite fundamental here: the word frequency decays rapidly in a well
defined way. After dealing with the first four words as exceptions (?the?, ?i?, ?and?, ?of?), all remaining
words follow a straight line on a log-log plot. This means that words satisfy Zipf?s law110 which
states that the item frequency is given by
n(x) / (x + c)
?? and hence log n(x) = ?? log(x + c) + const: (8.3.7)
This should already give us pause if we want to model words by count statistics and smoothing.
After all, we will significantly overestimate the frequency of the tail, also known as the infrequent
109 https://en.wikipedia.org/wiki/Stop_words
110 https://en.wikipedia.org/wiki/Zipf%27s_law
8.3. Language Models and the Dataset 313
words. But what about the other word combinations (such as bigrams, trigrams, and beyond)? Let
us see whether the bigram frequency behaves in the same manner as the unigram frequency.
bigram_tokens = [[pair for pair in zip(
line[:-1], line[1:])] for line in tokens]
bigram_vocab = d2l.Vocab(bigram_tokens)
print(bigram_vocab.token_freqs[:10])
[(('of', 'the'), 297), (('in', 'the'), 161), (('i', 'had'), 126), (('and', 'the'), 104), (('i
,!', 'was'), 104), (('the', 'time'), 97), (('it', 'was'), 94), (('to', 'the'), 81), (('as',
,!'i'), 75), (('of', 'a'), 69)]
One thing is notable here. Out of the 10 most frequent word pairs, 9 are composed of stop words
and only one is relevant to the actual book��the time�. Furthermore, let us see whether the trigram
frequency behaves in the same manner.
trigram_tokens = [[triple for triple in zip(line[:-2], line[1:-1], line[2:])]
for line in tokens]
trigram_vocab = d2l.Vocab(trigram_tokens)
print(trigram_vocab.token_freqs[:10])
[(('the', 'time', 'traveller'), 53), (('the', 'time', 'machine'), 24), (('the', 'medical',
,!'man'), 22), (('it', 'seemed', 'to'), 14), (('it', 'was', 'a'), 14), (('i', 'began', 'to'),
,! 13), (('i', 'did', 'not'), 13), (('i', 'saw', 'the'), 13), (('here', 'and', 'there'), 12),
,! (('i', 'could', 'see'), 12)]
Last, let us visualize the token frequency among these three gram models: unigrams, bigrams,
and trigrams.
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token',
ylabel='frequency', xscale='log', yscale='log',
legend=['unigram', 'bigram', 'trigram'])
The graph is quite exciting for a number of reasons. First, beyond unigram words, also sequences
of words appear to be following Zipf?s law, albeit with a lower exponent, depending on sequence
314 Chapter 8. Recurrent Neural Networks
length. Second, the number of distinct n-grams is not that large. This gives us hope that there is
quite a lot of structure in language. Third, many n-grams occur very rarely, which makes Laplace
smoothing rather unsuitable for language modeling. Instead, we will use deep learning based
models.
8.3.4 Training Data Preparation
Before introducing the model, let us assume we will use a neural network to train a language
model. Now the question is how to read minibatches of examples and labels at random. Since
sequence data is by its very nature sequential, we need to address the issue of processing it. We
did so in a rather ad-hoc manner when we introduced in Section 8.1. Let us formalize this a bit.
In Fig. 8.3.1, we visualized several possible ways to obtain 5-grams in a sentence, here a token is a
character. Note that we have quite some freedom since we could pick an arbitrary offset.
Fig. 8.3.1: Different offsets lead to different subsequences when splitting up text.
Hence, which one should we pick? In fact, all of them are equally good. But if we pick all offsets
we end up with rather redundant data due to overlap, particularly if the sequences are long.
Picking just a random set of initial positions is no good either since it does not guarantee uniform
coverage of the array. For instance, if we pick n elements at random out of a set of n with random
replacement, the probability for a particular element not being picked is (1 ?? 1/n)n ! e??1 . This
means that we cannot expect uniform coverage this way. Even randomly permuting a set of all
offsets does not offer good guarantees. Instead we can use a simple trick to get both coverage and
randomness: use a random offset, after which one uses the terms sequentially. We describe how
to accomplish this for both random sampling and sequential partitioning strategies below.
Random Sampling
The following code randomly generates a minibatch from the data each time. Here, the batch size
batch_size indicates the number of examples in each minibatch and num_steps is the length of the
sequence (or timesteps if we have a time series) included in each example. In random sampling,
each example is a sequence arbitrarily captured on the original sequence. The positions of two
adjacent random minibatches on the original sequence are not necessarily adjacent. The target is
to predict the next character based on what we have seen so far, hence the labels are the original
sequence, shifted by one character.
8.3. Language Models and the Dataset 315
def seq_data_iter_random(corpus, batch_size, num_steps): #@save
# Offset the iterator over the data for uniform starts
corpus = corpus[random.randint(0, num_steps):]
# Subtract 1 extra since we need to account for label
num_examples = ((len(corpus) - 1) // num_steps)
example_indices = list(range(0, num_examples * num_steps, num_steps))
random.shuffle(example_indices)
def data(pos):
# This returns a sequence of length `num_steps` starting from `pos`
return corpus[pos: pos + num_steps]
# Discard half empty batches
num_batches = num_examples // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# `batch_size` indicates the random examples read each time
batch_indices = example_indices[i:(i+batch_size)]
X = [data(j) for j in batch_indices]
Y = [data(j + 1) for j in batch_indices]
yield np.array(X), np.array(Y)
Let us generate an artificial sequence from 0 to 30. We assume that the batch size and numbers
of timesteps are 2 and 6 respectively. This means that depending on the offset we can generate
between 4 and 5 (x; y) pairs. With a minibatch size of 2, we only get 2 minibatches.
my_seq = list(range(30))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=6):
print('X: ', X, '\nY:', Y)
X: [[ 7. 8. 9. 10. 11. 12.]
[13. 14. 15. 16. 17. 18.]]
Y: [[ 8. 9. 10. 11. 12. 13.]
[14. 15. 16. 17. 18. 19.]]
X: [[ 1. 2. 3. 4. 5. 6.]
[19. 20. 21. 22. 23. 24.]]
Y: [[ 2. 3. 4. 5. 6. 7.]
[20. 21. 22. 23. 24. 25.]]
Sequential Partitioning
In addition to random sampling of the original sequence, we can also make the positions of two
adjacent random minibatches adjacent in the original sequence.
def seq_data_iter_consecutive(corpus, batch_size, num_steps): #@save
# Offset for the iterator over the data for uniform starts
offset = random.randint(0, num_steps)
# Slice out data: ignore `num_steps` and just wrap around
num_indices = ((len(corpus) - offset - 1) // batch_size) * batch_size
Xs = np.array(corpus[offset:offset+num_indices])
Ys = np.array(corpus[offset+1:offset+1+num_indices])
Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
num_batches = Xs.shape[1] // num_steps
(continues on next page)
316 Chapter 8. Recurrent Neural Networks
(continued from previous page)
for i in range(0, num_batches * num_steps, num_steps):
X = Xs[:, i:(i+num_steps)]
Y = Ys[:, i:(i+num_steps)]
yield X, Y
Using the same settings, print input X and label Y for each minibatch of examples read by sequential
partitioning. The positions of two adjacent minibatches on the original sequence are adjacent.
for X, Y in seq_data_iter_consecutive(my_seq, batch_size=2, num_steps=6):
print('X: ', X, '\nY:', Y)
X: [[ 3. 4. 5. 6. 7. 8.]
[16. 17. 18. 19. 20. 21.]]
Y: [[ 4. 5. 6. 7. 8. 9.]
[17. 18. 19. 20. 21. 22.]]
X: [[ 9. 10. 11. 12. 13. 14.]
[22. 23. 24. 25. 26. 27.]]
Y: [[10. 11. 12. 13. 14. 15.]
[23. 24. 25. 26. 27. 28.]]
Now we wrap the above two sampling functions to a class so that we can use it as a data iterator
later.
class SeqDataLoader: #@save
"""An iterator to load sequence data."""
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
if use_random_iter:
self.data_iter_fn = d2l.seq_data_iter_random
else:
self.data_iter_fn = d2l.seq_data_iter_consecutive
self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
self.batch_size, self.num_steps = batch_size, num_steps
def __iter__(self):
return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
Last, we define a function load_data_time_machine that returns both the data iterator and the
vocabulary, so we can use it similarly as other functions with load_data prefix.
def load_data_time_machine(batch_size, num_steps, #@save
use_random_iter=False, max_tokens=10000):
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab
8.3. Language Models and the Dataset 317
Summary
� Language models are an important technology for natural language processing.
� n-grams provide a convenient model for dealing with long sequences by truncating the dependence.
� Long sequences suffer from the problem that they occur very rarely or never.
� Zipf?s law governs the word distribution for not only unigrams but also the other n-grams.
� There is a lot of structure but not enough frequency to deal with infrequent word combinations
efficiently via Laplace smoothing.
� The main choices for sequence partitioning are picking between consecutive and random
sequences.
� Given the overall document length, it is usually acceptable to be slightly wasteful with the
documents and discard half-empty minibatches.
Exercises
1. Suppose there are 100; 000 words in the training dataset. How much word frequency and
multi-word adjacent frequency does a four-gram need to store?
2. Review the smoothed probability estimates. Why are they not accurate? Hint: we are dealing
with a contiguous sequence rather than singletons.
3. How would you model a dialogue?
4. Estimate the exponent of Zipf?s law for unigrams, bigrams, and trigrams.
5. What other minibatch data sampling methods can you think of?
6. Why is it a good idea to have a random offset?
� Does it really lead to a perfectly uniform distribution over the sequences on the document?
� What would you have to do to make things even more uniform?
7. If we want a sequence example to be a complete sentence, what kinds of problems does this
introduce in minibatch sampling? Why would we want to do this anyway?
Discussions111
8.4 Recurrent Neural Networks
In Section 8.3 we introduced n-gram models, where the conditional probability of word xt at position
t only depends on the n??1 previous words. If we want to check the possible effect of words
earlier than t ?? (n ?? 1) on xt, we need to increase n. However, the number of model parameters
would also increase exponentially with it, as we need to store jV jn numbers for a vocabulary V .
111 https://discuss.d2l.ai/t/117
318 Chapter 8. Recurrent Neural Networks
Hence, rather than modeling p(xt j xt??1; : : : ; xt??n+1) it is preferable to use a latent variable model
in which we have
p(xt j xt??1; : : : ; x1)  p(xt j xt??1; ht): (8.4.1)
Here ht is a latent variable that stores the sequence information. A latent variable is also called as
hidden variable, hidden state or hidden state variable. The hidden state at time t could be computed
based on both input xt and hidden state ht??1, that is
ht = f(xt; ht??1): (8.4.2)
For a sufficiently powerful function f, the latent variable model is not an approximation. After all,
ht could simply store all the data it observed so far. We discussed this in Section 8.1. But it could
potentially makes both computation and storage expensive.
Note that we also use h to denote the number of hidden units of a hidden layer. Hidden layers
and hidden states refer to two very different concepts. Hidden layers are, as explained, layers that
are hidden from view on the path from input to output. Hidden states are technically speaking
inputs to whatever we do at a given step. Instead, they can only be computed by looking at data
at previous iterations. In this sense they have much in common with latent variable models in
statistics, such as clustering or topic models where the clusters affect the output but cannot be
directly observed.
Recurrent neural networks are neural networks with hidden states. Before introducing this model,
let us first revisit the multi-layer perceptron introduced in Section 4.1.
8.4.1 Recurrent Networks Without Hidden States
Let us take a look at a multilayer perceptron with a single hidden layer. Given a minibatch of the
instances X 2 Rnd with sample size n and d inputs. Let the hidden layer?s activation function be
?. Hence, the hidden layer?s output H 2 Rnh is calculated as
H = ?(XWxh + bh): (8.4.3)
Here, we have the weight parameter Wxh 2 Rdh, bias parameter bh 2 R1h, and the number of
hidden units h, for the hidden layer.
The hidden variable H is used as the input of the output layer. The output layer is given by
O = HWhq + bq: (8.4.4)
Here, O 2 Rnq is the output variable, Whq 2 Rhq is the weight parameter, and bq 2 R1q is
the bias parameter of the output layer. If it is a classification problem, we can use softmax(O) to
compute the probability distribution of the output category.
This is entirely analogous to the regression problem we solved previously in Section 8.1, hence we
omit details. Suffice it to say that we can pick (xt; xt??1) pairs at random and estimate the parameters
W and b of our network via autograd and stochastic gradient descent.
8.4. Recurrent Neural Networks 319
8.4.2 Recurrent Networks with Hidden States
Matters are entirely different when we have hidden states. Let us look at the structure in some
more detail. Remember that we often call iteration t as time t in an optimization algorithm, time
in a recurrent neural network refers to steps within an iteration. Assume that we have Xt 2 Rnd,
t = 1; : : : ; T, in an iteration. And Ht 2 Rnh is the hidden variable of timestep t from the sequence.
Unlike the multilayer perceptron, here we save the hidden variable Ht??1 from the previous
timestep and introduce a new weight parameter Whh 2 Rhh, to describe how to use the
hidden variable of the previous timestep in the current timestep. Specifically, the calculation of
the hidden variable of the current timestep is determined by the input of the current timestep
together with the hidden variable of the previous timestep:
Ht = ?(XtWxh + Ht??1Whh + bh): (8.4.5)
Compared with (8.4.3), we added one more Ht??1Whh here. From the relationship between hidden
variables Ht and Ht??1 of adjacent timesteps, we know that those variables captured and retained
the sequence?s historical information up to the current timestep, just like the state or memory of
the neural network?s current timestep. Therefore, such a hidden variable is called a hidden state.
Since the hidden state uses the same definition of the previous timestep in the current timestep,
the computation of the equation above is recurrent, hence the name recurrent neural network
(RNN).
There are many different RNN construction methods. RNNs with a hidden state defined by the
equation above are very common. For timestep t, the output of the output layer is similar to the
computation in the multilayer perceptron:
Ot = HtWhq + bq: (8.4.6)
RNN parameters include the weight Wxh 2 Rdh; Whh 2 Rhh of the hidden layer with the bias
bh 2 R1h, and the weight Whq 2 Rhq of the output layer with the bias bq 2 R1q. It is worth mentioning
that RNNs always use these model parameters, even for different timesteps. Therefore,
the number of RNN model parameters does not grow as the number of timesteps increases.
Fig. 8.4.1 shows the computational logic of an RNN at three adjacent timesteps. In timestep t,
the computation of the hidden state can be treated as an entry of a fully connected layer with the
activation function ? after concatenating the input Xt with the hidden state Ht??1 of the previous
timestep. The output of the fully connected layer is the hidden state of the current timestep Ht.
Its model parameter is the concatenation of Wxh and Whh, with a bias of bh. The hidden state of
the current timestep t, Ht, will participate in computing the hidden state Ht+1 of the next timestep
t+1. What is more, Ht will become the input for Ot, the fully connected output layer of the current
timestep.
320 Chapter 8. Recurrent Neural Networks
Fig. 8.4.1: An RNN with a hidden state.
8.4.3 Steps in a Language Model
Now we illustrate how RNNs can be used to build a language model. For simplicity of illustration
we use words rather than characters as the inputs, since the former are easier to comprehend. Let
the minibatch size be 1, and the sequence of the text be the beginning of our dataset, i.e., �the time
machine by H. G. Wells�. Fig. 8.4.2 illustrates how to estimate the next word based on the present
and previous words. During the training process, we run a softmax operation on the output from
the output layer for each timestep, and then use the cross-entropy loss function to compute the
error between the result and the label. Due to the recurrent computation of the hidden state in
the hidden layer, the output of timestep 3, O3, is determined by the text sequence �the�, �time�,
and �machine� respectively. Since the next word of the sequence in the training data is �by�, the
loss of timestep 3 will depend on the probability distribution of the next word generated based on
the feature sequence �the�, �time�, �machine� and the label �by� of this timestep.
Fig. 8.4.2: Word-level RNN language model. The input and label sequences are the time machine
by H. and time machine by H. G. respectively.
In practice, each word is presented by a d dimensional vector, and we use a batch size n > 1.
Therefore, the input Xt at timestep t will be a nd matrix, which is identical to what we discussed
before.
8.4. Recurrent Neural Networks 321
8.4.4 Perplexity
Last, let us discuss about how to measure the sequence model quality. One way is to check how
surprising the text is. A good language model is able to predict with high accuracy tokens that what
we will see next. Consider the following continuations of the phrase �It is raining�, as proposed
by different language models:
1. �It is raining outside�
2. �It is raining banana tree�
3. �It is raining piouw;kcj pwepoiut�
In terms of quality, example 1 is clearly the best. The words are sensible and logically coherent.
While it might not quite accurately reflect which word follows semantically (�in San Francisco� and
�in winter� would have been perfectly reasonable extensions), the model is able to capture which
kind of word follows. Example 2 is considerably worse by producing a nonsensical extension.
Nonetheless, at least the model has learned how to spell words and some degree of correlation
between words. Last, example 3 indicates a poorly trained model that does not fit data properly.
We might measure the quality of the model by computing p(w), i.e., the likelihood of the sequence.
Unfortunately this is a number that is hard to understand and difficult to compare. After all,
shorter sequences are much more likely to occur than the longer ones, hence evaluating the model
on Tolstoy?s magnum opus �War and Peace�112 will inevitably produce a much smaller likelihood
than, say, on Saint-Exupery?s novella �The Little Prince�113. What is missing is the equivalent of
an average.
Information theory comes handy here and we will introduce more in Section 18.11. If we want to
compress text, we can ask about estimating the next symbol given the current set of symbols. A
lower bound on the number of bits is given by ??log2 p(xt j xt??1; : : : ; x1). A good language model
should allow us to predict the next word quite accurately. Thus, it should allow us to spend very
few bits on compressing the sequence. So we can measure it by the average number of bits that
we need to spend.
1
n
?n
t=1
??log p(xt j xt??1; : : : ; x1): (8.4.7)
This makes the performance on documents of different lengths comparable. For historical reasons,
scientists in natural language processing prefer to use a quantity called perplexity rather than
bitrate. In a nutshell, it is the exponential of the above:
PPL := exp
(
??1
n
?n
t=1
log p(xt j xt??1; : : : ; x1)
)
: (8.4.8)
It can be best understood as the harmonic mean of the number of real choices that we have when
deciding which word to pick next. Note that perplexity naturally generalizes the notion of the
cross-entropy loss defined when we introduced the softmax regression (Section 3.4). That is, for
a single symbol both definitions are identical bar the fact that one is the exponential of the other.
Let us look at a number of cases:
� In the best case scenario, the model always estimates the probability of the next symbol as
1. In this case the perplexity of the model is 1.
112 https://www.gutenberg.org/files/2600/2600-h/2600-h.htm
113 https://en.wikipedia.org/wiki/The_Little_Prince
322 Chapter 8. Recurrent Neural Networks
� In the worst case scenario, the model always predicts the probability of the label category
as 0. In this situation, the perplexity is infinite.
� At the baseline, the model predicts a uniform distribution over all tokens. In this case, the
perplexity equals the size of the dictionary len(vocab). In fact, if we were to store the sequence
without any compression, this would be the best we could do to encode it. Hence,
this provides a nontrivial upper bound that any model must satisfy.
Summary
� A network that uses recurrent computation is called a recurrent neural network (RNN).
� The hidden state of the RNN can capture historical information of the sequence up to the
current timestep.
� The number of RNN model parameters does not grow as the number of timesteps increases.
� We can create language models using a character-level RNN.
Exercises
1. If we use an RNN to predict the next character in a text sequence, how many output dimensions
do we need?
2. Can you design a mapping for which an RNN with hidden states is exact? Hint: what about
a finite number of words?
3. What happens to the gradient if you backpropagate through a long sequence?
4. What are some of the problems associated with the simple sequence model described above?
Discussions114
8.5 Implementation of Recurrent Neural Networks from Scratch
In this section we implement a language model introduced in Chapter 8 from scratch. It is based
on a character-level recurrent neural network trained on H. G. Wells? The Time Machine. As before,
we start by reading the dataset first, which is introduced in Section 8.3.
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import autograd, np, npx, gluon
npx.set_np()
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
114 https://discuss.d2l.ai/t/337
8.5. Implementation of Recurrent Neural Networks from Scratch 323
8.5.1 One-hot Encoding
Remember that each token is presented as a numerical index in train_iter. Feeding these indices
directly to the neural network might make it hard to learn. We often present each token as a more
expressive feature vector. The easiest representation is called one-hot encoding.
In a nutshell, we map each index to a different unit vector: assume that the number of different
tokens in the vocabulary is N (the len(vocab)) and the token indices range from 0 to N ?? 1. If
the index of a token is the integer i, then we create a vector ei of all 0s with a length of N and set
the element at position i to 1. This vector is the one-hot vector of the original token. The one-hot
vectors with indices 0 and 2 are shown below.
npx.one_hot(np.array([0, 2]), len(vocab))
array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
The shape of the minibatch we sample each time is (batch size, timestep). The one_hot function
transforms such a minibatch into a 3-D tensor with the last dimension equals to the vocabulary
size. We often transpose the input so that we will obtain a (timestep, batch size, vocabulary size)
output that fits into a sequence model easier.
X = np.arange(10).reshape(2, 5)
npx.one_hot(X.T, 28).shape
(5, 2, 28)
8.5.2 Initializing the Model Parameters
Next, we initialize the model parameters for a RNN model. The number of hidden units
num_hiddens is a tunable parameter.
def get_params(vocab_size, num_hiddens, device):
num_inputs = num_outputs = vocab_size
def normal(shape):
return np.random.normal(scale=0.01, size=shape, ctx=device)
# Hidden layer parameters
W_xh = normal((num_inputs, num_hiddens))
W_hh = normal((num_hiddens, num_hiddens))
b_h = np.zeros(num_hiddens, ctx=device)
# Output layer parameters
W_hq = normal((num_hiddens, num_outputs))
b_q = np.zeros(num_outputs, ctx=device)
# Attach gradients
params = [W_xh, W_hh, b_h, W_hq, b_q]
for param in params:
param.attach_grad()
return params
324 Chapter 8. Recurrent Neural Networks
8.5.3 RNN Model
First, we need an init_rnn_state function to return the hidden state at initialization. It returns a
tensor filled with 0 and with a shape of (batch size, number of hidden units). Using tuples makes
it easier to handle situations where the hidden state contains multiple variables (e.g., when combining
multiple layers in an RNN where each layer requires initializing).
def init_rnn_state(batch_size, num_hiddens, device):
return (np.zeros((batch_size, num_hiddens), ctx=device), )
The following rnn function defines how to compute the hidden state and output in a timestep. The
activation function here uses the tanh function. As described in :numref:sec_mlp, the mean value
of the tanh function is 0, when the elements are evenly distributed over the real numbers.
def rnn(inputs, state, params):
# Inputs shape: (num_steps, batch_size, vocab_size)
W_xh, W_hh, b_h, W_hq, b_q = params
H, = state
outputs = []
for X in inputs:
H = np.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h)
Y = np.dot(H, W_hq) + b_q
outputs.append(Y)
return np.concatenate(outputs, axis=0), (H,)
Now we have all functions defined, next we create a class to wrap these functions and store parameters.
class RNNModelScratch: #@save
"""A RNN Model based on scratch implementations."""
def __init__(self, vocab_size, num_hiddens, device,
get_params, init_state, forward):
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.params = get_params(vocab_size, num_hiddens, device)
self.init_state, self.forward_fn = init_state, forward
def __call__(self, X, state):
X = npx.one_hot(X.T, self.vocab_size)
return self.forward_fn(X, state, self.params)
def begin_state(self, batch_size, ctx):
return self.init_state(batch_size, self.num_hiddens, ctx)
Let us do a sanity check whether inputs and outputs have the correct dimensions, e.g., to ensure
that the dimensionality of the hidden state has not changed.
num_hiddens = 512
model = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
init_rnn_state, rnn)
state = model.begin_state(X.shape[0], d2l.try_gpu())
Y, new_state = model(X.as_in_context(d2l.try_gpu()), state)
Y.shape, len(new_state), new_state[0].shape
8.5. Implementation of Recurrent Neural Networks from Scratch 325
((10, 28), 1, (2, 512))
We can see that the output shape is (number steps batch size, vocabulary size), while the hidden
state shape remains the same, i.e., (batch size, number of hidden units).
8.5.4 Prediction
We first explain the predicting function so we can regularly check the prediction during training.
This function predicts the next num_predicts characters based on the prefix (a string containing
several characters). For the beginning of the sequence, we only update the hidden state. After
that we begin generating new characters and emitting them.
def predict_ch8(prefix, num_predicts, model, vocab, device): #@save
state = model.begin_state(batch_size=1, ctx=device)
outputs = [vocab[prefix[0]]]
get_input = lambda: np.array([outputs[-1]], ctx=device).reshape(1, 1)
for y in prefix[1:]: # Warmup state with prefix
_, state = model(get_input(), state)
outputs.append(vocab[y])
for _ in range(num_predicts): # Predict num_predicts steps
Y, state = model(get_input(), state)
outputs.append(int(Y.argmax(axis=1).reshape(1)))
return ''.join([vocab.idx_to_token[i] for i in outputs])
We test the predict_ch8 function first. Given that we did not train the network, it will generate
nonsensical predictions. We initialize it with the sequence traveller and have it generate 10 additional
characters.
predict_ch8('time traveller ', 10, model, vocab, d2l.try_gpu())
'time traveller iiiiiiiiii'
8.5.5 Gradient Clipping
For a sequence of length T, we compute the gradients over these T timesteps in an iteration, which
results in a chain of matrix-products with length O(T) during backpropagating. As mentioned
in Section 4.8, it might result in numerical instability, e.g., the gradients may either explode or
vanish, when T is large. Therefore, RNN models often need extra help to stabilize the training.
Recall that when solving an optimization problem, we take update steps for the weights w in the
general direction of the negative gradient gt on a minibatch, say w ??   gt. Let us further assume
that the objective is well behaved, i.e., it is Lipschitz continuous with constant L, i.e.,
jl(w) ?? l(w?
)j  L?w ?? w??: (8.5.1)
In this case we can safely assume that if we update the weight vector by   gt, we will not observe a
change by more than L?gt?. This is both a curse and a blessing. A curse since it limits the speed
of making progress, whereas a blessing since it limits the extent to which things can go wrong if
we move in the wrong direction.
326 Chapter 8. Recurrent Neural Networks
Sometimes the gradients can be quite large and the optimization algorithm may fail to converge.
We could address this by reducing the learning rate  or by some other higher order trick. But
what if we only rarely get large gradients? In this case such an approach may appear entirely
unwarranted. One alternative is to clip the gradients by projecting them back to a ball of a given
radius, say  via
g   min
(
1;

?g?
)
g: (8.5.2)
By doing so we know that the gradient norm never exceeds  and that the updated gradient is
entirely aligned with the original direction g. It also has the desirable side-effect of limiting the
influence any given minibatch (and within it any given sample) can exert on the weight vectors.
This bestows a certain degree of robustness to the model. Gradient clipping provides a quick
fix to the gradient exploding. While it does not entirely solve the problem, it is one of the many
techniques to alleviate it.
Below we define a function to clip the gradients of a model that is either a building from scratch
instance or a model constructed by the high-level APIs. Also note that we compute the gradient
norm over all parameters.
def grad_clipping(model, theta): #@save
if isinstance(model, gluon.Block):
params = [p.data() for p in model.collect_params().values()]
else:
params = model.params
norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))
if norm > theta:
for param in params:
param.grad[:] *= theta / norm
8.5.6 Training
Let us first define the function to train the model on one data epoch. It differs from the models
training of Section 3.6 in three places:
1. Different sampling methods for sequential data (random sampling and sequential partitioning)
will result in differences in the initialization of hidden states.
2. We clip the gradients before updating the model parameters. This ensures that the model
does not diverge even when gradients blow up at some point during the training process,
and it effectively reduces the step size automatically.
3. We use perplexity to evaluate the model. This ensures that sequences of different length are
comparable.
When the sequential partitioning is used, we initialize the hidden state at the beginning of each
epoch. Since the ith example in the next minibatch is adjacent to the current ith example, so the
next minibatch can use the current hidden state directly, we only detach the gradient so that we
compute the gradients within a minibatch. When using the random sampling, we need to reinitialize
the hidden state for each iteration since each example is sampled with a random position.
Same as the train_epoch_ch3 function in Section 3.6, we use generalized updater, which could be
either a high-level API trainer or a scratched implementation.
8.5. Implementation of Recurrent Neural Networks from Scratch 327
def train_epoch_ch8(model, train_iter, loss, updater, device, #@save
use_random_iter):
state, timer = None, d2l.Timer()
metric = d2l.Accumulator(2) # loss_sum, num_examples
for X, Y in train_iter:
if state is None or use_random_iter:
# Initialize state when either it is the first iteration or
# using random sampling.
state = model.begin_state(batch_size=X.shape[0], ctx=device)
else:
for s in state:
s.detach()
y = Y.T.reshape(-1)
X, y = X.as_in_ctx(device), y.as_in_ctx(device)
with autograd.record():
py, state = model(X, state)
l = loss(py, y).mean()
l.backward()
grad_clipping(model, 1)
updater(batch_size=1) # Since used mean already
metric.add(l * y.size, y.size)
return math.exp(metric[0]/metric[1]), metric[1]/timer.stop()
The training function again supports either we implement the model from scratch or using highlevel
APIs.
def train_ch8(model, train_iter, vocab, lr, num_epochs, device, #@save
use_random_iter=False):
# Initialize
loss = gluon.loss.SoftmaxCrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[1, num_epochs])
if isinstance(model, gluon.Block):
model.initialize(ctx=device, force_reinit=True,
init=init.Normal(0.01))
trainer = gluon.Trainer(model.collect_params(),
'sgd', {'learning_rate': lr})
updater = lambda batch_size: trainer.step(batch_size)
else:
updater = lambda batch_size: d2l.sgd(model.params, lr, batch_size)
predict = lambda prefix: predict_ch8(prefix, 50, model, vocab, device)
# Train and check the progress.
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(
model, train_iter, loss, updater, device, use_random_iter)
if epoch % 10 == 0:
print(predict('time traveller'))
animator.add(epoch+1, [ppl])
print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))
Now we can train a model. Since we only use 10; 000 tokens in the dataset, the model needs more
epochs to converge.
328 Chapter 8. Recurrent Neural Networks
num_epochs, lr = 500, 1
train_ch8(model, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
perplexity 1.0, 35037.7 tokens/sec on gpu(0)
time traveller it s against reason said filby what reason said
traveller it s against reason said filby what reason said
Finally let us check the results to use a random sampling iterator.
train_ch8(model, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),
use_random_iter=True)
perplexity 1.4, 33823.8 tokens/sec on gpu(0)
time traveller after the pauserequired for the proper assimilati
traveller it s against reason said filby what reason said
While implementing the above RNN model from scratch is instructive, it is not convenient. In the
next section we will see how to improve significantly on the current model and how to make it
faster and easier to implement.
8.5. Implementation of Recurrent Neural Networks from Scratch 329
Summary
� Sequence models need state initialization for training.
� Between sequential models you need to ensure to detach the gradients, to ensure that the
automatic differentiation does not propagate effects beyond the current sample.
� A simple RNN language model consists of an encoder, an RNN model, and a decoder.
� Gradient clipping prevents gradient explosion (but it cannot fix vanishing gradients).
� Perplexity calibrates model performance across different sequence length. It is the exponentiated
average of the cross-entropy loss.
� Sequential partitioning typically leads to better models.
Exercises
1. Show that one-hot encoding is equivalent to picking a different embedding for each object.
2. Adjust the hyperparameters to improve the perplexity.
� How low can you go? Adjust embeddings, hidden units, learning rate, etc.
� How well will it work on other books by H. G. Wells, e.g., The War of the Worlds115.
3. Modify the predict function such as to use sampling rather than picking the most likely next
character.
� What happens?
� Bias the model towards more likely outputs, e.g., by sampling from q(wt j
wt??1; : : : ;w1) / p(wt j wt??1; : : : ;w1) for  > 1.
4. Run the code in this section without clipping the gradient. What happens?
5. Change sequential partitioning so that it does not separate hidden states from the computational
graph. Does the running time change? How about the accuracy?
6. Replace the activation function used in this section with ReLU and repeat the experiments
in this section.
7. Prove that the perplexity is the inverse of the harmonic mean of the conditional word probabilities.
Discussions116
115 http://www.gutenberg.org/ebooks/36
116 https://discuss.d2l.ai/t/336
330 Chapter 8. Recurrent Neural Networks
8.6 Concise Implementation of Recurrent Neural Networks
While Section 8.5 was instructive to see how recurrent neural networks (RNNs) are implemented,
this is not convenient or fast. This section will show how to implement the same language model
more efficiently using functions provided by Gluon. We begin as before by reading the �Time
Machine� corpus.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn, rnn
npx.set_np()
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
8.6.1 Defining the Model
Gluon?s rnn module provides a recurrent neural network implementation (beyond many other sequence
models). We construct the recurrent neural network layer rnn_layer with a single hidden
layer and 256 hidden units, and initialize the weights.
num_hiddens = 256
rnn_layer = rnn.RNN(num_hiddens)
rnn_layer.initialize()
Initializing the state is straightforward. We invoke the member function rnn_layer.
begin_state(batch_size). This returns an initial state for each element in the minibatch. That
is, it returns an object of size (hidden layers, batch size, number of hidden units). The number of
hidden layers defaults to be 1. In fact, we have not even discussed yet what it means to have multiple
layers�this will happen in Section 9.3. For now, suffice it to say that multiple layers simply
amount to the output of one RNN being used as the input for the next RNN.
batch_size = 1
state = rnn_layer.begin_state(batch_size=batch_size)
len(state), state[0].shape
(1, (1, 1, 256))
With a state variable and an input, we can compute the output with the updated state.
num_steps = 1
X = np.random.uniform(size=(num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
Y.shape, len(state_new), state_new[0].shape
((1, 1, 256), 1, (1, 1, 256))
Similar to Section 8.5, we define an RNNModel block by subclassing the Block class for a complete
recurrent neural network. Note that rnn_layer only contains the hidden recurrent layers, we need
8.6. Concise Implementation of Recurrent Neural Networks 331
to create a separate output layer. While in the previous section, we have the output layer within
the rnn block.
#@save
class RNNModel(nn.Block):
def __init__(self, rnn_layer, vocab_size, **kwargs):
super(RNNModel, self).__init__(**kwargs)
self.rnn = rnn_layer
self.vocab_size = vocab_size
self.dense = nn.Dense(vocab_size)
def forward(self, inputs, state):
X = npx.one_hot(inputs.T, self.vocab_size)
Y, state = self.rnn(X, state)
# The fully connected layer will first change the shape of `Y` to
# (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is
# (`num_steps` * `batch_size`, `vocab_size`).
output = self.dense(Y.reshape(-1, Y.shape[-1]))
return output, state
def begin_state(self, *args, **kwargs):
return self.rnn.begin_state(*args, **kwargs)
8.6.2 Training and Predicting
Before training the model, let us make a prediction with the a model that has random weights.
device = d2l.try_gpu()
model = RNNModel(rnn_layer, len(vocab))
model.initialize(force_reinit=True, ctx=device)
d2l.predict_ch8('time traveller', 10, model, vocab, device)
'time travellervmjznnngii'
As is quite obvious, this model does not work at all. Next, we call train_ch8 with the same hyperparameters
defined in Section 8.5 and train our model with Gluon.
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 163715.1 tokens/sec on gpu(0)
time traveller smiled round at us then still they could contit y
traveller you can show bamplyshoolatarlit theories betal t
332 Chapter 8. Recurrent Neural Networks
Compared with the last section, this model achieves comparable perplexity, albeit within a shorter
period of time, due to the code being more optimized.
Summary
� Gluon?s rnn module provides an implementation at the recurrent neural network layer.
� Gluon?s nn.RNN instance returns the output and hidden state after forward computation. This
forward computation does not involve output layer computation.
� As before, the computational graph needs to be detached from previous steps for reasons of
efficiency.
Exercises
1. Compare the implementation with the previous section.
� Why does Gluon?s implementation run faster?
� If you observe a significant difference beyond speed, try to find the reason.
2. Can you make the model overfit?
� Increase the number of hidden units.
� Increase the number of iterations.
� What happens if you adjust the clipping parameter?
3. Implement the autoregressive model of the introduction to the current chapter using an
RNN.
4. What happens if you increase the number of hidden layers in the RNN model? Can you make
the model work?
5. How well can you compress the text using this model?
� How many bits do you need?
� Why does not everyone use this model for text compression? Hint: what about the
compressor itself?
8.6. Concise Implementation of Recurrent Neural Networks 333
Discussions117
8.7 Backpropagation Through Time
So far we repeatedly alluded to things like exploding gradients, vanishing gradients, truncating backprop,
and the need to detach the computational graph. For instance, in the previous section we invoked
s.detach() on the sequence. None of this was really fully explained, in the interest of being
able to build a model quickly and to see how it works. In this section we will delve a bit more deeply
into the details of backpropagation for sequence models and why (and how) the math works. For a
more detailed discussion about randomization and backpropagation also see the paper by (Tallec
& Ollivier, 2017).
We encountered some of the effects of gradient explosion when we first implemented recurrent
neural networks (Section 8.5). In particular, if you solved the problems in the problem set, you
would have seen that gradient clipping is vital to ensure proper convergence. To provide a better
understanding of this issue, this section will review how gradients are computed for sequence
models. Note that there is nothing conceptually new in how it works. After all, we are still merely
applying the chain rule to compute gradients. Nonetheless, it is worth while reviewing backpropagation
(Section 4.7) again.
Forward propagation in a recurrent neural network is relatively straightforward. Backpropagation
through time is actually a specific application of back propagation in recurrent neural networks. It
requires us to expand the recurrent neural network one timestep at a time to obtain the dependencies
between model variables and parameters. Then, based on the chain rule, we apply backpropagation
to compute and store gradients. Since sequences can be rather long, the dependency can
be rather lengthy. For instance, for a sequence of 1000 characters, the first symbol could potentially
have significant influence on the symbol at position 1000. This is not really computationally
feasible (it takes too long and requires too much memory) and it requires over 1000 matrix-vector
products before we would arrive at that very elusive gradient. This is a process fraught with computational
and statistical uncertainty. In the following we will elucidate what happens and how
to address this in practice.
8.7.1 A Simplified Recurrent Network
We start with a simplified model of how an RNN works. This model ignores details about the
specifics of the hidden state and how it is updated. These details are immaterial to the analysis
and would only serve to clutter the notation, but make it look more intimidating. In this simplified
model, we denote ht as the hidden state, xt as the input, and ot as the output at timestep t. In
addition, wh and wo indicate the weights of hidden states and the output layer, respectively. As a
result, the hidden states and outputs at each timesteps can be explained as
ht = f(xt; ht??1;wh) and ot = g(ht;wo): (8.7.1)
Hence, we have a chain of values f: : : ; (ht??1; xt??1; ot??1); (ht; xt; ot); : : :g that depend on each other
via recursive computation. The forward propagation is fairly straightforward. All we need is to
loop through the (xt; ht; ot) triples one step at a time. The discrepancy between outputs ot and the
desired targets yt is then evaluated by an objective function as
L(x; y;wh;wo) =
?T
t=1
l(yt; ot): (8.7.2)
117 https://discuss.d2l.ai/t/335
334 Chapter 8. Recurrent Neural Networks
For backpropagation, matters are a bit more tricky, especially when we compute the gradients
with regard to the parameters wh of the objective function L. To be specific, by the chain rule,
@whL =
?T
t=1
@whl(yt; ot)
=
?T
t=1
@ot l(yt; ot)@htg(ht;wh) [@whht] :
(8.7.3)
The first and the second part of the derivative is easy to compute. The third part @whht is where
things get tricky, since we need to compute the effect of the parameters on ht.
To derive the above gradient, assume that we have three sequences fatg; fbtg; fctg satisfying a0 =
0; a1 = b1, and at = bt + ctat??1 for t = 1; 2; : : :. Then for t  1, it is easy to show
at = bt +
?t??1
i=1
0
@
?t
j=i+1
cj
1
Abi: (8.7.4)
Now let us apply (8.7.4) with
at = @whht; (8.7.5)
bt = @whf(xt; ht??1;wh); (8.7.6)
ct = @ht??1f(xt; ht??1;wh): (8.7.7)
Therefore, at = bt + ctat??1 becomes the following recursion
@whht = @whf(xt; ht??1;w) + @hf(xt; ht??1;wh)@whht??1: (8.7.8)
By (8.7.4), the third part will be
@whht = @whf(xt; ht??1;wh) +
?t??1
i=1
0
@
?t
j=i+1
@hj??1f(xj ; hj??1;wh)
1
A@whf(xi; hi??1;wh): (8.7.9)
While we can use the chain rule to compute @wht recursively, this chain can get very long whenever
t is large. Let us discuss a number of strategies for dealing with this problem.
� Compute the full sum. This is very slow and gradients can blow up, since subtle changes in
the initial conditions can potentially affect the outcome a lot. That is, we could see things
similar to the butterfly effect where minimal changes in the initial conditions lead to disproportionate
changes in the outcome. This is actually quite undesirable in terms of the model
that we want to estimate. After all, we are looking for robust estimators that generalize well.
Hence this strategy is almost never used in practice.
� Truncate the sum after  steps. This is what we have been discussing so far. This leads to
an approximation of the true gradient, simply by terminating the sum above at @wht?? . The
approximation error is thus given by @hf(xt; ht??1;w)@wht??1 (multiplied by a product of gradients
involving @hf). In practice this works quite well. It is what is commonly referred to as
truncated BPTT (backpropgation through time). One of the consequences of this is that the
model focuses primarily on short-term influence rather than long-term consequences. This
is actually desirable, since it biases the estimate towards simpler and more stable models.
8.7. Backpropagation Through Time 335
� Randomized Truncation. Last we can replace @whht by a random variable which is correct
in expectation but which truncates the sequence. This is achieved by using a sequence of t
where E[t] = 1 and P(t = 0) = 1 ??  and furthermore P(t = ??1) = . We use this to
replace the gradient:
zt = @wf(xt; ht??1;w) + t@hf(xt; ht??1;w)@wht??1: (8.7.10)
It follows from the definition of t that E[zt] = @wht. Whenever t = 0 the expansion terminates at
that point. This leads to a weighted sum of sequences of varying lengths where long sequences are
rare but appropriately overweighted. (Tallec & Ollivier, 2017) proposed this in their paper. Unfortunately,
while appealing in theory, the model does not work much better than simple truncation,
most likely due to a number of factors. First, the effect of an observation after a number of backpropagation
steps into the past is quite sufficient to capture dependencies in practice. Second,
the increased variance counteracts the fact that the gradient is more accurate. Third, we actually
want models that have only a short range of interaction. Hence, BPTT has a slight regularizing
effect which can be desirable.
Fig. 8.7.1: From top to bottom: randomized BPTT, regularly truncated BPTT and full BPTT
Fig. 8.7.1 illustrates the three cases when analyzing the first few words of The Time Machine: * The
first row is the randomized truncation which partitions the text into segments of varying length. *
The second row is the regular truncated BPTT which breaks it into sequences of the same length.
* The third row is the full BPTT that leads to a computationally infeasible expression.
8.7.2 The Computational Graph
In order to visualize the dependencies between model variables and parameters during computation
in a recurrent neural network, we can draw a computational graph for the model, as shown
in Fig. 8.7.2. For example, the computation of the hidden states of timestep 3, h3, depends on
the model parameters Whx and Whh, the hidden state of the last timestep h2, and the input of the
current timestep x3.
336 Chapter 8. Recurrent Neural Networks
Fig. 8.7.2: Computational dependencies for a recurrent neural network model with three
timesteps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent
operators.
8.7.3 BPTT in Detail
After discussing the general principle, let us discuss BPTT in detail. By decomposing W into different
sets of weight matrices (Whx; Whh and Woh), we will get a simple linear latent variable model:
ht = Whxxt + Whhht??1 and ot = Wohht: (8.7.11)
Following the discussion in Section 4.7, we compute the gradients @L
@Whx
, @L
@Whh
, @L
@Woh
for
L(x; y; W) =
?T
t=1
l(ot; yt); (8.7.12)
where l() denotes the chosen loss function. Taking the derivatives with respect to Woh is fairly
straightforward and we obtain
@WohL =
?T
t=1
prod (@ot l(ot; yt); ht) ; (8.7.13)
where prod() indicates the product of two or more matrices.
The dependency on Whx and Whh is a bit more tricky since it involves a chain of derivatives. We
begin with
@WhhL =
?T
t=1
prod (@ot l(ot; yt); Woh; @Whhht) ;
@WhxL =
?T
t=1
prod (@ot l(ot; yt); Woh; @Whxht) :
(8.7.14)
After all, hidden states depend on each other and on past inputs. The key quantity is how past
hidden states affect future hidden states.
@ht ht+1 = W?
hh and thus @ht hT =
(
W?
hh
)T??t
: (8.7.15)
8.7. Backpropagation Through Time 337
Chaining terms together yields
@Whhht =
?t
j=1
(
W?
hh
)t??j
hj
@Whxht =
?t
j=1
(
W?
hh
)t??j
xj :
(8.7.16)
A number of things follow from this potentially very intimidating expression. First, it pays to store
intermediate results, i.e., powers of Whh as we work our way through the terms of the loss function
L. Second, this simple linear example already exhibits some key problems of long sequence
models: it involves potentially very large powers Wj
hh. In it, eigenvalues smaller than 1 vanish
for large j and eigenvalues larger than 1 diverge. This is numerically unstable and gives undue
importance to potentially irrelevant past detail. One way to address this is to truncate the sum
at a computationally convenient size. Later on in Chapter 9 we will see how more sophisticated
sequence models such as LSTMs can alleviate this further. In practice, this truncation is effected
by detaching the gradient after a given number of steps.
Summary
� Backpropagation through time is merely an application of backpropagation to sequence
models with a hidden state.
� Truncation is needed for computational convenience and numerical stability.
� High powers of matrices can lead to divergent and vanishing eigenvalues. This manifests
itself in the form of exploding or vanishing gradients.
� For efficient computation, intermediate values are cached.
Exercises
1. Assume that we have a symmetric matrix M 2 Rnn with eigenvalues i. Without loss of
generality, assume that they are ordered in ascending order i  i+1. Show that Mk has
eigenvalues ki
.
2. Prove that for a random vector x 2 Rn, with high probability Mkx will be very much aligned
with the largest eigenvector vn of M. Formalize this statement.
3. What does the above result mean for gradients in a recurrent neural network?
4. Besides gradient clipping, can you think of any other methods to cope with gradient explosion
in recurrent neural networks?
Discussions118
118 https://discuss.d2l.ai/t/334
338 Chapter 8. Recurrent Neural Networks
9 | Modern Recurrent Neural Networks
Although we have learned the basics of recurrent neural networks, they are not sufficient for a
practitioner to solve today?s sequence learning problems. For instance, given the numerical unstability
during gradient calculation, gated recurrent neural networks are much more common
in practice. We will begin by introducing two of such widely-used networks, namely gated recurrent
units (GRUs) and long short term memory (LSTM), with illustrations using the same language
modeling problem as introduced in Chapter 8.
Furthermore, we will modify recurrent neural networks with a single undirectional hidden layer.
We will describe deep architectures, and discuss the bidirectional design with both forward and
backward recursion. They are frequently adopted in modern recurrent networks.
In fact, a large portion of sequence learning problems such as automatic speech recognition, text
to speech, and machine translation, consider both inputs and outputs to be sequences of arbitrary
length. Finally, we will take machine translation as an example, and introduce the encoderdecoder
architecture based on recurrent neural networks and modern practices for such sequence
to sequence learning problems.
9.1 Gated Recurrent Units (GRU)
In the previous section, we discussed how gradients are calculated in a recurrent neural network.
In particular we found that long products of matrices can lead to vanishing or divergent gradients.
Let us briefly think about what such gradient anomalies mean in practice:
� We might encounter a situation where an early observation is highly significant for predicting
all future observations. Consider the somewhat contrived case where the first observation
contains a checksum and the goal is to discern whether the checksum is correct at the
end of the sequence. In this case, the influence of the first token is vital. We would like to
have some mechanisms for storing vital early information in a memory cell. Without such a
mechanism, we will have to assign a very large gradient to this observation, since it affects
all subsequent observations.
� We might encounter situations where some symbols carry no pertinent observation. For
instance, when parsing a web page there might be auxiliary HTML code that is irrelevant
for the purpose of assessing the sentiment conveyed on the page. We would like to have
some mechanism for skipping such symbols in the latent state representation.
� We might encounter situations where there is a logical break between parts of a sequence.
For instance, there might be a transition between chapters in a book, or a transition between
a bear and a bull market for securities. In this case it would be nice to have a means of
resetting our internal state representation.
339
A number of methods have been proposed to address this. One of the earliest is Long Short Term
Memory (LSTM) (Hochreiter & Schmidhuber, 1997) which we will discuss in Section 9.2. Gated
Recurrent Unit (GRU) (Cho et al., 2014) is a slightly more streamlined variant that often offers
comparable performance and is significantly faster to compute. See also (Chung et al., 2014) for
more details. Due to its simplicity, let us start with the GRU.
9.1.1 Gating the Hidden State
The key distinction between regular RNNs and GRUs is that the latter support gating of the hidden
state. This means that we have dedicated mechanisms for when a hidden state should be updated
and also when it should be reset. These mechanisms are learned and they address the concerns
listed above. For instance, if the first symbol is of great importance we will learn not to update
the hidden state after the first observation. Likewise, we will learn to skip irrelevant temporary
observations. Last, we will learn to reset the latent state whenever needed. We discuss this in
detail below.
Reset Gates and Update Gates
The first thing we need to introduce are reset and update gates. We engineer them to be vectors
with entries in (0; 1) such that we can perform convex combinations. For instance, a reset variable
would allow us to control how much of the previous state we might still want to remember.
Likewise, an update variable would allow us to control how much of the new state is just a copy of
the old state.
We begin by engineering gates to generate these variables. Fig. 9.1.1 illustrates the inputs for
both reset and update gates in a GRU, given the current timestep input Xt and the hidden state of
the previous timestep Ht??1. The output is given by a fully connected layer with a sigmoid as its
activation function.
Fig. 9.1.1: Reset and update gate in a GRU.
For a given timestep t, the minibatch input is Xt 2 Rnd (number of examples: n, number of
inputs: d) and the hidden state of the last timestep is Ht??1 2 Rnh (number of hidden states: h).
340 Chapter 9. Modern Recurrent Neural Networks
Then, the reset gate Rt 2 Rnh and update gate Zt 2 Rnh are computed as follows:
Rt = (XtWxr + Ht??1Whr + br);
Zt = (XtWxz + Ht??1Whz + bz):
(9.1.1)
Here, Wxr; Wxz 2 Rdh and Whr; Whz 2 Rhh are weight parameters and br; bz 2 R1h are biases.
We use a sigmoid function (as introduced in Section 4.1) to transform input values to the interval
(0; 1).
Reset Gates in Action
We begin by integrating the reset gate with a regular latent state updating mechanism. In a conventional
RNN, we would have an hidden state update of the form
Ht = tanh(XtWxh + Ht??1Whh + bh): (9.1.2)
This is essentially identical to the discussion of the previous section, albeit with a nonlinearity
in the form of tanh to ensure that the values of the hidden states remain in the interval (??1; 1).
If we want to be able to reduce the influence of the previous states we can multiply Ht??1 with Rt
elementwise. Whenever the entries in the reset gate Rt are close to 1, we recover a conventional
RNN. For all entries of the reset gate Rt that are close to 0, the hidden state is the result of an MLP
with Xt as input. Any pre-existing hidden state is thus reset to defaults. This leads to the following
candidate hidden state (it is a candidate since we still need to incorporate the action of the update
gate).
~ Ht = tanh(XtWxh + (Rt ? Ht??1) Whh + bh): (9.1.3)
Fig. 9.1.2 illustrates the computational flow after applying the reset gate. The symbol ? indicates
pointwise multiplication between tensors.
Fig. 9.1.2: Candidate hidden state computation in a GRU. The multiplication is carried out elementwise.
9.1. Gated Recurrent Units (GRU) 341
Update Gates in Action
Next we need to incorporate the effect of the update gate Zt, as shown in :numref:fig_gru_3. This
determines the extent to which the new state Ht is just the old state Ht??1 and by how much the
new candidate state ~ Ht is used. The gating variable Zt can be used for this purpose, simply by
taking elementwise convex combinations between both candidates. This leads to the final update
equation for the GRU.
Ht = Zt ? Ht??1 + (1 ?? Zt) ? ~ Ht: (9.1.4)
Fig. 9.1.3: Hidden state computation in a GRU. As before, the multiplication is carried out elementwise.
Whenever the update gate Zt is close to 1, we simply retain the old state. In this case the information
from Xt is essentially ignored, effectively skipping timestep t in the dependency chain. In
contrast, whenever Zt is close to 0, the new latent state Ht approaches the candidate latent state ~ Ht.
These designs can help us cope with the vanishing gradient problem in RNNs and better capture
dependencies for time series with large timestep distances. In summary, GRUs have the following
two distinguishing features:
� Reset gates help capture short-term dependencies in time series.
� Update gates help capture long-term dependencies in time series.
9.1.2 Implementation from Scratch
To gain a better understanding of the model, let us implement a GRU from scratch.
342 Chapter 9. Modern Recurrent Neural Networks
Reading the Dataset
We begin by reading The Time Machine corpus that we used in Section 8.5. The code for reading
the dataset is given below:
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import rnn
npx.set_np()
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
Initializing Model Parameters
The next step is to initialize the model parameters. We draw the weights from a Gaussian with
variance to be 0:01 and set the bias to 0. The hyperparameter num_hiddens defines the number of
hidden units. We instantiate all weights and biases relating to the update gate, the reset gate, and
the candidate hidden state itself. Subsequently, we attach gradients to all the parameters.
def get_params(vocab_size, num_hiddens, device):
num_inputs = num_outputs = vocab_size
def normal(shape):
return np.random.normal(scale=0.01, size=shape, ctx=device)
def three():
return (normal((num_inputs, num_hiddens)),
normal((num_hiddens, num_hiddens)),
np.zeros(num_hiddens, ctx=device))
W_xz, W_hz, b_z = three() # Update gate parameter
W_xr, W_hr, b_r = three() # Reset gate parameter
W_xh, W_hh, b_h = three() # Candidate hidden state parameter
# Output layer parameters
W_hq = normal((num_hiddens, num_outputs))
b_q = np.zeros(num_outputs, ctx=device)
# Attach gradients
params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
for param in params:
param.attach_grad()
return params
9.1. Gated Recurrent Units (GRU) 343
Defining the Model
Now we will define the hidden state initialization function init_gru_state. Just like the
init_rnn_state function defined in Section 8.5, this function returns a tensor with a shape (batch
size, number of hidden units) whose values are all zeros.
def init_gru_state(batch_size, num_hiddens, device):
return (np.zeros(shape=(batch_size, num_hiddens), ctx=device), )
Now we are ready to define the GRU model. Its structure is the same as the basic RNN cell, except
that the update equations are more complex.
def gru(inputs, state, params):
W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
H, = state
outputs = []
for X in inputs:
Z = npx.sigmoid(np.dot(X, W_xz) + np.dot(H, W_hz) + b_z)
R = npx.sigmoid(np.dot(X, W_xr) + np.dot(H, W_hr) + b_r)
H_tilda = np.tanh(np.dot(X, W_xh) + np.dot(R * H, W_hh) + b_h)
H = Z * H + (1 - Z) * H_tilda
Y = np.dot(H, W_hq) + b_q
outputs.append(Y)
return np.concatenate(outputs, axis=0), (H,)
Training and Prediction
Training and prediction work in exactly the same manner as before. After training for one epoch,
the perplexity and the output sentence will be like the following.
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,
init_gru_state, gru)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 13402.4 tokens/sec on gpu(0)
time traveller it s against reason said filby what reason said
traveller it s against reason said filby what reason said
344 Chapter 9. Modern Recurrent Neural Networks
9.1.3 Concise Implementation
In Gluon, we can directly call the GRU class in the rnn module. This encapsulates all the configuration
detail that we made explicit above. The code is significantly faster as it uses compiled
operators rather than Python for many details that we spelled out in detail before.
gru_layer = rnn.GRU(num_hiddens)
model = d2l.RNNModel(gru_layer, len(vocab))
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 188606.4 tokens/sec on gpu(0)
time traveller it s against reason said filby what reason said
traveller comint bee on it furedent a vegions of space and
9.1. Gated Recurrent Units (GRU) 345
Summary
� Gated recurrent neural networks are better at capturing dependencies for time series with
large timestep distances.
� Reset gates help capture short-term dependencies in time series.
� Update gates help capture long-term dependencies in time series.
� GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. They
can ignore sequences as needed.
Exercises
1. Compare runtime, perplexity, and the output strings for rnn.RNN and rnn.GRU implementations
with each other.
2. Assume that we only want to use the input for timestep t? to predict the output at timestep
t > t?. What are the best values for the reset and update gates for each timestep?
3. Adjust the hyperparameters and observe and analyze the impact on running time, perplexity,
and the written lyrics.
4. What happens if you implement only parts of a GRU? That is, implement a recurrent cell
that only has a reset gate. Likewise, implement a recurrent cell only with an update gate.
Discussions119
9.2 Long Short Term Memory (LSTM)
The challenge to address long-term information preservation and short-term input skipping in
latent variable models has existed for a long time. One of the earliest approaches to address this
was the LSTM (Hochreiter & Schmidhuber, 1997). It shares many of the properties of the Gated Recurrent
Unit (GRU). Interestingly, LSTM?s design is slightly more complex than GRU but predates
GRU by almost two decades.
Arguably it is inspired by logic gates of a computer. To control a memory cell we need a number
of gates. One gate is needed to read out the entries from the cell (as opposed to reading any other
cell). We will refer to this as the output gate. A second gate is needed to decide when to read data
into the cell. We refer to this as the input gate. Last, we need a mechanism to reset the contents of
the cell, governed by a forget gate. The motivation for such a design is the same as before, namely
to be able to decide when to remember and when to ignore inputs in the latent state via a dedicated
mechanism. Let us see how this works in practice.
119 https://discuss.d2l.ai/t/342
346 Chapter 9. Modern Recurrent Neural Networks
9.2.1 Gated Memory Cells
Three gates are introduced in LSTMs: the input gate, the forget gate, and the output gate. In addition
to that we will introduce the memory cell that has the same shape as the hidden state. Strictly
speaking this is just a fancy version of a hidden state, engineered to record additional information.
Input Gates, Forget Gates, and Output Gates
Just like with GRUs, the data feeding into the LSTM gates is the input at the current timestep Xt and
the hidden state of the previous timestep Ht??1. These inputs are processed by a fully connected
layer and a sigmoid activation function to compute the values of input, forget and output gates.
As a result, the three gates? all output values are in the range of [0; 1]. Fig. 9.2.1 illustrates the data
flow for the input, forget, and output gates.
Fig. 9.2.1: Calculation of input, forget, and output gates in an LSTM.
We assume that there are h hidden units, the minibatch is of size n, and number of inputs is d.
Thus, the input is Xt 2 Rnd and the hidden state of the last timestep is Ht??1 2 Rnh. Correspondingly,
the gates are defined as follows: the input gate is It 2 Rnh, the forget gate is Ft 2 Rnh,
and the output gate is Ot 2 Rnh. They are calculated as follows:
It = (XtWxi + Ht??1Whi + bi);
Ft = (XtWxf + Ht??1Whf + bf );
Ot = (XtWxo + Ht??1Who + bo);
(9.2.1)
where Wxi; Wxf ; Wxo 2 Rdh and Whi; Whf ; Who 2 Rhh are weight parameters and bi; bf ; bo 2
R1h are bias parameters.
9.2. Long Short Term Memory (LSTM) 347
Candidate Memory Cell
Next we design the memory cell. Since we have not specified the action of the various gates yet,
we first introduce the candidate memory cell ~Ct 2 Rnh. Its computation is similar to the three
gates described above, but using a tanh function with a value range for [??1; 1] as the activation
function. This leads to the following equation at timestep t.
~Ct = tanh(XtWxc + Ht??1Whc + bc): (9.2.2)
Here Wxc 2 Rdh and Whc 2 Rhh are weight parameters and bc 2 R1h is a bias parameter.
A quick illustration of the candidate memory cell is shown in Fig. 9.2.2.
Fig. 9.2.2: Computation of candidate memory cells in LSTM.
Memory Cell
In GRUs, we had a single mechanism to govern input and forgetting. Here in LSTMs we have two
parameters, It which governs how much we take new data into account via ~Ct and the forget parameter
Ft which addresses how much of the old memory cell content Ct??1 2 Rnh we retain.
Using the same pointwise multiplication trick as before, we arrive at the following update equation.
Ct = Ft ? Ct??1 + It ? ~Ct: (9.2.3)
If the forget gate is always approximately 1 and the input gate is always approximately 0, the past
memory cells Ct??1 will be saved over time and passed to the current timestep. This design was
introduced to alleviate the vanishing gradient problem and to better capture dependencies for
time series with long range dependencies. We thus arrive at the flow diagram in Fig. 9.2.3.
348 Chapter 9. Modern Recurrent Neural Networks
Fig. 9.2.3: Computation of memory cells in an LSTM. Here, the multiplication is carried out elementwise.
Hidden States
Last, we need to define how to compute the hidden state Ht 2 Rnh. This is where the output
gate comes into play. In LSTM it is simply a gated version of the tanh of the memory cell. This
ensures that the values of Ht are always in the interval (??1; 1). Whenever the output gate is 1 we
effectively pass all memory information through to the predictor, whereas for output 0 we retain
all the information only within the memory cell and perform no further processing. Fig. 9.2.4 has
a graphical illustration of the data flow.
Ht = Ot ? tanh(Ct): (9.2.4)
Fig. 9.2.4: Computation of the hidden state. Multiplication is elementwise.
9.2. Long Short Term Memory (LSTM) 349
9.2.2 Implementation from Scratch
Now let us implement an LSTM from scratch. As same as the experiments in the previous sections,
we first load data of The Time Machine.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import rnn
npx.set_np()
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
Initializing Model Parameters
Next we need to define and initialize the model parameters. As previously, the hyperparameter
num_hiddens defines the number of hidden units. We initialize weights following a Gaussian distribution
with 0:01 standard deviation, and we set the biases to 0.
def get_lstm_params(vocab_size, num_hiddens, device):
num_inputs = num_outputs = vocab_size
def normal(shape):
return np.random.normal(scale=0.01, size=shape, ctx=device)
def three():
return (normal((num_inputs, num_hiddens)),
normal((num_hiddens, num_hiddens)),
np.zeros(num_hiddens, ctx=device))
W_xi, W_hi, b_i = three() # Input gate parameters
W_xf, W_hf, b_f = three() # Forget gate parameters
W_xo, W_ho, b_o = three() # Output gate parameters
W_xc, W_hc, b_c = three() # Candidate cell parameters
# Output layer parameters
W_hq = normal((num_hiddens, num_outputs))
b_q = np.zeros(num_outputs, ctx=device)
# Attach gradients
params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
b_c, W_hq, b_q]
for param in params:
param.attach_grad()
return params
350 Chapter 9. Modern Recurrent Neural Networks
Defining the Model
In the initialization function, the hidden state of the LSTM needs to return an additional memory
cell with a value of 0 and a shape of (batch size, number of hidden units). Hence we get the
following state initialization.
def init_lstm_state(batch_size, num_hiddens, device):
return (np.zeros(shape=(batch_size, num_hiddens), ctx=device),
np.zeros(shape=(batch_size, num_hiddens), ctx=device))
The actual model is defined just like what we discussed before: providing three gates and an auxiliary
memory cell. Note that only the hidden state is passed to the output layer. The memory cells
Ct do not participate in the output computation directly.
def lstm(inputs, state, params):
[W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
W_hq, b_q] = params
(H, C) = state
outputs = []
for X in inputs:
I = npx.sigmoid(np.dot(X, W_xi) + np.dot(H, W_hi) + b_i)
F = npx.sigmoid(np.dot(X, W_xf) + np.dot(H, W_hf) + b_f)
O = npx.sigmoid(np.dot(X, W_xo) + np.dot(H, W_ho) + b_o)
C_tilda = np.tanh(np.dot(X, W_xc) + np.dot(H, W_hc) + b_c)
C = F * C + I * C_tilda
H = O * np.tanh(C)
Y = np.dot(H, W_hq) + b_q
outputs.append(Y)
return np.concatenate(outputs, axis=0), (H, C)
Training and Prediction
Let us train an LSTM as same as what we did in Section 9.1, by calling the RNNModelScratch function
as introduced in Section 8.5.
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
init_lstm_state, lstm)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 11345.6 tokens/sec on gpu(0)
time traveller it s against reason said filby what reason said
traveller but now you begin to seethe object of my investig
9.2. Long Short Term Memory (LSTM) 351
9.2.3 Concise Implementation
In Gluon, we can directly call the LSTM class in the rnn module. This encapsulates all the configuration
details that we made explicit above. The code is significantly faster as it uses compiled
operators rather than Python for many details that we spelled out in detail before.
lstm_layer = rnn.LSTM(num_hiddens)
model = d2l.RNNModel(lstm_layer, len(vocab))
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 172189.1 tokens/sec on gpu(0)
time traveller for so it will be convenient to speak of him was
traveller the parsporiccallime co bat exenp bas arount you
In many cases, LSTMs perform slightly better than GRUs but they are more costly to train and
execute due to the larger latent state size. LSTMs are the prototypical latent variable autoregressive
model with nontrivial state control. Many variants thereof have been proposed over the years, e.g.,
multiple layers, residual connections, different types of regularization. However, training LSTMs
and other sequence models (such as GRU) are quite costly due to the long range dependency of
the sequence. Later we will encounter alternative models such as Transformers that can be used
in some cases.
352 Chapter 9. Modern Recurrent Neural Networks
Summary
� LSTMs have three types of gates: input gates, forget gates, and output gates which control
the flow of information.
� The hidden layer output of LSTM includes hidden states and memory cells. Only hidden
states are passed into the output layer. Memory cells are entirely internal.
� LSTMs can cope with vanishing and exploding gradients.
Exercises
1. Adjust the hyperparameters. Observe and analyze the impact on runtime, perplexity, and
the generated output.
2. How would you need to change the model to generate proper words as opposed to sequences
of characters?
3. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden
dimension. Pay special attention to the training and inference cost.
4. Since the candidate memory cells ensure that the value range is between ??1 and 1 by using
the tanh function, why does the hidden state need to use the tanh function again to ensure
that the output value range is between ??1 and 1?
5. Implement an LSTM for time series prediction rather than character sequence prediction.
Discussions120
9.3 Deep Recurrent Neural Networks
Up to now, we only discussed recurrent neural networks with a single unidirectional hidden layer.
In it the specific functional form of how latent variables and observations interact was rather arbitrary.
This is not a big problem as long as we have enough flexibility to model different types
of interactions. With a single layer, however, this can be quite challenging. In the case of the
perceptron, we fixed this problem by adding more layers. Within RNNs this is a bit trickier, since
we first need to decide how and where to add extra nonlinearity. Our discussion below focuses
primarily on LSTMs, but it applies to other sequence models, too.
� We could add extra nonlinearity to the gating mechanisms. That is, instead of using a single
perceptron we could use multiple layers. This leaves the mechanism of the LSTM unchanged.
Instead it makes it more sophisticated. This would make sense if we were led to believe
that the LSTM mechanism describes some form of universal truth of how latent variable
autoregressive models work.
� We could stack multiple layers of LSTMs on top of each other. This results in a mechanism
that is more flexible, due to the combination of several simple layers. In particular, data
might be relevant at different levels of the stack. For instance, we might want to keep highlevel
data about financial market conditions (bear or bull market) available, whereas at a
lower level we only record shorter-term temporal dynamics.
120 https://discuss.d2l.ai/t/343
9.3. Deep Recurrent Neural Networks 353
Beyond all this abstract discussion it is probably easiest to understand the family of models we are
interested in by reviewing Fig. 9.3.1. It describes a deep recurrent neural network with L hidden
layers. Each hidden state is continuously passed to both the next timestep of the current layer and
the current timestep of the next layer.
Fig. 9.3.1: Architecture of a deep recurrent neural network.
9.3.1 Functional Dependencies
At timestep t we assume that we have a minibatch Xt 2 Rnd (number of examples: n, number of
inputs: d). The hidden state of hidden layer ? (? = 1; : : : ;L) is H(?)
t
2 Rnh (number of hidden units:
h), the output layer variable is Ot 2 Rnq (number of outputs: q) and a hidden layer activation
function fl for layer l. We compute the hidden state of layer 1 as before, using Xt as input. For all
subsequent layers, the hidden state of the previous layer is used in its place.
H(1)
t = f1
(
Xt; H(1)
t??1
)
;
H(l)
t = fl
(
H(l??1)
t ; H(l)
t??1
)
:
(9.3.1)
Finally, the output layer is only based on the hidden state of hidden layer L. We use the output
function g to address this:
Ot = g
(
H(L)
t
)
: (9.3.2)
Just as with multilayer perceptrons, the number of hidden layers L and number of hidden units h
are hyper parameters. In particular, we can pick a regular RNN, a GRU, or an LSTM to implement
the model.
354 Chapter 9. Modern Recurrent Neural Networks
9.3.2 Concise Implementation
Fortunately many of the logistical details required to implement multiple layers of an RNN are
readily available in Gluon. To keep things simple we only illustrate the implementation using
such built-in functionality. The code is very similar to the one we used previously for LSTMs. In
fact, the only difference is that we specify the number of layers explicitly rather than picking the
default of a single layer. Let us begin by importing the appropriate modules and loading data.
from d2l import mxnet as d2l
from mxnet import npx
from mxnet.gluon import rnn
npx.set_np()
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
The architectural decisions (such as choosing parameters) are very similar to those of previous
sections. We pick the same number of inputs and outputs as we have distinct tokens, i.e., vocab_
size. The number of hidden units is still 256. The only difference is that we now select a
nontrivial number of layers num_layers = 2.
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
device = d2l.try_gpu()
lstm_layer = rnn.LSTM(num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
9.3.3 Training
The actual invocation logic is identical to before. The only difference is that we now instantiate
two layers with LSTMs. This rather more complex architecture and the large number of epochs
slow down training considerably.
num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.1, 126635.9 tokens/sec on gpu(0)
time traveller it s against reason said filby what reason said
traveller it s against reason said filby what reason said
9.3. Deep Recurrent Neural Networks 355
Summary
� In deep recurrent neural networks, hidden state information is passed to the next timestep
of the current layer and the current timestep of the next layer.
� There exist many different flavors of deep RNNs, such as LSTMs, GRUs, or regular RNNs.
Conveniently these models are all available as parts of the rnn module in Gluon.
� Initialization of the models requires care. Overall, deep RNNs require considerable amount
of work (such as learning rate and clipping) to ensure proper convergence.
Exercises
1. Try to implement a two-layer RNN from scratch using the single layer implementation we
discussed in Section 8.5.
2. Replace the LSTM by a GRU and compare the accuracy.
3. Increase the training data to include multiple books. How low can you go on the perplexity
scale?
4. Would you want to combine sources of different authors when modeling text? Why is this a
good idea? What could go wrong?
Discussions121
9.4 Bidirectional Recurrent Neural Networks
So far we assumed that our goal is to model the next word given what we have seen so far, e.g., in
the context of a time series or in the context of a language model. While this is a typical scenario,
it is not the only one we might encounter. To illustrate the issue, consider the following three tasks
of filling in the blanks in a text:
1. I am _____
2. I am _____ very hungry.
121 https://discuss.d2l.ai/t/340
356 Chapter 9. Modern Recurrent Neural Networks
3. I am _____ very hungry, I could eat half a pig.
Depending on the amount of information available, we might fill the blanks with very different
words such as �happy�, �not�, and �very�. Clearly the end of the phrase (if available) conveys significant
information about which word to pick. A sequence model that is incapable of taking
advantage of this will perform poorly on related tasks. For instance, to do well in named entity
recognition (e.g., to recognize whether �Green� refers to �Mr. Green� or to the color) longer-range
context is equally vital. To get some inspiration for addressing the problem let us take a detour to
graphical models.
9.4.1 Dynamic Programming
This section serves to illustrate the dynamic programming problem. The specific technical details
do not matter for understanding the deep learning counterpart but they help in motivating why
one might use deep learning and why one might pick specific architectures.
If we want to solve the problem using graphical models we could for instance design a latent variable
model as follows. We assume that there exists some latent variable ht which governs the
emissions xt that we observe via p(xt j ht). Moreover, the transitions ht ! ht+1 are given by some
state transition probability p(ht + 1 j ht). The graphical model is then a Hidden Markov Model
(HMM) as in Fig. 9.4.1.
Fig. 9.4.1: Hidden Markov Model.
Thus, for a sequence of T observations we have the following joint probability distribution over
observed and hidden states:
p(x; h) = p(h1)p(x1 j h1)
?T
t=2
p(ht j ht??1)p(xt j ht): (9.4.1)
Now assume that we observe all xi with the exception of some xj and it is our goal to compute
p(xj j x??j), where x??j = (x1; x2; : : : ; xj??1). To accomplish this we need to sum over all possible
choices of h = (h1; : : : ; hT ). In case hi can take on k distinct values, this means that we need to
sum over kT terms�mission impossible! Fortunately there is an elegant solution for this: dynamic
programming. To see how it works, consider summing over the first two hidden variable h1 and
9.4. Bidirectional Recurrent Neural Networks 357
h2. This yields:
p(x) =
?
h1;:::;hT
p(x1; : : : ; xT ; h1; : : : ; hT )
=
?
h1;:::;hT
p(h1)p(x1 j h1)
?T
t=2
p(ht j ht??1)p(xt j ht)
=
?
h2;:::;hT
2
4
?
h1
p(h1)p(x1 j h1)p(h2 j h1)
3
5
| {z }
=:2(h2)
p(x2 j h2)
?T
t=3
p(ht j ht??1)p(xt j ht)
=
?
h3;:::;hT
2
4
?
h2
2(h2)p(x2 j h2)p(h3 j h2)
3
5
| {z }
=:3(h3)
p(x3 j h3)
?T
t=4
p(ht j ht??1)p(xt j ht)
= : : :
=
?
hT
T (hT )p(xT j hT ):
(9.4.2)
In general we have the forward recursion as
t+1(ht+1) =
?
ht
t(ht)p(xt j ht)p(ht+1 j ht): (9.4.3)
The recursion is initialized as 1(h1) = p(h1). In abstract terms this can be written as t+1 =
f(t; xt), where f is some learnable function. This looks very much like the update equation in
the hidden variable models we discussed so far in the context of RNNs. Entirely analogously to
the forward recursion, we can also start a backward recursion. This yields:
p(x) =
?
h1;:::;hT
p(x1; : : : ; xT ; h1; : : : ; hT )
=
?
h1;:::;hT
T???1
t=1
p(ht j ht??1)p(xt j ht)  p(hT j hT??1)p(xT j hT )
=
?
h1;:::;hT??1
T???1
t=1
p(ht j ht??1)p(xt j ht) 
2
4
?
hT
p(hT j hT??1)p(xT j hT )
3
5
| {z }
=:T??1(hT??1)
=
?
h1;:::;hT??2
T???2
t=1
p(ht j ht??1)p(xt j ht) 
2
4
?
hT??1
p(hT??1 j hT??2)p(xT??1 j hT??1)T??1(hT??1)
3
5
| {z }
=:T??2(hT??2)
= : : : ;
=
?
h1
p(h1)p(x1 j h1)1(h1):
(9.4.4)
We can thus write the backward recursion as
t??1(ht??1) =
?
ht
p(ht j ht??1)p(xt j ht)t(ht); (9.4.5)
358 Chapter 9. Modern Recurrent Neural Networks
with initialization T (hT ) = 1. These two recursions allow us to sum over T variables in O(kT)
(linear) time over all values of (h1; : : : ; hT ) rather than in exponential time. This is one of the
great benefits of the probabilistic inference with graphical models. It is a very special instance
of the (Aji & McEliece, 2000) proposed in 2000 by Aji and McEliece. Combining both forward and
backward pass, we are able to compute
p(xj j x??j) /
?
hj
j(hj)j(hj)p(xj j hj): (9.4.6)
Note that in abstract terms the backward recursion can be written as t??1 = g(t; xt), where g is a
learnable function. Again, this looks very much like an update equation, just running backwards
unlike what we have seen so far in RNNs. Indeed, HMMs benefit from knowing future data when
it is available. Signal processing scientists distinguish between the two cases of knowing and not
knowing future observations as interpolation v.s. extrapolation. See the introductory chapter of
the book by (Doucet et al., 2001) on sequential Monte Carlo algorithms for more details.
9.4.2 Bidirectional Model
If we want to have a mechanism in RNNs that offers comparable look-ahead ability as in HMMs,
we need to modify the recurrent net design that we have seen so far. Fortunately, this is easy conceptually.
Instead of running an RNN only in the forward mode starting from the first symbol, we
start another one from the last symbol running from back to front. Bidirectional recurrent neural
networks add a hidden layer that passes information in a backward direction to more flexibly process
such information. Fig. 9.4.2 illustrates the architecture of a bidirectional recurrent neural
network with a single hidden layer.
Fig. 9.4.2: Architecture of a bidirectional recurrent neural network.
In fact, this is not too dissimilar to the forward and backward recursion we encountered above.
The main distinction is that in the previous case these equations had a specific statistical meaning.
Now they are devoid of such easily accessible interpretation and we can just treat them as generic
functions. This transition epitomizes many of the principles guiding the design of modern deep
networks: first, use the type of functional dependencies of classical statistical models, and then
use the models in a generic form.
9.4. Bidirectional Recurrent Neural Networks 359
Definition
Bidirectional RNNs were introduced by (Schuster & Paliwal, 1997). For a detailed discussion of
the various architectures see also the paper by (Graves & Schmidhuber, 2005). Let us look at the
specifics of such a network.
For a given timestep t, the minibatch input is Xt 2 Rnd (number of examples: n, number of inputs:
d) and the hidden layer activation function is ?. In the bidirectional architecture, we assume
that the forward and backward hidden states for this timestep are
??!H t 2 Rnh and
 ??
H t 2 Rnh respectively.
Here h indicates the number of hidden units. We compute the forward and backward
hidden state updates as follows:
??!H t = ?(XtW(f)
xh +
??!H t??1W(f)
hh + b(f)
h );
 ??
H t = ?(XtW(b)
xh +
 ??
H t+1W(b)
hh + b(b)
h ):
(9.4.7)
Here, the weight parameters W(f)
xh
2 Rdh; W(f)
hh
2 Rhh; W(b)
xh
2 Rdh; and W(b)
hh
2 Rhh, and bias
parameters b(f)
h
2 R1h and b(b)
h
2 R1h are all model parameters.
Then we concatenate the forward and backward hidden states
??!H t and
 ??
H t to obtain the hidden
state Ht 2 Rn2h and feed it to the output layer. In deep bidirectional RNNs, the information
is passed on as input to the next bidirectional layer. Last, the output layer computes the output
Ot 2 Rnq (number of outputs: q):
Ot = HtWhq + bq: (9.4.8)
Here, the weight parameter Whq 2 R2hq and the bias parameter bq 2 R1q are the model parameters
of the output layer. The two directions can have different numbers of hidden units.
Computational Cost and Applications
One of the key features of a bidirectional RNN is that information from both ends of the sequence
is used to estimate the output. That is, we use information from both future and past observations
to predict the current one (a smoothing scenario). In the case of language models this is not quite
what we want. After all, we do not have the luxury of knowing the next to next symbol when
predicting the next one. Hence, if we were to use a bidirectional RNN naively we would not get a
very good accuracy: during training we have past and future data to estimate the present. During
test time we only have past data and thus poor accuracy (we will illustrate this in an experiment
below).
To add insult to injury, bidirectional RNNs are also exceedingly slow. The main reasons for this
are that they require both a forward and a backward pass and that the backward pass is dependent
on the outcomes of the forward propagation. Hence, gradients will have a very long dependency
chain.
In practice bidirectional layers are used very sparingly and only for a narrow set of applications,
such as filling in missing words, annotating tokens (e.g., for named entity recognition), or encoding
sequences wholesale as a step in a sequence processing pipeline (e.g., for machine translation).
In short, handle with care!
360 Chapter 9. Modern Recurrent Neural Networks
Training a Bidirectional RNN for the Wrong Application
If we were to ignore all advice regarding the fact that bidirectional LSTMs use past and future
data and simply apply it to language models, we will get estimates with acceptable perplexity.
Nonetheless, the ability of the model to predict future symbols is severely compromised as the
example below illustrates. Despite reasonable perplexity, it only generates gibberish even after
many iterations. We include the code below as a cautionary example against using them in the
wrong context.
from d2l import mxnet as d2l
from mxnet import npx
from mxnet.gluon import rnn
npx.set_np()
# Load data
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
# Define the model
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
lstm_layer = rnn.LSTM(num_hiddens, num_layers, bidirectional=True)
model = d2l.RNNModel(lstm_layer, len(vocab))
# Train the model
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
perplexity 1.2, 78537.3 tokens/sec on gpu(0)
time travellerererererererererererererererererererererererererer
travellerererererererererererererererererererererererererer
The output is clearly unsatisfactory for the reasons described above. For a discussion of more
effective uses of bidirectional models, please see the sentiment classification in Section 15.2.
9.4. Bidirectional Recurrent Neural Networks 361
Summary
� In bidirectional recurrent neural networks, the hidden state for each timestep is simultaneously
determined by the data prior to and after the current timestep.
� Bidirectional RNNs bear a striking resemblance with the forward-backward algorithm in
graphical models.
� Bidirectional RNNs are mostly useful for sequence embedding and the estimation of observations
given bidirectional context.
� Bidirectional RNNs are very costly to train due to long gradient chains.
Exercises
1. If the different directions use a different number of hidden units, how will the shape of Ht
change?
2. Design a bidirectional recurrent neural network with multiple hidden layers.
3. Implement a sequence classification algorithm using bidirectional RNNs. Hint: use the RNN
to embed each word and then aggregate (average) all embedded outputs before sending the
output into an MLP for classification. For instance, if we have (o1; o2; o3), we compute o =
1
3
?
i oi first and then use the latter for sentiment classification.
Discussions122
9.5 Machine Translation and the Dataset
So far we see how to use recurrent neural networks for language models, in which we predict the
next token given all previous tokens in an article. Now let us have a look at a different application,
machine translation, whose predict output is no longer a single token, but a list of tokens.
Machine translation (MT) refers to the automatic translation of a segment of text from one language
to another. Solving this problem with neural networks is often called neural machine translation
(NMT). Compared to language models (Section 8.3), in which the corpus only contains a
single language, machine translation dataset has at least two languages, the source language and
the target language. In addition, each sentence in the source language is mapped to the according
translation in the target language. Therefore, the data preprocessing for machine translation
data is different to the one for language models. This section is dedicated to demonstrate how to
pre-process such a dataset and then load into a set of minibatches.
from d2l import mxnet as d2l
from mxnet import np, npx, gluon
import os
npx.set_np()
122 https://discuss.d2l.ai/t/339
362 Chapter 9. Modern Recurrent Neural Networks
9.5.1 Reading and Preprocessing the Dataset
We first download a dataset that contains a set of English sentences with the corresponding French
translations. As can be seen that each line contains an English sentence with its French translation,
which are separated by a TAB.
#@save
d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
'94646ad1522d915e7b0f9296181140edcf86a4f5')
#@save
def read_data_nmt():
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r') as f:
return f.read()
raw_text = read_data_nmt()
print(raw_text[0:106])
Go. Va !
Hi. Salut !
Run! Cours?!
Run! Courez?!
Who? Qui ?
Wow! Ca alors?!
Fire! Au feu !
Help! A l'aide?!
We perform several preprocessing steps on the raw text data, including ignoring cases, replacing
UTF-8 non-breaking space with space, and adding space between words and punctuation marks.
#@save
def preprocess_nmt(text):
def no_space(char, prev_char):
return char in set(',.!') and prev_char != ' '
text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
out = [' ' + char if i > 0 and no_space(char, text[i-1]) else char
for i, char in enumerate(text)]
return ''.join(out)
text = preprocess_nmt(raw_text)
print(text[0:95])
go . va !
hi . salut !
run ! cours !
run ! courez !
who? qui ?
wow ! ca alors !
fire ! au feu !
9.5. Machine Translation and the Dataset 363
9.5.2 Tokenization
Different to using character tokens in Section 8.3, here a token is either a word or a punctuation
mark. The following function tokenizes the text data to return source and target. Each one is a list
of token list, with source[i] is the ith sentence in the source language and target[i] is the ith sentence
in the target language. To make the latter training faster, we sample the first num_examples
sentences pairs.
#@save
def tokenize_nmt(text, num_examples=None):
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target
source, target = tokenize_nmt(text)
source[0:3], target[0:3]
([['go', '.'], ['hi', '.'], ['run', '!']],
[['va', '!'], ['salut', '!'], ['cours', '!']])
We visualize the histogram of the number of tokens per sentence in the following figure. As can be
seen, a sentence in average contains 5 tokens, and most of the sentences have less than 10 tokens.
d2l.set_figsize()
d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],
label=['source', 'target'])
d2l.plt.legend(loc='upper right');
364 Chapter 9. Modern Recurrent Neural Networks
9.5.3 Vocabulary
Since the tokens in the source language could be different to the ones in the target language, we
need to build a vocabulary for each of them. Since we are using words instead of characters as
tokens, it makes the vocabulary size significantly large. Here we map every token that appears
less than 3 times into the <unk> token Section 8.2. In addition, we need other special tokens such
as padding and sentence beginnings.
src_vocab = d2l.Vocab(source, min_freq=3,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
len(src_vocab)
9140
9.5.4 Loading the Dataset
In language models, each example is a num_steps length sequence from the corpus, which may
be a segment of a sentence, or span over multiple sentences. In machine translation, an example
should contain a pair of source sentence and target sentence. These sentences might have
different lengths, while we need same length examples to form a minibatch.
One way to solve this problem is that if a sentence is longer than num_steps, we trim its length,
otherwise pad with a special <pad> token to meet the length. Therefore we could transform any
sentence to a fixed length.
#@save
def truncate_pad(line, num_steps, padding_token):
if len(line) > num_steps:
return line[:num_steps] # Trim
return line + [padding_token] * (num_steps - len(line)) # Pad
truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])
[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]
Now we can convert a list of sentences into an (num_example, num_steps) index array. We also
record the length of each sentence without the padding tokens, called valid length, which might
be used by some models. In addition, we add the special �<bos>� and �<eos>� tokens to the target
sentences so that our model will know the signals for starting and ending predicting.
#@save
def build_array(lines, vocab, num_steps, is_source):
lines = [vocab[l] for l in lines]
if not is_source:
lines = [[vocab['<bos>']] + l + [vocab['<eos>']] for l in lines]
array = np.array([truncate_pad(
l, num_steps, vocab['<pad>']) for l in lines])
valid_len = (array != vocab['<pad>']).sum(axis=1)
return array, valid_len
Then we can construct minibatches based on these arrays.
9.5. Machine Translation and the Dataset 365
9.5.5 Putting All Things Together
Finally, we define the function load_data_nmt to return the data iterator with the vocabularies for
source language and target language.
#@save
def load_data_nmt(batch_size, num_steps, num_examples=1000):
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=3,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=3,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array(
source, src_vocab, num_steps, True)
tgt_array, tgt_valid_len = build_array(
target, tgt_vocab, num_steps, False)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return src_vocab, tgt_vocab, data_iter
Let us read the first batch.
src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, num_steps=8)
for X, X_vlen, Y, Y_vlen in train_iter:
print('X:', X.astype('int32'))
print('valid lengths for X:', X_vlen)
print('Y:', Y.astype('int32'))
print('valid lengths for Y:', Y_vlen)
break
X: [[ 5 45 8 4 1 1 1 1]
[10 70 4 1 1 1 1 1]]
valid lengths for X: [4 3]
Y: [[ 2 6 55 47 4 3 1 1]
[ 2 9 0 4 3 1 1 1]]
valid lengths for Y: [6 5]
Summary
� Machine translation (MT) refers to the automatic translation of a segment of text from one
language to another.
� We read, preprocess, and tokenize the datasets from both source language and target language.
366 Chapter 9. Modern Recurrent Neural Networks
Exercises
1. Find a machine translation dataset online and process it.
Discussions123
9.6 Encoder-Decoder Architecture
The encoder-decoder architecture is a neural network design pattern. As shown in Fig. 9.6.1, the
architecture is partitioned into two parts, the encoder and the decoder. The encoder?s role is to
encode the inputs into state, which often contains several tensors. Then the state is passed into
the decoder to generate the outputs. In machine translation, the encoder transforms a source
sentence, e.g., �Hello world.�, into state, e.g., a vector, that captures its semantic information. The
decoder then uses this state to generate the translated target sentence, e.g., �Bonjour le monde.�.
Fig. 9.6.1: The encoder-decoder architecture.
In this section, we will show an interface to implement this encoder-decoder architecture.
9.6.1 Encoder
The encoder is a normal neural network that takes inputs, e.g., a source sentence, to return outputs.
from mxnet.gluon import nn
#@save
class Encoder(nn.Block):
"""The base encoder interface for the encoder-decoder architecture."""
def __init__(self, **kwargs):
super(Encoder, self).__init__(**kwargs)
def forward(self, X, *args):
raise NotImplementedError
9.6.2 Decoder
The decoder has an additional method init_state to parse the outputs of the encoder with possible
additional information, e.g., the valid lengths of inputs, to return the state it needs. In the
forward method, the decoder takes both inputs, e.g., a target sentence and the state. It returns
outputs, with potentially modified state if the encoder contains RNN layers.
123 https://discuss.d2l.ai/t/344
9.6. Encoder-Decoder Architecture 367
#@save
class Decoder(nn.Block):
"""The base decoder interface for the encoder-decoder architecture."""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)
def init_state(self, enc_outputs, *args):
raise NotImplementedError
def forward(self, X, state):
raise NotImplementedError
9.6.3 Model
The encoder-decoder model contains both an encoder and a decoder. We implement its forward
method for training. It takes both encoder inputs and decoder inputs, with optional additional
arguments. During computation, it first computes encoder outputs to initialize the decoder state,
and then returns the decoder outputs.
#@save
class EncoderDecoder(nn.Block):
"""The base class for the encoder-decoder architecture."""
def __init__(self, encoder, decoder, **kwargs):
super(EncoderDecoder, self).__init__(**kwargs)
self.encoder = encoder
self.decoder = decoder
def forward(self, enc_X, dec_X, *args):
enc_outputs = self.encoder(enc_X, *args)
dec_state = self.decoder.init_state(enc_outputs, *args)
return self.decoder(dec_X, dec_state)
Summary
� An encoder-decoder architecture is a neural network design pattern mainly in natural language
processing.
� An encoder is a network (FC, CNN, RNN, etc.) that takes the input, and outputs a feature
map, a vector or a tensor.
� A decoder is a network (usually the same network structure as encoder) that takes the feature
vector from the encoder, and gives the best closest match to the actual input or intended
output.
368 Chapter 9. Modern Recurrent Neural Networks
Exercises
1. Besides machine translation, can you think of another application scenarios where an
encoder-decoder architecture can fit?
2. Can you design a deep encoder-decoder architecture?
Discussions124
9.7 Sequence to Sequence
The sequence to sequence (seq2seq) model is based on the encoder-decoder architecture to generate
a sequence output for a sequence input, as demonstrated in Fig. 9.7.1. Both the encoder and
the decoder use recurrent neural networks (RNNs) to handle sequence inputs of variable length.
The hidden state of the encoder is used directly to initialize the decoder hidden state to pass information
from the encoder to the decoder.
Fig. 9.7.1: The sequence to sequence model architecture.
The layers in the encoder and the decoder are illustrated in Fig. 9.7.2.
Fig. 9.7.2: Layers in the encoder and the decoder.
In this section we will explain and implement the seq2seq model to train on the machine translation
dataset.
from d2l import mxnet as d2l
from mxnet import np, npx, init, gluon, autograd
(continues on next page)
124 https://discuss.d2l.ai/t/341
9.7. Sequence to Sequence 369
(continued from previous page)
from mxnet.gluon import nn, rnn
npx.set_np()
9.7.1 Encoder
Recall that the encoder of seq2seq can transform the inputs of variable length to a fixed-length
context vector c by encoding the sequence information into c. We usually use RNN layers within
the encoder. Suppose that we have an input sequence x1; : : : ; xT , where xt is the tth word. At
timestep t, the RNN will have two vectors as the input: the feature vector xt of xt and the hidden
state of the last timestep ht??1. Let us denote the transformation of the RNN?s hidden states by a
function f:
ht = f(xt; ht??1): (9.7.1)
Next, the encoder captures information of all the hidden states and encodes it into the context
vector c with a function q:
c = q(h1; : : : ; hT ): (9.7.2)
For example, if we choose q as q(h1; : : : ; hT ) = hT , then the context vector will be the final hidden
state hT .
So far what we describe above is a unidirectional RNN, where each timestep?s hidden state depends
only on the previous timesteps?. We can also use other forms of RNNs such as GRUs, LSTMs, and
bidirectional RNNs to encode the sequential input.
Now let us implement the seq2seq?s encoder. Here we use the word embedding layer to obtain the
feature vector according to the word index of the input language. Those feature vectors will be
fed to a multi-layer LSTM. The input for the encoder is a batch of sequences, which is 2-D tensor
with shape (batch size, sequence length). The encoder returns both the LSTM outputs, i.e., hidden
states of all the timesteps, as well as the hidden state and the memory cell of the final timestep.
#@save
class Seq2SeqEncoder(d2l.Encoder):
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqEncoder, self).__init__(**kwargs)
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)
def forward(self, X, *args):
# `X` shape: (`batch_size`, `seq_len`, `embed_size`)
X = self.embedding(X)
# RNN needs first axes to be timestep, i.e., `seq_len`
X = X.swapaxes(0, 1)
state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)
out, state = self.rnn(X, state)
# `out` shape: (`seq_len`, `batch_size`, `num_hiddens`)
# `state` shape: (`num_layers`, `batch_size`, `num_hiddens`),
# where "state" contains the hidden state and the memory cell
return out, state
370 Chapter 9. Modern Recurrent Neural Networks
Next, we will create a minibatch sequence input with a batch size of 4 and 7 timesteps. We assume
the number of hidden layers of the LSTM unit is 2 and the number of hidden units is 16. The output
shape returned by the encoder after performing forward calculation on the input is (number
of timesteps, batch size, number of hidden units). The shape of the multi-layer hidden state of
the gated recurrent unit in the final timestep is (number of hidden layers, batch size, number of
hidden units). For the gated recurrent unit, the state list contains only one element, which is the
hidden state. If long short-term memory is used, the state list will also contain another element,
which is the memory cell.
encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
num_layers=2)
encoder.initialize()
X = np.zeros((4, 7))
output, state = encoder(X)
output.shape
(7, 4, 16)
Since an LSTM is used, the state list will contain both the hidden state and the memory cell with
same shape (number of hidden layers, batch size, number of hidden units). However, if a GRU
is used, the state list will contain only one element�the hidden state in the final timestep with
shape (number of hidden layers, batch size, number of hidden units).
len(state), state[0].shape, state[1].shape
(2, (2, 4, 16), (2, 4, 16))
9.7.2 Decoder
As we just introduced, the context vector c encodes the information from the whole input sequence
x1; : : : ; xT . Suppose that the given outputs in the training set are y1; : : : ; yT? . At each
timestep t?, the conditional probability of output yt? will depend on the previous output sequence
y1; : : : ; yt???1 and the context vector c, i.e.,
P(yt? j y1; : : : ; yt???1; c): (9.7.3)
Hence, we can use another RNN as the decoder. At timestep t?, the decoder will update its hidden
state st? using three inputs: the feature vector yt???1 of yt???1, the context vector c, and the hidden
state of the last timestep st???1. Let us denote the transformation of the RNN?s hidden states within
the decoder by a function g:
st? = g(yt???1; c; st???1): (9.7.4)
When implementing the decoder, we directly use the hidden state of the encoder in the final
timestep as the initial hidden state of the decoder. This requires that the encoder and decoder
RNNs have the same numbers of layers and hidden units. The LSTM forward calculation of the
decoder is similar to that of the encoder. The only difference is that we add a dense layer after
the LSTM layers, where the hidden size is the vocabulary size. The dense layer will predict the
confidence score for each word.
9.7. Sequence to Sequence 371
#@save
class Seq2SeqDecoder(d2l.Decoder):
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqDecoder, self).__init__(**kwargs)
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)
self.dense = nn.Dense(vocab_size, flatten=False)
def init_state(self, enc_outputs, *args):
return enc_outputs[1]
def forward(self, X, state):
X = self.embedding(X).swapaxes(0, 1)
out, state = self.rnn(X, state)
# Make the batch to be the first dimension to simplify loss computation
out = self.dense(out).swapaxes(0, 1)
return out, state
We create a decoder with the same hyperparameters as the encoder. As we can see, the output
shape is changed to (batch size, the sequence length, vocabulary size).
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,
num_hiddens=16, num_layers=2)
decoder.initialize()
state = decoder.init_state(encoder(X))
out, state = decoder(X, state)
out.shape, len(state), state[0].shape, state[1].shape
((4, 7, 10), 2, (2, 4, 16), (2, 4, 16))
9.7.3 The Loss Function
For each timestep, the decoder outputs a vocabulary-size confidence score vector to predict words.
Similar to language modeling, we can apply softmax to obtain the probabilities and then use crossentropy
loss to calculate the loss. Note that we padded the target sentences to make them have
the same length, but we do not need to compute the loss on the padding symbols.
To implement the loss function that filters out some entries, we will use an operator called SequenceMask.
It can specify to mask the first dimension (axis=0) or the second one (axis=1). If the
second one is chosen, given a valid length vector len and 2-dim input X, this operator sets X[i,
len[i]:] = 0 for all i?s.
X = np.array([[1, 2, 3], [4, 5, 6]])
npx.sequence_mask(X, np.array([1, 2]), True, axis=1)
array([[1., 0., 0.],
[4., 5., 0.]])
Apply to n-dim tensor X, it sets X[i, len[i]:, :, ..., :] = 0. In addition, we can specify the
filling value such as ??1 as shown below.
372 Chapter 9. Modern Recurrent Neural Networks
X = np.ones((2, 3, 4))
npx.sequence_mask(X, np.array([1, 2]), True, value=-1, axis=1)
array([[[ 1., 1., 1., 1.],
[-1., -1., -1., -1.],
[-1., -1., -1., -1.]],
[[ 1., 1., 1., 1.],
[ 1., 1., 1., 1.],
[-1., -1., -1., -1.]]])
Now we can implement the masked version of the softmax cross-entropy loss. Note that each
Gluon loss function allows to specify per-example weights, in default they are 1s. Then we can
just use a zero weight for each example we would like to remove. So our customized loss function
accepts an additional valid_len argument to ignore some failing elements in each sequence.
#@save
class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):
# `pred` shape: (`batch_size`, `seq_len`, `vocab_size`)
# `label` shape: (`batch_size`, `seq_len`)
# `valid_len` shape: (`batch_size`, )
def forward(self, pred, label, valid_len):
# weights shape: (batch_size, seq_len, 1)
weights = np.expand_dims(np.ones_like(label), axis=-1)
weights = npx.sequence_mask(weights, valid_len, True, axis=1)
return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)
For a sanity check, we create identical three sequences, keep 4 elements for the first sequence, 2
elements for the second sequence, and none for the last one. Then the first example loss should
be 2 times larger than the second one, and the last loss should be 0.
loss = MaskedSoftmaxCELoss()
loss(np.ones((3, 4, 10)), np.ones((3, 4)), np.array([4, 2, 0]))
array([2.3025851, 1.1512926, 0. ])
9.7.4 Training
During training, if the target sequence has length n, we feed the first n??1 tokens into the decoder
as inputs, and the last n ?? 1 tokens are used as ground truth label.
#@save
def train_s2s_ch9(model, data_iter, lr, num_epochs, device):
model.initialize(init.Xavier(), force_reinit=True, ctx=device)
trainer = gluon.Trainer(model.collect_params(),
'adam', {'learning_rate': lr})
loss = MaskedSoftmaxCELoss()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[1, num_epochs], ylim=[0, 0.25])
for epoch in range(1, num_epochs + 1):
(continues on next page)
9.7. Sequence to Sequence 373
(continued from previous page)
timer = d2l.Timer()
metric = d2l.Accumulator(2) # loss_sum, num_tokens
for batch in data_iter:
X, X_vlen, Y, Y_vlen = [x.as_in_ctx(device) for x in batch]
Y_input, Y_label, Y_vlen = Y[:, :-1], Y[:, 1:], Y_vlen-1
with autograd.record():
Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)
l = loss(Y_hat, Y_label, Y_vlen)
l.backward()
d2l.grad_clipping(model, 1)
num_tokens = Y_vlen.sum()
trainer.step(num_tokens)
metric.add(l.sum(), num_tokens)
if epoch % 10 == 0:
animator.add(epoch, (metric[0]/metric[1],))
print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
f'tokens/sec on {str(device)}')
Next, we create a model instance and set hyperparameters. Then, we can train the model.
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()
src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2SeqEncoder(
len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqDecoder(
len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
model = d2l.EncoderDecoder(encoder, decoder)
train_s2s_ch9(model, train_iter, lr, num_epochs, device)
loss 0.026, 8491.3 tokens/sec on gpu(0)
374 Chapter 9. Modern Recurrent Neural Networks
9.7.5 Predicting
Here we implement the simplest method, greedy search, to generate an output sequence. As illustrated
in Fig. 9.7.3, during predicting, we feed the same �<bos>� token to the decoder as training
at timestep 0. But the input token for a later timestep is the predicted token from the previous
timestep.
Fig. 9.7.3: Sequence to sequence model predicting with greedy search
#@save
def predict_s2s_ch9(model, src_sentence, src_vocab, tgt_vocab, num_steps,
device):
src_tokens = src_vocab[src_sentence.lower().split(' ')]
enc_valid_len = np.array([len(src_tokens)], ctx=device)
src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
enc_X = np.array(src_tokens, ctx=device)
# Add the batch size dimension
enc_outputs = model.encoder(np.expand_dims(enc_X, axis=0),
enc_valid_len)
dec_state = model.decoder.init_state(enc_outputs, enc_valid_len)
dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)
predict_tokens = []
for _ in range(num_steps):
Y, dec_state = model.decoder(dec_X, dec_state)
# The token with highest score is used as the next timestep input
dec_X = Y.argmax(axis=2)
py = dec_X.squeeze(axis=0).astype('int32').item()
if py == tgt_vocab['<eos>']:
break
predict_tokens.append(py)
return ' '.join(tgt_vocab.to_tokens(predict_tokens))
Try several examples:
for sentence in ['Go .', 'Wow !', "I'm OK .", 'I won !']:
print(sentence + ' => ' + predict_s2s_ch9(
model, sentence, src_vocab, tgt_vocab, num_steps, device))
Go . => va !
Wow ! => <unk> !
I'm OK . => je vais bien .
I won ! => je l'ai emporte !
9.7. Sequence to Sequence 375
Summary
� The sequence to sequence (seq2seq) model is based on the encoder-decoder architecture to
generate a sequence output from a sequence input.
� We use multiple LSTM layers for both the encoder and the decoder.
Exercises
1. Can you think of other use cases of seq2seq besides neural machine translation?
2. What if the input sequence in the example of this section is longer?
3. If we do not use the SequenceMask in the loss function, what may happen?
Discussions125
9.8 Beam Search
In Section 9.7, we discussed how to train an encoder-decoder with input and output sequences
that are both of variable length. In this section, we are going to introduce how to use the encoderdecoder
to predict sequences of variable length.
As in Section 9.5, when preparing to train the dataset, we normally attach a special symbol �<eos>�
after each sentence to indicate the termination of the sequence. We will continue to use this mathematical
symbol in the discussion below. For ease of discussion, we assume that the output of the
decoder is a sequence of text. Let the size of output text dictionary Y (contains special symbol
�<eos>�) be jYj, and the maximum length of the output sequence be T?. There are a total O(jYjT?
)
types of possible output sequences. All the subsequences after the special symbol �<eos>� in these
output sequences will be discarded. Besides, we still denote the context vector as c, which encodes
information of all the hidden states from the input.
9.8.1 Greedy Search
First, we will take a look at a simple solution: greedy search. For any timestep t? of the output
sequence, we are going to search for the word with the highest conditional probability from jYj
numbers of words, with
yt? = argmax
y2Y
P(y j y1; : : : ; yt???1; c) (9.8.1)
as the output. Once the �<eos>� symbol is detected, or the output sequence has reached its maximum
length T?, the output is completed.
As we mentioned in our discussion of the decoder, the conditional probability of generating an
output sequence based on the input sequence is
?T?
t?=1 P(yt? j y1; : : : ; yt???1; c). We will take the output
sequence with the highest conditional probability as the optimal sequence. The main problem
with greedy search is that there is no guarantee that the optimal sequence will be obtained.
125 https://discuss.d2l.ai/t/345
376 Chapter 9. Modern Recurrent Neural Networks
Take a look at the example below. We assume that there are four words �A�, �B�, �C�, and �<eos>�
in the output dictionary. The four numbers under each timestep in Fig. 9.8.1 represent the conditional
probabilities of generating �A�, �B�, �C�, and �<eos>� at that timestep, respectively. At each
timestep, greedy search selects the word with the highest conditional probability. Therefore, the
output sequence �A�, �B�, �C�, and �<eos>� will be generated in Fig. 9.8.1. The conditional probability
of this output sequence is 0:5  0:4  0:4  0:6 = 0:048.
Fig. 9.8.1: The four numbers under each timestep represent the conditional probabilities of generating
�A�, �B�, �C�, and �<eos>� at that timestep, respectively. At each timestep, greedy search
selects the word with the highest conditional probability.
Now, we will look at another example shown in Fig. 9.8.2. Unlike in Fig. 9.8.1, the following figure
Fig. 9.8.2 selects the word �C�, which has the second highest conditional probability at timestep
2. Since the output subsequences of timesteps 1 and 2, on which timestep 3 is based, are changed
from �A� and �B� in Fig. 9.8.1 to �A� and �C� in Fig. 9.8.2, the conditional probability of each word
generated at timestep 3 has also changed in Fig. 9.8.2. We choose the word �B�, which has the highest
conditional probability. Now, the output subsequences of timestep 4 based on the first three
timesteps are �A�, �C�, and �B�, which are different from �A�, �B�, and �C� in Fig. 9.8.1. Therefore,
the conditional probability of generating each word in timestep 4 in Fig. 9.8.2 is also different
from that in Fig. 9.8.1. We find that the conditional probability of the output sequence �A�, �C�,
�B�, and �<eos>� at the current timestep is 0:5  0:3  0:6  0:6 = 0:054, which is higher than the
conditional probability of the output sequence obtained by greedy search. Therefore, the output
sequence �A�, �B�, �C�, and �<eos>� obtained by the greedy search is not an optimal sequence.
Fig. 9.8.2: The four numbers under each timestep represent the conditional probabilities of generating
�A�, �B�, �C�, and �<eos>� at that timestep. At timestep 2, the word �C�, which has the second
highest conditional probability, is selected.
9.8. Beam Search 377
9.8.2 Exhaustive Search
If the goal is to obtain the optimal sequence, we may consider using exhaustive search: an exhaustive
examination of all possible output sequences, which outputs the sequence with the highest
conditional probability.
Although we can use an exhaustive search to obtain the optimal sequence, its computational overhead
O(jYjT?
) is likely to be excessively high. For example, when jYj = 10000 and T? = 10, we
will need to evaluate 1000010 = 1040 sequences. This is next to impossible to complete. The
computational overhead of greedy search is O(jYj T?), which is usually significantly less than the
computational overhead of an exhaustive search. For example, when jYj = 10000 and T? = 10,
we only need to evaluate 10000  10 = 1  105 sequences.
9.8.3 Beam Search
Beam search is an improved algorithm based on greedy search. It has a hyperparameter named
beam size, k. At timestep 1, we select k words with the highest conditional probability to be the first
word of the k candidate output sequences. For each subsequent timestep, we are going to select
the k output sequences with the highest conditional probability from the total of k jYj possible
output sequences based on the k candidate output sequences from the previous timestep. These
will be the candidate output sequences for that timestep. Finally, we will filter out the sequences
containing the special symbol �<eos>� from the candidate output sequences of each timestep and
discard all the subsequences after it to obtain a set of final candidate output sequences.
Fig. 9.8.3: The beam search process. The beam size is 2 and the maximum length of the output
sequence is 3. The candidate output sequences are A, C, AB, CE, ABD, and CED.
Fig. 9.8.3 demonstrates the process of beam search with an example. Suppose that the vocabulary
of the output sequence contains only five elements: Y = fA;B;C;D;Eg where one of them is
a special symbol �<eos>�. Set beam size to 2, the maximum length of the output sequence to 3.
At timestep 1 of the output sequence, suppose the words with the highest conditional probability
378 Chapter 9. Modern Recurrent Neural Networks
P(y1 j c) are A and C. At timestep 2, for all y2 2 Y; we compute
P(A; y2 j c) = P(A j c)P(y2 j A; c) (9.8.2)
and
P(C; y2 j c) = P(C j c)P(y2 j C; c); (9.8.3)
and pick the largest two among these 10 values, say
P(A;B j c) and P(C;E j c): (9.8.4)
Then at timestep 3, for all y3 2 Y, we compute
P(A;B; y3 j c) = P(A;B j c)P(y3 j A;B; c) (9.8.5)
and
P(C;E; y3 j c) = P(C;E j c)P(y3 j C;E; c); (9.8.6)
and pick the largest two among these 10 values, say
P(A;B;D j c) and P(C;E;D j c): (9.8.7)
As a result, we obtain 6 candidates output sequences: (1) A; (2) C; (3) A, B; (4) C, E; (5) A, B, D;
and (6) C, E, D. In the end, we will get the set of final candidate output sequences based on these
6 sequences.
In the set of final candidate output sequences, we will take the sequence with the highest score as
the output sequence from those below:
1
L log P(y1; : : : ; yL) =
1
L
?L
t?=1
log P(yt? j y1; : : : ; yt???1; c); (9.8.8)
Here, L is the length of the final candidate sequence and the selection for  is generally 0.75. The
L on the denominator is a penalty on the logarithmic addition scores for the longer sequences
above. The computational overhead O(k jYj T?) of the beam search can be obtained through analysis.
The result is between the computational overhead of greedy search and exhaustive search.
In addition, greedy search can be treated as a beam search with a beam size of 1. Beam search
strikes a balance between computational overhead and search quality using a flexible beam size
of k.
Summary
� Methods for predicting variable-length sequences include greedy search, exhaustive search,
and beam search.
� Beam search strikes a balance between computational overhead and search quality using a
flexible beam size.
9.8. Beam Search 379
Exercises
1. Can we treat an exhaustive search as a beam search with a special beam size? Why?
2. We used language models to generate sentences in Section 8.5. Which kind of search does
this output use? Can you improve it?
Discussions126
126 https://discuss.d2l.ai/t/338
380 Chapter 9. Modern Recurrent Neural Networks
10 | Attention Mechanisms
As a bit of a historical digression, attention research is an enormous field with a long history in
cognitive neuroscience. Focalization, concentration of consciousness are of the essence of attention,
which enable the human to prioritize the perception in order to deal effectively with others.
As a result, we do not process all the information that is available in the sensory input. At any
time, we are aware of only a small fraction of the information in the environment. In cognitive
neuroscience, there are several types of attention such as selective attention, covert attention, and
spatial attention. The theory ignites the spark in recent deep learning is the feature integration theory
of the selective attention, which was developed by Anne Treisman and Garry Gelade through
the paper (Treisman & Gelade, 1980) in 1980. This paper declares that when perceiving a stimulus,
features are registered early, automatically, and in parallel, while objects are identified separately
and at a later stage in processing. The theory has been one of the most influential psychological
models of human visual attention.
However, we will not indulge in too much theory of attention in neuroscience, but rather focus on
applying the attention idea in deep learning, where attention can be seen as a generalized pooling
method with bias alignment over inputs. In this chapter, we will provide you with some intuition
about how to transform the attention idea to the concrete mathematics models, and make them
work.
10.1 Attention Mechanisms
In Section 9.7, we encode the source sequence input information in the recurrent unit state and
then pass it to the decoder to generate the target sequence. A token in the target sequence may
closely relate to one or more tokens in the source sequence, instead of the whole source sequence.
For example, when translating �Hello world.� to �Bonjour le monde.�, �Bonjour� maps to �Hello�
and �monde� maps to �world�. In the seq2seq model, the decoder may implicitly select the corresponding
information from the state passed by the encoder. The attention mechanism, however,
makes this selection explicit.
Attention is a generalized pooling method with bias alignment over inputs. The core component
in the attention mechanism is the attention layer, or called attention for simplicity. An input of the
attention layer is called a query. For a query, attention returns an output based on the memory�a
set of key-value pairs encoded in the attention layer. To be more specific, assume that the memory
contains n key-value pairs, (k1; v1); : : : ; (kn; vn), with ki 2 Rdk , vi 2 Rdv . Given a query q 2 Rdq ,
the attention layer returns an output o 2 Rdv with the same shape as the value.
381
Fig. 10.1.1: The attention layer returns an output based on the input query and its memory.
The full process of attention mechanism is expressed in Fig. 10.1.2. To compute the output of
attention, we first use a score function  that measures the similarity between the query and the
key. So for each key k1; : : : ; kn, we compute the scores a1; : : : ; an by
ai = (q; ki): (10.1.1)
Next we use softmax to obtain the attention weights, i.e.,
b = softmax(a) , where bi =
?exp(ai)
j exp(aj)
; b = [b1; : : : ; bn]T : (10.1.2)
Finally, the output is a weighted sum of the values:
o =
?n
i=1
bivi: (10.1.3)
Fig. 10.1.2: The attention output is a weighted sum of the values.
Different choices of the score function lead to different attention layers. Below, we introduce two
commonly used attention layers. Before diving into the implementation, we first express two operators
to get you up and running: a masked version of the softmax operator masked_softmax and
a specialized dot operator batch_dot.
382 Chapter 10. Attention Mechanisms
import math
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
The masked softmax takes a 3-dimensional input and enables us to filter out some elements by
specifying a valid length for the last dimension. (Refer to Section 9.5 for the definition of a valid
length.) As a result, any value outside the valid length will be masked as 0. Let us implement the
masked_softmax function.
#@save
def masked_softmax(X, valid_len):
"""Perform softmax by filtering out some elements."""
# X: 3-D tensor, valid_len: 1-D or 2-D tensor
if valid_len is None:
return npx.softmax(X)
else:
shape = X.shape
if valid_len.ndim == 1:
valid_len = valid_len.repeat(shape[1], axis=0)
else:
valid_len = valid_len.reshape(-1)
# Fill masked elements with a large negative, whose exp is 0
X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_len, True,
axis=1, value=-1e6)
return npx.softmax(X).reshape(shape)
To illustrate how this function works, we construct two 24 matrices as the input. In addition, we
specify that the valid length equals to 2 for the first example, and 3 for the second example. Then,
as we can see from the following outputs, the values outside valid lengths are masked as zero.
masked_softmax(np.random.uniform(size=(2, 2, 4)), np.array([2, 3]))
array([[[0.488994 , 0.511006 , 0. , 0. ],
[0.43654838, 0.56345165, 0. , 0. ]],
[[0.28817102, 0.3519408 , 0.3598882 , 0. ],
[0.29034293, 0.25239873, 0.45725834, 0. ]]])
Moreover, the second operator batch_dot takes two inputs X and Y with shapes (b; n;m) and
(b; m; k), respectively, and returns an output with shape (b; n; k). To be specific, it computes b
dot products for i = f1; : : : ; bg, i.e.,
Z[i; :; :] = X[i; :; :]Y [i; :; :]: (10.1.4)
npx.batch_dot(np.ones((2, 1, 3)), np.ones((2, 3, 2)))
array([[[3., 3.]],
[[3., 3.]]])
10.1. Attention Mechanisms 383
10.1.1 Dot Product Attention
Equipped with the above two operators: masked_softmax and batch_dot, let us dive into the details
of two widely used attention layers. The first one is the dot product attention: it assumes that the
query has the same dimension as the keys, namely q; ki 2 Rd for all i. The dot product attention
computes the scores by a dot product between the query and a key, which is then divided by
p
d
to minimize the unrelated influence of the dimension d on the scores. In other words,
(q; k) = ?q; k?/
p
d: (10.1.5)
Beyond the single-dimensional queries and keys, we can always generalize them to multidimensional
queries and keys. Assume that Q 2 Rmd contains m queries and K 2 Rnd has
all the n keys. We can compute all mn scores by
(Q; K) = QK?
/
p
d: (10.1.6)
With (10.1.6), we can implement the dot product attention layer DotProductAttention that supports
a batch of queries and key-value pairs. In addition, for regularization we also use a dropout
layer.
#@save
class DotProductAttention(nn.Block):
def __init__(self, dropout, **kwargs):
super(DotProductAttention, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)
# `query`: (`batch_size`, #queries, `d`)
# `key`: (`batch_size`, #kv_pairs, `d`)
# `value`: (`batch_size`, #kv_pairs, `dim_v`)
# `valid_len`: either (`batch_size`, ) or (`batch_size`, xx)
def forward(self, query, key, value, valid_len=None):
d = query.shape[-1]
# Set transpose_b=True to swap the last two dimensions of key
scores = npx.batch_dot(query, key, transpose_b=True) / math.sqrt(d)
attention_weights = self.dropout(masked_softmax(scores, valid_len))
return npx.batch_dot(attention_weights, value)
Let us test the class DotProductAttention in a toy example. First, create two batches, where each
batch has one query and 10 key-value pairs. Via the valid_len argument, we specify that we will
check the first 2 key-value pairs for the first batch and 6 for the second one. Therefore, even though
both batches have the same query and key-value pairs, we obtain different outputs.
atten = DotProductAttention(dropout=0.5)
atten.initialize()
keys = np.ones((2, 10, 2))
values = np.arange(40).reshape(1, 10, 4).repeat(2, axis=0)
atten(np.ones((2, 1, 2)), keys, values, np.array([2, 6]))
array([[[ 2. , 3. , 4. , 5. ]],
[[10. , 11. , 12.000001, 13. ]]])
As we can see above, dot product attention simply multiplies the query and key together, and
hopes to derive their similarities from there. Whereas, the query and key may not be of the same
dimension. To address such an issue, we may resort to the MLP attention.
384 Chapter 10. Attention Mechanisms
10.1.2 MLP Attention
In MLP attention, we project both query and keys intoRh by learnable weights parameters. Assume
that the learnable weights are Wk 2 Rhdk , Wq 2 Rhdq , and v 2 Rh. Then the score function is
defined by
(k; q) = v?tanh(Wkk + Wqq): (10.1.7)
Intuitively, you can imagine Wkk+Wqq as concatenating the key and value in the feature dimension
and feeding them to a single hidden layer perceptron with hidden layer size h and output
layer size 1. In this hidden layer, the activation function is tanh and no bias is applied. Now let us
implement the MLP attention.
#@save
class MLPAttention(nn.Block):
def __init__(self, units, dropout, **kwargs):
super(MLPAttention, self).__init__(**kwargs)
# Use flatten=False to keep query's and key's 3-D shapes
self.W_k = nn.Dense(units, use_bias=False, flatten=False)
self.W_q = nn.Dense(units, use_bias=False, flatten=False)
self.v = nn.Dense(1, use_bias=False, flatten=False)
self.dropout = nn.Dropout(dropout)
def forward(self, query, key, value, valid_len):
query, key = self.W_q(query), self.W_k(key)
# Expand query to (`batch_size`, #queries, 1, units), and key to
# (`batch_size`, 1, #kv_pairs, units). Then plus them with broadcast
features = np.expand_dims(query, axis=2) + np.expand_dims(key, axis=1)
features = np.tanh(features)
scores = np.squeeze(self.v(features), axis=-1)
attention_weights = self.dropout(masked_softmax(scores, valid_len))
return npx.batch_dot(attention_weights, value)
To test the above MLPAttention class, we use the same inputs as in the previous toy example. As
we can see below, despite MLPAttention containing an additional MLP model, we obtain the same
outputs as for DotProductAttention.
atten = MLPAttention(units=8, dropout=0.1)
atten.initialize()
atten(np.ones((2, 1, 2)), keys, values, np.array([2, 6]))
array([[[ 2. , 3. , 4. , 5. ]],
[[10. , 11. , 12.000001, 13. ]]])
10.1. Attention Mechanisms 385
Summary
� An attention layer explicitly selects related information.
� An attention layer?s memory consists of key-value pairs, so its output is close to the values
whose keys are similar to the queries.
� Two commonly used attention models are dot product attention and MLP attention.
Exercises
1. What are the advantages and disadvantages for dot product attention and MLP attention,
respectively?
Discussions127
10.2 Sequence to Sequence with Attention Mechanisms
In this section, we add the attention mechanism to the sequence to sequence (seq2seq) model as
introduced in Section 9.7 to explicitly aggregate states with weights. Fig. 10.2.1 shows the model
architecture for encoding and decoding at the timestep t. Here, the memory of the attention layer
consists of all the information that the encoder has seen�the encoder output at each timestep.
During the decoding, the decoder output from the previous timestep t ?? 1 is used as the query.
The output of the attention model is viewed as the context information, and such context is concatenated
with the decoder input Dt. Finally, we feed the concatenation into the decoder.
Fig. 10.2.1: The second timestep in decoding for the sequence to sequence model with attention
mechanism.
To illustrate the overall architecture of seq2seq with attention model, the layer structure of its
encoder and decoder is shown in Fig. 10.2.2.
127 https://discuss.d2l.ai/t/346
386 Chapter 10. Attention Mechanisms
Fig. 10.2.2: The layers in the sequence to sequence model with attention mechanism.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import rnn, nn
npx.set_np()
10.2.1 Decoder
Since the encoder of seq2seq with attention mechanisms is the same as Seq2SeqEncoder in Section
9.7, we will just focus on the decoder. We add an MLP attention layer (MLPAttention) which has
the same hidden size as the LSTM layer in the decoder. Then we initialize the state of the decoder
by passing three items from the encoder:
� the encoder outputs of all timesteps: they are used as the attention layer?s memory with
identical keys and values;
� the hidden state of the encoder?s final timestep: it is used as the initial decoder?s hidden
state;
� the encoder valid length: so the attention layer will not consider the padding tokens within
the encoder outputs.
At each timestep of the decoding, we use the hidden state of the decoder?s last RNN layer as the
query for the attention layer. The attention model?s output is then concatenated with the input embedding
vector to feed into the RNN layer. Although the RNN layer hidden state also contains history
information from decoder, the attention output explicitly selects the encoder outputs based
on enc_valid_len, so that the attention output suspends other irrelevant information.
Let us implement the Seq2SeqAttentionDecoder, and see how it differs from the decoder in
seq2seq from Section 9.7.2.
class Seq2SeqAttentionDecoder(d2l.Decoder):
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
self.attention_cell = d2l.MLPAttention(num_hiddens, dropout)
(continues on next page)
10.2. Sequence to Sequence with Attention Mechanisms 387
(continued from previous page)
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)
self.dense = nn.Dense(vocab_size, flatten=False)
def init_state(self, enc_outputs, enc_valid_len, *args):
outputs, hidden_state = enc_outputs
# Transpose `outputs` to (`batch_size`, `seq_len`, `num_hiddens`)
return (outputs.swapaxes(0, 1), hidden_state, enc_valid_len)
def forward(self, X, state):
enc_outputs, hidden_state, enc_valid_len = state
X = self.embedding(X).swapaxes(0, 1)
outputs = []
for x in X:
# `query` shape: (`batch_size`, 1, `num_hiddens`)
query = np.expand_dims(hidden_state[0][-1], axis=1)
# `context` has same shape as `query`
context = self.attention_cell(
query, enc_outputs, enc_outputs, enc_valid_len)
# Concatenate on the feature dimension
x = np.concatenate((context, np.expand_dims(x, axis=1)), axis=-1)
# Reshape `x` to (1, `batch_size`, `embed_size` + `num_hiddens`)
out, hidden_state = self.rnn(x.swapaxes(0, 1), hidden_state)
outputs.append(out)
outputs = self.dense(np.concatenate(outputs, axis=0))
return outputs.swapaxes(0, 1), [enc_outputs, hidden_state,
enc_valid_len]
Now we can test the seq2seq with attention model. To be consistent with the model without attention
in Section 9.7, we use the same hyperparameters for vocab_size, embed_size, num_hiddens,
and num_layers. As a result, we get the same decoder output shape, but the state structure is
changed.
encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8,
num_hiddens=16, num_layers=2)
encoder.initialize()
decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,
num_hiddens=16, num_layers=2)
decoder.initialize()
X = np.zeros((4, 7))
state = decoder.init_state(encoder(X), None)
out, state = decoder(X, state)
out.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape
((4, 7, 10), 3, (4, 7, 16), 2, (2, 4, 16))
388 Chapter 10. Attention Mechanisms
10.2.2 Training
Similar to Section 9.7.4, we try a toy model by applying the same training hyperparameters and
the same training loss. As we can see from the result, since the sequences in the training dataset
are relatively short, the additional attention layer does not lead to a significant improvement. Due
to the computational overhead of both the encoder?s and the decoder?s attention layers, this model
is much slower than the seq2seq model without attention.
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(batch_size, num_steps)
encoder = d2l.Seq2SeqEncoder(
len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
model = d2l.EncoderDecoder(encoder, decoder)
d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, device)
loss 0.033, 3660.9 tokens/sec on gpu(0)
Last, we predict several sample examples.
for sentence in ['Go .', 'Wow !', "I'm OK .", 'I won !']:
print(sentence + ' => ' + d2l.predict_s2s_ch9(
model, sentence, src_vocab, tgt_vocab, num_steps, device))
Go . => <unk> !
Wow ! => <unk> !
I'm OK . => je vais bien .
I won ! => j'ai gagne !
10.2. Sequence to Sequence with Attention Mechanisms 389
Summary
� The seq2seq model with attention adds an additional attention layer to the model without
attention.
� The decoder of the seq2seq with attention model passes three items from the encoder: the
encoder outputs of all timesteps, the hidden state of the encoder?s final timestep, and the
encoder valid length.
Exercises
1. Compare Seq2SeqAttentionDecoder and Seq2seqDecoder by using the same parameters and
checking their losses.
2. Can you think of any use cases where Seq2SeqAttentionDecoder will outperform
Seq2seqDecoder?
Discussions128
10.3 Transformer
In previous chapters, we have covered major neural network architectures such as convolution
neural networks (CNNs) and recurrent neural networks (RNNs). Let us recap their pros and cons:
� CNNs are easy to parallelize at a layer but cannot capture the variable-length sequential dependency
very well.
� RNNs are able to capture the long-range, variable-length sequential information, but suffer
from inability to parallelize within a sequence.
To combine the advantages from both CNNs and RNNs, (Vaswani et al., 2017) designed a novel
architecture using the attention mechanism. This architecture, which is called as Transformer,
achieves parallelization by capturing recurrence sequence with attention and at the same time
encodes each item?s position in the sequence. As a result, Transformer leads to a compatible model
with significantly shorter training time.
Similar to the seq2seq model in Section 9.7, Transformer is also based on the encoder-decoder
architecture. However, Transformer differs to the former by replacing the recurrent layers in
seq2seq with multi-head attention layers, incorporating the position-wise information through position
encoding, and applying layer normalization. We compare Transformer and seq2seq side-byside
in Fig. 10.3.1.
Overall, these two models are similar to each other: the source sequence embeddings are fed
into n repeated blocks. The outputs of the last block are then used as attention memory for the
decoder. The target sequence embeddings are similarly fed into n repeated blocks in the decoder,
and the final outputs are obtained by applying a dense layer with vocabulary size to the last block?s
outputs.
128 https://discuss.d2l.ai/t/347
390 Chapter 10. Attention Mechanisms
Multi-head
attention
Add & norm
Positional
encoding
Embedding
Positionwise
FFN
Add & norm
Muti-head
attention
Add & norm
Positionwise
FFN
Add & norm
Sources
Dense
Masked
multi-head
attention
Add & norm
Embedding
Targets
x n
n x
State
Embedding
Sources
Decoder
n x
Embedding
Targets
Dense
Encoder
Attention
Recurrent layer Recurrent layer x n
Seq2seq with Attention
Transformer
Positional
+ + encoding
Fig. 10.3.1: The Transformer architecture.
On the flip side, Transformer differs from the seq2seq with attention model in the following:
1. Transformer block: a recurrent layer in seq2seq is replaced by a Transformer block. This
block contains a multi-head attention layer and a position-wise feed-forward network with two
layers for the encoder. For the decoder, another multi-head attention layer is used to take
the encoder state.
2. Add and norm: the inputs and outputs of both the multi-head attention layer or the positionwise
feed-forward network, are processed by two �add and norm� layer that contains a residual
structure and a layer normalization layer.
3. Position encoding: since the self-attention layer does not distinguish the item order in a sequence,
a positional encoding layer is used to add sequential information into each sequence
item.
In the rest of this section, we will equip you with each new component introduced by Transformer,
and get you up and running to construct a machine translation model.
from d2l import mxnet as d2l
import math
from mxnet import autograd, np, npx
from mxnet.gluon import nn
npx.set_np()
10.3. Transformer 391
10.3.1 Multi-Head Attention
Before the discussion of the multi-head attention layer, let us quick express the self-attention architecture.
The self-attention model is a normal attention model, with its query, its key, and its
value being copied exactly the same from each item of the sequential inputs. As we illustrate in
Fig. 10.3.2, self-attention outputs a same-length sequential output for each input item. Compared
with a recurrent layer, output items of a self-attention layer can be computed in parallel and,
therefore, it is easy to obtain a highly-efficient implementation.
Fig. 10.3.2: Self-attention architecture.
The multi-head attention layer consists of h parallel self-attention layers, each one is called a head.
For each head, before feeding into the attention layer, we project the queries, keys, and values with
three dense layers with hidden sizes pq, pk, and pv, respectively. The outputs of these h attention
heads are concatenated and then processed by a final dense layer.
Fig. 10.3.3: Multi-head attention
Assume that the dimension for a query, a key, and a value are dq, dk, and dv, respectively. Then,
for each head i = 1; : : : ; h, we can train learnable parameters W(i)
q 2 Rpqdq , W(i)
k
2 Rpkdk , and
W(i)
v 2 Rpvdv . Therefore, the output for each head is
o(i) = attention(W(i)
q q; W(i)
k k; W(i)
v v); (10.3.1)
where attention can be any attention layer, such as the DotProductAttention and MLPAttention as
we introduced in Section 10.1.
After that, the output with length pv from each of the h attention heads are concatenated to be an
output of length hpv, which is then passed the final dense layer with do hidden units. The weights
392 Chapter 10. Attention Mechanisms
of this dense layer can be denoted by Wo 2 Rdohpv . As a result, the multi-head attention output
will be
o = Wo
2
64
o(1)
...
o(h)
3
75
: (10.3.2)
Now we can implement the multi-head attention. Assume that the multi-head attention contain
the number heads num_heads = h, the hidden size num_hiddens = pq = pk = pv are the same for
the query, key, and value dense layers. In addition, since the multi-head attention keeps the same
dimensionality between its input and its output, we have the output feature size do = num_hiddens
as well.
#@save
class MultiHeadAttention(nn.Block):
def __init__(self, num_hiddens, num_heads, dropout, use_bias=False, **kwargs):
super(MultiHeadAttention, self).__init__(**kwargs)
self.num_heads = num_heads
self.attention = d2l.DotProductAttention(dropout)
self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
def forward(self, query, key, value, valid_len):
# For self-attention, `query`, `key`, and `value` shape:
# (`batch_size`, `seq_len`, `dim`), where `seq_len` is the length of
# input sequence. `valid_len` shape is either (`batch_size`, ) or
# (`batch_size`, `seq_len`).
# Project and transpose `query`, `key`, and `value` from
# (`batch_size`, `seq_len`, `num_hiddens`) to
# (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
query = transpose_qkv(self.W_q(query), self.num_heads)
key = transpose_qkv(self.W_k(key), self.num_heads)
value = transpose_qkv(self.W_v(value), self.num_heads)
if valid_len is not None:
# Copy `valid_len` by `num_heads` times
if valid_len.ndim == 1:
valid_len = np.tile(valid_len, self.num_heads)
else:
valid_len = np.tile(valid_len, (self.num_heads, 1))
# For self-attention, `output` shape:
# (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
output = self.attention(query, key, value, valid_len)
# `output_concat` shape: (`batch_size`, `seq_len`, `num_hiddens`)
output_concat = transpose_output(output, self.num_heads)
return self.W_o(output_concat)
Here are the definitions of the transpose functions transpose_qkv and transpose_output, which
are the inverse of each other.
10.3. Transformer 393
#@save
def transpose_qkv(X, num_heads):
# Input `X` shape: (`batch_size`, `seq_len`, `num_hiddens`).
# Output `X` shape:
# (`batch_size`, `seq_len`, `num_heads`, `num_hiddens` / `num_heads`)
X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)
# `X` shape:
# (`batch_size`, `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
X = X.transpose(0, 2, 1, 3)
# `output` shape:
# (`batch_size` * `num_heads`, `seq_len`, `num_hiddens` / `num_heads`)
output = X.reshape(-1, X.shape[2], X.shape[3])
return output
#@save
def transpose_output(X, num_heads):
# A reversed version of `transpose_qkv`
X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
X = X.transpose(0, 2, 1, 3)
return X.reshape(X.shape[0], X.shape[1], -1)
Let us test the MultiHeadAttention model in the a toy example. Create a multi-head attention
with the hidden size do = 100, the output will share the same batch size and sequence length as
the input, but the last dimension will be equal to the num_hiddens = 100.
cell = MultiHeadAttention(90, 9, 0.5)
cell.initialize()
X = np.ones((2, 4, 5))
valid_len = np.array([2, 3])
cell(X, X, X, valid_len).shape
(2, 4, 90)
10.3.2 Position-wise Feed-Forward Networks
Another key component in the Transformer block is called position-wise feed-forward network
(FFN). It accepts a 3-dimensional input with shape (batch size, sequence length, feature size). The
position-wise FFN consists of two dense layers that applies to the last dimension. Since the same
two dense layers are used for each position item in the sequence, we referred to it as position-wise.
Indeed, it is equivalent to applying two 1  1 convolution layers.
Below, the PositionWiseFFN shows how to implement a position-wise FFN with two dense layers
of hidden size ffn_num_hiddens and pw_num_outputs, respectively.
#@save
class PositionWiseFFN(nn.Block):
def __init__(self, ffn_num_hiddens, pw_num_outputs, **kwargs):
super(PositionWiseFFN, self).__init__(**kwargs)
self.dense1 = nn.Dense(ffn_num_hiddens, flatten=False,
(continues on next page)
394 Chapter 10. Attention Mechanisms
(continued from previous page)
activation='relu')
self.dense2 = nn.Dense(pw_num_outputs, flatten=False)
def forward(self, X):
return self.dense2(self.dense1(X))
Similar to the multi-head attention, the position-wise feed-forward network will only change the
last dimension size of the input�the feature dimension. In addition, if two items in the input
sequence are identical, the according outputs will be identical as well.
ffn = PositionWiseFFN(4, 8)
ffn.initialize()
ffn(np.ones((2, 3, 4)))[0]
array([[ 9.15348239e-04, -7.27669394e-04, 1.14063594e-04,
-8.76279722e-04, -1.02867256e-03, 8.02748313e-04,
-4.53725770e-05, 2.15598906e-04],
[ 9.15348239e-04, -7.27669394e-04, 1.14063594e-04,
-8.76279722e-04, -1.02867256e-03, 8.02748313e-04,
-4.53725770e-05, 2.15598906e-04],
[ 9.15348239e-04, -7.27669394e-04, 1.14063594e-04,
-8.76279722e-04, -1.02867256e-03, 8.02748313e-04,
-4.53725770e-05, 2.15598906e-04]])
10.3.3 Add and Norm
Besides the above two components in the Transformer block, the �add and norm� within the block
also plays a key role to connect the inputs and outputs of other layers smoothly. To explain, we
add a layer that contains a residual structure and a layer normalization after both the multi-head
attention layer and the position-wise FFN network. Layer normalization is similar to batch normalization
in Section 7.5. One difference is that the mean and variances for the layer normalization
are calculated along the last dimension, e.g X.mean(axis=-1) instead of the first batch dimension,
e.g., X.mean(axis=0). Layer normalization prevents the range of values in the layers from changing
too much, which allows faster training and better generalization ability.
MXNet has both LayerNorm and BatchNorm implemented within the nn block. Let us call both of
them and see the difference in the example below.
layer = nn.LayerNorm()
layer.initialize()
batch = nn.BatchNorm()
batch.initialize()
X = np.array([[1, 2], [2, 3]])
# Compute mean and variance from `X` in the training mode
with autograd.record():
print('layer norm:', layer(X), '\nbatch norm:', batch(X))
layer norm: [[-0.99998 0.99998]
[-0.99998 0.99998]]
(continues on next page)
10.3. Transformer 395
(continued from previous page)
batch norm: [[-0.99998 -0.99998]
[ 0.99998 0.99998]]
Now let us implement the connection block AddNorm together. AddNorm accepts two inputs X and
Y . We can deem X as the original input in the residual network, and Y as the outputs from either
the multi-head attention layer or the position-wise FFN network. In addition, we apply dropout
on Y for regularization.
#@save
class AddNorm(nn.Block):
def __init__(self, dropout, **kwargs):
super(AddNorm, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)
self.ln = nn.LayerNorm()
def forward(self, X, Y):
return self.ln(self.dropout(Y) + X)
Due to the residual connection, X and Y should have the same shape.
add_norm = AddNorm(0.5)
add_norm.initialize()
add_norm(np.ones((2, 3, 4)), np.ones((2, 3, 4))).shape
(2, 3, 4)
10.3.4 Positional Encoding
Unlike the recurrent layer, both the multi-head attention layer and the position-wise feed-forward
network compute the output of each item in the sequence independently. This feature enables
us to parallelize the computation, but it fails to model the sequential information for a given sequence.
To better capture the sequential information, the Transformer model uses the positional
encoding to maintain the positional information of the input sequence.
To explain, assume that X 2 Rld is the embedding of an example, where l is the sequence length
and d is the embedding size. This positional encoding layer encodes X?s position P 2 Rld and
outputs P + X.
The position P is a 2-D matrix, where i refers to the order in the sentence, and j refers to the
position along the embedding vector dimension. In this way, each value in the origin sequence is
then maintained using the equations below:
Pi;2j = sin(i/100002j/d); (10.3.3)
Pi;2j+1 = cos(i/100002j/d); (10.3.4)
for i = 0; : : : ; l ?? 1 and j = 0; : : : ; ?(d ?? 1)/2?.
Fig. 10.3.4 illustrates the positional encoding.
396 Chapter 10. Attention Mechanisms
Fig. 10.3.4: Positional encoding.
#@save
class PositionalEncoding(nn.Block):
def __init__(self, num_hiddens, dropout, max_len=1000):
super(PositionalEncoding, self).__init__()
self.dropout = nn.Dropout(dropout)
# Create a long enough `P`
self.P = np.zeros((1, max_len, num_hiddens))
X = np.arange(0, max_len).reshape(-1, 1) / np.power(
10000, np.arange(0, num_hiddens, 2) / num_hiddens)
self.P[:, :, 0::2] = np.sin(X)
self.P[:, :, 1::2] = np.cos(X)
def forward(self, X):
X = X + self.P[:, :X.shape[1], :].as_in_ctx(X.ctx)
return self.dropout(X)
Now we test the PositionalEncoding class with a toy model for 4 dimensions. As we can see, the
4th dimension has the same frequency as the 5th but with different offset (i.e. phase) because one
is produced by a sine function and the other is produced by a cosine function. The 6th and 7th
dimensions have lower frequency.
pe = PositionalEncoding(20, 0)
pe.initialize()
Y = pe(np.zeros((1, 100, 20)))
d2l.plot(np.arange(100), Y[0, :, 4:8].T, figsize=(6, 2.5),
legend=["dim %d" % p for p in [4, 5, 6, 7]])
10.3. Transformer 397
10.3.5 Encoder
Armed with all the essential components of Transformer, let us first build a Transformer encoder
block. This encoder contains a multi-head attention layer, a position-wise feed-forward
network, and two �add and norm� connection blocks. As shown in the code, for both of the attention
model and the positional FFN model in the EncoderBlock, their outputs? dimension are equal
to the num_hiddens. This is due to the nature of the residual block, as we need to add these outputs
back to the original value during �add and norm�.
#@save
class EncoderBlock(nn.Block):
def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias=False, **kwargs):
super(EncoderBlock, self).__init__(**kwargs)
self.attention = MultiHeadAttention(num_hiddens, num_heads, dropout,
use_bias)
self.addnorm1 = AddNorm(dropout)
self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
self.addnorm2 = AddNorm(dropout)
def forward(self, X, valid_len):
Y = self.addnorm1(X, self.attention(X, X, X, valid_len))
return self.addnorm2(Y, self.ffn(Y))
Due to the residual connections, this block will not change the input shape. It means that the
num_hiddens argument should be equal to the input size of the last dimension. In our toy example
below, num_hiddens = 24, ffn_num_hiddens = 48, num_heads = 8, and dropout = 0:5.
X = np.ones((2, 100, 24))
encoder_blk = EncoderBlock(24, 48, 8, 0.5)
encoder_blk.initialize()
encoder_blk(X, valid_len).shape
(2, 100, 24)
Now it comes to the implementation of the entire Transformer encoder. With the Transformer
encoder, n blocks of EncoderBlock stack up one after another. Because of the residual connection,
the embedding layer size d is same as the Transformer block output size. Also note that we multiply
the embedding output by
p
d to prevent its values from being too small.
#@save
class TransformerEncoder(d2l.Encoder):
def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout, use_bias=False, **kwargs):
super(TransformerEncoder, self).__init__(**kwargs)
self.num_hiddens = num_hiddens
self.embedding = nn.Embedding(vocab_size, num_hiddens)
self.pos_encoding = PositionalEncoding(num_hiddens, dropout)
self.blks = nn.Sequential()
for _ in range(num_layers):
self.blks.add(
EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias))
(continues on next page)
398 Chapter 10. Attention Mechanisms
(continued from previous page)
def forward(self, X, valid_len, *args):
X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
for blk in self.blks:
X = blk(X, valid_len)
return X
Let us create an encoder with two stacked Transformer encoder blocks, whose hyperparameters
are the same as before. Similar to the previous toy example?s parameters, we add two more parameters
vocab_size to be 200 and num_layers to be 2 here.
encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)
encoder.initialize()
encoder(np.ones((2, 100)), valid_len).shape
(2, 100, 24)
10.3.6 Decoder
The Transformer decoder block looks similar to the Transformer encoder block. However, besides
the two sub-layers (the multi-head attention layer and the positional encoding network), the
decoder Transformer block contains a third sub-layer, which applies multi-head attention on the
output of the encoder stack. Similar to the Transformer encoder block, the Transformer decoder
block employs �add and norm�, i.e., the residual connections and the layer normalization to connect
each of the sub-layers.
To be specific, at timestep t, assume that xt is the current input, i.e., the query. As illustrated in
Fig. 10.3.5, the keys and values of the self-attention layer consist of the current query with all the
past queries x1; : : : ; xt??1.
Fig. 10.3.5: Predict at timestep t for a self-attention layer.
During training, the output for the t-query could observe all the previous key-value pairs. It results
in an different behavior from prediction. Thus, during prediction we can eliminate the unnecessary
information by specifying the valid length to be t for the tth query.
class DecoderBlock(nn.Block):
# `i` means it is the i-th block in the decoder
def __init__(self, num_hiddens, ffn_num_hiddens, num_heads,
(continues on next page)
10.3. Transformer 399
(continued from previous page)
dropout, i, **kwargs):
super(DecoderBlock, self).__init__(**kwargs)
self.i = i
self.attention1 = MultiHeadAttention(num_hiddens, num_heads, dropout)
self.addnorm1 = AddNorm(dropout)
self.attention2 = MultiHeadAttention(num_hiddens, num_heads, dropout)
self.addnorm2 = AddNorm(dropout)
self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
self.addnorm3 = AddNorm(dropout)
def forward(self, X, state):
enc_outputs, enc_valid_len = state[0], state[1]
# `state[2][i]` contains the past queries for this block
if state[2][self.i] is None:
key_values = X
else:
key_values = np.concatenate((state[2][self.i], X), axis=1)
state[2][self.i] = key_values
if autograd.is_training():
batch_size, seq_len, _ = X.shape
# Shape: (batch_size, seq_len), the values in the j-th column
# are j+1
valid_len = np.tile(np.arange(1, seq_len + 1, ctx=X.ctx),
(batch_size, 1))
else:
valid_len = None
X2 = self.attention1(X, key_values, key_values, valid_len)
Y = self.addnorm1(X, X2)
Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_len)
Z = self.addnorm2(Y, Y2)
return self.addnorm3(Z, self.ffn(Z)), state
Similar to the Transformer encoder block, num_hiddens should be equal to the last dimension size
of X.
decoder_blk = DecoderBlock(24, 48, 8, 0.5, 0)
decoder_blk.initialize()
X = np.ones((2, 100, 24))
state = [encoder_blk(X, valid_len), valid_len, [None]]
decoder_blk(X, state)[0].shape
(2, 100, 24)
The construction of the entire Transformer decoder is identical to the Transformer encoder, except
for the additional dense layer to obtain the output confidence scores.
Let us implement the Transformer decoder TransformerDecoder. Besides the regular hyperparameters
such as the vocab_size and num_hiddens, the Transformer decoder also needs the Transformer
encoder?s outputs enc_outputs and env_valid_len.
class TransformerDecoder(d2l.Decoder):
def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,
(continues on next page)
400 Chapter 10. Attention Mechanisms
(continued from previous page)
num_heads, num_layers, dropout, **kwargs):
super(TransformerDecoder, self).__init__(**kwargs)
self.num_hiddens = num_hiddens
self.num_layers = num_layers
self.embedding = nn.Embedding(vocab_size, num_hiddens)
self.pos_encoding = PositionalEncoding(num_hiddens, dropout)
self.blks = nn.Sequential()
for i in range(num_layers):
self.blks.add(
DecoderBlock(num_hiddens, ffn_num_hiddens, num_heads,
dropout, i))
self.dense = nn.Dense(vocab_size, flatten=False)
def init_state(self, enc_outputs, env_valid_len, *args):
return [enc_outputs, env_valid_len, [None]*self.num_layers]
def forward(self, X, state):
X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
for blk in self.blks:
X, state = blk(X, state)
return self.dense(X), state
10.3.7 Training
Finally, we can build an encoder-decoder model with the Transformer architecture. Similar to
the seq2seq with attention model in Section 10.2, we use the following hyperparameters: two
Transformer blocks with both the embedding size and the block output size to be 32. In addition,
we use 4 heads, and set the hidden size to be twice larger than the output size.
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.0, 64, 10
lr, num_epochs, device = 0.005, 100, d2l.try_gpu()
ffn_num_hiddens, num_heads = 64, 4
src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(batch_size, num_steps)
encoder = TransformerEncoder(
len(src_vocab), num_hiddens, ffn_num_hiddens, num_heads, num_layers,
dropout)
decoder = TransformerDecoder(
len(src_vocab), num_hiddens, ffn_num_hiddens, num_heads, num_layers,
dropout)
model = d2l.EncoderDecoder(encoder, decoder)
d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, device)
loss 0.034, 3190.5 tokens/sec on gpu(0)
10.3. Transformer 401
As we can see from the training time and accuracy, compared with the seq2seq model with attention
model, Transformer runs faster per epoch, and converges faster at the beginning.
We can use the trained Transformer to translate some simple sentences.
for sentence in ['Go .', 'Wow !', "I'm OK .", 'I won !']:
print(sentence + ' => ' + d2l.predict_s2s_ch9(
model, sentence, src_vocab, tgt_vocab, num_steps, device))
Go . => va <unk> !
Wow ! => <unk> !
I'm OK . => ca va <unk> va <unk> va <unk> tom .
I won ! => je l'ai emporte gagne !
Summary
� The Transformer model is based on the encoder-decoder architecture.
� Multi-head attention layer contains h parallel attention layers.
� Position-wise feed-forward network consists of two dense layers that apply to the last dimension.
� Layer normalization differs from batch normalization by normalizing along the last dimension
(the feature dimension) instead of the first (batch size) dimension.
� Positional encoding is the only place that adds positional information to the Transformer
model.
402 Chapter 10. Attention Mechanisms
Exercises
1. Try a larger size of epochs and compare the loss between seq2seq model and Transformer
model.
2. Can you think of any other benefit of positional encoding?
3. Compare layer normalization and batch normalization, when shall we apply which?
Discussions129
129 https://discuss.d2l.ai/t/348
10.3. Transformer 403
404 Chapter 10. Attention Mechanisms
11 | Optimization Algorithms
If you read the book in sequence up to this point you already used a number of advanced optimization
algorithms to train deep learning models. They were the tools that allowed us to continue
updating model parameters and to minimize the value of the loss function, as evaluated on the
training set. Indeed, anyone content with treating optimization as a black box device to minimize
objective functions in a simple setting might well content oneself with the knowledge that there
exists an array of incantations of such a procedure (with names such as �Adam�, �NAG�, or �SGD�).
To do well, however, some deeper knowledge is required. Optimization algorithms are important
for deep learning. On one hand, training a complex deep learning model can take hours, days, or
even weeks. The performance of the optimization algorithm directly affects the model?s training
efficiency. On the other hand, understanding the principles of different optimization algorithms
and the role of their parameters will enable us to tune the hyperparameters in a targeted manner
to improve the performance of deep learning models.
In this chapter, we explore common deep learning optimization algorithms in depth. Almost all
optimization problems arising in deep learning are nonconvex. Nonetheless, the design and analysis
of algorithms in the context of convex problems has proven to be very instructive. It is for
that reason that this section includes a primer on convex optimization and the proof for a very
simple stochastic gradient descent algorithm on a convex objective function.
11.1 Optimization and Deep Learning
In this section, we will discuss the relationship between optimization and deep learning as well
as the challenges of using optimization in deep learning. For a deep learning problem, we will
usually define a loss function first. Once we have the loss function, we can use an optimization
algorithm in attempt to minimize the loss. In optimization, a loss function is often referred to as
the objective function of the optimization problem. By tradition and convention most optimization
algorithms are concerned with minimization. If we ever need to maximize an objective there
is a simple solution: just flip the sign on the objective.
405
11.1.1 Optimization and Estimation
Although optimization provides a way to minimize the loss function for deep learning, in essence,
the goals of optimization and deep learning are fundamentally different. The former is primarily
concerned with minimizing an objective whereas the latter is concerned with finding a suitable
model, given a finite amount of data. In Section 4.4, we discussed the difference between these
two goals in detail. For instance, training error and generalization error generally differ: since the
objective function of the optimization algorithm is usually a loss function based on the training
dataset, the goal of optimization is to reduce the training error. However, the goal of statistical
inference (and thus of deep learning) is to reduce the generalization error. To accomplish the
latter we need to pay attention to overfitting in addition to using the optimization algorithm to
reduce the training error. We begin by importing a few libraries for this chapter.
%matplotlib inline
from d2l import mxnet as d2l
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()
Next we define two functions, the expected function f and the empirical function g, to illustrate
this issue. Here the g is less smooth than f since we have only a finite amount of data.
def f(x): return x * np.cos(np.pi * x)
def g(x): return f(x) + 0.2 * np.cos(5 * np.pi * x)
The graph below illustrates that the minimum of the training error may be at a different location
than the minimum of the expected error (or of the test error).
def annotate(text, xy, xytext): #@save
d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
arrowprops=dict(arrowstyle='->'))
x = np.arange(0.5, 1.5, 0.01)
d2l.set_figsize((4.5, 2.5))
d2l.plot(x, [f(x), g(x)], 'x', 'risk')
annotate('empirical risk', (1.0, -1.2), (0.5, -1.1))
annotate('expected risk', (1.1, -1.05), (0.95, -0.5))
406 Chapter 11. Optimization Algorithms
11.1.2 Optimization Challenges in Deep Learning
In this chapter, we are going to focus specifically on the performance of the optimization algorithm
in minimizing the objective function, rather than a model?s generalization error. In Section 3.1
we distinguished between analytical solutions and numerical solutions in optimization problems.
In deep learning, most objective functions are complicated and do not have analytical solutions.
Instead, we must use numerical optimization algorithms. The optimization algorithms below all
fall into this category.
There are many challenges in deep learning optimization. Some of the most vexing ones are local
minima, saddle points and vanishing gradients. Let us have a look at a few of them.
Local Minima
For the objective function f(x), if the value of f(x) at x is smaller than the values of f(x) at any
other points in the vicinity of x, then f(x) could be a local minimum. If the value of f(x) at x is
the minimum of the objective function over the entire domain, then f(x) is the global minimum.
For example, given the function
f(x) = x  cos(x) for ?? 1:0  x  2:0; (11.1.1)
we can approximate the local minimum and global minimum of this function.
x = np.arange(-1.0, 2.0, 0.01)
d2l.plot(x, [f(x), ], 'x', 'f(x)')
annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))
annotate('global minimum', (1.1, -0.95), (0.6, 0.8))
The objective function of deep learning models usually has many local optima. When the numerical
solution of an optimization problem is near the local optimum, the numerical solution
obtained by the final iteration may only minimize the objective function locally, rather than globally,
as the gradient of the objective function?s solutions approaches or becomes zero. Only some
degree of noise might knock the parameter out of the local minimum. In fact, this is one of the
beneficial properties of stochastic gradient descent where the natural variation of gradients over
minibatches is able to dislodge the parameters from local minima.
11.1. Optimization and Deep Learning 407
Saddle Points
Besides local minima, saddle points are another reason for gradients to vanish. A saddle point130
is any location where all gradients of a function vanish but which is neither a global nor a local
minimum. Consider the function f(x) = x3. Its first and second derivative vanish for x = 0.
Optimization might stall at the point, even though it is not a minimum.
x = np.arange(-2.0, 2.0, 0.01)
d2l.plot(x, [x**3], 'x', 'f(x)')
annotate('saddle point', (0, -0.2), (-0.52, -5.0))
Saddle points in higher dimensions are even more insidious, as the example below shows. Consider
the function f(x; y) = x2 ?? y2. It has its saddle point at (0; 0). This is a maximum with
respect to y and a minimum with respect to x. Moreover, it looks like a saddle, which is where this
mathematical property got its name.
x, y = np.meshgrid(np.linspace(-1.0, 1.0, 101), np.linspace(-1.0, 1.0, 101))
z = x**2 - y**2
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.plot([0], [0], [0], 'rx')
ticks = [-1, 0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y');
130 https://en.wikipedia.org/wiki/Saddle_point
408 Chapter 11. Optimization Algorithms
We assume that the input of a function is a k-dimensional vector and its output is a scalar, so its
Hessian matrix will have k eigenvalues (refer to Section 18.1). The solution of the function could
be a local minimum, a local maximum, or a saddle point at a position where the function gradient
is zero:
� When the eigenvalues of the function?s Hessian matrix at the zero-gradient position are all
positive, we have a local minimum for the function.
� When the eigenvalues of the function?s Hessian matrix at the zero-gradient position are all
negative, we have a local maximum for the function.
� When the eigenvalues of the function?s Hessian matrix at the zero-gradient position are negative
and positive, we have a saddle point for the function.
For high-dimensional problems the likelihood that at least some of the eigenvalues are negative
is quite high. This makes saddle points more likely than local minima. We will discuss some exceptions
to this situation in the next section when introducing convexity. In short, convex functions
are those where the eigenvalues of the Hessian are never negative. Sadly, though, most deep
learning problems do not fall into this category. Nonetheless it is a great tool to study optimization
algorithms.
Vanishing Gradients
Probably the most insidious problem to encounter are vanishing gradients. For instance, assume
that we want to minimize the function f(x) = tanh(x) and we happen to get started at x = 4.
As we can see, the gradient of f is close to nil. More specifically f?(x) = 1 ?? tanh2(x) and thus
f?(4) = 0:0013. Consequently optimization will get stuck for a long time before we make progress.
This turns out to be one of the reasons that training deep learning models was quite tricky prior
to the introduction of the ReLU activation function.
x = np.arange(-2.0, 5.0, 0.01)
d2l.plot(x, [np.tanh(x)], 'x', 'f(x)')
annotate('vanishing gradient', (4, 1), (2, 0.0))
11.1. Optimization and Deep Learning 409
As we saw, optimization for deep learning is full of challenges. Fortunately there exists a robust
range of algorithms that perform well and that are easy to use even for beginners. Furthermore,
it is not really necessary to find the best solution. Local optima or even approximate solutions
thereof are still very useful.
Summary
� Minimizing the training error does not guarantee that we find the best set of parameters to
minimize the expected error.
� The optimization problems may have many local minima.
� The problem may have even more saddle points, as generally the problems are not convex.
� Vanishing gradients can cause optimization to stall. Often a reparameterization of the problem
helps. Good initialization of the parameters can be beneficial, too.
Exercises
1. Consider a simple multilayer perceptron with a single hidden layer of, say, d dimensions in
the hidden layer and a single output. Show that for any local minimum there are at least d!
equivalent solutions that behave identically.
2. Assume that we have a symmetric random matrix M where the entries Mij = Mji are each
drawn from some probability distribution pij . Furthermore assume that pij(x) = pij(??x),
i.e., that the distribution is symmetric (see e.g., (Wigner, 1958) for details).
� Prove that the distribution over eigenvalues is also symmetric. That is, for any eigenvector
v the probability that the associated eigenvalue  satisfies P( > 0) = P( < 0).
� Why does the above not imply P( > 0) = 0:5?
3. What other challenges involved in deep learning optimization can you think of?
4. Assume that you want to balance a (real) ball on a (real) saddle.
� Why is this hard?
� Can you exploit this effect also for optimization algorithms?
410 Chapter 11. Optimization Algorithms
Discussions131
11.2 Convexity
Convexity plays a vital role in the design of optimization algorithms. This is largely due to the fact
that it is much easier to analyze and test algorithms in this context. In other words, if the algorithm
performs poorly even in the convex setting we should not hope to see great results otherwise.
Furthermore, even though the optimization problems in deep learning are generally nonconvex,
they often exhibit some properties of convex ones near local minima. This can lead to exciting
new optimization variants such as (Izmailov et al., 2018).
11.2.1 Basics
Let us begin with the basics.
Sets
Sets are the basis of convexity. Simply put, a set X in a vector space is convex if for any a; b 2 X
the line segment connecting a and b is also in X. In mathematical terms this means that for all
 2 [0; 1] we have
  a + (1 ?? )  b 2 X whenever a; b 2 X: (11.2.1)
This sounds a bit abstract. Consider the picture Fig. 11.2.1. The first set is not convex since there
are line segments that are not contained in it. The other two sets suffer no such problem.
Fig. 11.2.1: Three shapes, the left one is nonconvex, the others are convex
Definitions on their own are not particularly useful unless you can do something with them. In
this case we can look at unions and intersections as shown in Fig. 11.2.2. Assume that X and Y
are convex sets. Then X \ Y is also convex. To see this, consider any a; b 2 X \ Y . Since X and Y
are convex, the line segments connecting a and b are contained in both X and Y . Given that, they
also need to be contained in X \ Y , thus proving our first theorem.
131 https://discuss.d2l.ai/t/349
11.2. Convexity 411
Fig. 11.2.2: The intersection between two convex sets is convex
We can strengthen this result with little effort: given convex sets Xi, their intersection \iXi is
convex. To see that the converse is not true, consider two disjoint sets X \Y = ?. Now pick a 2 X
and b 2 Y . The line segment in :numref:fig_nonconvex connecting a and b needs to contain some
part that is neither in X nor Y , since we assumed that X \ Y = ?. Hence the line segment is not
in X [ Y either, thus proving that in general unions of convex sets need not be convex.
Fig. 11.2.3: The union of two convex sets need not be convex
Typically the problems in deep learning are defined on convex domains. For instance Rd is a
convex set (after all, the line between any two points in Rd remains in Rd). In some cases we work
with variables of bounded length, such as balls of radius r as defined by fxjx 2 Rd and ?x?2  rg.
Functions
Now that we have convex sets we can introduce convex functions f. Given a convex set X a function
defined on it f : X ! R is convex if for all x; x? 2 X and for all  2 [0; 1] we have
f(x) + (1 ?? )f(x
?
)  f(x + (1 ?? )x
?
): (11.2.2)
To illustrate this let us plot a few functions and check which ones satisfy the requirement. We
need to import a few libraries.
%matplotlib inline
from d2l import mxnet as d2l
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()
Let us define a few functions, both convex and nonconvex.
412 Chapter 11. Optimization Algorithms
f = lambda x: 0.5 * x**2 # Convex
g = lambda x: np.cos(np.pi * x) # Nonconvex
h = lambda x: np.exp(0.5 * x) # Convex
x, segment = np.arange(-2, 2, 0.01), np.array([-1.5, 1])
d2l.use_svg_display()
_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))
for ax, func in zip(axes, [f, g, h]):
d2l.plot([x, segment], [func(x), func(segment)], axes=ax)
As expected, the cosine function is nonconvex, whereas the parabola and the exponential function
are. Note that the requirement thatX is a convex set is necessary for the condition to make sense.
Otherwise the outcome of f(x + (1 ?? )x?) might not be well defined. Convex functions have a
number of desirable properties.
Jensen�s Inequality
One of the most useful tools is Jensen?s inequality. It amounts to a generalization of the definition
of convexity:
?
i
if(xi)  f
(
?
i
ixi
)
and Ex[f(x)]  f (Ex[x]) ; (11.2.3)
where i are nonnegative real numbers such that
?
i i = 1. In other words, the expectation of a
convex function is larger than the convex function of an expectation. To prove the first inequality
we repeatedly apply the definition of convexity to one term in the sum at a time. The expectation
can be proven by taking the limit over finite segments.
One of the common applications of Jensen?s inequality is with regard to the log-likelihood of partially
observed random variables. That is, we use
EyP(y)[??log P(x j y)]  ??log P(x): (11.2.4)
This follows since
?
P(y)P(x j y)dy = P(x). This is used in variational methods. Here y is typically
the unobserved random variable, P(y) is the best guess of how it might be distributed and P(x) is
the distribution with y integrated out. For instance, in clustering y might be the cluster labels and
P(x j y) is the generative model when applying cluster labels.
11.2. Convexity 413
11.2.2 Properties
Convex functions have a few useful properties. We describe them as follows.
No Local Minima
In particular, convex functions do not have local minima. Let us assume the contrary and prove
it wrong. If x 2 X is a local minimum there exists some neighborhood of x for which f(x) is
the smallest value. Since x is only a local minimum there has to be another x? 2 X for which
f(x?) < f(x). However, by convexity the function values on the entire line x + (1 ?? )x? have to
be less than f(x) since for  2 [0; 1)
f(x) > f(x) + (1 ?? )f(x
?
)  f(x + (1 ?? )x
?
): (11.2.5)
This contradicts the assumption that f(x) is a local minimum. For instance, the function f(x) =
(x + 1)(x ?? 1)2 has a local minimum for x = 1. However, it is not a global minimum.
f = lambda x: (x-1)**2 * (x+1)
d2l.set_figsize()
d2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')
The fact that convex functions have no local minima is very convenient. It means that if we minimize
functions we cannot �get stuck�. Note, though, that this does not mean that there cannot
be more than one global minimum or that there might even exist one. For instance, the function
f(x) = max(jxj??1; 0) attains its minimum value over the interval [??1; 1]. Conversely, the function
f(x) = exp(x) does not attain a minimum value on R. For x ! ??1 it asymptotes to 0, however
there is no x for which f(x) = 0.
414 Chapter 11. Optimization Algorithms
Convex Functions and Sets
Convex functions define convex sets as below-sets. They are defined as
Sb := fxjx 2 X and f(x)  bg: (11.2.6)
Such sets are convex. Let us prove this quickly. Remember that for any x; x? 2 Sb we need to
show that x + (1 ?? )x? 2 Sb as long as  2 [0; 1]. But this follows directly from the definition of
convexity since f(x + (1 ?? )x?)  f(x) + (1 ?? )f(x?)  b.
Have a look at the function f(x; y) = 0:5x2 + cos(2y) below. It is clearly nonconvex. The level
sets are correspondingly nonconvex. In fact, they are typically composed of disjoint sets.
x, y = np.meshgrid(np.linspace(-1.0, 1.0, 101), np.linspace(-1.0, 1.0, 101))
z = x**2 + 0.5 * np.cos(2 * np.pi * y)
# Plot the 3D surface
d2l.set_figsize((6, 4))
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.contour(x, y, z, offset=-1)
ax.set_zlim(-1, 1.5)
# Adjust labels
for func in [d2l.plt.xticks, d2l.plt.yticks, ax.set_zticks]:
func([-1, 0, 1])
/var/lib/jenkins/miniconda3/envs/d2l-en-release-0/lib/python3.7/site-packages/numpy/core/
,!_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested?
,!sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths?
,!or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when?
,!creating the ndarray
return array(a, dtype, copy=False, order=order, subok=True)
11.2. Convexity 415
Derivatives and Convexity
Whenever the second derivative of a function exists it is very easy to check for convexity. All we
need to do is check whether @2
xf(x) ? 0, i.e., whether all of its eigenvalues are nonnegative. For
instance, the function f(x) = 1
2
?x?22
is convex since @2
xf = 1, i.e., its derivative is the identity
matrix.
The first thing to realize is that we only need to prove this property for one-dimensional functions.
After all, in general we can always define some function g(z) = f(x + z  v). This function has the
first and second derivatives g? = (@xf)?v and g?? = v?(@2
xf)v respectively. In particular, g??  0 for
all v whenever the Hessian of f is positive semidefinite, i.e., whenever all of its eigenvalues are
greater equal than zero. Hence back to the scalar case.
To see that f??(x)  0 for convex functions we use the fact that
1
2
f(x + ?) +
1
2
f(x ?? ?)  f
(
x + ?
2
+
x ?? ?
2
)
= f(x): (11.2.7)
Since the second derivative is given by the limit over finite differences it follows that
f
??
(x) = lim
?!0
f(x + ?) + f(x ?? ?) ?? 2f(x)
?2
 0: (11.2.8)
To see that the converse is true we use the fact that f??  0 implies that f? is a monotonically
increasing function. Let a < x < b be three points in R. We use the mean value theorem to
express
f(x) ?? f(a) = (x ?? a)f
?
() for some  2 [a; x] and
f(b) ?? f(x) = (b ?? x)f
?
() for some  2 [x; b]:
(11.2.9)
By monotonicity f?()  f?(), hence
f(b) ?? f(a) = f(b) ?? f(x) + f(x) ?? f(a)
= (b ?? x)f
?
() + (x ?? a)f
?
()
 (b ?? a)f
?
():
(11.2.10)
By geometry it follows that f(x) is below the line connecting f(a) and f(b), thus proving convexity.
We omit a more formal derivation in favor of a graph below.
f = lambda x: 0.5 * x**2
x = np.arange(-2, 2, 0.01)
axb, ab = np.array([-1.5, -0.5, 1]), np.array([-1.5, 1])
d2l.set_figsize()
d2l.plot([x, axb, ab], [f(x) for x in [x, axb, ab]], 'x', 'f(x)')
d2l.annotate('a', (-1.5, f(-1.5)), (-1.5, 1.5))
d2l.annotate('b', (1, f(1)), (1, 1.5))
d2l.annotate('x', (-0.5, f(-0.5)), (-1.5, f(-0.5)))
416 Chapter 11. Optimization Algorithms
11.2.3 Constraints
One of the nice properties of convex optimization is that it allows us to handle constraints efficiently.
That is, it allows us to solve problems of the form:
minimize
x
f(x)
subject to ci(x)  0 for all i 2 f1; : : : ;Ng:
(11.2.11)
Here f is the objective and the functions ci are constraint functions. To see what this does consider
the case where c1(x) = ?x?2??1. In this case the parameters x are constrained to the unit ball. If a
second constraint is c2(x) = v?x+b, then this corresponds to all x lying on a halfspace. Satisfying
both constraints simultaneously amounts to selecting a slice of a ball as the constraint set.
Lagrange Function
In general, solving a constrained optimization problem is difficult. One way of addressing it stems
from physics with a rather simple intuition. Imagine a ball inside a box. The ball will roll to the
place that is lowest and the forces of gravity will be balanced out with the forces that the sides of
the box can impose on the ball. In short, the gradient of the objective function (i.e., gravity) will
be offset by the gradient of the constraint function (need to remain inside the box by virtue of the
walls �pushing back�). Note that any constraint that is not active (i.e., the ball does not touch the
wall) will not be able to exert any force on the ball.
Skipping over the derivation of the Lagrange function L (see e.g., the book by Boyd and Vandenberghe
for details (Boyd & Vandenberghe, 2004)) the above reasoning can be expressed via the
following saddlepoint optimization problem:
L(x; ) = f(x) +
?
i
ici(x) where i  0: (11.2.12)
Here the variables i are the so-called Lagrange Multipliers that ensure that a constraint is properly
enforced. They are chosen just large enough to ensure that ci(x)  0 for all i. For instance, for
any x for which ci(x) < 0 naturally, we?d end up picking i = 0. Moreover, this is a saddlepoint
optimization problem where one wants to maximize L with respect to  and simultaneously minimize
it with respect to x. There is a rich body of literature explaining how to arrive at the function
L(x; ). For our purposes it is sufficient to know that the saddlepoint of L is where the original
constrained optimization problem is solved optimally.
11.2. Convexity 417
Penalties
One way of satisfying constrained optimization problems at least approximately is to adapt the Lagrange
function L. Rather than satisfying ci(x)  0 we simply add ici(x) to the objective function
f(x). This ensures that the constraints will not be violated too badly.
In fact, we have been using this trick all along. Consider weight decay in Section 4.5. In it we add

2
?w?2 to the objective function to ensure that w does not grow too large. Using the constrained
optimization point of view we can see that this will ensure that ?w?2 ?? r2  0 for some radius r.
Adjusting the value of  allows us to vary the size of w.
In general, adding penalties is a good way of ensuring approximate constraint satisfaction. In
practice this turns out to be much more robust than exact satisfaction. Furthermore, for nonconvex
problems many of the properties that make the exact approach so appealing in the convex
case (e.g., optimality) no longer hold.
Projections
An alternative strategy for satisfying constraints are projections. Again, we encountered them
before, e.g., when dealing with gradient clipping in Section 8.5. There we ensured that a gradient
has length bounded by c via
g   g  min(1; c/?g?): (11.2.13)
This turns out to be a projection of g onto the ball of radius c. More generally, a projection on a
(convex) set X is defined as
ProjX(x) = argmin
x?2X
?x ?? x??2: (11.2.14)
It is thus the closest point in X to x. This sounds a bit abstract. Fig. 11.2.4 explains it somewhat
more clearly. In it we have two convex sets, a circle and a diamond. Points inside the set (yellow)
remain unchanged. Points outside the set (black) are mapped to the closest point inside the set
(red). While for L2 balls this leaves the direction unchanged, this need not be the case in general,
as can be seen in the case of the diamond.
Fig. 11.2.4: Convex Projections
One of the uses for convex projections is to compute sparse weight vectors. In this case we project
w onto an L1 ball (the latter is a generalized version of the diamond in the picture above).
418 Chapter 11. Optimization Algorithms
Summary
In the context of deep learning the main purpose of convex functions is to motivate optimization
algorithms and help us understand them in detail. In the following we will see how gradient
descent and stochastic gradient descent can be derived accordingly.
� Intersections of convex sets are convex. Unions are not.
� The expectation of a convex function is larger than the convex function of an expectation
(Jensen?s inequality).
� A twice-differentiable function is convex if and only if its second derivative has only nonnegative
eigenvalues throughout.
� Convex constraints can be added via the Lagrange function. In practice simply add them
with a penalty to the objective function.
� Projections map to points in the (convex) set closest to the original point.
Exercises
1. Assume that we want to verify convexity of a set by drawing all lines between points within
the set and checking whether the lines are contained.
� Prove that it is sufficient to check only the points on the boundary.
� Prove that it is sufficient to check only the vertices of the set.
2. Denote by Bp[r] := fxjx 2 Rd and ?x?p  rg the ball of radius r using the p-norm. Prove that
Bp[r] is convex for all p  1.
3. Given convex functions f and g show that max(f; g) is convex, too. Prove that min(f; g) is
not convex.
4. Prove that the normalization of the softmax function is convex. More specifically prove the
convexity of f(x) = log
?
i exp(xi).
5. Prove that linear subspaces are convex sets, i.e., X = fxjWx = bg.
6. Prove that in the case of linear subspaces with b = 0 the projection ProjX can be written as
Mx for some matrix M.
7. Show that for convex twice differentiable functions f we can write f(x+?) = f(x)+?f?(x)+
1
2 ?2f??(x + ) for some  2 [0; ?].
8. Given a vector w 2 Rd with ?w?1 > 1 compute the projection on the ?1 unit ball.
� As intermediate step write out the penalized objective ?w??w??22
+?w??1 and compute
the solution for a given  > 0.
� Can you find the ?right? value of  without a lot of trial and error?
9. Given a convex setX and two vectors x and y prove that projections never increase distances,
i.e., ?x ?? y?  ?ProjX(x) ?? ProjX(y)?.
Discussions132
132 https://discuss.d2l.ai/t/350
11.2. Convexity 419
11.3 Gradient Descent
In this section we are going to introduce the basic concepts underlying gradient descent. This is
brief by necessity. See e.g., (Boyd & Vandenberghe, 2004) for an in-depth introduction to convex
optimization. Although the latter is rarely used directly in deep learning, an understanding of
gradient descent is key to understanding stochastic gradient descent algorithms. For instance,
the optimization problem might diverge due to an overly large learning rate. This phenomenon
can already be seen in gradient descent. Likewise, preconditioning is a common technique in
gradient descent and carries over to more advanced algorithms. Let us start with a simple special
case.
11.3.1 Gradient Descent in One Dimension
Gradient descent in one dimension is an excellent example to explain why the gradient descent
algorithm may reduce the value of the objective function. Consider some continuously differentiable
real-valued function f : R ! R. Using a Taylor expansion (Section 18.3) we obtain that
f(x + ?) = f(x) + ?f
?
(x) + O(?2): (11.3.1)
That is, in first approximation f(x + ?) is given by the function value f(x) and the first derivative
f?(x) at x. It is not unreasonable to assume that for small ? moving in the direction of the negative
gradient will decrease f. To keep things simple we pick a fixed step size  > 0 and choose ? =
??f?(x). Plugging this into the Taylor expansion above we get
f(x ?? f
?
(x)) = f(x) ?? f
?2(x) + O(2f
?2(x)): (11.3.2)
If the derivative f?(x) ?= 0 does not vanish we make progress since f?2(x) > 0. Moreover, we can
always choose  small enough for the higher order terms to become irrelevant. Hence we arrive
at
f(x ?? f
?
(x)) ? f(x): (11.3.3)
This means that, if we use
x   x ?? f
?
(x) (11.3.4)
to iterate x, the value of function f(x) might decline. Therefore, in gradient descent we first choose
an initial value x and a constant  > 0 and then use them to continuously iterate x until the stop
condition is reached, for example, when the magnitude of the gradient jf?(x)j is small enough or
the number of iterations has reached a certain value.
For simplicity we choose the objective function f(x) = x2 to illustrate how to implement gradient
descent. Although we know that x = 0 is the solution to minimize f(x), we still use this simple
function to observe how x changes. As always, we begin by importing all required modules.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
420 Chapter 11. Optimization Algorithms
f = lambda x: x**2 # Objective function
gradf = lambda x: 2 * x # Its derivative
Next, we use x = 10 as the initial value and assume  = 0:2. Using gradient descent to iterate x for
10 times we can see that, eventually, the value of x approaches the optimal solution.
def gd(eta):
x = 10.0
results = [x]
for i in range(10):
x -= eta * gradf(x)
results.append(float(x))
print('epoch 10, x:', x)
return results
res = gd(0.2)
epoch 10, x: 0.06046617599999997
The progress of optimizing over x can be plotted as follows.
def show_trace(res):
n = max(abs(min(res)), abs(max(res)))
f_line = np.arange(-n, n, 0.01)
d2l.set_figsize()
d2l.plot([f_line, res], [[f(x) for x in f_line], [f(x) for x in res]],
'x', 'f(x)', fmts=['-', '-o'])
show_trace(res)
11.3. Gradient Descent 421
Learning Rate
The learning rate  can be set by the algorithm designer. If we use a learning rate that is too small,
it will cause x to update very slowly, requiring more iterations to get a better solution. To show
what happens in such a case, consider the progress in the same optimization problem for  = 0:05.
As we can see, even after 10 steps we are still very far from the optimal solution.
show_trace(gd(0.05))
epoch 10, x: 3.4867844009999995
Conversely, if we use an excessively high learning rate, jf?(x)j might be too large for the firstorder
Taylor expansion formula. That is, the term O(2f?2(x)) in :eqref:gd-taylor might become
significant. In this case, we cannot guarantee that the iteration of x will be able to lower the value
of f(x). For example, when we set the learning rate to  = 1:1, x overshoots the optimal solution
x = 0 and gradually diverges.
show_trace(gd(1.1))
epoch 10, x: 61.917364224000096
422 Chapter 11. Optimization Algorithms
Local Minima
To illustrate what happens for nonconvex functions consider the case of f(x) = x  cos cx. This
function has infinitely many local minima. Depending on our choice of learning rate and depending
on how well conditioned the problem is, we may end up with one of many solutions.
The example below illustrates how an (unrealistically) high learning rate will lead to a poor local
minimum.
c = np.array(0.15 * np.pi)
f = lambda x: x * np.cos(c * x)
gradf = lambda x: np.cos(c * x) - c * x * np.sin(c * x)
show_trace(gd(2))
epoch 10, x: -1.5281651
11.3.2 Multivariate Gradient Descent
Now that we have a better intuition of the univariate case, let us consider the situation where
x 2 Rd. That is, the objective function f : Rd ! R maps vectors into scalars. Correspondingly its
gradient is multivariate, too. It is a vector consisting of d partial derivatives:
?f(x) =
[
@f(x)
@x1
;
@f(x)
@x2
; : : : ;
@f(x)
@xd
]?
: (11.3.5)
Each partial derivative element @f(x)/@xi in the gradient indicates the rate of change of f at x
with respect to the input xi. As before in the univariate case we can use the corresponding Taylor
approximation for multivariate functions to get some idea of what we should do. In particular, we
have that
f(x + ?) = f(x) + ?
??f(x) + O(???2): (11.3.6)
In other words, up to second order terms in ? the direction of steepest descent is given by the
negative gradient???f(x). Choosing a suitable learning rate  > 0 yields the prototypical gradient
descent algorithm:
x   x ?? ?f(x): (11.3.7)
11.3. Gradient Descent 423
To see how the algorithm behaves in practice let us construct an objective function f(x) = x21
+2x22
with a two-dimensional vector x = [x1; x2]? as input and a scalar as output. The gradient is given
by ?f(x) = [2x1; 4x2]?. We will observe the trajectory of x by gradient descent from the initial
position [??5;??2]. We need two more helper functions. The first uses an update function and
applies it 20 times to the initial value. The second helper visualizes the trajectory of x.
def train_2d(trainer, steps=20): #@save
"""Optimize a 2-dim objective function with a customized trainer."""
# s1 and s2 are internal state variables and will
# be used later in the chapter
x1, x2, s1, s2 = -5, -2, 0, 0
results = [(x1, x2)]
for i in range(steps):
x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
results.append((x1, x2))
return results
def show_trace_2d(f, results): #@save
"""Show the trace of 2D variables during optimization."""
d2l.set_figsize()
d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1),
np.arange(-3.0, 1.0, 0.1))
d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
d2l.plt.xlabel('x1')
d2l.plt.ylabel('x2')
Next, we observe the trajectory of the optimization variable x for learning rate  = 0:1. We can see
that after 20 steps the value of x approaches its minimum at [0; 0]. Progress is fairly well-behaved
albeit rather slow.
f = lambda x1, x2: x1 ** 2 + 2 * x2 ** 2 # Objective
gradf = lambda x1, x2: (2 * x1, 4 * x2) # Gradient
def gd(x1, x2, s1, s2):
(g1, g2) = gradf(x1, x2) # Compute gradient
return (x1 - eta * g1, x2 - eta * g2, 0, 0) # Update variables
eta = 0.1
show_trace_2d(f, train_2d(gd))
424 Chapter 11. Optimization Algorithms
11.3.3 Adaptive Methods
As we could see in Section 11.3.1, getting the learning rate  �just right� is tricky. If we pick it too
small, we make no progress. If we pick it too large, the solution oscillates and in the worst case it
might even diverge. What if we could determine  automatically or get rid of having to select a step
size at all? Second order methods that look not only at the value and gradient of the objective but
also at its curvature can help in this case. While these methods cannot be applied to deep learning
directly due to the computational cost, they provide useful intuition into how to design advanced
optimization algorithms that mimic many of the desirable properties of the algorithms outlined
below.
Newton�s Method
Reviewing the Taylor expansion of f there is no need to stop after the first term. In fact, we can
write it as
f(x + ?) = f(x) + ?
??f(x) +
1
2
?
????
f(x)? + O(???3): (11.3.8)
To avoid cumbersome notation we define Hf := ???f(x) to be the Hessian of f. This is a d  d
matrix. For small d and simple problems Hf is easy to compute. For deep networks, on the other
hand, Hf may be prohibitively large, due to the cost of storing O(d2) entries. Furthermore it may
be too expensive to compute via backpropagation as we would need to apply backpropagation
to the backpropagation call graph. For now let us ignore such considerations and look at what
algorithm we?d get.
After all, the minimum of f satisfies?f(x) = 0. Taking derivatives of (11.3.8) with regard to ? and
ignoring higher order terms we arrive at
?f(x) + Hf ? = 0 and hence ? = ??H
??1
f
?f(x): (11.3.9)
That is, we need to invert the Hessian Hf as part of the optimization problem.
For f(x) = 1
2x2 we have ?f(x) = x and Hf = 1. Hence for any x we obtain ? = ??x. In other
words, a single step is sufficient to converge perfectly without the need for any adjustment! Alas,
we got a bit lucky here since the Taylor expansion was exact. Let us see what happens in other
problems.
c = np.array(0.5)
f = lambda x: np.cosh(c * x) # Objective
gradf = lambda x: c * np.sinh(c * x) # Derivative
hessf = lambda x: c**2 * np.cosh(c * x) # Hessian
def newton(eta=1):
x = 10.0
results = [x]
for i in range(10):
x -= eta * gradf(x) / hessf(x)
results.append(float(x))
print('epoch 10, x:', x)
return results
show_trace(newton())
11.3. Gradient Descent 425
epoch 10, x: 0.0
Now let us see what happens when we have a nonconvex function, such as f(x) = x cos(cx). After
all, note that in Newton?s method we end up dividing by the Hessian. This means that if the second
derivative is negative we would walk into the direction of increasing f. That is a fatal flaw of the
algorithm. Let us see what happens in practice.
c = np.array(0.15 * np.pi)
f = lambda x: x * np.cos(c * x)
gradf = lambda x: np.cos(c * x) - c * x * np.sin(c * x)
hessf = lambda x: - 2 * c * np.sin(c * x) - x * c**2 * np.cos(c * x)
show_trace(newton())
epoch 10, x: 26.834133
This went spectacularly wrong. How can we fix it? One way would be to �fix� the Hessian by taking
its absolute value instead. Another strategy is to bring back the learning rate. This seems to defeat
the purpose, but not quite. Having second order information allows us to be cautious whenever
the curvature is large and to take longer steps whenever the objective is flat. Let us see how this
426 Chapter 11. Optimization Algorithms
works with a slightly smaller learning rate, say  = 0:5. As we can see, we have quite an efficient
algorithm.
show_trace(newton(0.5))
epoch 10, x: 7.26986
Convergence Analysis
We only analyze the convergence rate for convex and three times differentiable f, where at its
minimum x the second derivative is nonzero, i.e., where f??(x) > 0. The multivariate proof is
a straightforward extension of the argument below and omitted since it doesn?t help us much in
terms of intuition.
Denote by xk the value of x at the k-th iteration and let ek := xk??x be the distance from optimality.
By Taylor series expansion we have that the condition f?(x) = 0 can be written as
0 = f
?
(xk ?? ek) = f
?
(xk) ?? ekf
??
(xk) +
1
2
e2
kf
???
(k): (11.3.10)
This holds for some k 2 [xk ?? ek; xk]. Recall that we have the update xk+1 = xk ?? f?(xk)/f??(xk).
Dividing the above expansion by f??(xk) yields
ek ?? f
?
(xk)/f
??
(xk) =
1
2
e2
kf
???
(k)/f
??
(xk): (11.3.11)
Plugging in the update equations leads to the following bound ek+1  e2
kf???(k)/f?(xk). Consequently,
whenever we are in a region of bounded f???(k)/f??(xk)  c, we have a quadratically
decreasing error ek+1  ce2
k.
As an aside, optimization researchers call this linear convergence, whereas a condition such as
ek+1  ek would be called a constant rate of convergence. Note that this analysis comes with
a number of caveats: We do not really have much of a guarantee when we will reach the region
of rapid convergence. Instead, we only know that once we reach it, convergence will be very
quick. Second, this requires that f is well-behaved up to higher order derivatives. It comes down
to ensuring that f does not have any �surprising� properties in terms of how it might change its
values.
11.3. Gradient Descent 427
Preconditioning
Quite unsurprisingly computing and storing the full Hessian is very expensive. It is thus desirable
to find alternatives. One way to improve matters is by avoiding to compute the Hessian in
its entirety but only compute the diagonal entries. While this is not quite as good as the full Newton
method, it is still much better than not using it. Moreover, estimates for the main diagonal
elements are what drives some of the innovation in stochastic gradient descent optimization algorithms.
This leads to update algorithms of the form
x   x ?? diag(Hf )
??1?f(x): (11.3.12)
To see why this might be a good idea consider a situation where one variable denotes height in millimeters
and the other one denotes height in kilometers. Assuming that for both the natural scale
is in meters we have a terrible mismatch in parameterizations. Using preconditioning removes
this. Effectively preconditioning with gradient descent amounts to selecting a different learning
rate for each coordinate.
Gradient Descent with Line Search
One of the key problems in gradient descent was that we might overshoot the goal or make insufficient
progress. A simple fix for the problem is to use line search in conjunction with gradient
descent. That is, we use the direction given by ?f(x) and then perform binary search as to which
step length  minimizes f(x ?? ?f(x)).
This algorithm converges rapidly (for an analysis and proof see e.g., (Boyd & Vandenberghe,
2004)). However, for the purpose of deep learning this is not quite so feasible, since each step
of the line search would require us to evaluate the objective function on the entire dataset. This is
way too costly to accomplish.
Summary
� Learning rates matter. Too large and we diverge, too small and we do not make progress.
� Gradient descent can get stuck in local minima.
� In high dimensions adjusting the learning rate is complicated.
� Preconditioning can help with scale adjustment.
� Newton?s method is a lot faster once it has started working properly in convex problems.
� Beware of using Newton?s method without any adjustments for nonconvex problems.
Exercises
1. Experiment with different learning rates and objective functions for gradient descent.
2. Implement line search to minimize a convex function in the interval [a; b].
� Do you need derivatives for binary search, i.e., to decide whether to pick [a; (a + b)/2]
or [(a + b)/2; b].
� How rapid is the rate of convergence for the algorithm?
428 Chapter 11. Optimization Algorithms
� Implement the algorithm and apply it to minimizing log(exp(x) + exp(??2  x ?? 3)).
3. Design an objective function defined onR2 where gradient descent is exceedingly slow. Hint:
scale different coordinates differently.
4. Implement the lightweight version of Newton?s method using preconditioning:
� Use diagonal Hessian as preconditioner.
� Use the absolute values of that rather than the actual (possibly signed) values.
� Apply this to the problem above.
5. Apply the algorithm above to a number of objective functions (convex or not). What happens
if you rotate coordinates by 45 degrees?
Discussions133
11.4 Stochastic Gradient Descent
In this section, we are going to introduce the basic principles of stochastic gradient descent.
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np, npx
npx.set_np()
11.4.1 Stochastic Gradient Updates
In deep learning, the objective function is usually the average of the loss functions for each example
in the training dataset. We assume that fi(x) is the loss function of the training dataset with n
examples, an index of i, and parameter vector of x, then we have the objective function
f(x) =
1
n
?n
i=1
fi(x): (11.4.1)
The gradient of the objective function at x is computed as
?f(x) =
1
n
?n
i=1
?fi(x): (11.4.2)
If gradient descent is used, the computing cost for each independent variable iteration is O(n),
which grows linearly with n. Therefore, when the model training dataset is large, the cost of gradient
descent for each iteration will be very high.
Stochastic gradient descent (SGD) reduces computational cost at each iteration. At each iteration
of stochastic gradient descent, we uniformly sample an index i 2 f1; : : : ; ng for data examples at
random, and compute the gradient ?fi(x) to update x:
x   x ?? ?fi(x): (11.4.3)
133 https://discuss.d2l.ai/t/351
11.4. Stochastic Gradient Descent 429
Here,  is the learning rate. We can see that the computing cost for each iteration drops from
O(n) of the gradient descent to the constantO(1). We should mention that the stochastic gradient
?fi(x) is the unbiased estimate of gradient ?f(x).
Ei?fi(x) =
1
n
?n
i=1
?fi(x) = ?f(x): (11.4.4)
This means that, on average, the stochastic gradient is a good estimate of the gradient.
Now, we will compare it to gradient descent by adding random noise with a mean of 0 and a variance
of 1 to the gradient to simulate a SGD.
f = lambda x1, x2: x1 ** 2 + 2 * x2 ** 2 # Objective
gradf = lambda x1, x2: (2 * x1, 4 * x2) # Gradient
def sgd(x1, x2, s1, s2):
global lr # Learning rate scheduler
(g1, g2) = gradf(x1, x2)
# Simulate noisy gradient
g1 += np.random.normal(0.0, 1, (1,))
g2 += np.random.normal(0.0, 1, (1,))
eta_t = eta * lr() # Learning rate at time t
return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0) # Update variables
eta = 0.1
lr = (lambda: 1) # Constant learning rate
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50))
/var/lib/jenkins/miniconda3/envs/d2l-en-release-0/lib/python3.7/site-packages/numpy/core/
,!_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested?
,!sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths?
,!or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when?
,!creating the ndarray
return array(a, dtype, copy=False, order=order, subok=True)
As we can see, the trajectory of the variables in the SGD is much more noisy than the one we
observed in gradient descent in the previous section. This is due to the stochastic nature of the
gradient. That is, even when we arrive near the minimum, we are still subject to the uncertainty
430 Chapter 11. Optimization Algorithms
injected by the instantaneous gradient via ?fi(x). Even after 50 steps the quality is still not so
good. Even worse, it will not improve after additional steps (we encourage the reader to experiment
with a larger number of steps to confirm this on his own). This leaves us with the only
alternative�change the learning rate . However, if we pick this too small, we will not make any
meaningful progress initially. On the other hand, if we pick it too large, we will not get a good
solution, as seen above. The only way to resolve these conflicting goals is to reduce the learning
rate dynamically as optimization progresses.
This is also the reason for adding a learning rate function lr into the sgd step function. In the
example above any functionality for learning rate scheduling lies dormant as we set the associated
lr function to be constant, i.e., lr = (lambda: 1).
11.4.2 Dynamic Learning Rate
Replacing  with a time-dependent learning rate (t) adds to the complexity of controlling convergence
of an optimization algorithm. In particular, need to figure out how rapidly  should decay.
If it is too quick, we will stop optimizing prematurely. If we decrease it too slowly, we waste too
much time on optimization. There are a few basic strategies that are used in adjusting  over time
(we will discuss more advanced strategies in a later chapter):
(t) = i if ti  t  ti+1 piecewise constant
(t) = 0  e
??t exponential
(t) = 0  (t + 1)
?? polynomial
(11.4.5)
In the first scenario we decrease the learning rate, e.g., whenever progress in optimization has
stalled. This is a common strategy for training deep networks. Alternatively we could decrease it
much more aggressively by an exponential decay. Unfortunately this leads to premature stopping
before the algorithm has converged. A popular choice is polynomial decay with  = 0:5. In the
case of convex optimization there are a number of proofs which show that this rate is well behaved.
Let us see what this looks like in practice.
def exponential():
global ctr
ctr += 1
return math.exp(-0.1 * ctr)
ctr = 1
lr = exponential # Set up learning rate
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000))
11.4. Stochastic Gradient Descent 431
As expected, the variance in the parameters is significantly reduced. However, this comes at the
expense of failing to converge to the optimal solution x = (0; 0). Even after 1000 steps are we are
still very far away from the optimal solution. Indeed, the algorithm fails to converge at all. On the
other hand, if we use a polynomial decay where the learning rate decays with the inverse square
root of the number of steps convergence is good.
def polynomial():
global ctr
ctr += 1
return (1 + 0.1 * ctr)**(-0.5)
ctr = 1
lr = polynomial # Set up learning rate
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50))
There exist many more choices for how to set the learning rate. For instance, we could start with a
small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We could even alternate
between smaller and larger learning rates. There exists a large variety of such schedules.
For now let us focus on learning rate schedules for which a comprehensive theoretical analysis
is possible, i.e., on learning rates in a convex setting. For general nonconvex problems it is very
difficult to obtain meaningful convergence guarantees, since in general minimizing nonlinear
432 Chapter 11. Optimization Algorithms
nonconvex problems is NP hard. For a survey see e.g., the excellent lecture notes134 of Tibshirani
2015.
11.4.3 Convergence Analysis for Convex Objectives
The following is optional and primarily serves to convey more intuition about the problem. We
limit ourselves to one of the simplest proofs, as described by (Nesterov & Vial, 2000). Significantly
more advanced proof techniques exist, e.g., whenever the objective function is particularly well
behaved. (Hazan et al., 2008) show that for strongly convex functions, i.e., for functions that can
be bounded from below by x?Qx, it is possible to minimize them in a small number of steps while
decreasing the learning rate like (t) = 0/(t+1). Unfortunately this case never really occurs in
deep learning and we are left with a much more slowly decreasing rate in practice.
Consider the case where
wt+1 = wt ?? t@wl(xt; w): (11.4.6)
In particular, assume that xt is drawn from some distribution P(x) and that l(x; w) is a convex
function in w for all x. Last denote by
R(w) = ExP [l(x; w)] (11.4.7)
the expected risk and by R its minimum with regard to w. Last let w be the minimizer (we
assume that it exists within the domain which w is defined). In this case we can track the distance
between the current parameter wt and the risk minimizer w and see whether it improves over
time:
?wt+1 ?? w?2 = ?wt ?? t@wl(xt; w) ?? w?2
= ?wt ?? w?2 + 2
t
?@wl(xt; w)?2 ?? 2t ?wt ?? w
; @wl(xt; w)? :
(11.4.8)
The gradient @wl(xt; w) can be bounded from above by some Lipschitz constant L, hence we have
that
2
t
?@wl(xt; w)?2  2
t L2: (11.4.9)
We are mostly interested in how the distance between wt and w changes in expectation. In fact,
for any specific sequence of steps the distance might well increase, depending on whichever xt
we encounter. Hence we need to bound the inner product. By convexity we have that
l(xt; w
)  l(xt; wt) + ?w ?? wt; @wl(xt; wt)? : (11.4.10)
Using both inequalities and plugging it into the above we obtain a bound on the distance between
parameters at time t + 1 as follows:
?wt ?? w?2 ?? ?wt+1 ?? w?2  2t(l(xt; wt) ?? l(xt; w
)) ?? 2
t L2: (11.4.11)
This means that we make progress as long as the expected difference between current loss and
the optimal loss outweighs tL2. Since the former is bound to converge to 0 it follows that the
learning rate t also needs to vanish.
Next we take expectations over this expression. This yields
Ewt
[
?wt ?? w?2]
?? Ewt+1jwt
[
?wt+1 ?? w?2]
 2t[E[R[wt]] ?? R

] ?? 2
t L2: (11.4.12)
134 https://www.stat.cmu.edu/~ryantibs/convexopt-F15/lectures/26-nonconvex.pdf
11.4. Stochastic Gradient Descent 433
The last step involves summing over the inequalities for t 2 ft; : : : ; Tg. Since the sum telescopes
and by dropping the lower term we obtain
?w0 ?? w?2  2
?T
t=1
t[E[R[wt]] ?? R

] ?? L2
?T
t=1
2
t : (11.4.13)
Note that we exploited that w0 is given and thus the expectation can be dropped. Last define
 w :=
?T
t=1 twt ?T
t=1 t
: (11.4.14)
Then by convexity it follows that
?
t
tE[R[wt]] 
?
t  [E[  w]] : (11.4.15)
Plugging this into the above inequality yields the bound
[E[  w]] ?? R
  r2 + L2?T
t=1 2
t
2
?T
t=1 t
: (11.4.16)
Here r2 := ?w0 ?? w?2 is a bound on the distance between the initial choice of parameters and
the final outcome. In short, the speed of convergence depends on how rapidly the loss function
changes via the Lipschitz constant L and how far away from optimality the initial value is r. Note
that the bound is in terms of  w rather than wT . This is the case since  w is a smoothed version of
the optimization path. Now let us analyze some choices for t.
� Known Time Horizon. Whenever r;L and T are known we can pick  = r/L
p
T. This yields
as upper bound rL(1+1/T)/2
p
T < rL/
p
T. That is, we converge with rate O(1/
p
T) to the
optimal solution.
� Unknown Time Horizon. Whenever we want to have a good solution for any time T we can
pick  = O(1/
p
T). This costs us an extra logarithmic factor and it leads to an upper bound
of the form O(log T/
p
T).
Note that for strongly convex losses l(x; w?)  l(x; w) + ?w? ?? w; @wl(x; w)? + 
2
?w ?? w??2 we can
design even more rapidly converging optimization schedules. In fact, an exponential decay in 
leads to a bound of the form O(log T/T).
11.4.4 Stochastic Gradients and Finite Samples
So far we have played a bit fast and loose when it comes to talking about stochastic gradient descent.
We posited that we draw instances xi, typically with labels yi from some distribution p(x; y)
and that we use this to update the weights w in some manner. In particular, for a finite sample size
we simply argued that the discrete distribution p(x; y) = 1
n
?n
i=1 xi(x)yi(y) allows us to perform
SGD over it.
However, this is not really what we did. In the toy examples in the current section we simply
added noise to an otherwise non-stochastic gradient, i.e., we pretended to have pairs (xi; yi). It
turns out that this is justified here (see the exercises for a detailed discussion). More troubling is
that in all previous discussions we clearly did not do this. Instead we iterated over all instances
exactly once. To see why this is preferable consider the converse, namely that we are sampling
434 Chapter 11. Optimization Algorithms
n observations from the discrete distribution with replacement. The probability of choosing an
element i at random is N??1. Thus to choose it at least once is
P(choose i) = 1 ?? P(omit i) = 1 ?? (1 ?? N
??1)N  1 ?? e
??1  0:63: (11.4.17)
A similar reasoning shows that the probability ( of picking a sample exactly once is given by N
1
)
N??1(1 ?? N??1)N??1 = N??1
N (1 ?? N??1)N  e??1  0:37. This leads to an increased variance
and decreased data efficiency relative to sampling without replacement. Hence, in practice we
perform the latter (and this is the default choice throughout this book). Last note that repeated
passes through the dataset traverse it in a different random order.
Summary
� For convex problems we can prove that for a wide choice of learning rates Stochastic Gradient
Descent will converge to the optimal solution.
� For deep learning this is generally not the case. However, the analysis of convex problems
gives us useful insight into how to approach optimization, namely to reduce the learning
rate progressively, albeit not too quickly.
� Problems occur when the learning rate is too small or too large. In practice a suitable learning
rate is often found only after multiple experiments.
� When there are more examples in the training dataset, it costs more to compute each iteration
for gradient descent, so SGD is preferred in these cases.
� Optimality guarantees for SGD are in general not available in nonconvex cases since the
number of local minima that require checking might well be exponential.
Exercises
1. Experiment with different learning rate schedules for SGD and with different numbers of
iterations. In particular, plot the distance from the optimal solution (0; 0) as a function of
the number of iterations.
2. Prove that for the function f(x1; x2) = x21+2x22
adding normal noise to the gradient is equivalent
to minimizing a loss function l(x; w) = (x1??w1)2+2(x2??w2)2 where x is drawn from
a normal distribution.
� Derive mean and variance of the distribution for x.
� Show that this property holds in general for objective functions f(x) = 1
2 (x??)?Q(x??
) for Q ? 0.
3. Compare convergence of SGD when you sample from f(x1; y1); : : : ; (xm; ym)g with replacement
and when you sample without replacement.
4. How would you change the SGD solver if some gradient (or rather some coordinate associated
with it) was consistently larger than all other gradients?
5. Assume that f(x) = x2(1 + sin x). How many local minima does f have? Can you change f
in such a way that to minimize it one needs to evaluate all local minima?
Discussions135
135 https://discuss.d2l.ai/t/352
11.4. Stochastic Gradient Descent 435
11.5 Minibatch Stochastic Gradient Descent
So far we encountered two extremes in the approach to gradient based learning: Section 11.3 uses
the full dataset to compute gradients and to update parameters, one pass at a time. Conversely
Section 11.4 processes one observation at a time to make progress. Each of them has its own drawbacks.
Gradient Descent is not particularly data efficient whenever data is very similar. Stochastic
Gradient Descent is not particularly computationally efficient since CPUs and GPUs cannot exploit
the full power of vectorization. This suggests that there might be a happy medium, and in fact,
that?s what we have been using so far in the examples we discussed.
11.5.1 Vectorization and Caches
At the heart of the decision to use minibatches is computational efficiency. This is most easily
understood when considering parallelization to multiple GPUs and multiple servers. In this case
we need to send at least one image to each GPU. With 8 GPUs per server and 16 servers we already
arrive at a minibatch size of 128.
Things are a bit more subtle when it comes to single GPUs or even CPUs. These devices have multiple
types of memory, often multiple type of compute units and different bandwidth constraints
between them. For instance, a CPU has a small number of registers and then L1, L2 and in some
cases even L3 cache (which is shared between the different processor cores). These caches are of
increasing size and latency (and at the same time they are of decreasing bandwidth). Suffice it to
say, the processor is capable of performing many more operations than what the main memory
interface is able to provide.
� A 2GHz CPU with 16 cores and AVX-512 vectorization can process up to 2  109  16  32 = 1012
bytes per second. The capability of GPUs easily exceeds this number by a factor of 100.
On the other hand, a midrange server processor might not have much more than 100 GB/s
bandwidth, i.e., less than one tenth of what would be required to keep the processor fed. To
make matters worse, not all memory access is created equal: first, memory interfaces are
typically 64 bit wide or wider (e.g., on GPUs up to 384 bit), hence reading a single byte incurs
the cost of a much wider access.
� There is significant overhead for the first access whereas sequential access is relatively cheap
(this is often called a burst read). There are many more things to keep in mind, such as
caching when we have multiple sockets, chiplets and other structures. A detailed discussion
of this is beyond the scope of this section. See e.g., this Wikipedia article136 for a more indepth
discussion.
The way to alleviate these constraints is to use a hierarchy of CPU caches which are actually fast
enough to supply the processor with data. This is the driving force behind batching in deep learning.
To keep matters simple, consider matrix-matrix multiplication, say A = BC. We have a number
of options for calculating A. For instance we could try the following:
1. We could compute Aij = Bi;:C?
:;j , i.e., we could compute it element-wise by means of dot
products.
2. We could compute A:;j = BC?
:;j , i.e., we could compute it one column at a time. Likewise we
could compute A one row Ai;: at a time.
3. We could simply compute A = BC.
136 https://en.wikipedia.org/wiki/Cache_hierarchy
436 Chapter 11. Optimization Algorithms
4. We could break B and C into smaller block matrices and compute A one block at a time.
If we follow the first option, we will need to copy one row and one column vector into the CPU
each time we want to compute an element Aij . Even worse, due to the fact that matrix elements
are aligned sequentially we are thus required to access many disjoint locations for one of the two
vectors as we read them from memory. The second option is much more favorable. In it, we are
able to keep the column vector C:;j in the CPU cache while we keep on traversing through B. This
halves the memory bandwidth requirement with correspondingly faster access. Of course, option
3 is most desirable. Unfortunately, most matrices might not entirely fit into cache (this is what we
are discussing after all). However, option 4 offers a practically useful alternative: we can move
blocks of the matrix into cache and multiply them locally. Optimized libraries take care of this for
us. Let us have a look at how efficient these operations are in practice.
Beyond computational efficiency, the overhead introduced by Python and by the deep learning
framework itself is considerable. Recall that each time we execute a command the Python interpreter
sends a command to the MXNet engine which needs to insert it into the computational
graph and deal with it during scheduling. Such overhead can be quite detrimental. In short, it is
highly advisable to use vectorization (and matrices) whenever possible.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
timer = d2l.Timer()
A = np.zeros((256, 256))
B = np.random.normal(0, 1, (256, 256))
C = np.random.normal(0, 1, (256, 256))
Element-wise assignment simply iterates over all rows and columns of B and C respectively to
assign the value to A.
# Compute A = BC one element at a time
timer.start()
for i in range(256):
for j in range(256):
A[i, j] = np.dot(B[i, :], C[:, j])
A.wait_to_read()
timer.stop()
60.47386956214905
A faster strategy is to perform column-wise assignment.
# Compute A = BC one column at a time
timer.start()
for j in range(256):
A[:, j] = np.dot(B, C[:, j])
A.wait_to_read()
timer.stop()
11.5. Minibatch Stochastic Gradient Descent 437
0.18439102172851562
Last, the most effective manner is to perform the entire operation in one block. Let us see what
the respective speed of the operations is.
# Compute A = BC in one go
timer.start()
A = np.dot(B, C)
A.wait_to_read()
timer.stop()
# Multiply and add count as separate operations (fused in practice)
gigaflops = [2/i for i in timer.times]
print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '
f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')
performance in Gigaflops: element 0.033, column 10.847, full 2337.311
11.5.2 Minibatches
In the past we took it for granted that we would read minibatches of data rather than single observations
to update parameters. We now give a brief justification for it. Processing single observations
requires us to perform many single matrix-vector (or even vector-vector) multiplications, which is
quite expensive and which incurs a significant overhead on behalf of the underlying deep learning
framework. This applies both to evaluating a network when applied to data (often referred to as
inference) and when computing gradients to update parameters. That is, this applies whenever
we perform w   w ?? tgt where
gt = @wf(xt; w) (11.5.1)
We can increase the computational efficiency of this operation by applying it to a minibatch of
observations at a time. That is, we replace the gradient gt over a single observation by one over a
small batch
gt = @w
1
jBtj
?
i2Bt
f(xi; w) (11.5.2)
Let us see what this does to the statistical properties of gt: since both xt and also all elements
of the minibatch Bt are drawn uniformly at random from the training set, the expectation of the
gradient remains unchanged. The variance, on the other hand, is reduced significantly. Since the
minibatch gradient is composed of b := jBtj independent gradients which are being averaged, its
standard deviation is reduced by a factor of b
??1
2 . This, by itself, is a good thing, since it means that
the updates are more reliably aligned with the full gradient.
Naively this would indicate that choosing a large minibatch Bt would be universally desirable.
Alas, after some point, the additional reduction in standard deviation is minimal when compared
to the linear increase in computational cost. In practice we pick a minibatch that is large enough
to offer good computational efficiency while still fitting into the memory of a GPU. To illustrate the
savings let us have a look at some code. In it we perform the same matrix-matrix multiplication,
but this time broken up into �minibatches� of 64 columns at a time.
438 Chapter 11. Optimization Algorithms
timer.start()
for j in range(0, 256, 64):
A[:, j:j+64] = np.dot(B, C[:, j:j+64])
timer.stop()
print(f'performance in Gigaflops: block {2 / timer.times[3]:.3f}')
performance in Gigaflops: block 548.490
As we can see, the computation on the minibatch is essentially as efficient as on the full matrix. A
word of caution is in order. In Section 7.5 we used a type of regularization that was heavily dependent
on the amount of variance in a minibatch. As we increase the latter, the variance decreases
and with it the benefit of the noise-injection due to batch normalization. See e.g., (Ioffe, 2017) for
details on how to rescale and compute the appropriate terms.
11.5.3 Reading the Dataset
Let us have a look at how minibatches are efficiently generated from data. In the following we use
a dataset developed by NASA to test the wing noise from different aircraft137 to compare these optimization
algorithms. For convenience we only use the first 1; 500 examples. The data is whitened
for preprocessing, i.e., we remove the mean and rescale the variance to 1 per coordinate.
#@save
d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
'76e5be1548fd8222e5074cf0faae75edff8cf93f')
#@save
def get_data_ch11(batch_size=10, n=1500):
data = np.genfromtxt(d2l.download('airfoil'),
dtype=np.float32, delimiter='\t')
data = (data - data.mean(axis=0)) / data.std(axis=0)
data_iter = d2l.load_array(
(data[:n, :-1], data[:n, -1]), batch_size, is_train=True)
return data_iter, data.shape[1]-1
11.5.4 Implementation from Scratch
Recall the minibatch SGD implementation from Section 3.2. In the following we provide a slightly
more general implementation. For convenience it has the same call signature as the other optimization
algorithms introduced later in this chapter. Specifically, we add the status input states
and place the hyperparameter in dictionary hyperparams. In addition, we will average the loss of
each minibatch example in the training function, so the gradient in the optimization algorithm
does not need to be divided by the batch size.
def sgd(params, states, hyperparams):
for p in params:
p[:] -= hyperparams['lr'] * p.grad
137 https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise
11.5. Minibatch Stochastic Gradient Descent 439
Next, we implement a generic training function to facilitate the use of the other optimization algorithms
introduced later in this chapter. It initializes a linear regression model and can be used
to train the model with minibatch SGD and other algorithms introduced subsequently.
#@save
def train_ch11(trainer_fn, states, hyperparams, data_iter,
feature_dim, num_epochs=2):
# Initialization
w = np.random.normal(scale=0.01, size=(feature_dim, 1))
b = np.zeros(1)
w.attach_grad()
b.attach_grad()
net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
# Train
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
with autograd.record():
l = loss(net(X), y).mean()
l.backward()
trainer_fn([w, b], states, hyperparams)
n += X.shape[0]
if n % 200 == 0:
timer.stop()
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss),))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
return timer.cumsum(), animator.Y[0]
Let us see how optimization proceeds for batch gradient descent. This can be achieved by setting
the minibatch size to 1500 (i.e., to the total number of examples). As a result the model parameters
are updated only once per epoch. There is little progress. In fact, after 6 steps progress stalls.
def train_sgd(lr, batch_size, num_epochs=2):
data_iter, feature_dim = get_data_ch11(batch_size)
return train_ch11(
sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)
gd_res = train_sgd(1, 1500, 10)
loss: 0.254, 0.071 sec/epoch
440 Chapter 11. Optimization Algorithms
When the batch size equals 1, we use SGD for optimization. For simplicity of implementation we
picked a constant (albeit small) learning rate. In SGD, the model parameters are updated whenever
an example is processed. In our case this amounts to 1500 updates per epoch. As we can see,
the decline in the value of the objective function slows down after one epoch. Although both the
procedures processed 1500 examples within one epoch, SGD consumes more time than gradient
descent in our experiment. This is because SGD updated the parameters more frequently and
since it is less efficient to process single observations one at a time.
sgd_res = train_sgd(0.005, 1)
loss: 0.247, 0.434 sec/epoch
Last, when the batch size equals 100, we use minibatch SGD for optimization. The time required
per epoch is shorter than the time needed for SGD and the time for batch gradient descent.
mini1_res = train_sgd(.4, 100)
loss: 0.245, 0.008 sec/epoch
11.5. Minibatch Stochastic Gradient Descent 441
Reducing the batch size to 10, the time for each epoch increases because the workload for each
batch is less efficient to execute.
mini2_res = train_sgd(.05, 10)
loss: 0.247, 0.052 sec/epoch
Finally, we compare the time vs. loss for the preview four experiments. As can be seen, despite
SGD converges faster than GD in terms of number of examples processed, it uses more time to
reach the same loss than GD because that computing gradient example by example is not efficient.
Minibatch SGD is able to trade-off the convergence speed and computation efficiency. A minibatch
size 10 is more efficient than SGD; a minibatch size 100 even outperforms GD in terms of runtime.
d2l.set_figsize([6, 3])
d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),
'time (sec)', 'loss', xlim=[1e-2, 10],
legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])
d2l.plt.gca().set_xscale('log')
442 Chapter 11. Optimization Algorithms
11.5.5 Concise Implementation
In Gluon, we can use the Trainer class to call optimization algorithms. This is used to implement
a generic training function. We will use this throughout the current chapter.
#@save
def train_concise_ch11(tr_name, hyperparams, data_iter, num_epochs=2):
# Initialization
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(init.Normal(sigma=0.01))
trainer = gluon.Trainer(net.collect_params(), tr_name, hyperparams)
loss = gluon.loss.L2Loss()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(X.shape[0])
n += X.shape[0]
if n % 200 == 0:
timer.stop()
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss),))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
Using Gluon to repeat the last experiment shows identical behavior.
data_iter, _ = get_data_ch11(10)
train_concise_ch11('sgd', {'learning_rate': 0.05}, data_iter)
loss: 0.243, 0.045 sec/epoch
11.5. Minibatch Stochastic Gradient Descent 443
Summary
� Vectorization makes code more efficient due to reduced overhead arising from the deep
learning framework and due to better memory locality and caching on CPUs and GPUs.
� There is a trade-off between statistical efficiency arising from SGD and computational efficiency
arising from processing large batches of data at a time.
� Minibatch stochastic gradient descent offers the best of both worlds: computational and
statistical efficiency.
� In minibatch SGD we process batches of data obtained by a random permutation of the training
data (i.e., each observation is processed only once per epoch, albeit in random order).
� It is advisable to decay the learning rates during training.
� In general, minibatch SGD is faster than SGD and gradient descent for convergence to a
smaller risk, when measured in terms of clock time.
Exercises
1. Modify the batch size and learning rate and observe the rate of decline for the value of the
objective function and the time consumed in each epoch.
2. Read the MXNet documentation and use the Trainer class set_learning_rate function to
reduce the learning rate of the minibatch SGD to 1/10 of its previous value after each epoch.
3. Compare minibatch SGD with a variant that actually samples with replacement from the training
set. What happens?
4. An evil genie replicates your dataset without telling you (i.e., each observation occurs twice
and your dataset grows to twice its original size, but nobody told you). How does the behavior
of SGD, minibatch SGD and that of gradient descent change?
Discussions138
138 https://discuss.d2l.ai/t/353
444 Chapter 11. Optimization Algorithms
11.6 Momentum
In Section 11.4 we reviewed what happens when performing stochastic gradient descent, i.e.,
when performing optimization where only a noisy variant of the gradient is available. In particular,
we noticed that for noisy gradients we need to be extra cautious when it comes to choosing
the learning rate in the face of noise. If we decrease it too rapidly, convergence stalls. If we are
too lenient, we fail to converge to a good enough solution since noise keeps on driving us away
from optimality.
11.6.1 Basics
In this section, we will explore more effective optimization algorithms, especially for certain types
of optimization problems that are common in practice.
Leaky Averages
The previous section saw us discussing minibatch SGD as a means for accelerating computation.
It also had the nice side-effect that averaging gradients reduced the amount of variance. The minibatch
SGD can be calculated by:
gt;t??1 = @w
1
jBtj
?
i2Bt
f(xi; wt??1) =
1
jBtj
?
i2Bt
hi;t??1: (11.6.1)
To keep the notation simple, here we used hi;t??1 = @wf(xi; wt??1) as the SGD for sample i using
the weights updated at time t ?? 1. It would be nice if we could benefit from the effect of variance
reduction even beyond averaging gradients on a mini-batch. One option to accomplish this task
is to replace the gradient computation by a �leaky average�:
vt = vt??1 + gt;t??1 (11.6.2)
for some  2 (0; 1). This effectively replaces the instantaneous gradient by one that?s been averaged
over multiple past gradients. v is called momentum. It accumulates past gradients similar to
how a heavy ball rolling down the objective function landscape integrates over past forces. To see
what is happening in more detail let us expand vt recursively into
vt = 2vt??2 + gt??1;t??2 + gt;t??1 = : : : ;=
?t??1
=0
 gt??;t????1: (11.6.3)
Large  amounts to a long-range average, whereas small  amounts to only a slight correction
relative to a gradient method. The new gradient replacement no longer points into the direction
of steepest descent on a particular instance any longer but rather in the direction of a weighted
average of past gradients. This allows us to realize most of the benefits of averaging over a batch
without the cost of actually computing the gradients on it. We will revisit this averaging procedure
in more detail later.
The above reasoning formed the basis for what is now known as accelerated gradient methods,
such as gradients with momentum. They enjoy the additional benefit of being much more effective
in cases where the optimization problem is ill-conditioned (i.e., where there are some directions
where progress is much slower than in others, resembling a narrow canyon). Furthermore,
they allow us to average over subsequent gradients to obtain more stable directions of descent.
11.6. Momentum 445
Indeed, the aspect of acceleration even for noise-free convex problems is one of the key reasons
why momentum works and why it works so well.
As one would expect, due to its efficacy momentum is a well-studied subject in optimization for
deep learning and beyond. See e.g., the beautiful expository article139 by (Goh, 2017) for an indepth
analysis and interactive animation. It was proposed by (Polyak, 1964). (Nesterov, 2018)
has a detailed theoretical discussion in the context of convex optimization. Momentum in deep
learning has been known to be beneficial for a long time. See e.g., the discussion by (Sutskever et
al., 2013) for details.
An Ill-conditioned Problem
To get a better understanding of the geometric properties of the momentum method we revisit
gradient descent, albeit with a significantly less pleasant objective function. Recall that in Section
11.3 we used f(x) = x21
+ 2x22
, i.e., a moderately distorted ellipsoid objective. We distort this
function further by stretching it out in the x1 direction via
f(x) = 0:1x21
+ 2x22
: (11.6.4)
As before f has its minimum at (0; 0). This function is very flat in the direction of x1. Let us
see what happens when we perform gradient descent as before on this new function. We pick a
learning rate of 0:4.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
eta = 0.4
def f_2d(x1, x2):
return 0.1 * x1 ** 2 + 2 * x2 ** 2
def gd_2d(x1, x2, s1, s2):
return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)
d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
139 https://distill.pub/2017/momentum/
446 Chapter 11. Optimization Algorithms
By construction, the gradient in the x2 direction is much higher and changes much more rapidly
than in the horizontal x1 direction. Thus we are stuck between two undesirable choices: if we
pick a small learning rate we ensure that the solution does not diverge in the x2 direction but we
are saddled with slow convergence in the x1 direction. Conversely, with a large learning rate we
progress rapidly in the x1 direction but diverge in x2. The example below illustrates what happens
even after a slight increase in learning rate from 0:4 to 0:6. Convergence in the x1 direction
improves but the overall solution quality is much worse.
eta = 0.6
d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))
The Momentum Method
The momentum method allows us to solve the gradient descent problem described above. Looking
at the optimization trace above we might intuit that averaging gradients over the past would
work well. After all, in the x1 direction this will aggregate well-aligned gradients, thus increasing
the distance we cover with every step. Conversely, in the x2 direction where gradients oscillate,
an aggregate gradient will reduce step size due to oscillations that cancel each other out. Using vt
instead of the gradient gt yields the following update equations:
vt   vt??1 + gt;t??1;
xt   xt??1 ?? tvt:
(11.6.5)
Note that for  = 0 we recover regular gradient descent. Before delving deeper into the mathematical
properties let us have a quick look at how the algorithm behaves in practice.
def momentum_2d(x1, x2, v1, v2):
v1 = beta * v1 + 0.2 * x1
v2 = beta * v2 + 4 * x2
return x1 - eta * v1, x2 - eta * v2, v1, v2
eta, beta = 0.6, 0.5
d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))
11.6. Momentum 447
As we can see, even with the same learning rate that we used before, momentum still converges
well. Let us see what happens when we decrease the momentum parameter. Halving it to  =
0:25 leads to a trajectory that barely converges at all. Nonetheless, it is a lot better than without
momentum (when the solution diverges).
eta, beta = 0.6, 0.25
d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))
Note that we can combine momentum with SGD and in particular, minibatch-SGD. The only
change is that in that case we replace the gradients gt;t??1 with gt. Last, for convenience we initialize
v0 = 0 at time t = 0. Let us look at what leaky averaging actually does to the updates.
448 Chapter 11. Optimization Algorithms
Effective Sample Weight
Recall that vt =
?t??1
=0  gt??;t????1. In the limit the terms add up to
?1
=0  = 1
1?? . In other
words, rather than taking a step of size  in GD or SGD we take a step of size 
1?? while at the same
time, dealing with a potentially much better behaved descent direction. These are two benefits in
one. To illustrate how weighting behaves for different choices of  consider the diagram below.
d2l.set_figsize()
betas = [0.95, 0.9, 0.6, 0]
for beta in betas:
x = d2l.numpy(np.arange(40))
d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')
d2l.plt.xlabel('time')
d2l.plt.legend();
11.6.2 Practical Experiments
Let us see how momentum works in practice, i.e., when used within the context of a proper optimizer.
For this we need a somewhat more scalable implementation.
Implementation from Scratch
Compared with (minibatch) SGD the momentum method needs to maintain a set of auxiliary variables,
i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem).
In the implementation below we call these variables states.
def init_momentum_states(feature_dim):
v_w = np.zeros((feature_dim, 1))
v_b = np.zeros(1)
return (v_w, v_b)
def sgd_momentum(params, states, hyperparams):
for p, v in zip(params, states):
v[:] = hyperparams['momentum'] * v + p.grad
p[:] -= hyperparams['lr'] * v
11.6. Momentum 449
Let us see how this works in practice.
def train_momentum(lr, momentum, num_epochs=2):
d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),
{'lr': lr, 'momentum': momentum}, data_iter,
feature_dim, num_epochs)
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
train_momentum(0.02, 0.5)
loss: 0.242, 0.064 sec/epoch
When we increase the momentum hyperparameter momentum to 0.9, it amounts to a significantly
larger effective sample size of 1
1??0:9 = 10. We reduce the learning rate slightly to 0:01 to keep
matters under control.
train_momentum(0.01, 0.9)
loss: 0.251, 0.067 sec/epoch
450 Chapter 11. Optimization Algorithms
Reducing the learning rate further addresses any issue of non-smooth optimization problems.
Setting it to 0:005 yields good convergence properties.
train_momentum(0.005, 0.9)
loss: 0.247, 0.056 sec/epoch
Concise Implementation
There is very little to do in Gluon since the standard sgd solver already had momentum built in.
Setting matching parameters yields a very similar trajectory.
d2l.train_concise_ch11('sgd', {'learning_rate': 0.005, 'momentum': 0.9},
data_iter)
loss: 0.243, 0.048 sec/epoch
11.6. Momentum 451
11.6.3 Theoretical Analysis
So far the 2D example of f(x) = 0:1x21
+2x22
seemed rather contrived. We will now see that this is
actually quite representative of the types of problem one might encounter, at least in the case of
minimizing convex quadratic objective functions.
Quadratic Convex Functions
Consider the function
h(x) =
1
2
x?Qx + x?c + b: (11.6.6)
This is a general quadratic function. For positive definite matrices Q ? 0, i.e., for matrices with
positive eigenvalues this has a minimizer at x = ??Q??1c with minimum value b??1
2 c?Q??1c. Hence
we can rewrite h as
h(x) =
1
2
(x ?? Q??1c)
?Q(x ?? Q??1c) + b ?? 1
2
c?Q??1c: (11.6.7)
The gradient is given by @xf(x) = Q(x ?? Q??1c). That is, it is given by the distance between x
and the minimizer, multiplied by Q. Consequently also the momentum is a linear combination of
terms Q(xt ?? Q??1c).
Since Q is positive definite it can be decomposed into its eigensystem via Q = O?O for an orthogonal
(rotation) matrix O and a diagonal matrix  of positive eigenvalues. This allows us to
perform a change of variables from x to z := O(x??Q??1c) to obtain a much simplified expression:
h(z) =
1
2
z?
z + b
?
: (11.6.8)
Here c? = b ?? 1
2 c?Q??1c. Since O is only an orthogonal matrix this does not perturb the gradients
in a meaningful way. Expressed in terms of z gradient descent becomes
zt = zt??1 ?? zt??1 = (I ?? )zt??1: (11.6.9)
The important fact in this expression is that gradient descent does not mix between different
eigenspaces. That is, when expressed in terms of the eigensystem of Q the optimization problem
proceeds in a coordinate-wise manner. This also holds for momentum.
vt = vt??1 + zt??1
zt = zt??1 ??  (vt??1 + zt??1)
= (I ?? )zt??1 ?? vt??1:
(11.6.10)
In doing this we just proved the following theorem: Gradient Descent with and without momentum
for a convex quadratic function decomposes into coordinate-wise optimization in the direction
of the eigenvectors of the quadratic matrix.
452 Chapter 11. Optimization Algorithms
Scalar Functions
Given the above result let us see what happens when we minimize the function f(x) = 
2x2. For
gradient descent we have
xt+1 = xt ?? xt = (1 ?? )xt: (11.6.11)
Whenever j1 ?? j < 1 this optimization converges at an exponential rate since after t steps we
have xt = (1 ?? )tx0. This shows how the rate of convergence improves initially as we increase
the learning rate  until  = 1. Beyond that things diverge and for  > 2 the optimization
problem diverges.
lambdas = [0.1, 1, 10, 19]
eta = 0.1
d2l.set_figsize((6, 4))
for lam in lambdas:
t = d2l.numpy(np.arange(20))
d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')
d2l.plt.xlabel('time')
d2l.plt.legend();
To analyze convergence in the case of momentum we begin by rewriting the update equations in
terms of two scalars: one for x and one for the momentum v. This yields:
[
vt+1
xt+1
]
=
[
 
?? (1 ?? )
] [
vt
xt
]
= R(; ; )
[
vt
xt
]
: (11.6.12)
We used R to denote the 2  2 governing convergence behavior. After t steps the initial choice
[v0; x0] becomes R(; ; )t[v0; x0]. Hence, it is up to the eigenvalues of R to determine the speed
of convergence. See the Distill post140 of (Goh, 2017) for a great animation and (Flammarion &
140 https://distill.pub/2017/momentum/
11.6. Momentum 453
Bach, 2015) for a detailed analysis. One can show that 0 <  < 2 + 2 momentum converges.
This is a larger range of feasible parameters when compared to 0 <  < 2 for gradient descent. It
also suggests that in general large values of  are desirable. Further details require a fair amount
of technical detail and we suggest that the interested reader consult the original publications.
Summary
� Momentum replaces gradients with a leaky average over past gradients. This accelerates
convergence significantly.
� It is desirable for both noise-free gradient descent and (noisy) stochastic gradient descent.
� Momentum prevents stalling of the optimization process that is much more likely to occur
for stochastic gradient descent.
� The effective number of gradients is given by 1
1?? due to exponentiated downweighting of
past data.
� In the case of convex quadratic problems this can be analyzed explicitly in detail.
� Implementation is quite straightforward but it requires us to store an additional state vector
(momentum v).
Exercises
1. Use other combinations of momentum hyperparameters and learning rates and observe and
analyze the different experimental results.
2. Try out GD and momentum for a quadratic problem where you have multiple eigenvalues,
i.e., f(x) = 1
2
?
i ix2i
, e.g., i = 2??i. Plot how the values of x decrease for the initialization
xi = 1.
3. Derive minimum value and minimizer for h(x) = 1
2 x?Qx + x?c + b.
4. What changes when we perform SGD with momentum? What happens when we use minibatch
SGD with momentum? Experiment with the parameters?
Discussions141
11.7 Adagrad
Let us begin by considering learning problems with features that occur infrequently.
141 https://discuss.d2l.ai/t/354
454 Chapter 11. Optimization Algorithms
11.7.1 Sparse Features and Learning Rates
Imagine that we are training a language model. To get good accuracy we typically want to decrease
the learning rate as we keep on training, usually at a rate of O(t
??1
2 ) or slower. Now consider a
model training on sparse features, i.e., features that occur only infrequently. This is common for
natural language, e.g., it is a lot less likely that we will see the word preconditioning than learning.
However, it is also common in other areas such as computational advertising and personalized
collaborative filtering. After all, there are many things that are of interest only for a small number
of people.
Parameters associated with infrequent features only receive meaningful updates whenever these
features occur. Given a decreasing learning rate we might end up in a situation where the parameters
for common features converge rather quickly to their optimal values, whereas for infrequent
features we are still short of observing them sufficiently frequently before their optimal values can
be determined. In other words, the learning rate either decreases too slowly for frequent features
or too quickly for infrequent ones.
A possible hack to redress this issue would be to count the number of times we see a particular
feature and to use this as a clock for adjusting learning rates. That is, rather than choosing a
learning rate of the form  = p0
t+c we could use i = p 0
s(i;t)+c
. Here s(i; t) counts the number of
nonzeros for feature i that we have observed up to time t. This is actually quite easy to implement
at no meaningful overhead. However, it fails whenever we do not quite have sparsity but rather
just data where the gradients are often very small and only rarely large. After all, it is unclear
where one would draw the line between something that qualifies as an observed feature or not.
Adagrad by (Duchi et al., 2011) addresses this by replacing the rather crude counter s(i; t) by an
aggregate of the squares of previously observed gradients. In particular, it uses s(i; t + 1) =
s(i; t) + (@if(x))2 as a means to adjust the learning rate. This has two benefits: first, we no longer
need to decide just when a gradient is large enough. Second, it scales automatically with the magnitude
of the gradients. Coordinates that routinely correspond to large gradients are scaled down
significantly, whereas others with small gradients receive a much more gentle treatment. In practice
this leads to a very effective optimization procedure for computational advertising and related
problems. But this hides some of the additional benefits inherent in Adagrad that are best understood
in the context of preconditioning.
11.7.2 Preconditioning
Convex optimization problems are good for analyzing the characteristics of algorithms. After all,
for most nonconvex problems it is difficult to derive meaningful theoretical guarantees, but intuition
and insight often carry over. Let us look at the problem of minimizing f(x) = 1
2 x?Qx+c?x+b.
As we saw in Section 11.6, it is possible to rewrite this problem in terms of its eigendecomposition
Q = U?U to arrive at a much simplified problem where each coordinate can be solved
individually:
f(x) =  f(x) =
1
2
x?
x + c?x + b: (11.7.1)
Here we used x = Ux and consequently c = Uc. The modified problem has as its minimizer
x = ??
??1c and minimum value ??1
2c?
??1c + b. This is much easier to compute since  is a
diagonal matrix containing the eigenvalues of Q.
If we perturb c slightly we would hope to find only slight changes in the minimizer of f. Unfortunately
this is not the case. While slight changes in c lead to equally slight changes in c, this is not
11.7. Adagrad 455
the case for the minimizer of f (and of  f respectively). Whenever the eigenvalues i are large we
will see only small changes in xi and in the minimum of  f. Conversely, for small i changes in xi
can be dramatic. The ratio between the largest and the smallest eigenvalue is called the condition
number of an optimization problem.
 =
1
d
: (11.7.2)
If the condition number  is large, it is difficult to solve the optimization problem accurately. We
need to ensure that we are careful in getting a large dynamic range of values right. Our analysis
leads to an obvious, albeit somewhat naive question: couldn?t we simply �fix� the problem by distorting
the space such that all eigenvalues are 1. In theory this is quite easy: we only need the
eigenvalues and eigenvectors of Q to rescale the problem from x to one in z := 
1
2 Ux. In the
new coordinate system x?Qx could be simplified to ?z?2. Alas, this is a rather impractical suggestion.
Computing eigenvalues and eigenvectors is in general much more expensive than solving the
actual problem.
While computing eigenvalues exactly might be expensive, guessing them and computing them
even somewhat approximately may already be a lot better than not doing anything at all. In particular,
we could use the diagonal entries of Q and rescale it accordingly. This is much cheaper
than computing eigenvalues.
~Q = diag??1
2 (Q)Qdiag??1
2 (Q): (11.7.3)
In this case we have ~Qij = Qij/
v
QiiQjj and specifically ~Qii = 1 for all i. In most cases this
simplifies the condition number considerably. For instance, the cases we discussed previously,
this would entirely eliminate the problem at hand since the problem is axis aligned.
Unfortunately we face yet another problem: in deep learning we typically do not even have access
to the second derivative of the objective function: for x 2 Rd the second derivative even on a
minibatch may require O(d2) space and work to compute, thus making it practically infeasible.
The ingenious idea of Adagrad is to use a proxy for that elusive diagonal of the Hessian that is both
relatively cheap to compute and effective�the magnitude of the gradient itself.
In order to see why this works, let us look at  f(x). We have that
@x  f(x) = x + c = (x ?? x0) ; (11.7.4)
where x0 is the minimizer of  f. Hence the magnitude of the gradient depends both on  and the
distance from optimality. If x ?? x0 didn?t change, this would be all that?s needed. After all, in this
case the magnitude of the gradient @x  f(x) suffices. Since AdaGrad is a stochastic gradient descent
algorithm, we will see gradients with nonzero variance even at optimality. As a result we can safely
use the variance of the gradients as a cheap proxy for the scale of the Hessian. A thorough analysis
is beyond the scope of this section (it would be several pages). We refer the reader to (Duchi et al.,
2011) for details.
456 Chapter 11. Optimization Algorithms
11.7.3 The Algorithm
Let us formalize the discussion from above. We use the variable st to accumulate past gradient
variance as follows.
gt = @wl(yt; f(xt; w));
st = st??1 + g2t
;
wt = wt??1 ?? p 
st + ?
 gt:
(11.7.5)
Here the operation are applied coordinate wise. That is, v2 has entries v2
i . Likewise p1
v has entries
p1
vi and u  v has entries uivi. As before  is the learning rate and ? is an additive constant that
ensures that we do not divide by 0. Last, we initialize s0 = 0.
Just like in the case of momentum we need to keep track of an auxiliary variable, in this case to
allow for an individual learning rate per coordinate. This does not increase the cost of Adagrad
significantly relative to SGD, simply since the main cost is typically to compute l(yt; f(xt; w)) and
its derivative.
Note that accumulating squared gradients in st means that st grows essentially at linear rate (somewhat
slower than linearly in practice, since the gradients initially diminish). This leads to an
O(t
??1
2 ) learning rate, albeit adjusted on a per coordinate basis. For convex problems this is perfectly
adequate. In deep learning, though, we might want to decrease the learning rate rather
more slowly. This led to a number of Adagrad variants that we will discuss in the subsequent
chapters. For now let us see how it behaves in a quadratic convex problem. We use the same
problem as before:
f(x) = 0:1x21
+ 2x22
: (11.7.6)
We are going to implement Adagrad using the same learning rate previously, i.e.,  = 0:4. As
we can see, the iterative trajectory of the independent variable is smoother. However, due to the
cumulative effect of st, the learning rate continuously decays, so the independent variable does
not move as much during later stages of iteration.
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np, npx
npx.set_np()
def adagrad_2d(x1, x2, s1, s2):
eps = 1e-6
g1, g2 = 0.2 * x1, 4 * x2
s1 += g1 ** 2
s2 += g2 ** 2
x1 -= eta / math.sqrt(s1 + eps) * g1
x2 -= eta / math.sqrt(s2 + eps) * g2
return x1, x2, s1, s2
def f_2d(x1, x2):
return 0.1 * x1 ** 2 + 2 * x2 ** 2
eta = 0.4
d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))
11.7. Adagrad 457
As we increase the learning rate to 2 we see much better behavior. This already indicates that the
decrease in learning rate might be rather aggressive, even in the noise-free case and we need to
ensure that parameters converge appropriately.
eta = 2
d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))
11.7.4 Implementation from Scratch
Just like the momentum method, Adagrad needs to maintain a state variable of the same shape as
the parameters.
def init_adagrad_states(feature_dim):
s_w = np.zeros((feature_dim, 1))
s_b = np.zeros(1)
return (s_w, s_b)
def adagrad(params, states, hyperparams):
eps = 1e-6
for p, s in zip(params, states):
(continues on next page)
458 Chapter 11. Optimization Algorithms
(continued from previous page)
s[:] += np.square(p.grad)
p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)
Compared to the experiment in Section 11.5 we use a larger learning rate to train the model.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(adagrad, init_adagrad_states(feature_dim),
{'lr': 0.1}, data_iter, feature_dim);
loss: 0.242, 0.062 sec/epoch
11.7.5 Concise Implementation
Using the Trainer instance of the algorithm adagrad, we can invoke the Adagrad algorithm in
Gluon.
d2l.train_concise_ch11('adagrad', {'learning_rate': 0.1}, data_iter)
loss: 0.242, 0.082 sec/epoch
11.7. Adagrad 459
Summary
� Adagrad decreases the learning rate dynamically on a per-coordinate basis.
� It uses the magnitude of the gradient as a means of adjusting how quickly progress is
achieved - coordinates with large gradients are compensated with a smaller learning rate.
� Computing the exact second derivative is typically infeasible in deep learning problems due
to memory and computational constraints. The gradient can be a useful proxy.
� If the optimization problem has a rather uneven uneven structure Adagrad can help mitigate
the distortion.
� Adagrad is particularly effective for sparse features where the learning rate needs to decrease
more slowly for infrequently occurring terms.
� On deep learning problems Adagrad can sometimes be too aggressive in reducing learning
rates. We will discuss strategies for mitigating this in the context of Section 11.10.
Exercises
1. Prove that for an orthogonal matrix U and a vector c the following holds: ?c ?? ?2 = ?Uc ??
U?2. Why does this mean that the magnitude of perturbations does not change after an
orthogonal change of variables?
2. Try out Adagrad for f(x) = 0:1x21
+ 2x22
and also for the objective function was rotated by 45
degrees, i.e., f(x) = 0:1(x1 + x2)2 + 2(x1 ?? x2)2. Does it behave differently?
3. Prove Gerschgorin?s circle theorem142 which states that eigenvalues i of a matrix M satisfy
ji ?? Mjj j 
?
k?=j
jMjkj for at least one choice of j.
4. What does Gerschgorin?s theorem tell us about the eigenvalues of the diagonally preconditioned
matrix diag??1
2 (M)Mdiag??1
2 (M)?
5. Try out Adagrad for a proper deep network, such as Section 6.6 when applied to Fashion
MNIST.
6. How would you need to modify Adagrad to achieve a less aggressive decay in learning rate?
142 https://en.wikipedia.org/wiki/Gershgorin_circle_theorem
460 Chapter 11. Optimization Algorithms
Discussions143
11.8 RMSProp
One of the key issues in Section 11.7 is that the learning rate decreases at a predefined schedule
of effectively O(t
??1
2 ). While this is generally appropriate for convex problems, it might not be
ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise
adaptivity of Adagrad is highly desirable as a preconditioner.
(Tieleman & Hinton, 2012) proposed the RMSProp algorithm as a simple fix to decouple rate
scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accumulates the
squares of the gradient gt into a state vector st = st??1+g2t
. As a result st keeps on growing without
bound due to the lack of normalization, essentially linearly as the algorithm converges.
One way of fixing this problem would be to use st/t. For reasonable distributions of gt this will
converge. Unfortunately it might take a very long time until the limit behavior starts to matter
since the procedure remembers the full trajectory of values. An alternative is to use a leaky average
in the same way we used in the momentum method, i.e., st   
st??1 + (1 ?? 
)g2t
for some
parameter 
 > 0. Keeping all other parts unchanged yields RMSProp.
11.8.1 The Algorithm
Let us write out the equations in detail.
st   
st??1 + (1 ?? 
)g2t
;
xt   xt??1 ?? p 
st + ?
? gt:
(11.8.1)
The constant ? > 0 is typically set to 10??6 to ensure that we do not suffer from division by zero
or overly large step sizes. Given this expansion we are now free to control the learning rate 
independently of the scaling that is applied on a per-coordinate basis. In terms of leaky averages
we can apply the same reasoning as previously applied in the case of the momentum method.
Expanding the definition of st yields
st = (1 ?? 
)g2t
+ 
st??1
= (1 ?? 
)
(
g2t
+ 
g2t
??1 + 
2gt??2 + : : : ;
)
:
(11.8.2)
As before in Section 11.6 we use 1+
+
2+: : : ;= 1
1??
 . Hence the sum of weights is normalized to 1
with a half-life time of an observation of 
??1. Let us visualize the weights for the past 40 timesteps
for various choices of 
.
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np, npx
npx.set_np()
143 https://discuss.d2l.ai/t/355
11.8. RMSProp 461
d2l.set_figsize()
gammas = [0.95, 0.9, 0.8, 0.7]
for gamma in gammas:
x = d2l.numpy(np.arange(40))
d2l.plt.plot(x, (1-gamma) * gamma ** x, label=f'gamma = {gamma:.2f}')
d2l.plt.xlabel('time');
11.8.2 Implementation from Scratch
As before we use the quadratic function f(x) = 0:1x21
+ 2x22
to observe the trajectory of RMSProp.
Recall that in Section 11.7, when we used Adagrad with a learning rate of 0.4, the variables moved
only very slowly in the later stages of the algorithm since the learning rate decreased too quickly.
Since  is controlled separately this does not happen with RMSProp.
def rmsprop_2d(x1, x2, s1, s2):
g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6
s1 = gamma * s1 + (1 - gamma) * g1 ** 2
s2 = gamma * s2 + (1 - gamma) * g2 ** 2
x1 -= eta / math.sqrt(s1 + eps) * g1
x2 -= eta / math.sqrt(s2 + eps) * g2
return x1, x2, s1, s2
def f_2d(x1, x2):
return 0.1 * x1 ** 2 + 2 * x2 ** 2
eta, gamma = 0.4, 0.9
d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))
462 Chapter 11. Optimization Algorithms
Next, we implement RMSProp to be used in a deep network. This is equally straightforward.
def init_rmsprop_states(feature_dim):
s_w = np.zeros((feature_dim, 1))
s_b = np.zeros(1)
return (s_w, s_b)
def rmsprop(params, states, hyperparams):
gamma, eps = hyperparams['gamma'], 1e-6
for p, s in zip(params, states):
s[:] = gamma * s + (1 - gamma) * np.square(p.grad)
p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)
We set the initial learning rate to 0.01 and the weighting term 
 to 0.9. That is, s aggregates on
average over the past 1/(1 ?? 
) = 10 observations of the square gradient.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),
{'lr': 0.01, 'gamma': 0.9}, data_iter, feature_dim);
loss: 0.243, 0.077 sec/epoch
11.8. RMSProp 463
11.8.3 Concise Implementation
Since RMSProp is a rather popular algorithm it is also available in the Trainer instance. All we
need to do is instantiate it using an algorithm named rmsprop, assigning 
 to the parameter gamma1.
d2l.train_concise_ch11('rmsprop', {'learning_rate': 0.01, 'gamma1': 0.9},
data_iter)
loss: 0.243, 0.042 sec/epoch
Summary
� RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale
coefficients.
� RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique
to adjust the coefficient-wise preconditioner.
� The learning rate needs to be scheduled by the experimenter in practice.
� The coefficient 
 determines how long the history is when adjusting the per-coordinate
scale.
Exercises
1. What happens experimentally if we set 
 = 1? Why?
2. Rotate the optimization problem to minimize f(x) = 0:1(x1 + x2)2 + 2(x1 ?? x2)2. What
happens to the convergence?
3. Try out what happens to RMSProp on a real machine learning problem, such as training on
Fashion-MNIST. Experiment with different choices for adjusting the learning rate.
4. Would you want to adjust 
 as optimization progresses? How sensitive is RMSProp to this?
Discussions144
144 https://discuss.d2l.ai/t/356
464 Chapter 11. Optimization Algorithms
11.9 Adadelta
Adadelta is yet another variant of AdaGrad (Section 11.7). The main difference lies in the fact that
it decreases the amount by which the learning rate is adaptive to coordinates. Moreover, traditionally
it referred to as not having a learning rate since it uses the amount of change itself as
calibration for future change. The algorithm was proposed in (Zeiler, 2012). It is fairly straightforward,
given the discussion of previous algorithms so far.
11.9.1 The Algorithm
In a nutshell, Adadelta uses two state variables, st to store a leaky average of the second moment
of the gradient and?xt to store a leaky average of the second moment of the change of parameters
in the model itself. Note that we use the original notation and naming of the authors for compatibility
with other publications and implementations (there is no other real reason why one should
use different Greek variables to indicate a parameter serving the same purpose in momentum,
Adagrad, RMSProp, and Adadelta).
Here are the technical details of Adadelta. Given the parameter du jour is , we obtain the following
leaky updates similarly to Section 11.8:
st = st??1 + (1 ?? )g2t
: (11.9.1)
The difference to Section 11.8 is that we perform updates with the rescaled gradient g?
t, i.e.,
xt = xt??1 ?? g?
t: (11.9.2)
So what is the rescaled gradient g?
t? We can calculate it as follows:
g?
t =
p
?pxt??1 + ?
st + ?
? gt; (11.9.3)
where ?xt??1 is the leaky average of the squared rescaled gradients g?
t. We initialize ?x0 to be 0
and update it at each step with g?
t, i.e.,
?xt = ?xt??1 + (1 ?? )g?
t
2; (11.9.4)
and ? (a small value such as 10??5) is added to maintain numerical stability.
11.9.2 Implementation
Adadelta needs to maintain two state variables for each variable, st and ?xt. This yields the following
implementation.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
def init_adadelta_states(feature_dim):
s_w, s_b = np.zeros((feature_dim, 1)), np.zeros(1)
delta_w, delta_b = np.zeros((feature_dim, 1)), np.zeros(1)
(continues on next page)
11.9. Adadelta 465
(continued from previous page)
return ((s_w, delta_w), (s_b, delta_b))
def adadelta(params, states, hyperparams):
rho, eps = hyperparams['rho'], 1e-5
for p, (s, delta) in zip(params, states):
# In-place updates via [:]
s[:] = rho * s + (1 - rho) * np.square(p.grad)
g = (np.sqrt(delta + eps) / np.sqrt(s + eps)) * p.grad
p[:] -= g
delta[:] = rho * delta + (1 - rho) * g * g
Choosing  = 0:9 amounts to a half-life time of 10 for each parameter update. This tends to work
quite well. We get the following behavior.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(adadelta, init_adadelta_states(feature_dim),
{'rho': 0.9}, data_iter, feature_dim);
loss: 0.243, 0.096 sec/epoch
For a concise implementation we simply use the adadelta algorithm from the Trainer class. This
yields the following one-liner for a much more compact invocation.
d2l.train_concise_ch11('adadelta', {'rho': 0.9}, data_iter)
loss: 0.244, 0.101 sec/epoch
466 Chapter 11. Optimization Algorithms
Summary
� Adadelta has no learning rate parameter. Instead, it uses the rate of change in the parameters
itself to adapt the learning rate.
� Adadelta requires two state variables to store the second moments of gradient and the
change in parameters.
� Adadelta uses leaky averages to keep a running estimate of the appropriate statistics.
Exercises
1. Adjust the value of . What happens?
2. Show how to implement the algorithm without the use of g?
t. Why might this be a good idea?
3. Is Adadelta really learning rate free? Could you find optimization problems that break
Adadelta?
4. Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.
Discussions145
11.10 Adam
In the discussions leading up to this section we encountered a number of techniques for efficient
optimization. Let us recap them in detail here:
� We saw that Section 11.4 is more effective than Gradient Descent when solving optimization
problems, e.g., due to its inherent resilience to redundant data.
� We saw that Section 11.5 affords significant additional efficiency arising from vectorization,
using larger sets of observations in one minibatch. This is the key to efficient multi-machine,
multi-GPU and overall parallel processing.
145 https://discuss.d2l.ai/t/357
11.10. Adam 467
� Section 11.6 added a mechanism for aggregating a history of past gradients to accelerate
convergence.
� Section 11.7 used per-coordinate scaling to allow for a computationally efficient preconditioner.
� Section 11.8 decoupled per-coordinate scaling from a learning rate adjustment.
Adam (Kingma & Ba, 2014) combines all these techniques into one efficient learning algorithm.
As expected, this is an algorithm that has become rather popular as one of the more robust and
effective optimization algorithms to use in deep learning. It is not without issues, though. In
particular, (Reddi et al., 2019) show that there are situations where Adam can diverge due to poor
variance control. In a follow-up work (Zaheer et al., 2018) proposed a hotfix to Adam, called Yogi
which addresses these issues. More on this later. For now let us review the Adam algorithm.
11.10.1 The Algorithm
One of the key components of Adam is that it uses exponential weighted moving averages (also
known as leaky averaging) to obtain an estimate of both the momentum and also the second moment
of the gradient. That is, it uses the state variables
vt   1vt??1 + (1 ?? 1)gt;
st   2st??1 + (1 ?? 2)g2t
:
(11.10.1)
Here 1 and 2 are nonnegative weighting parameters. Common choices for them are 1 = 0:9
and 2 = 0:999. That is, the variance estimate moves much more slowly than the momentum term.
Note that if we initialize v0 = s0 = 0 we have a significant amount of bias initially towards smaller
values. This can be addressed by using the fact that
?t
i=0 i = 1??t
1?? to re-normalize terms. Correspondingly
the normalized state variables are given by
^vt =
vt
1 ?? t1
and ^st =
st
1 ?? t2
: (11.10.2)
Armed with the proper estimates we can now write out the update equations. First, we rescale the
gradient in a manner very much akin to that of RMSProp to obtain
g?
t =
vv^t
^st + ?
: (11.10.3)
Unlike RMSProp our update uses the momentum ^vt rather than the gradient itself. Moreover,
there is a slight cosmetic difference as the rescaling happens using p 1
^st+? instead of p 1
^st+? . The
former works arguably slightly better in practice, hence the deviation from RMSProp. Typically
we pick ? = 10??6 for a good trade-off between numerical stability and fidelity.
Now we have all the pieces in place to compute updates. This is slightly anticlimactic and we have
a simple update of the form
xt   xt??1 ?? g?
t: (11.10.4)
Reviewing the design of Adam its inspiration is clear. Momentum and scale are clearly visible in
the state variables. Their rather peculiar definition forces us to debias terms (this could be fixed
by a slightly different initialization and update condition). Second, the combination of both terms
is pretty straightforward, given RMSProp. Last, the explicit learning rate  allows us to control the
step length to address issues of convergence.
468 Chapter 11. Optimization Algorithms
11.10.2 Implementation
Implementing Adam from scratch is not very daunting. For convenience we store the timestep
counter t in the hyperparams dictionary. Beyond that all is straightforward.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
def init_adam_states(feature_dim):
v_w, v_b = np.zeros((feature_dim, 1)), np.zeros(1)
s_w, s_b = np.zeros((feature_dim, 1)), np.zeros(1)
return ((v_w, s_w), (v_b, s_b))
def adam(params, states, hyperparams):
beta1, beta2, eps = 0.9, 0.999, 1e-6
for p, (v, s) in zip(params, states):
v[:] = beta1 * v + (1 - beta1) * p.grad
s[:] = beta2 * s + (1 - beta2) * np.square(p.grad)
v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps)
hyperparams['t'] += 1
We are ready to use Adam to train the model. We use a learning rate of  = 0:01.
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(adam, init_adam_states(feature_dim),
{'lr': 0.01, 't': 1}, data_iter, feature_dim);
loss: 0.244, 0.094 sec/epoch
A more concise implementation is straightforward since adam is one of the algorithms provided
as part of the Gluon trainer optimization library. Hence we only need to pass configuration parameters
for an implementation in Gluon.
11.10. Adam 469
d2l.train_concise_ch11('adam', {'learning_rate': 0.01}, data_iter)
loss: 0.243, 0.049 sec/epoch
11.10.3 Yogi
One of the problems of Adam is that it can fail to converge even in convex settings when the second
moment estimate in st blows up. As a fix (Zaheer et al., 2018) proposed a refined update (and
initialization) for st. To understand what?s going on, let us rewrite the Adam update as follows:
st   st??1 + (1 ?? 2)
(
g2t
?? st??1
)
: (11.10.5)
Whenever g2t
has high variance or updates are sparse, st might forget past values too quickly. A
possible fix for this is to replace g2t
??st??1 by g2t
?sgn(g2t
??st??1). Now the magnitude of the update
no longer depends on the amount of deviation. This yields the Yogi updates
st   st??1 + (1 ?? 2)g2t? sgn(g2t
?? st??1): (11.10.6)
The authors furthermore advise to initialize the momentum on a larger initial batch rather than
just initial pointwise estimate. We omit the details since they are not material to the discussion
and since even without this convergence remains pretty good.
def yogi(params, states, hyperparams):
beta1, beta2, eps = 0.9, 0.999, 1e-3
for p, (v, s) in zip(params, states):
v[:] = beta1 * v + (1 - beta1) * p.grad
s[:] = s + (1 - beta2) * np.sign(
np.square(p.grad) - s) * np.square(p.grad)
v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps)
hyperparams['t'] += 1
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(yogi, init_adam_states(feature_dim),
{'lr': 0.01, 't': 1}, data_iter, feature_dim);
470 Chapter 11. Optimization Algorithms
loss: 0.242, 0.107 sec/epoch
Summary
� Adam combines features of many optimization algorithms into a fairly robust update rule.
� Created on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic gradient
� Adam uses bias correction to adjust for a slow startup when estimating momentum and a
second moment.
� For gradients with significant variance we may encounter issues with convergence. They
can be amended by using larger minibatches or by switching to an improved estimate for st.
Yogi offers such an alternative.
Exercises
1. Adjust the learning rate and observe and analyze the experimental results.
2. Can you rewrite momentum and second moment updates such that it does not require bias
correction?
3. Why do you need to reduce the learning rate  as we converge?
4. Try to construct a case for which Adam diverges and Yogi converges?
Discussions146
146 https://discuss.d2l.ai/t/358
11.10. Adam 471
11.11 Learning Rate Scheduling
So far we primarily focused on optimization algorithms for how to update the weight vectors rather
than on the rate at which they are being updated. Nonetheless, adjusting the learning rate is often
just as important as the actual algorithm. There are a number of aspects to consider:
� Most obviously the magnitude of the learning rate matters. If it is too large, optimization
diverges, if it is too small, it takes too long to train or we end up with a suboptimal result.
We saw previously that the condition number of the problem matters (see e.g., Section 11.6
for details). Intuitively it is the ratio of the amount of change in the least sensitive direction
vs. the most sensitive one.
� Secondly, the rate of decay is just as important. If the learning rate remains large we may
simply end up bouncing around the minimum and thus not reach optimality. Section 11.5
discussed this in some detail and we analyzed performance guarantees in Section 11.4. In
short, we want the rate to decay, but probably more slowly than O(t
??1
2 ) which would be a
good choice for convex problems.
� Another aspect that is equally important is initialization. This pertains both to how the parameters
are set initially (review Section 4.8 for details) and also how they evolve initially.
This goes under the moniker of warmup, i.e., how rapidly we start moving towards the solution
initially. Large steps in the beginning might not be beneficial, in particular since the
initial set of parameters is random. The initial update directions might be quite meaningless,
too.
� Lastly, there are a number of optimization variants that perform cyclical learning rate adjustment.
This is beyond the scope of the current chapter. We recommend the reader to
review details in (Izmailov et al., 2018), e.g., how to obtain better solutions by averaging
over an entire path of parameters.
Given the fact that there is a lot of detail needed to manage learning rates, most deep learning
frameworks have tools to deal with this automatically. In the current chapter we will review the
effects that different schedules have on accuracy and also show how this can be managed efficiently
via a learning rate scheduler.
11.11.1 Toy Problem
We begin with a toy problem that is cheap enough to compute easily, yet sufficiently nontrivial to
illustrate some of the key aspects. For that we pick a slightly modernized version of LeNet (relu
instead of sigmoid activation, MaxPooling rather than AveragePooling), as applied to Fashion-
MNIST. Moreover, we hybridize the network for performance. Since most of the code is standard
we just introduce the basics without further detailed discussion. See Chapter 6 for a refresher as
needed.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, lr_scheduler, np, npx
from mxnet.gluon import nn
npx.set_np()
net = nn.HybridSequential()
net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='relu'),
(continues on next page)
472 Chapter 11. Optimization Algorithms
(continued from previous page)
nn.MaxPool2D(pool_size=2, strides=2),
nn.Conv2D(channels=16, kernel_size=5, activation='relu'),
nn.MaxPool2D(pool_size=2, strides=2),
nn.Dense(120, activation='relu'),
nn.Dense(84, activation='relu'),
nn.Dense(10))
net.hybridize()
loss = gluon.loss.SoftmaxCrossEntropyLoss()
device = d2l.try_gpu()
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
# The code is almost identical to `d2l.train_ch6` that defined in the lenet
# section of chapter convolutional neural networks
def train(net, train_iter, test_iter, num_epochs, loss, trainer, device):
net.initialize(force_reinit=True, ctx=device, init=init.Xavier())
animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
metric = d2l.Accumulator(3) # train_loss, train_acc, num_examples
for i, (X, y) in enumerate(train_iter):
X, y = X.as_in_ctx(device), y.as_in_ctx(device)
with autograd.record():
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
trainer.step(X.shape[0])
metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])
train_loss = metric[0] / metric[2]
train_acc = metric[1] / metric[2]
if (i + 1) % 50 == 0:
animator.add(epoch + i / len(train_iter),
(train_loss, train_acc, None))
test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
animator.add(epoch + 1, (None, None, test_acc))
print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '
f'test acc {test_acc:.3f}')
Let us have a look at what happens if we invoke this algorithm with default settings, such as a
learning rate of 0:3 and train for 30 iterations. Note how the training accuracy keeps on increasing
while progress in terms of test accuracy stalls beyond a point. The gap between both curves
indicates overfitting.
lr, num_epochs = 0.3, 30
net.initialize(force_reinit=True, ctx=device, init=init.Xavier())
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.141, train acc 0.947, test acc 0.884
11.11. Learning Rate Scheduling 473
11.11.2 Schedulers
One way of adjusting the learning rate is to set it explicitly at each step. This is conveniently
achieved by the set_learning_rate method. We could adjust it downward after every epoch (or
even after every minibatch), e.g., in a dynamic manner in response to how optimization is progressing.
trainer.set_learning_rate(0.1)
print(f'learning rate is now {trainer.learning_rate:.2f}')
learning rate is now 0.10
More generally we want to define a scheduler. When invoked with the number of updates it returns
the appropriate value of the learning rate. Let us define a simple one that sets the learning rate to
 = 0(t + 1)
??1
2 .
class SquareRootScheduler:
def __init__(self, lr=0.1):
self.lr = lr
def __call__(self, num_update):
return self.lr * pow(num_update + 1.0, -0.5)
Let us plot its behavior over a range of values.
scheduler = SquareRootScheduler(lr=1.0)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
474 Chapter 11. Optimization Algorithms
Now let us see how this plays out for training on Fashion-MNIST. We simply provide the scheduler
as an additional argument to the training algorithm.
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.295, train acc 0.893, test acc 0.884
This worked quite a bit better than previously. Two things stand out: the curve was rather more
smooth than previously. Secondly, there was less overfitting. Unfortunately it is not a wellresolved
question as to why certain strategies lead to less overfitting in theory. There is some
argument that a smaller stepsize will lead to parameters that are closer to zero and thus simpler.
However, this does not explain the phenomenon entirely since we do not really stop early but
simply reduce the learning rate gently.
11.11. Learning Rate Scheduling 475
11.11.3 Policies
While we cannot possibly cover the entire variety of learning rate schedulers, we attempt to give
a brief overview of popular policies below. Common choices are polynomial decay and piecewise
constant schedules. Beyond that, cosine learning rate schedules have been found to work well
empirically on some problems. Lastly, on some problems it is beneficial to warm up the optimizer
prior to using large learning rates.
Factor Scheduler
One alternative to a polynomial decay would be a multiplicative one, that is t+1   t   for
 2 (0; 1). To prevent the learning rate from decaying beyond a reasonable lower bound the
update equation is often modified to t+1   max(min; t  ).
class FactorScheduler:
def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):
self.factor = factor
self.stop_factor_lr = stop_factor_lr
self.base_lr = base_lr
def __call__(self, num_update):
self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)
return self.base_lr
scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)
d2l.plot(np.arange(50), [scheduler(t) for t in range(50)])
This can also be accomplished by a built-in scheduler in MXNet via the lr_scheduler.
FactorScheduler object. It takes a few more parameters, such as warmup period, warmup mode
(linear or constant), the maximum number of desired updates, etc.; Going forward we will use
the built-in schedulers as appropriate and only explain their functionality here. As illustrated, it
is fairly straightforward to build your own scheduler if needed.
476 Chapter 11. Optimization Algorithms
Multi Factor Scheduler
A common strategy for training deep networks is to keep the learning rate piecewise constant and
to decrease it by a given amount every so often. That is, given a set of times when to decrease the
rate, such as s = f5; 10; 20g decrease t+1   t   whenever t 2 s. Assuming that the values are
halved at each step we can implement this as follows.
scheduler = lr_scheduler.MultiFactorScheduler(step=[15, 30], factor=0.5,
base_lr=0.5)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
The intuition behind this piecewise constant learning rate schedule is that one lets optimization
proceed until a stationary point has been reached in terms of the distribution of weight vectors.
Then (and only then) do we decrease the rate such as to obtain a higher quality proxy to a good
local minimum. The example below shows how this can produce ever slightly better solutions.
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.203, train acc 0.923, test acc 0.895
11.11. Learning Rate Scheduling 477
Cosine Scheduler
A rather perplexing heuristic was proposed by (Loshchilov & Hutter, 2016). It relies on the observation
that we might not want to decrease the learning rate too drastically in the beginning and
moreover, that we might want to �refine� the solution in the end using a very small learning rate.
This results in a cosine-like schedule with the following functional form for learning rates in the
range t 2 [0; T].
t = T +
0 ?? T
2
(1 + cos(t/T)) (11.11.1)
Here 0 is the initial learning rate, T is the target rate at time T. Furthermore, for t > T we simply
pin the value to T without increasing it again. In the following example, we set the max update
step T = 20.
scheduler = lr_scheduler.CosineScheduler(max_update=20, base_lr=0.5,
final_lr=0.01)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
In the context of computer vision this schedule can lead to improved results. Note, though, that
such improvements are not guaranteed (as can be seen below).
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.345, train acc 0.877, test acc 0.868
478 Chapter 11. Optimization Algorithms
Warmup
In some cases initializing the parameters is not sufficient to guarantee a good solution. This particularly
a problem for some advanced network designs that may lead to unstable optimization
problems. We could address this by choosing a sufficiently small learning rate to prevent divergence
in the beginning. Unfortunately this means that progress is slow. Conversely, a large learning
rate initially leads to divergence.
A rather simple fix for this dilemma is to use a warmup period during which the learning rate
increases to its initial maximum and to cool down the rate until the end of the optimization process.
For simplicity one typically uses a linear increase for this purpose. This leads to a schedule of the
form indicated below.
scheduler = lr_scheduler.CosineScheduler(20, warmup_steps=5, base_lr=0.5,
final_lr=0.01)
d2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
Note that the network converges better initially (in particular observe the performance during the
first 5 epochs).
11.11. Learning Rate Scheduling 479
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'lr_scheduler': scheduler})
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)
train loss 0.424, train acc 0.846, test acc 0.835
Warmup can be applied to any scheduler (not just cosine). For a more detailed discussion of learning
rate schedules and many more experiments see also (Gotmare et al., 2018). In particular they
find that a warmup phase limits the amount of divergence of parameters in very deep networks.
This makes intuitively sense since we would expect significant divergence due to random initialization
in those parts of the network that take the most time to make progress in the beginning.
Summary
� Decreasing the learning rate during training can lead to improved accuracy and (most perplexingly)
reduced overfitting of the model.
� A piecewise decrease of the learning rate whenever progress has plateaued is effective in
practice. Essentially this ensures that we converge efficiently to a suitable solution and only
then reduce the inherent variance of the parameters by reducing the learning rate.
� Cosine schedulers are popular for some computer vision problems. See e.g., GluonCV147 for
details of such a scheduler.
� A warmup period before optimization can prevent divergence.
� Optimization serves multiple purposes in deep learning. Besides minimizing the training
objective, different choices of optimization algorithms and learning rate scheduling can lead
to rather different amounts of generalization and overfitting on the test set (for the same
amount of training error).
147 http://gluon-cv.mxnet.io
480 Chapter 11. Optimization Algorithms
Exercises
1. Experiment with the optimization behavior for a given fixed learning rate. What is the best
model you can obtain this way?
2. How does convergence change if you change the exponent of the decrease in the learning
rate? Use PolyScheduler for your convenience in the experiments.
3. Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet. How
does it affect performance relative to other schedulers?
4. How long should warmup last?
5. Can you connect optimization and sampling? Start by using results from (Welling & Teh,
2011) on Stochastic Gradient Langevin Dynamics.
Discussions148
148 https://discuss.d2l.ai/t/359
11.11. Learning Rate Scheduling 481
482 Chapter 11. Optimization Algorithms
12 | Computational Performance
In deep learning, datasets are usually large and model computation is complex. Therefore, we
are always very concerned about computing performance. This chapter will focus on the important
factors that affect computing performance: imperative programming, symbolic programming,
asynchronous programing, automatic parallel computation, and multi-GPU computation.
By studying this chapter, you should be able to further improve the computing performance of
the models that have been implemented in the previous chapters, for example, by reducing the
model training time without affecting the accuracy of the model.
12.1 Compilers and Interpreters
So far, this book has focused on imperative programming, which makes use of statements such as
print, + or if to change a program?s state. Consider the following example of a simple imperative
program.
def add(a, b):
return a + b
def fancy_func(a, b, c, d):
e = add(a, b)
f = add(c, d)
g = add(e, f)
return g
print(fancy_func(1, 2, 3, 4))
10
Python is an interpreted language. When evaluating fancy_func it performs the operations making
up the function?s body in sequence. That is, it will evaluate e = add(a, b) and it will store the
results as variable e, thereby changing the program?s state. The next two statements f = add(c,
d) and g = add(e, f) will be excecuted similarly, performing additions and storing the results as
variables. Fig. 12.1.1 illustrates the flow of data.
483
Fig. 12.1.1: Data flow in an imperative program.
Although imperative programming is convenient, it may be inefficient. On one hand, even if the
add function is repeatedly called throughout fancy_func, Python will execute the three function
calls individually. If these are executed, say, on a GPU (or even on multiple GPUs), the overhead
arising from the Python interpreter can become overwhelming. Moreover, it will need to save
the variable values of e and f until all the statements in fancy_func have been executed. This is
because we do not know whether the variables e and f will be used by other parts of the program
after the statements e = add(a, b) and f = add(c, d) have been executed.
12.1.1 Symbolic Programming
Consider the alternative, symbolic programming where computation is usually performed only
once the process has been fully defined. This strategy is used by multiple deep learning frameworks,
including Theano, Keras and TensorFlow (the latter two have since acquired imperative
extensions). It usually involves the following steps:
1. Define the operations to be executed.
2. Compile the operations into an executable program.
3. Provide the required inputs and call the compiled program for execution.
This allows for a significant amount of optimization. First off, we can skip the Python interpreter
in many cases, thus removing a performance bottleneck that can become significant on multiple
fast GPUs paired with a single Python thread on a CPU. Secondly, a compiler might optimize and
rewrite the above code into print((1 + 2) + (3 + 4)) or even print(10). This is possible since a
compiler gets to see the full code before turning it into machine instructions. For instance, it can
release memory (or never allocate it) whenever a variable is no longer needed. Or it can transform
the code entirely into an equivalent piece. To get a better idea consider the following simulation
of imperative programming (it is Python after all) below.
def add_():
return '''
def add(a, b):
return a + b
'''
def fancy_func_():
return '''
def fancy_func(a, b, c, d):
e = add(a, b)
f = add(c, d)
(continues on next page)
484 Chapter 12. Computational Performance
(continued from previous page)
g = add(e, f)
return g
'''
def evoke_():
return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'
prog = evoke_()
print(prog)
y = compile(prog, '', 'exec')
exec(y)
def add(a, b):
return a + b
def fancy_func(a, b, c, d):
e = add(a, b)
f = add(c, d)
g = add(e, f)
return g
print(fancy_func(1, 2, 3, 4))
10
The differences between imperative (interpreted) programming and symbolic programming are
as follows:
� Imperative programming is easier. When imperative programming is used in Python, the
majority of the code is straightforward and easy to write. It is also easier to debug imperative
programming code. This is because it is easier to obtain and print all relevant intermediate
variable values, or use Python?s built-in debugging tools.
� Symbolic programming is more efficient and easier to port. It makes it easier to optimize
the code during compilation, while also having the ability to port the program into a format
independent of Python. This allows the program to be run in a non-Python environment,
thus avoiding any potential performance issues related to the Python interpreter.
12.1.2 Hybrid Programming
Historically most deep learning frameworks choose between an imperative or a symbolic approach.
For example, Theano, TensorFlow (inspired by the latter), Keras and CNTK formulate
models symbolically. Conversely, Chainer and PyTorch take an imperative approach. An imperative
mode was added to TensorFlow 2.0 (via Eager) and Keras in later revisions. When designing
Gluon, developers considered whether it would be possible to combine the benefits of both programming
models. This led to a hybrid model that lets users develop and debug using pure imperative
programming, while having the ability to convert most programs into symbolic programs to
be run when product-level computing performance and deployment are required.
In practice this means that we build models using either the HybridBlock or the HybridSequential
and HybridConcurrent classes. By default, they are executed in the same way Block or Sequential
and Concurrent classes are executed in imperative programming. HybridSequential is a subclass
of HybridBlock (just like Sequential subclasses Block). When the hybridize function is called,
12.1. Compilers and Interpreters 485
Gluon compiles the model into the form used in symbolic programming. This allows one to optimize
the compute-intensive components without sacrifices in the way a model is implemented.
We will illustrate the benefits below, focusing on sequential models and blocks only (the concurrent
composition works analogously).
12.1.3 HybridSequential
The easiest way to get a feel for how hybridization works is to consider deep networks with multiple
layers. Conventionally the Python interpreter will need to execute the code for all layers to
generate an instruction that can then be forwarded to a CPU or a GPU. For a single (fast) compute
device this does not cause any major issues. On the other hand, if we use an advanced 8-GPU
server such as an AWS P3dn.24xlarge instance Python will struggle to keep all GPUs busy. The
single-threaded Python interpreter becomes the bottleneck here. Let us see how we can address
this for significant parts of the code by replacing Sequential by HybridSequential. We begin by
defining a simple MLP.
from d2l import mxnet as d2l
from mxnet import np, npx
from mxnet.gluon import nn
npx.set_np()
# factory for networks
def get_net():
net = nn.HybridSequential()
net.add(nn.Dense(256, activation='relu'),
nn.Dense(128, activation='relu'),
nn.Dense(2))
net.initialize()
return net
x = np.random.normal(size=(1, 512))
net = get_net()
net(x)
array([[ 0.16526176, -0.14005631]])
By calling the hybridize function, we are able to compile and optimize the computation in the
MLP. The model?s computation result remains unchanged.
net.hybridize()
net(x)
array([[ 0.16526176, -0.14005631]])
This seems almost too good to be true: simply designate a block to be HybridSequential, write
the same code as before and invoke hybridize. Once this happens the network is optimized (we
will benchmark the performance below). Unfortunately this does not work magically for every
layer. That said, the blocks provided by Gluon are by default subclasses of HybridBlock and thus
hybridizable. A layer will not be optimized if it inherits from the Block instead.
486 Chapter 12. Computational Performance
Acceleration by Hybridization
To demonstrate the performance improvement gained by compilation we compare the time
needed to evaluate net(x) before and after hybridization. Let us define a function to measure
this time first. It will come handy throughout the chapter as we set out to measure (and improve)
performance.
#@save
class Benchmark:
def __init__(self, description='Done'):
self.description = description
def __enter__(self):
self.timer = d2l.Timer()
return self
def __exit__(self, *args):
print(f'{self.description}: {self.timer.stop():.4f} sec')
Now we can invoke the network twice, once with and once without hybridization.
net = get_net()
with Benchmark('Without hybridization'):
for i in range(1000): net(x)
npx.waitall()
net.hybridize()
with Benchmark('With hybridization'):
for i in range(1000): net(x)
npx.waitall()
Without hybridization: 0.8433 sec
With hybridization: 0.3059 sec
As is observed in the above results, after a HybridSequential instance calls the hybridize function,
computing performance is improved through the use of symbolic programming.
Serialization
One of the benefits of compiling the models is that we can serialize (save) the model and its parameters
to disk. This allows us to store a model in a manner that is independent of the front-end
language of choice. This allows us to deploy trained models to other devices and easily use other
front-end programming languages. At the same time the code is often faster than what can be
achieved in imperative programming. Let us see the export method in action.
net.export('my_mlp')
!ls -lh my_mlp*
-rw-r--r-- 1 jenkins jenkins 643K Aug 9 21:34 my_mlp-0000.params
-rw-r--r-- 1 jenkins jenkins 3.0K Aug 9 21:34 my_mlp-symbol.json
12.1. Compilers and Interpreters 487
The model is decomposed into a (large binary) parameter file and a JSON description of the program
required to execute to compute the model. The files can be read by other front-end languages
supported by Python or MXNet, such as C++, R, Scala, and Perl. Let us have a look at the model
description.
!head my_mlp-symbol.json
{
"nodes": [
{
"op": "null",
"name": "data",
"inputs": []
},
{
"op": "null",
"name": "dense3_weight",
Things are slightly more tricky when it comes to models that resemble code more closely. Basically
hybridization needs to deal with control flow and Python overhead in a much more immediate
manner. Moreover,
Contrary to the Block instance, which needs to use the forward function, for a HybridBlock instance
we need to use the hybrid_forward function.
Earlier, we demonstrated that, after calling the hybridize method, the model is able to achieve superior
computing performance and portability. Note, though that hybridization can affect model
flexibility, in particular in terms of control flow. We will illustrate how to design more general
models and also how compilation will remove spurious Python elements.
class HybridNet(nn.HybridBlock):
def __init__(self, **kwargs):
super(HybridNet, self).__init__(**kwargs)
self.hidden = nn.Dense(4)
self.output = nn.Dense(2)
def hybrid_forward(self, F, x):
print('module F: ', F)
print('value x: ', x)
x = F.npx.relu(self.hidden(x))
print('result : ', x)
return self.output(x)
The code above implements a simple network with 4 hidden units and 2 outputs. hybrid_forward
takes an additional argument - the module F. This is needed since, depending on whether the code
has been hybridized or not, it will use a slightly different library (ndarray or symbol) for processing.
Both classes perform very similar functions and MXNet automatically determines the argument.
To understand what is going on we print the arguments as part of the function invocation.
net = HybridNet()
net.initialize()
x = np.random.normal(size=(1, 3))
net(x)
488 Chapter 12. Computational Performance
module F: <module 'mxnet.ndarray' from '/var/lib/jenkins/miniconda3/envs/d2l-en-release-0/
,!lib/python3.7/site-packages/mxnet/ndarray/__init__.py'>
value x: [[-0.6338663 0.40156594 0.46456942]]
result : [[0.01641375 0. 0. 0. ]]
array([[0.00097611, 0.00019453]])
Repeating the forward computation will lead to the same output (we omit details). Now let us see
what happens if we invoke the hybridize method.
net.hybridize()
net(x)
module F: <module 'mxnet.symbol' from '/var/lib/jenkins/miniconda3/envs/d2l-en-release-0/
,!lib/python3.7/site-packages/mxnet/symbol/__init__.py'>
value x: <_Symbol data>
result : <_Symbol hybridnet0_relu0>
array([[0.00097611, 0.00019453]])
Instead of using ndarray we now use the symbol module for F. Moreover, even though the input is
of ndarray type, the data flowing through the network is now converted to symbol type as part of
the compilation process. Repeating the function call leads to a surprising outcome:
net(x)
array([[0.00097611, 0.00019453]])
This is quite different from what we saw previously. All print statements, as defined in hybrid_
forward are omitted. Indeed, after hybridization the execution of net(x) does not involve
the Python interpreter any longer. This means that any spurious Python code is omitted (such
as print statements) in favor of a much more streamlined execution and better performance. Instead,
MXNet directly calls the C++ backend. Also note that some functions are not supported in
the symbol module (like asnumpy) and operations in-place like a += b and a[:] = a + b must
be rewritten as a = a + b. Nonetheless, compilation of models is worth the effort whenever
speed matters. The benefit can range from small percentage points to more than twice the speed,
depending on the complexity of the model, the speed of the CPU and the speed and number of
GPUs.
Summary
� Imperative programming makes it easy to design new models since it is possible to write
code with control flow and the ability to use a large amount of the Python software ecosystem.
� Symbolic programming requires that we specify the program and compile it before executing
it. The benefit is improved performance.
� MXNet is able to combine the advantages of both approaches as needed.
12.1. Compilers and Interpreters 489
� Models constructed by the HybridSequential and HybridBlock classes are able to convert
imperative programs into symbolic programs by calling the hybridize method.
Exercises
1. Design a network using the HybridConcurrent class. Alternatively look at Networks with Parallel
Concatenations (GoogLeNet) (page 274) for a network to compose.
2. Add x.asnumpy() to the first line of the hybrid_forward function of the HybridNet class in
this section. Execute the code and observe the errors you encounter. Why do they happen?
3. What happens if we add control flow, i.e., the Python statements if and for in the hybrid_
forward function?
4. Review the models that interest you in the previous chapters and use the HybridBlock class
or HybridSequential class to implement them.
Discussions149
12.2 Asynchronous Computation
Today?s computers are highly parallel systems, consisting of multiple CPU cores (often multiple
threads per core), multiple processing elements per GPU and often multiple GPUs per device. In
short, we can process many different things at the same time, often on different devices. Unfortunately
Python is not a great way of writing parallel and asynchronous code, at least not with
some extra help. After all, Python is single-threaded and this is unlikely to change in the future.
Deep learning frameworks such as MXNet and TensorFlow utilize an asynchronous programming
model to improve performance (PyTorch uses Python?s own scheduler leading to a different performance
trade-off). Hence, understanding how asynchronous programming works helps us to
develop more efficient programs, by proactively reducing computational requirements and mutual
dependencies. This allows us to reduce memory overhead and increase processor utilization.
We begin by importing the necessary libraries.
from d2l import mxnet as d2l
import numpy, os, subprocess
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
npx.set_np()
12.2.1 Asynchrony via Backend
For a warmup consider the following toy problem - we want to generate a random matrix and
multiply it. Let us do that both in NumPy and in MXNet NP to see the difference.
with d2l.Benchmark('numpy'):
for _ in range(10):
a = numpy.random.normal(size=(1000, 1000))
b = numpy.dot(a, a)
(continues on next page)
149 https://discuss.d2l.ai/t/360
490 Chapter 12. Computational Performance
(continued from previous page)
with d2l.Benchmark('mxnet.np'):
for _ in range(10):
a = np.random.normal(size=(1000, 1000))
b = np.dot(a, a)
numpy: 0.9055 sec
mxnet.np: 0.0041 sec
This is orders of magnitude faster. At least it seems to be so. Since both are executed on the same
processor something else must be going on. Forcing MXNet to finish all computation prior to
returning shows what happened previously: computation is being executed by the backend while
the frontend returns control to Python.
with d2l.Benchmark():
for _ in range(10):
a = np.random.normal(size=(1000, 1000))
b = np.dot(a, a)
npx.waitall()
Done: 0.6968 sec
Broadly speaking, MXNet has a frontend for direct interaction with the users, e.g., via Python,
as well as a backend used by the system to perform the computation. As shown in Fig. 12.2.1,
users can write MXNet programs in various frontend languages, such as Python, R, Scala, and C++.
Regardless of the frontend programming language used, the execution of MXNet programs occurs
primarily in the backend of C++ implementations. Operations issued by the frontend language are
passed on to the backend for execution. The backend manages its own threads that continuously
collect and execute queued tasks. Note that for this to work the backend must be able to keep track
of the dependencies between various steps in the computational graph. Hence, it is not possible
to parallelize operations that depend on each other.
Fig. 12.2.1: Programming Frontends.
Let us look at another toy example to understand the dependency graph a bit better.
x = np.ones((1, 2))
y = np.ones((1, 2))
(continues on next page)
12.2. Asynchronous Computation 491
(continued from previous page)
z = x * y + 2
z
array([[3., 3.]])
Fig. 12.2.2: Dependencies.
The code snippet above is also illustrated in Fig. 12.2.2. Whenever the Python frontend thread
executes one of the first three statements, it simply returns the task to the backend queue. When
the last statement?s results need to be printed, the Python frontend thread will wait for the C++
backend thread to finish computing result of the variable z. One benefit of this design is that the
Python frontend thread does not need to perform actual computations. Thus, there is little impact
on the program?s overall performance, regardless of Python?s performance. Fig. 12.2.3 illustrates
how frontend and backend interact.
Fig. 12.2.3: Frontend and Backend.
12.2.2 Barriers and Blockers
There are a number of operations that will force Python to wait for completion: * Most obviously
npx.waitall() waits until all computation has completed, regardless of when the compute instructions
were issued. In practice it is a bad idea to use this operator unless absolutely necessary
since it can lead to poor performance. * If we just want to wait until a specific variable is available
we can call z.wait_to_read(). In this case MXNet blocks return to Python until the variable z has
been computed. Other computation may well continue afterwards.
Let us see how this works in practice:
492 Chapter 12. Computational Performance
with d2l.Benchmark('waitall'):
b = np.dot(a, a)
npx.waitall()
with d2l.Benchmark('wait_to_read'):
b = np.dot(a, a)
b.wait_to_read()
waitall: 0.0043 sec
wait_to_read: 0.0043 sec
Both operations take approximately the same time to complete. Besides the obvious blocking operations
we recommend that the reader is aware of implicit blockers. Printing a variable clearly
requires the variable to be available and is thus a blocker. Lastly, conversions to NumPy via z.
asnumpy() and conversions to scalars via z.item() are blocking, since NumPy has no notion of
asynchrony. It needs access to the values just like the print function. Copying small amounts
of data frequently from MXNet?s scope to NumPy and back can destroy performance of an otherwise
efficient code, since each such operation requires the computational graph to evaluate all
intermediate results needed to get the relevant term before anything else can be done.
with d2l.Benchmark('numpy conversion'):
b = np.dot(a, a)
b.asnumpy()
with d2l.Benchmark('scalar conversion'):
b = np.dot(a, a)
b.sum().item()
numpy conversion: 0.0046 sec
scalar conversion: 0.0155 sec
12.2.3 Improving Computation
On a heavily multithreaded system (even regular laptops have 4 threads or more and on multisocket
servers this number can exceed 256) the overhead of scheduling operations can become
significant. This is why it is highly desirable to have computation and scheduling occur asynchronously
and in parallel. To illustrate the benefit of doing this let us see what happens if we
increment a variable by 1 multiple times, both in sequence or asynchronously. We simulate synchronous
execution by inserting a wait_to_read() barrier in between each addition.
with d2l.Benchmark('synchronous'):
for _ in range(1000):
y = x + 1
y.wait_to_read()
with d2l.Benchmark('asynchronous'):
for _ in range(1000):
y = x + 1
y.wait_to_read()
12.2. Asynchronous Computation 493
synchronous: 0.0624 sec
asynchronous: 0.0572 sec
A slightly simplified interaction between the Python frontend thread and the C++ backend thread
can be summarized as follows:
1. The frontend orders the backend to insert the calculation task y = x + 1 into the queue.
2. The backend then receives the computation tasks from the queue and performs the actual
computations.
3. The backend then returns the computation results to the frontend.
Assume that the durations of these three stages are t1; t2 and t3, respectively. If we do not use
asynchronous programming, the total time taken to perform 1000 computations is approximately
1000(t1 + t2 + t3). If asynchronous programming is used, the total time taken to perform 1000
computations can be reduced to t1 + 1000t2 + t3 (assuming 1000t2 > 999t1), since the frontend
does not have to wait for the backend to return computation results for each loop.
12.2.4 Improving Memory Footprint
Imagine a situation where we keep on inserting operations into the backend by executing Python
code on the frontend. For instance, the frontend might insert a large number of minibatch tasks
within a very short time. After all, if no meaningful computation happens in Python this can
be done quite quickly. If each of these tasks can be launched quickly at the same time this may
cause a spike in memory usage. Given a finite amount of memory available on GPUs (and even
on CPUs) this can lead to resource contention or even program crashes. Some readers might have
noticed that previous training routines made use of synchronization methods such as item or even
asnumpy.
We recommend to use these operations carefully, e.g., for each minibatch, such as to balance
computational efficiency and memory footprint. To illustrate what happens let us implement a
simple training loop for a deep network and measure its memory consumption and timing. Below
is the mock data generator and deep network.
def data_iter():
timer = d2l.Timer()
num_batches, batch_size = 150, 1024
for i in range(num_batches):
X = np.random.normal(size=(batch_size, 512))
y = np.ones((batch_size,))
yield X, y
if (i + 1) % 50 == 0:
print(f'batch {i + 1}, time {timer.stop():.4f} sec')
net = nn.Sequential()
net.add(nn.Dense(2048, activation='relu'),
nn.Dense(512, activation='relu'), nn.Dense(1))
net.initialize()
trainer = gluon.Trainer(net.collect_params(), 'sgd')
loss = gluon.loss.L2Loss()
Next we need a tool to measure the memory footprint of our code. We use a relatively primitive
ps call to accomplish this (note that the latter only works on Linux and MacOS). For a much more
494 Chapter 12. Computational Performance
detailed analysis of what is going on here use e.g., Nvidia?s Nsight150 or Intel?s vTune151.
def get_mem():
res = subprocess.check_output(['ps', 'u', '-p', str(os.getpid())])
return int(str(res).split()[15]) / 1e3
Before we can begin testing we need to initialize the parameters of the network and process one
batch. Otherwise it would be tricky to see what the additional memory consumption is. See Section
5.3 for further details related to initialization.
for X, y in data_iter():
break
loss(y, net(X)).wait_to_read()
To ensure that we do not overflow the task buffer on the backend we insert a wait_to_read call for
the loss function at the end of each loop. This forces the forward propagation to complete before
a new forward propagation is commenced. Note that a (possibly more elegant) alternative would
have been to track the loss in a scalar variable and to force a barrier via the item call.
mem = get_mem()
with d2l.Benchmark('time per epoch'):
for X, y in data_iter():
with autograd.record():
l = loss(y, net(X))
l.backward()
trainer.step(X.shape[0])
l.wait_to_read() # Barrier before a new batch
npx.waitall()
print(f'increased memory: {get_mem() - mem:f} MB')
batch 50, time 3.0298 sec
batch 100, time 6.0744 sec
batch 150, time 9.1045 sec
time per epoch: 9.1362 sec
increased memory: 5.200000 MB
As we see, the timing of the minibatches lines up quite nicely with the overall runtime of the optimization
code. Moreover, memory footprint only increases slightly. Now let us see what happens
if we drop the barrier at the end of each minibatch.
mem = get_mem()
with d2l.Benchmark('time per epoch'):
for X, y in data_iter():
with autograd.record():
l = loss(y, net(X))
l.backward()
trainer.step(X.shape[0])
npx.waitall()
print(f'increased memory: {get_mem() - mem:f} MB')
150 https://developer.nvidia.com/nsight-compute-2019_5
151 https://software.intel.com/en-us/vtune
12.2. Asynchronous Computation 495
batch 50, time 0.1171 sec
batch 100, time 0.2335 sec
batch 150, time 0.3488 sec
time per epoch: 9.1249 sec
increased memory: 2.040000 MB
Even though the time to issue instructions for the backend is an order of magnitude smaller, we
still need to perform computation. Consequently a large amount of intermediate results cannot
be released and may pile up in memory. While this didn?t cause any issues in the toy example
above, it might well have resulted in out of memory situations when left unchecked in real world
scenarios.
Summary
� MXNet decouples the Python frontend from an execution backend. This allows for fast asynchronous
insertion of commands into the backend and associated parallelism.
� Asynchrony leads to a rather responsive frontend. However, use caution not to overfill the
task queue since it may lead to excessive memory consumption.
� It is recommended to synchronize for each minibatch to keep frontend and backend approximately
synchronized.
� Be aware of the fact that conversions from MXNet?s memory management to Python will
force the backend to wait until the specific variable is ready. print, asnumpy and item all have
this effect. This can be desirable but a carless use of synchronization can ruin performance.
� Chip vendors offer sophisticated performance analysis tools to obtain a much more finegrained
insight into the efficiency of deep learning.
Exercises
1. We mentioned above that using asynchronous computation can reduce the total amount of
time needed to perform 1000 computations to t1 + 1000t2 + t3. Why do we have to assume
1000t2 > 999t1 here?
2. How would you need to modify the training loop if you wanted to have an overlap of one
minibatch each? I.e., if you wanted to ensure that batch bt finishes before batch bt+2 commences?
3. What might happen if we want to execute code on CPUs and GPUs simultaneously? Should
you still insist on synchronizing after every minibatch has been issued?
4. Measure the difference between waitall and wait_to_read. Hint: perform a number of
instructions and synchronize for an intermediate result.
Discussions152
152 https://discuss.d2l.ai/t/361
496 Chapter 12. Computational Performance
12.3 Automatic Parallelism
MXNet automatically constructs computational graphs at the backend. Using a computational
graph, the system is aware of all the dependencies, and can selectively execute multiple noninterdependent
tasks in parallel to improve speed. For instance, Fig. 12.2.2 in Section 12.2 initializes
two variables independently. Consequently the system can choose to execute them in
parallel.
Typically, a single operator will use all the computational resources on all CPUs or on a single
GPU. For example, the dot operator will use all cores (and threads) on all CPUs, even if there
are multiple CPU processors on a single machine. The same applies to a single GPU. Hence parallelization
is not quite so useful single-device computers. With multiple devices things matter
more. While parallelization is typically most relevant between multiple GPUs, adding the local
CPU will increase performance slightly. See e.g., (Hadjis et al., 2016) for a paper that focuses on
training computer vision models combining a GPU and a CPU. With the convenience of an automatically
parallelizing framework we can accomplish the same goal in a few lines of Python code.
More broadly, our discussion of automatic parallel computation focuses on parallel computation
using both CPUs and GPUs, as well as the parallelization of computation and communication. We
begin by importing the required packages and modules. Note that we need at least one GPU to
run the experiments in this section.
from d2l import mxnet as d2l
from mxnet import np, npx
npx.set_np()
12.3.1 Parallel Computation on CPUs and GPUs
Let us start by defining a reference workload to test - the run function below performs 10 matrixmatrix
multiplications on the device of our choosing using data allocated into two variables, x_cpu
and x_gpu.
def run(x):
return [x.dot(x) for _ in range(10)]
x_cpu = np.random.uniform(size=(2000, 2000))
x_gpu = np.random.uniform(size=(6000, 6000), ctx=d2l.try_gpu())
Now we apply the function to the data. To ensure that caching does not play a role in the results
we warm up the devices by performing a single pass on each of them prior to measuring.
run(x_cpu) # Warm-up both devices
run(x_gpu)
npx.waitall()
with d2l.Benchmark('CPU time'):
run(x_cpu)
npx.waitall()
with d2l.Benchmark('GPU time'):
run(x_gpu)
npx.waitall()
12.3. Automatic Parallelism 497
CPU time: 0.2868 sec
GPU time: 0.3007 sec
If we remove the waitall() between both tasks the system is free to parallelize computation on
both devices automatically.
with d2l.Benchmark('CPU & GPU'):
run(x_cpu)
run(x_gpu)
npx.waitall()
CPU & GPU: 0.3005 sec
In the above case the total execution time is less than the sum of its parts, since MXNet automatically
schedules computation on both CPU and GPU devices without the need for sophisticated
code on behalf of the user.
12.3.2 Parallel Computation and Communication
In many cases we need to move data between different devices, say between CPU and GPU, or between
different GPUs. This occurs e.g., when we want to perform distributed optimization where
we need to aggregate the gradients over multiple accelerator cards. Let us simulate this by computing
on the GPU and then copying the results back to the CPU.
def copy_to_cpu(x):
return [y.copyto(npx.cpu()) for y in x]
with d2l.Benchmark('Run on GPU'):
y = run(x_gpu)
npx.waitall()
with d2l.Benchmark('Copy to CPU'):
y_cpu = copy_to_cpu(y)
npx.waitall()
Run on GPU: 0.3038 sec
Copy to CPU: 1.0119 sec
This is somewhat inefficient. Note that we could already start copying parts of y to the CPU while
the remainder of the list is still being computed. This situation occurs, e.g., when we compute the
(backprop) gradient on a minibatch. The gradients of some of the parameters will be available
earlier than that of others. Hence it works to our advantage to start using PCI-Express bus bandwidth
while the GPU is still running. Removing waitall between both parts allows us to simulate
this scenario.
with d2l.Benchmark('Run on GPU and copy to CPU'):
y = run(x_gpu)
y_cpu = copy_to_cpu(y)
npx.waitall()
498 Chapter 12. Computational Performance
Run on GPU and copy to CPU: 1.0859 sec
The total time required for both operations is (as expected) significantly less than the sum of their
parts. Note that this task is different from parallel computation as it uses a different resource: the
bus between CPU and GPUs. In fact, we could compute on both devices and communicate, all at
the same time. As noted above, there is a dependency between computation and communication:
y[i] must be computed before it can be copied to the CPU. Fortunately, the system can copy y[i-1]
while computing y[i] to reduce the total running time.
We conclude with an illustration of the computational graph and its dependencies for a simple
two-layer MLP when training on a CPU and two GPUs, as depicted in Fig. 12.3.1. It would be quite
painful to schedule the parallel program resulting from this manually. This is where it is advantageous
to have a graph based compute backend for optimization.
Fig. 12.3.1: Two layer MLP on a CPU and 2 GPUs.
12.3. Automatic Parallelism 499
Summary
� Modern systems have a variety of devices, such as multiple GPUs and CPUs. They can be
used in parallel, asynchronously.
� Modern systems also have a variety of resources for communication, such as PCI Express,
storage (typically SSD or via network), and network bandwidth. They can be used in parallel
for peak efficiency.
� The backend can improve performance through through automatic parallel computation
and communication.
Exercises
1. 10 operations were performed in the run function defined in this section. There are no dependencies
between them. Design an experiment to see if MXNet will automatically execute
them in parallel.
2. When the workload of an individual operator is sufficiently small, parallelization can help
even on a single CPU or GPU. Design an experiment to verify this.
3. Design an experiment that uses parallel computation on CPU, GPU and communication between
both devices.
4. Use a debugger such as NVIDIA?s Nsight to verify that your code is efficient.
5. Designing computation tasks that include more complex data dependencies, and run experiments
to see if you can obtain the correct results while improving performance.
Discussions153
12.4 Hardware
Building systems with great performance requires a good understanding of the algorithms and
models to capture the statistical aspects of the problem. At the same time it is also indispensable
to have at least a modicum of knowledge of the underlying hardware. The current section is no
substitute for a proper course on hardware and systems design. Instead, it might serve as a starting
point for understanding why some algorithms are more efficient than others and how to achieve
good throughput. Good design can easily make a difference of an order of magnitude and, in turn,
this can make the difference between being able to train a network (e.g., in a week) or not at all (in
3 months, thus missing the deadline). We will start by looking at computers. Then we will zoom in
to look more carefully at CPUs and GPUs. Lastly we zoom out to review how multiple computers
are connected in a server center or in the cloud. This is not a GPU purchase guide. For this review
Section 19.5. An introduction to cloud computing with AWS can be found in Section 19.3.
Impatient readers may be able to get by with Fig. 12.4.1. It is taken from Colin Scott?s interactive
post154 which gives a good overview of the progress over the past decade. The original numbers are
due to Jeff Dean?s Stanford talk from 2010155. The discussion below explains some of the rationale
for these numbers and how they can guide us in designing algorithms. The discussion below is
153 https://discuss.d2l.ai/t/362
154 https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html
155 https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf
500 Chapter 12. Computational Performance
very high level and cursory. It is clearly no substitute for a proper course but rather just meant to
provide enough information for a statistical modeler to make suitable design decisions. For an
in-depth overview of computer architecture we refer the reader to (Hennessy & Patterson, 2011)
or a recent course on the subject, such as the one by Arste Asanovic156.
Fig. 12.4.1: Latency Numbers every Programmer should know.
12.4.1 Computers
Most deep learning researchers have access to a computer with a fair amount of memory, compute,
some form of an accelerator such as a GPU, or multiples thereof. It consists of several key
components:
� A processor, also referred to as CPU which is able to execute the programs we give it (in
addition to running an operating system and many other things), typically consisting of 8 or
more cores.
� Memory (RAM) to store and retrieve the results from computation, such as weight vectors,
activations, often training data.
� An Ethernet network connection (sometimes multiple) with speeds ranging from 1Gbit/s to
100Gbit/s (on high end servers more advanced interconnects can be found).
� A high speed expansion bus (PCIe) to connect the system to one or more GPUs. Servers have
up to 8 accelerators, often connected in an advanced topology, desktop systems have 1-2,
depending on the budget of the user and the size of the power supply.
� Durable storage, such as a magnetic harddrive (HDD), solid state (SSD), in many cases connected
using the PCIe bus, provides efficient transfer of training data to the system and storage
of intermediate checkpoints as needed.
Fig. 12.4.2: Connectivity of components
156 http://inst.eecs.berkeley.edu/~cs152/sp19/
12.4. Hardware 501
As Fig. 12.4.2 indicates, most components (network, GPU, storage) are connected to the CPU across
the PCI Express bus. It consists of multiple lanes that are directly attached to the CPU. For instance
AMD?s Threadripper 3 has 64 PCIe 4.0 lanes, each of which is capable 16 Gbit/s data transfer in both
directions. The memory is directly attached to the CPU with a total bandwidth of up to 100 GB/s.
When we run code on a computer we need to shuffle data to the processors (CPU or GPU), perform
computation and then move the results off the processor back to RAM and durable storage. Hence,
in order to get good performance we need to make sure that this works seamlessly without any
one of the systems becoming a major bottleneck. For instance, if we cannot load images quickly
enough the processor will not have any work to do. Likewise, if we cannot move matrices quickly
enough to the CPU (or GPU), its processing elements will starve. Finally, if we want to synchronize
multiple computers across the network, the latter should not slow down computation. One option
is to interleave communication and computation. Let us have a look at the various components in
more detail.
12.4.2 Memory
At its most basic memory is used to store data that needs to be readily accessible. At present CPU
RAM is typically of the DDR4157 variety, offering 20-25GB/s bandwidth per module. Each module
has a 64 bit wide bus. Typically pairs of memory modules are used to allow for multiple channels.
CPUs have between 2 and 4 memory channels, i.e., they have between 40GB/s and 100GB/s peak
memory bandwidth. Often there are two banks per channel. For instance AMD?s Zen 3 Threadripper
has 8 slots.
While these numbers are impressive, indeed, they only tell part of the story. When we want to
read a portion from memory we first need to tell the memory module where the information can
be found. That is, we first need to send the address to RAM. Once this accomplished we can choose
to read just a single 64bit record or a long sequence of records. The latter is called burst read. In
a nutshell, sending an address to memory and setting up the transfer takes approximately 100ns
(details depend on the specific timing coefficients of the memory chips used), every subsequent
transfer takes only 0.2ns. In short, the first read is 500 times as expensive as subsequent ones! We
could perform up to 10; 000; 000 random reads per second. This suggests that we avoid random
memory access as far as possible and use burst reads (and writes) instead.
Matters are a bit more complex when we take into account that we have multiple banks. Each bank
can read memory largely independently. This means two things: the effective number of random
reads is up to 4x higher, provided that they are spread evenly across memory. It also means that
it is still a bad idea to perform random reads since burst reads are 4x faster, too. Secondly, due
to memory alignment to 64 bit boundaries it is a good idea to align any datastructures with the
same boundaries. Compilers do this pretty much automatically158 when the appropriate flags are
set. Curious readers are encouraged to review a lecture on DRAMs such as the one by Zeshan
Chishti159.
GPU memory is subject to even higher bandwidth requirements since they have many more processing
elements than CPUs. By and large there are two options to address them. One is to make
the memory bus significantly wider. For instance NVIDIA?s RTX 2080 Ti has a 352 bit wide bus.
This allows for much more information to be transferred at the same time. Secondly, GPUs use
specific high-performance memory. Consumer grade devices, such as NVIDIA?s RTX and Titan
157 https://en.wikipedia.org/wiki/DDR4_SDRAM
158 https://en.wikipedia.org/wiki/Data_structure_alignment
159 http://web.cecs.pdx.edu/~zeshan/ece585_lec5.pdf
502 Chapter 12. Computational Performance
series typically use GDDR6160 chips with over 500 GB/s aggregate bandwidth. An alternative is to
use HBM (high bandwidth memory) modules. They use a very different interface and connect
directly with GPUs on a dedicated silicon wafer. This makes them very expensive and their use is
typically limited to high end server chips, such as the NVIDIA Volta V100 series of accelerators.
Quite unsurprisingly GPU memory is much smaller than CPU memory due to its higher cost. For
our purposes, by and large their performance characteristics are similar, just a lot faster. We can
safely ignore the details for the purpose of this book. They only matter when tuning GPU kernels
for high throughput.
12.4.3 Storage
We saw that some of the key characteristics of RAM were bandwidth and latency. The same is true
for storage devices, just that the differences can be even more extreme.
Hard Disks have been in use for over half a century. In a nutshell they contain a number of spinning
platters with heads that can be positioned to read / write at any given track. High end end
disks hold up to 16TB on 9 platters. One of the key benefits of HDDs is that they are relatively inexpensive.
One of their many downsides are their typically catastrophic failure modes and their
relatively high read latency.
To understand the latter, consider the fact that HDDs spin at around 7,200 RPM. If they were much
faster they would shatter due to the centrifugal force exerted on the platters. This has a major
downside when it comes to accessing a specific sector on the disk: we need to wait until the platter
has rotated in position (we can move the heads but not accelerate the actual disks). Hence it can
take over 8ms until the requested data is available. A common way this is expressed is to say that
HDDs can operate at approximately 100 IOPs. This number has essentially remained unchanged
for the past two decades. Worse still, it is equally difficult to increase bandwidth (it is in the order
of 100-200 MB/s). After all, each head reads a track of bits, hence the bit rate only scales with
the square root of the information density. As a result HDDs are quickly becoming relegated to
archival storage and low-grade storage for very large datasets.
Solid State Drives use Flash memory to store information persistently. This allows for much faster
access to stored records. Modern SSDs can operate at 100,000 to 500,000 IOPs, i.e., up to 3 orders
of magnitude faster than HDDs. Furthermore, their bandwidth can reach 1-3GB/s, i.e., one order
of magnitude faster than HDDs. These improvements sound almost too good to be true. Indeed,
they come with a number of caveats, due to the way SSDs are designed.
� SSDs store information in blocks (256 KB or larger). They can only be written as a whole,
which takes significant time. Consequently bit-wise random writes on SSD have very poor
performance. Likewise, writing data in general takes significant time since the block has
to be read, erased and then rewritten with new information. By now SSD controllers and
firmware have developed algorithms to mitigate this. Nonetheless writes can be much
slower, in particular for QLC (quad level cell) SSDs. The key for improved performance is to
maintain a queue of operations, to prefer reads and to write in large blocks if possible.
� The memory cells in SSDs wear out relatively quickly (often already after a few thousand
writes). Wear-level protection algorithms are able to spread the degradation over many cells.
That said, it is not recommended to use SSDs for swap files or for large aggregations of logfiles.
� Lastly, the massive increase in bandwidth has forced computer designers to attach SSDs directly
to the PCIe bus. The drives capable of handling this, referred to as NVMe (Non Volatile
160 https://en.wikipedia.org/wiki/GDDR6_SDRAM
12.4. Hardware 503
Memory enhanced), can use up to 4 PCIe lanes. This amounts to up to 8GB/s on PCIe 4.0.
Cloud Storage provides a configurable range of performance. That is, the assignment of storage
to virtual machines is dynamic, both in terms of quantity and in terms speed, as chosen by the
user. We recommend that the user increase the provisioned number of IOPs whenever latency is
too high, e.g., during training with many small records.
12.4.4 CPUs
Central Processing Units (CPUs) are the centerpiece of any computer (as before we give a very
high level description focusing primarily on what matters for efficient deep learning models).
They consist of a number of key components: processor cores which are able to execute machine
code, a bus connecting them (the specific topology differs significantly between processor models,
generations and vendors), and caches to allow for higher bandwidth and lower latency memory
access than what is possible by reads from main memory. Lastly, almost all modern CPUs
contain vector processing units to aid with high performance linear algebra and convolutions, as
they are common in media processing and machine learning.
Fig. 12.4.3: Intel Skylake consumer quad-core CPU
Fig. 12.4.3 depicts an Intel Skylake consumer grade quad-core CPU. It has an integrated GPU,
caches, and a ringbus connecting the four cores. Peripherals (Ethernet, WiFi, Bluetooth, SSD
controller, USB, etc.) are either part of the chipset or directly attached (PCIe) to the CPU.
504 Chapter 12. Computational Performance
Microarchitecture
Each of the processor cores consists of a rather sophisticated set of components. While details
differ between generations and vendors, the basic functionality is pretty much standard. The
front end loads instructions and tries to predict which path will be taken (e.g., for control flow).
Instructions are then decoded from assembly code to microinstructions. Assembly code is often
not the lowest level code that a processor executes. Instead, complex instructions may be decoded
into a set of more lower level operations. These are then processed by the actual execution core.
Often the latter is capable of performing many operations simultaneously. For instance, the ARM
Cortex A77 core of Fig. 12.4.4 is able to perform up to 8 operations simultaneously.
Fig. 12.4.4: ARM Cortex A77 Microarchitecture Overview
This means that efficient programs might be able to perform more than one instruction per clock
cycle, provided that they can be carried out independently. Not all units are created equal. Some
specialize in integer instructions whereas others are optimized for floating point performance.
To increase throughput, the processor might also follow multiple codepaths simultaneously in a
branching instruction and then discard the results of the branches not taken. This is why branch
prediction units matter (on the frontend) such that only the most promising paths are pursued.
Vectorization
Deep learning is extremely compute hungry. Hence, to make CPUs suitable for machine learning,
one needs to perform many operations in one clock cycle. This is achieved via vector units. They
have different names: on ARM they are called NEON, on x86 the latest generation is referred to as
AVX2161 units. A common aspect is that they are able to perform SIMD (single instruction multiple
data) operations. Fig. 12.4.5 shows how 8 short integers can be added in one clock cycle on ARM.
161 https://en.wikipedia.org/wiki/Advanced_Vector_Extensions
12.4. Hardware 505
Fig. 12.4.5: 128 bit NEON vectorization
Depending on architecture choices, such registers are up to 512 bit long, allowing for the combination
of up to 64 pairs of numbers. For instance, we might be multiplying two numbers and adding
them to a third, which is also known as a fused multiply-add. Intel?s OpenVino162 uses these to
achieve respectable throughput for deep learning on server grade CPUs. Note, though, that this
number is entirely dwarved by what GPUs are capable of achieving. For instance, NVIDIA?s RTX
2080 Ti has 4,352 CUDA cores, each of which is capable of processing such an operation at any
time.
Cache
Consider the following situation: we have a modest CPU core with 4 cores as depicted in Fig. 12.4.3
above, running at 2GHz frequency. Moreover, let us assume that we have an IPC (instructions per
clock) count of 1 and that the units have AVX2 with 256bit width enabled. Let us furthermore assume
that at least one of the registers used for AVX2 operations needs to be retrieved from memory.
This means that the CPU consumes 4x256bit = 1kbit of data per clock cycle. Unless we are
able to transfer 2  109  128 = 256  109 bytes to the processor per second the processing elements
are going to starve. Unfortunately the memory interface of such a chip only supports 20-40 GB/s
data transfer, i.e., one order of magnitude less. The fix is to avoid loading new data from memory
as far as possible and rather to cache it locally on the CPU. This is where caches come in handy
(see this Wikipedia article163 for a primer). Commonly the following names / concepts are used:
� Registers are strictly speaking not part of the cache. They help stage instructions. That said,
CPU registers are memory locations that a CPU can access at clock speed without any delay
penalty. CPUs have tens of registers. It is up to the compiler (or programmer) to use registers
efficiently. For instance the C programming language has a register keyword.
� L1 caches are the first line of defense against high memory bandwidth requirements. L1
caches are tiny (typical sizes might be 32-64kB) and often split into data and instructions
caches. When data is found in the L1 cache access is very fast. If it cannot be found there,
the search progresses down the cache hierarchy.
� L2 caches are the next stop. Depending on architecture design and processor size they might
be exclusive. They might be accessible only by a given core or shared between multiple
cores. L2 caches are larger (typically 256-512kB per core) and slower than L1. Furthermore,
162 https://01.org/openvinotoolkit
163 https://en.wikipedia.org/wiki/Cache_hierarchy
506 Chapter 12. Computational Performance
to access something in L2 we first need to check to realize that the data is not in L1, which
adds a small amount of extra latency.
� L3 caches are shared between multiple cores and can be quite large. AMD?s Epyc 3 server
CPUs have a whopping 256MB of cache spread across multiple chiplets. More typical numbers
are in the 4-8MB range.
Predicting which memory elements will be needed next is one of the key optimization parameters
in chip design. For instance, it is advisable to traverse memory in a forward direction since
most caching algorithms will try to read ahead rather than backwards. Likewise, keeping memory
access patterns local is a good way of improving performance. Adding caches is a double-edge
sword. On one hand they ensure that the processor cores do not starve of data. At the same time
they increase chip size, using up area that otherwise could have been spent on increasing processing
power. Moreover, cache misses can be expensive. Consider the worst case scenario, depicted
in Fig. 12.4.6. A memory location is cached on processor 0 when a thread on processor 1 requests
the data. To obtain it, processor 0 needs to stop what it is doing, write the information back to
main memory and then let processor 1 read it from memory. During this operation both processors
wait. Quite potentially such code runs more slowly on multiple processors when compared to
an efficient single-processor implementation. This is one more reason for why there is a practical
limit to cache sizes (besides their physical size).
Fig. 12.4.6: False sharing (image courtesy of Intel)
12.4.5 GPUs and other Accelerators
It is not an exaggeration to claim that deep learning would not have been successful without GPUs.
By the same token, it is quite reasonable to argue that GPU manufacturers? fortunes have been
increased significantly due to deep learning. This co-evolution of hardware and algorithms has
led to a situation where for better or worse deep learning is the preferable statistical modeling
paradigm. Hence it pays to understand the specific benefits that GPUs and related accelerators
such as the TPU (Jouppi et al., 2017) offer.
Of note is a distinction that is often made in practice: accelerators are optimized either for training
or inference. For the latter we only need to compute the forward propagation in a network.
No storage of intermediate data is needed for backpropagation. Moreover, we may not need very
precise computation (FP16 or INT8 typically suffice). On the other hand, during training all intermediate
results need storing to compute gradients. Moreover, accumulating gradients requires
higher precision to avoid numerical underflow (or overflow). This means that FP16 (or mixed
12.4. Hardware 507
precision with FP32) is the minimum required. All of this necessitates faster and larger memory
(HBM2 vs. GDDR6) and more processing power. For instance, NVIDIA?s Turing164 T4 GPUs are
optimized for inference whereas the V100 GPUs are preferable for training.
Recall Fig. 12.4.5. Adding vector units to a processor core allowed us to increase throughput significantly
(in the example in the figure we were able to perform 16 operations simultaneously).
What if we added operations that optimized not just operations between vectors but also between
matrices? This strategy led to Tensor Cores (more on this shortly). Secondly, what if we added
many more cores? In a nutshell, these two strategies summarize the design decisions in GPUs.
Fig. 12.4.7 gives an overview over a basic processing block. It contains 16 integer and 16 floating
point units. In addition to that, two Tensor Cores accelerate a narrow subset of additional
operations relevant for deep learning. Each Streaming Multiprocessor (SM) consists of four such
blocks.
Fig. 12.4.7: NVIDIA Turing Processing Block (image courtesy of NVIDIA)
12 streaming multiprocessors are then grouped into graphics processing clusters which make up
the high-end TU102 processors. Ample memory channels and an L2 cache complement the setup.
Fig. 12.4.8 has the relevant details. One of the reasons for designing such a device is that individual
blocks can be added or removed as needed to allow for more compact chips and to deal with yield
issues (faulty modules might not be activated). Fortunately programming such devices is well
hidden from the casual deep learning researcher beneath layers of CUDA and framework code.
In particular, more than one of the programs might well be executed simultaneously on the GPU,
provided that there are available resources. Nonetheless it pays to be aware of the limitations of
the devices to avoid picking models that do not fit into device memory.
164 https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/
508 Chapter 12. Computational Performance
Fig. 12.4.8: NVIDIA Turing Architecture (image courtesy of NVIDIA)
A last aspect that is worth mentioning in more detail are TensorCores. They are an example of
a recent trend of adding more optimized circuits that are specifically effective for deep learning.
For instance, the TPU added a systolic array (Kung, 1988) for fast matrix multiplication. There
the design was to support a very small number (one for the first generation of TPUs) of large operations.
TensorCores are at the other end. They are optimized for small operations involving
between 4x4 and 16x16 matrices, depending on their numerical precision. Fig. 12.4.9 gives an
overview of the optimizations.
Fig. 12.4.9: NVIDIA TensorCores in Turing (image courtesy of NVIDIA)
Obviously when optimizing for computation we end up making certain compromises. One of
them is that GPUs are not very good at handling interrupts and sparse data. While there are notable
exceptions, such as Gunrock165 (Wang et al., 2016), the access pattern of sparse matrices and
vectors do not go well with the high bandwidth burst read operations where GPUs excel. Matching
both goals is an area of active research. See e.g., DGL166, a library tuned for deep learning on
165 https://github.com/gunrock/gunrock
166 http://dgl.ai
12.4. Hardware 509
graphs.
12.4.6 Networks and Buses
Whenever a single device is insufficient for optimization we need to transfer data to and from it
to synchronize processing. This is where networks and buses come in handy. We have a number
of design parameters: bandwidth, cost, distance and flexibility. On one end we have WiFi which
has a pretty good range, is very easy to use (no wires, after all), cheap but it offers comparatively
mediocre bandwidth and latency. No machine learning researcher within their right mind would
use it to build a cluster of servers. In what follows we focus on interconnects that are suitable for
deep learning.
� PCIe is a dedicated bus for very high bandwidth point to point connections (up to 16 Gbs on
PCIe 4.0) per lane. Latency is in the order of single-digit microseconds (5 ?s). PCIe links
are precious. Processors only have a limited number of them: AMD?s EPYC 3 has 128 lanes,
Intel?s Xeon has up to 48 lanes per chip; on desktop grade CPUs the numbers are 20 (Ryzen
9) and 16 (Core i9) respectively. Since GPUs have typically 16 lanes this limits the number of
GPUs that can connect to the CPU at full bandwidth. After all, they need to share the links
with other high bandwidth peripherals such as storage and Ethernet. Just like with RAM
access, large bulk transfers are preferable due to reduced packet overhead.
� Ethernet is the most commonly used way of connecting computers. While it is significantly
slower than PCIe, it is very cheap and resilient to install and covers much longer distances.
Typical bandwidth for low-grade servers is 1 GBit/s. Higher end devices (e.g., C5 instances167
in the cloud) offer between 10 and 100 GBit/s bandwidth. As in all previous cases data transmission
has significant overheads. Note that we almost never use raw Ethernet directly but
rather a protocol that is executed on top of the physical interconnect (such as UDP or TCP/IP).
This adds further overhead. Like PCIe, Ethernet is designed to connect two devices, e.g., a
computer and a switch.
� Switches allow us to connect multiple devices in a manner where any pair of them can
carry out a (typically full bandwidth) point to point connection simultaneously. For instance,
Ethernet switches might connect 40 servers at high cross-sectional bandwidth. Note
that switches are not unique to traditional computer networks. Even PCIe lanes can be
switched168. This occurs e.g., to connect a large number of GPUs to a host processor, as
is the case for the P2 instances169.
� NVLink is an alternative to PCIe when it comes to very high bandwidth interconnects. It
offers up to 300 Gbit/s data transfer rate per link. Server GPUs (Volta V100) have 6 links
whereas consumer grade GPUs (RTX 2080 Ti) have only one link, operating at a reduced 100
Gbit/s rate. We recommend to use NCCL170 to achieve high data transfer between GPUs.
167 https://aws.amazon.com/ec2/instance-types/c5/
168 https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches
169 https://aws.amazon.com/ec2/instance-types/p2/
170 https://github.com/NVIDIA/nccl
510 Chapter 12. Computational Performance
Summary
� Devices have overheads for operations. Hence it is important to aim for a small number of
large transfers rather than many small ones. This applies to RAM, SSDs, Networks and GPUs.
� Vectorization is key for performance. Make sure you are aware of the specific abilities of your
accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 operations, NVIDIA
Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA Turing shines at FP16, INT8
and INT4 operations.
� Numerical overflow due to small datatypes can be a problem during training (and to a lesser
extent during inference).
� Aliasing can significantly degrade performance. For instance, memory alignment on 64 bit
CPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea to keep
convolution sizes aligned e.g., to TensorCores.
� Match your algorithms to the hardware (memory footprint, bandwidth, etc.). Great speedup
(orders of magnitude) can be achieved when fitting the parameters into caches.
� We recommend that you sketch out the performance of a novel algorithm on paper before
verifying the experimental results. Discrepancies of an order-of-magnitude or more are reasons
for concern.
� Use profilers to debug performance bottlenecks.
� Training and inference hardware have different sweet spots in terms of price / performance.
12.4.7 More Latency Numbers
The summary in Table 12.4.1 and Table 12.4.2 are due to Eliot Eshelman171 who maintains an
updated version of the numbers as a GitHub Gist172.
Table 12.4.1: Common Latency Numbers.
Action Time Notes
L1 cache reference/hit 1.5 ns 4 cycles
Floating-point add/mult/FMA 1.5 ns 4 cycles
L2 cache reference/hit 5 ns 12 ~ 17 cycles
Branch mispredict 6 ns 15 ~ 20 cycles
L3 cache hit (unshared cache) 16 ns 42 cycles
L3 cache hit (shared in another core) 25 ns 65 cycles
Mutex lock/unlock 25 ns
L3 cache hit (modified in another core) 29 ns 75 cycles
L3 cache hit (on a remote CPU socket) 40 ns 100 ~ 300 cycles (40 ~ 116 ns)
QPI hop to a another CPU (per hop) 40 ns
64MB memory ref. (local CPU) 46 ns TinyMemBench on Broadwell E5-2690v4
64MB memory ref. (remote CPU) 70 ns TinyMemBench on Broadwell E5-2690v4
256MB memory ref. (local CPU) 75 ns TinyMemBench on Broadwell E5-2690v4
Intel Optane random write 94 ns UCSD Non-Volatile Systems Lab
continues on next page
171 https://gist.github.com/eshelman
172 https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646
12.4. Hardware 511
Table 12.4.1 � continued from previous page
Action Time Notes
256MB memory ref. (remote CPU) 120 ns TinyMemBench on Broadwell E5-2690v4
Intel Optane random read 305 ns UCSD Non-Volatile Systems Lab
Send 4KB over 100 Gbps HPC fabric 1 ?s MVAPICH2 over Intel Omni-Path
Compress 1KB with Google Snappy 3 ?s
Send 4KB over 10 Gbps ethernet 10 ?s
Write 4KB randomly to NVMe SSD 30 ?s DC P3608 NVMe SSD (QOS 99% is 500?s)
Transfer 1MB to/from NVLink GPU 30 ?s ~33GB/s on NVIDIA 40GB NVLink
Transfer 1MB to/from PCI-E GPU 80 ?s ~12GB/s on PCIe 3.0 x16 link
Read 4KB randomly from NVMe SSD 120 ?s DC P3608 NVMe SSD (QOS 99%)
Read 1MB sequentially from NVMe SSD 208 ?s ~4.8GB/s DC P3608 NVMe SSD
Write 4KB randomly to SATA SSD 500 ?s DC S3510 SATA SSD (QOS 99.9%)
Read 4KB randomly from SATA SSD 500 ?s DC S3510 SATA SSD (QOS 99.9%)
Round trip within same datacenter 500 ?s One-way ping is ~250?s
Read 1MB sequentially from SATA SSD 2 ms ~550MB/s DC S3510 SATA SSD
Read 1MB sequentially from disk 5 ms ~200MB/s server HDD
Random Disk Access (seek+rotation) 10 ms
Send packet CA->Netherlands->CA 150 ms
Table 12.4.2: Latency Numbers for NVIDIA Tesla GPUs.
Action Time Notes
GPU Shared Memory access 30 ns 30~90 cycles (bank conflicts add latency)
GPU Global Memory access 200 ns 200~800 cycles
Launch CUDA kernel on GPU 10 ?s Host CPU instructs GPU to start kernel
Transfer 1MB to/from NVLink GPU 30 ?s ~33GB/s on NVIDIA 40GB NVLink
Transfer 1MB to/from PCI-E GPU 80 ?s ~12GB/s on PCI-Express x16 link
Exercises
1. Write C code to test whether there is any difference in speed between accessing memory
aligned or misaligned relative to the external memory interface. Hint: be careful of caching
effects.
2. Test the difference in speed between accessing memory in sequence or with a given stride.
3. How could you measure the cache sizes on a CPU?
4. How would you lay out data across multiple memory channels for maximum bandwidth?
How would you lay it out if you had many small threads?
5. An enterprise class HDD is spinning at 10,000 rpm. What is the absolutely minimum time
an HDD needs to spend worst case before it can read data (you can assume that heads move
almost instantaneously)? Why are 2.5� HDDs becoming popular for commercial servers (relative
to 3.5� and 5.25� drives)?
6. Assume that an HDD manufacturer increases the storage density from 1 Tbit per square inch
to 5 Tbit per square inch. How much information can you store on a ring on a 2.5� HDD? Is
there a difference between the inner and outer tracks?
512 Chapter 12. Computational Performance
7. The AWS P2 instances have 16 K80 Kepler GPUs. Use lspci on a p2.16xlarge and a p2.8xlarge
instance to understand how the GPUs are connected to the CPUs. Hint: keep your eye out
for PCI PLX bridges.
8. Going from 8 bit to 16 bit datatypes increases the amount of silicon approximately by 4x.
Why? Why might NVIDIA have added INT4 operations to their Turing GPUs.
9. Given 6 high speed links between GPUs (such as for the Volta V100 GPUs), how would you
connect 8 of them? Look up the connectivity used in the P3.16xlarge servers.
10. How much faster is it to read forward through memory vs. reading backwards? Does this
number differ between different computers and CPU vendors? Why? Write C code and experiment
with it.
11. Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs need a
cache?
12. Measure the packet overhead when sending messages across the Ethernet. Look up the difference
between UDP and TCP/IP connections.
13. Direct Memory Access allows devices other than the CPU to write (and read) directly to
(from) memory. Why is this a good idea?
14. Look at the performance numbers for the Turing T4 GPU. Why does the performance ?only?
double as you go from FP16 to INT8 and INT4?
15. What is the shortest time it should take for a packet on a roundtrip between San Francisco
and Amsterdam? Hint: you can assume that the distance is 10,000km.
Discussions173
12.5 Training on Multiple GPUs
So far we discussed how to train models efficiently on CPUs and GPUs. We even showed how deep
learning frameworks such as MXNet (and TensorFlow) allow one to parallelize computation and
communication automatically between them in Section 12.3. Lastly, we showed in Section 5.6
how to list all available GPUs on a computer using nvidia-smi. What we did not discuss is how to
actually parallelize deep learning training (we omit any discussion of inference on multiple GPUs
here as it is a rather rarely used and advanced topic that goes beyond the scope of this book).
Instead, we implied in passing that one would somehow split the data across multiple devices and
make it work. The present section fills in the details and shows how to train a network in parallel
when starting from scratch. Details on how to take advantage of functionality in Gluon is relegated
to Section 12.6. We assume that the reader is familiar with minibatch SGD algorithms such as the
ones described in Section 11.5.
173 https://discuss.d2l.ai/t/363
12.5. Training on Multiple GPUs 513
12.5.1 Splitting the Problem
Let us start with a simple computer vision problem and a slightly archaic network, e.g., with multiple
layers of convolutions, pooling, and possibly a few dense layers in the end. That is, let us
start with a network that looks quite similar to LeNet (LeCun et al., 1998) or AlexNet (Krizhevsky
et al., 2012). Given multiple GPUs (2 if it is a desktop server, 4 on a g4dn.12xlarge, 8 on an AWS
p3.16xlarge, or 16 on a p2.16xlarge), we want to partition training in a manner as to achieve good
speedup while simultaneously benefitting from simple and reproducible design choices. Multiple
GPUs, after all, increase both memory and compute ability. In a nutshell, we have a number of
choices, given a minibatch of training data that we want to classify.
� We could partition the network layers across multiple GPUs. That is, each GPU takes as input
the data flowing into a particular layer, processes data across a number of subsequent layers
and then sends the data to the next GPU.
� This allows us to process data with larger networks when compared to what a single
GPU could handle.
� Memory footprint per GPU can be well controlled (it is a fraction of the total network
footprint)
� The interface between layers (and thus GPUs) requires tight synchronization. This can
be tricky, in particular if the computational workloads are not properly matched between
layers. The problem is exacerbated for large numbers of GPUs.
� The interface between layers requires large amounts of data transfer (activations, gradients).
This may overwhelm the bandwidth of the GPU buses.
� Compute intensive, yet sequential operations are nontrivial to partition. See e.g.,
(Mirhoseini et al., 2017) for a best effort in this regard. It remains a difficult problem
and it is unclear whether it is possible to achieve good (linear) scaling on nontrivial
problems. We do not recommend it unless there is excellent framework / OS support
for chaining together multiple GPUs.
� We could split the work required by individual layers. For instance, rather than computing
64 channels on a single GPU we could split up the problem across 4 GPUs, each of which
generate data for 16 channels. Likewise, for a dense layer we could split the number of
output neurons. Fig. 12.5.1 illustrates this design. The figure is taken from (Krizhevsky et al.,
2012) where this strategy was used to deal with GPUs that had a very small memory footprint
(2GB at the time).
� This allows for good scaling in terms of computation, provided that the number of channels
(or neurons) is not too small.
� Multiple GPUs can process increasingly larger networks since the memory available
scales linearly.
� We need a very large number of synchronization / barrier operations since each layer
depends on the results from all other layers.
� The amount of data that needs to be transferred is potentially even larger than when
distributing layers across GPUs. We do not recommend this approach due to its bandwidth
cost and complexity.
514 Chapter 12. Computational Performance
Fig. 12.5.1: Model parallelism in the original AlexNet design due to limited GPU memory.
� Lastly we could partition data across multiple GPUs. This way all GPUs perform the same
type of work, albeit on different observations. Gradients are aggregated between GPUs after
each minibatch.
� This is the simplest approach and it can be applied in any situation.
� Adding more GPUs does not allow us to train larger models.
� We only need to synchronize after each minibatch. That said, it is highly desirable to
start exchanging gradients parameters already while others are still being computed.
� Large numbers of GPUs lead to very large minibatch sizes, thus reducing training efficiency.
By and large, data parallelism is the most convenient way to proceed, provided that we have access
to GPUs with sufficiently large memory. See also (Li et al., 2014) for a detailed description of
partitioning for distributed training. GPU memory used to be a problem in the early days of deep
learning. By now this issue has been resolved for all but the most unusual cases. We focus on data
parallelism in what follows.
12.5.2 Data Parallelism
Assume that there are k GPUs on a machine. Given the model to be trained, each GPU will maintain
a complete set of model parameters independently. Training proceeds as follows (see Fig. 12.5.2
for details on data parallel training on two GPUs).
12.5. Training on Multiple GPUs 515
Fig. 12.5.2: Calculation of minibatch stochastic gradient using data parallelism and two GPUs.
� In any iteration of training, given a random minibatch, we split the examples in the batch
into k portions and distribute them evenly across the GPUs.
� Each GPU calculates loss and gradient of the model parameters based on the minibatch subset
it was assigned and the model parameters it maintains.
� The local gradients of each of the k GPUs are aggregated to obtain the current minibatch
stochastic gradient.
� The aggregate gradient is re-distributed to each GPU.
� Each GPU uses this minibatch stochastic gradient to update the complete set of model parameters
that it maintains.
A comparison of different ways of parallelization on multiple GPUs is depicted in Fig. 12.5.3. Note
that in practice we increase the minibatch size k-fold when training on k GPUs such that each GPU
has the same amount of work to do as if we were training on a single GPU only. On a 16 GPU server
this can increase the minibatch size considerably and we may have to increase the learning rate
accordingly. Also note that Section 7.5 needs to be adjusted (e.g., by keeping a separate batch
norm coefficient per GPU). In what follows we will use Section 6.6 as the toy network to illustrate
multi-GPU training. As always we begin by importing the relevant packages and modules.
516 Chapter 12. Computational Performance
Fig. 12.5.3: Parallelization on multiple GPUs. From left to right - original problem, network partitioning,
layer partitioning, data parallelism.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
npx.set_np()
12.5.3 A Toy Network
We use LeNet as introduced in Section 6.6. We define it from scratch to illustrate parameter exchange
and synchronization in detail.
# Initialize model parameters
scale = 0.01
W1 = np.random.normal(scale=scale, size=(20, 1, 3, 3))
b1 = np.zeros(20)
W2 = np.random.normal(scale=scale, size=(50, 20, 5, 5))
b2 = np.zeros(50)
W3 = np.random.normal(scale=scale, size=(800, 128))
b3 = np.zeros(128)
W4 = np.random.normal(scale=scale, size=(128, 10))
b4 = np.zeros(10)
params = [W1, b1, W2, b2, W3, b3, W4, b4]
# Define the model
def lenet(X, params):
h1_conv = npx.convolution(data=X, weight=params[0], bias=params[1],
kernel=(3, 3), num_filter=20)
h1_activation = npx.relu(h1_conv)
h1 = npx.pooling(data=h1_activation, pool_type='avg', kernel=(2, 2),
stride=(2, 2))
(continues on next page)
12.5. Training on Multiple GPUs 517
(continued from previous page)
h2_conv = npx.convolution(data=h1, weight=params[2], bias=params[3],
kernel=(5, 5), num_filter=50)
h2_activation = npx.relu(h2_conv)
h2 = npx.pooling(data=h2_activation, pool_type='avg', kernel=(2, 2),
stride=(2, 2))
h2 = h2.reshape(h2.shape[0], -1)
h3_linear = np.dot(h2, params[4]) + params[5]
h3 = npx.relu(h3_linear)
y_hat = np.dot(h3, params[6]) + params[7]
return y_hat
# Cross-entropy loss function
loss = gluon.loss.SoftmaxCrossEntropyLoss()
12.5.4 Data Synchronization
For efficient multi-GPU training we need two basic operations: firstly we need to have the ability
to distribute a list of parameters to multiple devices and to attach gradients (get_params). Without
parameters it is impossible to evaluate the network on a GPU. Secondly, we need the ability to sum
parameters across multiple devices, i.e., we need an allreduce function.
def get_params(params, device):
new_params = [p.copyto(device) for p in params]
for p in new_params:
p.attach_grad()
return new_params
Let us try it out by copying the model parameters of lenet to gpu(0).
new_params = get_params(params, d2l.try_gpu(0))
print('b1 weight:', new_params[1])
print('b1 grad:', new_params[1].grad)
b1 weight: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] @gpu(0)
b1 grad: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] @gpu(0)
Since we didn?t perform any computation yet, the gradient with regard to the bias weights is still 0.
Now let us assume that we have a vector distributed across multiple GPUs. The following allreduce
function adds up all vectors and broadcasts the result back to all GPUs. Note that for this to
work we need to copy the data to the device accumulating the results.
def allreduce(data):
for i in range(1, len(data)):
data[0][:] += data[i].copyto(data[0].ctx)
for i in range(1, len(data)):
data[0].copyto(data[i])
Let us test this by creating vectors with different values on different devices and aggregate them.
518 Chapter 12. Computational Performance
data = [np.ones((1, 2), ctx=d2l.try_gpu(i)) * (i + 1) for i in range(2)]
print('before allreduce:\n', data[0], '\n', data[1])
allreduce(data)
print('after allreduce:\n', data[0], '\n', data[1])
before allreduce:
[[1. 1.]] @gpu(0)
[[2. 2.]] @gpu(1)
after allreduce:
[[3. 3.]] @gpu(0)
[[3. 3.]] @gpu(1)
12.5.5 Distributing Data
We need a simple utility function to distribute a minibatch evenly across multiple GPUs. For instance,
on 2 GPUs we?d like to have half of the data to be copied to each of the GPUs. Since it is
more convenient and more concise, we use the built-in split and load function in Gluon (to try it
out on a 4  5 matrix).
data = np.arange(20).reshape(4, 5)
devices = [npx.gpu(0), npx.gpu(1)]
split = gluon.utils.split_and_load(data, devices)
print('input :', data)
print('load into', devices)
print('output:', split)
input : [[ 0. 1. 2. 3. 4.]
[ 5. 6. 7. 8. 9.]
[10. 11. 12. 13. 14.]
[15. 16. 17. 18. 19.]]
load into [gpu(0), gpu(1)]
output: [array([[0., 1., 2., 3., 4.],
[5., 6., 7., 8., 9.]], ctx=gpu(0)), array([[10., 11., 12., 13., 14.],
[15., 16., 17., 18., 19.]], ctx=gpu(1))]
For later reuse we define a split_batch function which splits both data and labels.
#@save
def split_batch(X, y, devices):
"""Split `X` and `y` into multiple devices."""
assert X.shape[0] == y.shape[0]
return (gluon.utils.split_and_load(X, devices),
gluon.utils.split_and_load(y, devices))
12.5. Training on Multiple GPUs 519
12.5.6 Training
Now we can implement multi-GPU training on a single minibatch. Its implementation is primarily
based on the data parallelism approach described in this section. We will use the auxiliary functions
we just discussed, allreduce and split_and_load, to synchronize the data among multiple
GPUs. Note that we do not need to write any specific code to achieve parallelism. Since the computational
graph does not have any dependencies across devices within a minibatch, it is executed
in parallel automatically.
def train_batch(X, y, device_params, devices, lr):
X_shards, y_shards = split_batch(X, y, devices)
with autograd.record(): # Loss is calculated separately on each GPU
losses = [loss(lenet(X_shard, device_W), y_shard)
for X_shard, y_shard, device_W in zip(
X_shards, y_shards, device_params)]
for l in losses: # Back Propagation is performed separately on each GPU
l.backward()
# Sum all gradients from each GPU and broadcast them to all GPUs
for i in range(len(device_params[0])):
allreduce([device_params[c][i].grad for c in range(len(devices))])
# The model parameters are updated separately on each GPU
for param in device_params:
d2l.sgd(param, lr, X.shape[0]) # Here, we use a full-size batch
Now, we can define the training function. It is slightly different from the ones used in the previous
chapters: we need to allocate the GPUs and copy all the model parameters to all devices. Obviously
each batch is processed using train_batch to deal with multiple GPUs. For convenience
(and conciseness of code) we compute the accuracy on a single GPU (this is inefficient since the
other GPUs are idle).
def train(num_gpus, batch_size, lr):
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
devices = [d2l.try_gpu(i) for i in range(num_gpus)]
# Copy model parameters to num_gpus GPUs
device_params = [get_params(params, d) for d in devices]
# num_epochs, times, acces = 10, [], []
num_epochs = 10
animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
timer = d2l.Timer()
for epoch in range(num_epochs):
timer.start()
for X, y in train_iter:
# Perform multi-GPU training for a single minibatch
train_batch(X, y, device_params, devices, lr)
npx.waitall()
timer.stop()
# Verify the model on GPU 0
animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(
lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))
print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
f'on {str(devices)}')
520 Chapter 12. Computational Performance
12.5.7 Experiment
Let us see how well this works on a single GPU. We use a batch size of 256 and a learning rate of
0.2.
train(num_gpus=1, batch_size=256, lr=0.2)
test acc: 0.85, 2.5 sec/epoch on [gpu(0)]
By keeping the batch size and learning rate unchanged and changing the number of GPUs to 2,
we can see that the improvement in test accuracy is roughly the same as in the results from the
previous experiment. In terms of the optimization algorithms, they are identical. Unfortunately
there is no meaningful speedup to be gained here: the model is simply too small; moreover we only
have a small dataset, where our slightly unsophisticated approach to implementing multi-GPU
training suffered from significant Python overhead. We will encounter more complex models and
more sophisticated ways of parallelization going forward. Let us see what happens nonetheless
for Fashion-MNIST.
train(num_gpus=2, batch_size=256, lr=0.2)
test acc: 0.86, 4.2 sec/epoch on [gpu(0), gpu(1)]
12.5. Training on Multiple GPUs 521
Summary
� There are multiple ways to split deep network training over multiple GPUs. We could split
them between layers, across layers, or across data. The former two require tightly choreographed
data transfers. Data parallelism is the simplest strategy.
� Data parallel training is straightforward. However, it increases the effective minibatch size
to be efficient.
� Data is split across multiple GPUs, each GPU executes its own forward and backward operation
and subsequently gradients are aggregated and results broadcast back to the GPUs.
� Large minibatches may require a slightly increased learning rate.
Exercises
1. When training on multiple GPUs, change the minibatch size from b to k  b, i.e., scale it up by
the number of GPUs.
2. Compare accuracy for different learning rates. How does it scale with the number of GPUs.
3. Implement a more efficient allreduce that aggregates different parameters on different GPUs
(why is this more efficient in the first place).
4. Implement multi-GPU test accuracy computation.
Discussions174
12.6 Concise Implementation for Multiple GPUs
Implementing parallelism from scratch for every new model is no fun. Moreover, there is significant
benefit in optimizing synchronization tools for high performance. In the following we will
show how to do this using Gluon. The math and the algorithms are the same as in Section 12.5. As
before we begin by importing the required modules (quite unsurprisingly you will need at least
two GPUs to run this notebook).
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
174 https://discuss.d2l.ai/t/364
522 Chapter 12. Computational Performance
12.6.1 A Toy Network
Let us use a slightly more meaningful network than LeNet from the previous section that?s still
sufficiently easy and quick to train. We pick a ResNet-18 variant (He et al., 2016a). Since the input
images are tiny we modify it slightly. In particular, the difference to Section 7.6 is that we use a
smaller convolution kernel, stride, and padding at the beginning. Moreover, we remove the maxpooling
layer.
#@save
def resnet18(num_classes):
"""A slightly modified ResNet-18 model."""
def resnet_block(num_channels, num_residuals, first_block=False):
blk = nn.Sequential()
for i in range(num_residuals):
if i == 0 and not first_block:
blk.add(d2l.Residual(
num_channels, use_1x1conv=True, strides=2))
else:
blk.add(d2l.Residual(num_channels))
return blk
net = nn.Sequential()
# This model uses a smaller convolution kernel, stride, and padding and
# removes the maximum pooling layer
net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
nn.BatchNorm(), nn.Activation('relu'))
net.add(resnet_block(64, 2, first_block=True),
resnet_block(128, 2),
resnet_block(256, 2),
resnet_block(512, 2))
net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
return net
12.6.2 Parameter Initialization and Logistics
The initialize method allows us to set initial defaults for parameters on a device of our choice.
For a refresher see Section 4.8. What is particularly convenient is that it also lets us initialize the
network on multiple devices simultaneously. Let us try how this works in practice.
net = resnet18(10)
# get a list of GPUs
devices = d2l.try_all_gpus()
# initialize the network on all of them
net.initialize(init=init.Normal(sigma=0.01), ctx=devices)
Using the split_and_load function introduced in the previous section we can divide a minibatch
of data and copy portions to the list of devices provided by the context variable. The network
object automatically uses the appropriate GPU to compute the value of the forward propagation.
As before we generate 4 observations and split them over the GPUs.
x = np.random.uniform(size=(4, 1, 28, 28))
x_shards = gluon.utils.split_and_load(x, devices)
net(x_shards[0]), net(x_shards[1])
12.6. Concise Implementation for Multiple GPUs 523
(array([[ 2.2610238e-06, 2.2045988e-06, -5.4046791e-06, 1.2869939e-06,
5.1373154e-06, -3.8298003e-06, 1.4338991e-07, 5.4683424e-06,
-2.8279187e-06, -3.9651113e-06],
[ 2.0698699e-06, 2.0084674e-06, -5.6382478e-06, 1.0498467e-06,
5.5506403e-06, -4.1065500e-06, 6.0830121e-07, 5.4521761e-06,
-3.7365019e-06, -4.1891644e-06]], ctx=gpu(0)),
array([[ 2.4629785e-06, 2.6015518e-06, -5.4362636e-06, 1.2938228e-06,
5.6387912e-06, -4.1360131e-06, 3.5758842e-07, 5.5125242e-06,
-3.1957325e-06, -4.2976335e-06],
[ 1.9431677e-06, 2.2600434e-06, -5.2698206e-06, 1.4807442e-06,
5.4830939e-06, -3.9678885e-06, 7.5754315e-08, 5.6764375e-06,
-3.2530229e-06, -4.0943960e-06]], ctx=gpu(1)))
Once data passes through the network, the corresponding parameters are initialized on the device
the data passed through. This means that initialization happens on a per-device basis. Since we
picked GPU 0 and GPU 1 for initialization, the network is initialized only there, and not on the
CPU. In fact, the parameters do not even exist on the device. We can verify this by printing out the
parameters and observing any errors that might arise.
weight = net[0].params.get('weight')
try:
weight.data()
except RuntimeError:
print('not initialized on cpu')
weight.data(devices[0])[0], weight.data(devices[1])[0]
not initialized on cpu
(array([[[ 0.01382882, -0.01183044, 0.01417866],
[-0.00319718, 0.00439528, 0.02562625],
[-0.00835081, 0.01387452, -0.01035946]]], ctx=gpu(0)),
array([[[ 0.01382882, -0.01183044, 0.01417866],
[-0.00319718, 0.00439528, 0.02562625],
[-0.00835081, 0.01387452, -0.01035946]]], ctx=gpu(1)))
Lastly let us replace the code to evaluate the accuracy by one that works in parallel across multiple
devices. This serves as a replacement of the evaluate_accuracy_gpu function from Section 6.6.
The main difference is that we split a batch before invoking the network. All else is essentially
identical.
#@save
def evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):
# Query the list of devices
devices = list(net.collect_params().values())[0].list_ctx()
metric = d2l.Accumulator(2) # num_corrected_examples, num_examples
for features, labels in data_iter:
X_shards, y_shards = split_f(features, labels, devices)
# Run in parallel
pred_shards = [net(X_shard) for X_shard in X_shards]
metric.add(sum(float(d2l.accuracy(pred_shard, y_shard)) for
pred_shard, y_shard in zip(
(continues on next page)
524 Chapter 12. Computational Performance
(continued from previous page)
pred_shards, y_shards)), labels.size)
return metric[0] / metric[1]
12.6.3 Training
As before, the training code needs to perform a number of basic functions for efficient parallelism:
� Network parameters need to be initialized across all devices.
� While iterating over the dataset minibatches are to be divided across all devices.
� We compute the loss and its gradient in parallel across devices.
� Losses are aggregated (by the trainer method) and parameters are updated accordingly.
In the end we compute the accuracy (again in parallel) to report the final value of the network.
The training routine is quite similar to implementations in previous chapters, except that we need
to split and aggregate data.
def train(num_gpus, batch_size, lr):
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
ctx = [d2l.try_gpu(i) for i in range(num_gpus)]
net.initialize(init=init.Normal(sigma=0.01), ctx=ctx, force_reinit=True)
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
timer, num_epochs = d2l.Timer(), 10
animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])
for epoch in range(num_epochs):
timer.start()
for features, labels in train_iter:
X_shards, y_shards = d2l.split_batch(features, labels, ctx)
with autograd.record():
losses = [loss(net(X_shard), y_shard) for X_shard, y_shard
in zip(X_shards, y_shards)]
for l in losses:
l.backward()
trainer.step(batch_size)
npx.waitall()
timer.stop()
animator.add(epoch + 1, (evaluate_accuracy_gpus(net, test_iter),))
print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '
f'on {str(ctx)}')
12.6. Concise Implementation for Multiple GPUs 525
12.6.4 Experiments
Let us see how this works in practice. As a warmup we train the network on a single GPU.
train(num_gpus=1, batch_size=256, lr=0.1)
test acc: 0.93, 13.1 sec/epoch on [gpu(0)]
Next we use 2 GPUs for training. Compared to LeNet the model for ResNet-18 is considerably
more complex. This is where parallelization shows its advantage. The time for computation is
meaningfully larger than the time for synchronizing parameters. This improves scalability since
the overhead for parallelization is less relevant.
train(num_gpus=2, batch_size=512, lr=0.2)
test acc: 0.93, 6.8 sec/epoch on [gpu(0), gpu(1)]
526 Chapter 12. Computational Performance
Summary
� Gluon provides primitives for model initialization across multiple devices by providing a
context list.
� Data is automatically evaluated on the devices where the data can be found.
� Take care to initialize the networks on each device before trying to access the parameters on
that device. Otherwise you will encounter an error.
� The optimization algorithms automatically aggregate over multiple GPUs.
Exercises
1. This section uses ResNet-18. Try different epochs, batch sizes, and learning rates. Use more
GPUs for computation. What happens if you try this on a p2.16xlarge instance with 16 GPUs?
2. Sometimes, different devices provide different computing power. We could use the GPUs
and the CPU at the same time. How should we divide the work? Is it worth the effort? Why?
Why not?
3. What happens if we drop npx.waitall()? How would you modify training such that you have
an overlap of up to two steps for parallelism?
Discussions175
12.7 Parameter Servers
As we move from single GPUs to multiple GPUs and then to multiple servers containing multiple
GPUs, possibly all spread out across multiple racks and network switches our algorithms for
distributed and parallel training need to become much more sophisticated. Details matter since
different interconnects have very different bandwidth (e.g., NVLink can offer up to 100GB/s across
6 links in an appropriate setting, PCIe 3.0 16x lanes offer 16GB/s while even high speed 100 GbE
Ethernet only amounts to 10GB/s). At the same time it is unreasonable to expect that a statistical
modeler be an expert in networking and systems.
The core idea of the parameter server was introduced in (Smola & Narayanamurthy, 2010) in the
context of distributed latent variable models. A description of the push and pull semantics then
followed in (Ahmed et al., 2012) and a description of the system and an open source library followed
in (Li et al., 2014). In the following we will motivate the components needed for efficiency.
175 https://discuss.d2l.ai/t/365
12.7. Parameter Servers 527
12.7.1 Data Parallel Training
Let us review the data parallel training approach to distributed training. We will use this to the
exclusion of all others in this section since it is significantly simpler to implement in practice.
There are virtually no use cases (besides deep learning on graphs) where any other strategy for
parallelism is preferred since GPUs have plenty of memory nowadays. Fig. 12.7.1 describes the
variant of data parallelism that we implemented in the previous section. The key aspect in it is
that the aggregation of gradients occurs on GPU0 before the updated parameters are rebroadcast
to all GPUs.
Fig. 12.7.1: Left: single GPU training; Right: a variant of multi-GPU training. It proceeds as follows.
(1) we compute loss and gradient, (2) all gradients are aggregated on one GPU, (3) parameter
update happens and the parameters are re-distributed to all GPUs.
In retrospect, the decision to aggregate on GPU0 seems rather ad-hoc. After all, we might just as
well aggregate on the CPU. In fact, we could even decide to aggregate some of the parameters on
one GPU and some others on another. Provided that the optimization algorithm supports this,
there is no real reason for why we could not. For instance, if we have four parameter vectors
v1; : : : ; v4 with associated gradients g1; : : : ; g4 we could aggregate the gradients on one GPU each.
gi =
?
j2GPUs
gij (12.7.1)
528 Chapter 12. Computational Performance
This reasoning seems arbitrary and frivolous. After all, the math is the same throughout. However,
we are dealing with real physical hardware where different buses have different bandwidth
as discussed in Section 12.4. Consider a real 4-way GPU server as described in Fig. 12.7.2. If it
is particularly well connected, it might have a 100 GbE network card. More typical numbers are
in the 1-10 GbE range with an effective bandwidth of 100MB/s to 1GB/s. Since the CPUs have too
few PCIe lanes to connect to all GPUs directly (e.g., consumer grade Intel CPUs have 24 lanes) we
need a multiplexer176. The bandwidth from the CPU on a 16x Gen3 link is 16GB/s. This is also the
speed at which each of the GPUs is connected to the switch. This means that it is more effective to
communicate between the devices.
Fig. 12.7.2: A 4-way GPU server.
For the sake of the argument let us assume that the gradients ?weight? 160MB. In this case it takes
30ms to send the gradients from all 3 remaining GPUs to the fourth one (each transfer takes 10ms
= 160MB / 16 GB/s). Add another 30ms to transmit the weight vectors back we arrive at a total of
60ms. If we send all data to the CPU we incur a penalty of 40ms since each of the four GPUs needs
to send the data to the CPU, yielding a total of 80ms. Lastly assume that we are able to split the
gradients into 4 parts of 40MB each. Now we can aggregate each of the parts on a different GPU
simultaneously since the PCIe switch offers a full-bandwidth operation between all links. Instead
of 30ms this takes 7.5ms, yielding a total of 15ms for a synchronization operation. In short, depending
on how we synchronize parameters the same operation can take anywhere from 15ms to
80ms. Fig. 12.7.3 depicts the different strategies for exchanging parameters.
176 https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches
12.7. Parameter Servers 529
Fig. 12.7.3: Synchronization strategies.
Note that we have yet another tool at our disposal when it comes to improving performance: in a
deep network it takes some time to compute all gradients from the top to the bottom. We can begin
synchronizing gradients for some parameter groups even while we are still busy computing them
for others (the technical details for that are somewhat involved). See e.g., (Sergeev & DelBalso,
2018) for details on how to do this in Horovod177.
12.7.2 Ring Synchronization
When it comes to synchronization on modern deep learning hardware we often encounter significantly
bespoke network connectivity. For instance, the AWS P3.16xlarge and NVIDIA DGX-2
instances share the connectivity structure of Fig. 12.7.4. Each GPU connects to a host CPU via a
PCIe link which operates at best at 16 GB/s. Additionally each GPU also has 6 NVLink connections,
each of which is capable of transferring 300 Gbit/s bidirectionally. This amounts to around 18 GB/s
per link per direction. In short, the aggregate NVLink bandwidth is significantly higher than the
PCIe bandwidth. The question is how to use it most efficiently.
177 https://github.com/horovod/horovod
530 Chapter 12. Computational Performance
Fig. 12.7.4: NVLink connectivity on 8GPU V100 servers (image courtesy of NVIDIA).
It turns out (Wang et al., 2018) that the optimal synchronization strategy is to decompose the network
into two rings and to use them to synchronize data directly. Fig. 12.7.5 illustrates that the
network can be decomposed into one ring (1-2-3-4-5-6-7-8-1) with double NVLink bandwidth and
into one (1-4-6-3-5-8-2-7-1) with regular bandwidth. Designing an efficient synchronization protocol
in this case is nontrivial.
12.7. Parameter Servers 531
Fig. 12.7.5: Decomposition of the NVLink network into two rings.
Consider the following thought experiment: given a ring of n compute nodes (or GPUs) we can
send gradients from the first to the second node. There it is added to the local gradient and sent
on to the third node, and so on. After n ?? 1 steps the aggregate gradient can be found in the lastvisited
node. That is, the time to aggregate gradients grows linearly with the number of nodes.
But if we do this the algorithm is quite inefficient. After all, at any time there is only one of the
nodes communicating. What if we broke the gradients into n chunks and started synchronizing
chunk i starting at node i. Since each chunk is of size 1/n the total time is now (n ?? 1)/n  1. In
other words, the time spent to aggregate gradients does not grow as we increase the size of the ring.
This is quite an astonishing result. Fig. 12.7.6 illustrates the sequence of steps on n = 4 nodes.
532 Chapter 12. Computational Performance
Fig. 12.7.6: Ring synchronization across 4 nodes. Each node starts transmitting parts of gradients
to its left neighbor until the assembled gradient can be found in its right neighbor.
If we use the same example of synchronizing 160MB across 8 V100 GPUs we arrive at approximately
2  160MB/(3  18GB/s)  6ms This is quite a bit better than using the PCIe bus, even though we are
now using 8 GPUs. Note that in practice these numbers are quite a bit worse, since deep learning
frameworks often fail to assemble communication into large burst transfers. Moreover, timing is
critical. Note that there is a common misconception that ring synchronization is fundamentally
different from other synchronization algorithms. The only difference is that the synchronization
path is somewhat more elaborate when compared to a simple tree.
12.7.3 Multi-Machine Training
Distributed training on multiple machines adds a further challenge: we need to communicate
with servers that are only connected across a comparatively lower bandwidth fabric which can be
over an order of magnitude slower in some cases. Synchronization across devices is tricky. After
all, different machines running training code will have subtly different speed. Hence we need to
synchronize them if we want to use synchronous distributed optimization. Fig. 12.7.7 illustrates
how distributed parallel training occurs.
1. A (different) batch of data is read on each machine, split across multiple GPUs and transferred
to GPU memory. There predictions and gradients are computed on each GPU batch
separately.
2. The gradients from all local GPUs are aggregated on one GPU (or alternatively parts of it are
aggregated over different GPUs.
3. The gradients are sent to the CPU.
4. The CPU sends the gradients to a central parameter server which aggregates all the gradients.
12.7. Parameter Servers 533
5. The aggregate gradients are then used to update the weight vectors and the updated weight
vectors are broadcast back to the individual CPUs.
6. The information is sent to one (or multiple) GPUs.
7. The updated weight vectors are spread across all GPUs.
Fig. 12.7.7: Multi-machine multi-GPU distributed parallel training.
Each of these operations seems rather straightforward. And, indeed, they can be carried out efficiently
within a single machine. Once we look at multiple machines, though, we can see that
the central parameter server becomes the bottleneck. After all, the bandwidth per server is limited,
hence for m workers the time it takes to send all gradients to the server is O(m). We can
break through this barrier by increasing the number of servers to n. At this point each server only
needs to store O(1/n) of the parameters, hence the total time for updates and optimization becomes
O(m/n). Matching both numbers yields constant scaling regardless of how many workers
we are dealing with. In practice we use the same machines both as workers and as servers. Fig.
12.7.8 illustrates the design. See also (Li et al., 2014) for details. In particular, ensuring that multiple
machines work without unreasonable delays is nontrivial. We omit details on barriers and
will only briefly touch on synchronous and asynchronous updates below.
534 Chapter 12. Computational Performance
Fig. 12.7.8: Top - a single parameter server is a bottleneck since its bandwidth is finite. Bottom -
multiple parameter servers store parts of the parameters with aggregate bandwidth.
12.7.4 (key,value) Stores
Implementing the steps required for distributed multi-GPU training in practice is nontrivial. In
particular, given the many different choices that we might encounter. This is why it pays to use a
common abstraction, namely that of a (key,value) store with redefined update semantics. Across
many servers and many GPUs the gradient computation can be defined as
gi =
?
k2workers
?
j2GPUs
gijk: (12.7.2)
The key aspect in this operation is that it is a commutative reduction, that is, it turns many vectors
into one and the order in which the operation is applied does not matter. This is great for our
purposes since we do not (need to) have fine grained control over when which gradient is received.
Note that it is possible for us to perform the reduction stagewise. Furthermore, note that this
operation is independent between blocks i pertaining to different parameters (and gradients).
This allows us to define the following two operations: push, which accumulates gradients, and
pull, which retrieves aggregate gradients. Since we have many different sets of gradients (after
all, we have many layers), we need to index the gradients with a key i. This similarity to (key,value)
stores, such as the one introduced in Dynamo (DeCandia et al., 2007) is not by coincidence. They,
12.7. Parameter Servers 535
too, satisfy many similar characteristics, in particular when it comes to distributing the parameters
across multiple servers.
� push(key, value) sends a particular gradient (the value) from a worker to a common storage.
There the parameter is aggregated, e.g., by summing it up.
� pull(key, value) retrieves an aggregate parameter from common storage, e.g., after combining
the gradients from all workers.
By hiding all the complexity about synchronization behind a simple push and pull operation we
can decouple the concerns of the statistical modeler who wants to be able to express optimization
in simple terms and the systems engineer who needs to deal with the complexity inherent in distributed
synchronization. In the next section we will experiment with such a (key,value) store in
practice.
Summary
� Synchronization needs to be highly adaptive to specific network infrastructure and connectivity
within a server. This can make a significant difference to the time it takes to synchronize.
� Ring-synchronization can be optimal for P3 and DGX-2 servers. For others possibly not so
much.
� A hierarchical synchronization strategy works well when adding multiple parameter servers
for increased bandwidth.
� Asynchronous communication (while computation is still ongoing) can improve performance.
Exercises
1. Can you increase the ring synchronization even further? Hint: you can send messages in
both directions.
2. Fully asynchronous. Some delays permitted?
3. Fault tolerance. How? What if we lose a server? Is this a problem?
4. Checkpointing
5. Tree aggregation. Can you do it faster?
6. Other reductions (commutative semiring).
Discussions178
178 https://discuss.d2l.ai/t/366
536 Chapter 12. Computational Performance
13 | Computer Vision
Many applications in the area of computer vision are closely related to our daily lives, now and in
the future, whether medical diagnostics, driverless vehicles, camera monitoring, or smart filters.
In recent years, deep learning technology has greatly enhanced computer vision systems? performance.
It can be said that the most advanced computer vision applications are nearly inseparable
from deep learning.
We have introduced deep learning models commonly used in the area of computer vision in the
chapter �Convolutional Neural Networks� and have practiced simple image classification tasks. In
this chapter, we will introduce image augmentation and fine tuning methods and apply them to
image classification. Then, we will explore various methods of object detection. After that, we
will learn how to use fully convolutional networks to perform semantic segmentation on images.
Then, we explain how to use style transfer technology to generate images that look like the cover of
this book. Finally, we will perform practice exercises on two important computer vision datasets
to review the content of this chapter and the previous chapters.
13.1 Image Augmentation
We mentioned that large-scale datasets are prerequisites for the successful application of deep
neural networks in Section 7.1. Image augmentation technology expands the scale of training
datasets by making a series of random changes to the training images to produce similar, but different,
training examples. Another way to explain image augmentation is that randomly changing
training examples can reduce a model?s dependence on certain properties, thereby improving its
capability for generalization. For example, we can crop the images in different ways, so that the
objects of interest appear in different positions, reducing the model?s dependence on the position
where objects appear. We can also adjust the brightness, color, and other factors to reduce
model?s sensitivity to color. It can be said that image augmentation technology contributed greatly
to the success of AlexNet. In this section, we will discuss this technology, which is widely used in
computer vision.
First, import the packages or modules required for the experiment in this section.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, image, init, np, npx
from mxnet.gluon import nn
npx.set_np()
537
13.1.1 Common Image Augmentation Method
In this experiment, we will use an image with a shape of 400  500 as an example.
d2l.set_figsize()
img = image.imread('../img/cat1.jpg')
d2l.plt.imshow(img.asnumpy());
Most image augmentation methods have a certain degree of randomness. To make it easier for
us to observe the effect of image augmentation, we next define the auxiliary function apply. This
function runs the image augmentation method aug multiple times on the input image img and
shows all results.
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
Y = [aug(img) for _ in range(num_rows * num_cols)]
d2l.show_images(Y, num_rows, num_cols, scale=scale)
Flipping and Cropping
Flipping the image left and right usually does not change the category of the object. This is one
of the earliest and most widely used methods of image augmentation. Next, we use the transforms
module to create the RandomFlipLeftRight instance, which introduces a 50% chance that
the image is flipped left and right.
apply(img, gluon.data.vision.transforms.RandomFlipLeftRight())
538 Chapter 13. Computer Vision
Flipping up and down is not as commonly used as flipping left and right. However, at least for this
example image, flipping up and down does not hinder recognition. Next, we create a RandomFlip-
TopBottom instance for a 50% chance of flipping the image up and down.
apply(img, gluon.data.vision.transforms.RandomFlipTopBottom())
In the example image we used, the cat is in the middle of the image, but this may not be the case
for all images. In Section 6.5, we explained that the pooling layer can reduce the sensitivity of the
convolutional layer to the target location. In addition, we can make objects appear at different
positions in the image in different proportions by randomly cropping the image. This can also
reduce the sensitivity of the model to the target position.
In the following code, we randomly crop a region with an area of 10% to 100% of the original
area, and the ratio of width to height of the region is randomly selected from between 0.5 and 2.
Then, the width and height of the region are both scaled to 200 pixels. Unless otherwise stated, the
random number between a and b in this section refers to a continuous value obtained by uniform
sampling in the interval [a; b].
shape_aug = gluon.data.vision.transforms.RandomResizedCrop(
(200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)
13.1. Image Augmentation 539
Changing the Color
Another augmentation method is changing colors. We can change four aspects of the image color:
brightness, contrast, saturation, and hue. In the example below, we randomly change the brightness
of the image to a value between 50% (1 ?? 0:5) and 150% (1 + 0:5) of the original image.
apply(img, gluon.data.vision.transforms.RandomBrightness(0.5))
Similarly, we can randomly change the hue of the image.
apply(img, gluon.data.vision.transforms.RandomHue(0.5))
540 Chapter 13. Computer Vision
We can also create a RandomColorJitter instance and set how to randomly change the brightness,
contrast, saturation, and hue of the image at the same time.
color_aug = gluon.data.vision.transforms.RandomColorJitter(
brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
apply(img, color_aug)
Overlying Multiple Image Augmentation Methods
In practice, we will overlay multiple image augmentation methods. We can overlay the different
image augmentation methods defined above and apply them to each image by using a Compose
instance.
augs = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.RandomFlipLeftRight(), color_aug, shape_aug])
apply(img, augs)
13.1. Image Augmentation 541
13.1.2 Using an Image Augmentation Training Model
Next, we will look at how to apply image augmentation in actual training. Here, we use the CIFAR-
10 dataset, instead of the Fashion-MNIST dataset we have been using. This is because the position
and size of the objects in the Fashion-MNIST dataset have been normalized, and the differences in
color and size of the objects in CIFAR-10 dataset are more significant. The first 32 training images
in the CIFAR-10 dataset are shown below.
d2l.show_images(gluon.data.vision.CIFAR10(
train=True)[0:32][0], 4, 8, scale=0.8);
In order to obtain definitive results during prediction, we usually only apply image augmentation
to the training example, and do not use image augmentation with random operations during prediction.
Here, we only use the simplest random left-right flipping method. In addition, we use
a ToTensor instance to convert minibatch images into the format required by MXNet, i.e., 32-bit
floating point numbers with the shape of (batch size, number of channels, height, width) and
value range between 0 and 1.
542 Chapter 13. Computer Vision
train_augs = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.RandomFlipLeftRight(),
gluon.data.vision.transforms.ToTensor()])
test_augs = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.ToTensor()])
Next, we define an auxiliary function to make it easier to read the image and apply image augmentation.
The transform_first function provided by Gluon?s dataset applies image augmentation to
the first element of each training example (image and label), i.e., the element at the top of the
image. For detailed descriptions of DataLoader, refer to Section 3.5.
def load_cifar10(is_train, augs, batch_size):
return gluon.data.DataLoader(
gluon.data.vision.CIFAR10(train=is_train).transform_first(augs),
batch_size=batch_size, shuffle=is_train,
num_workers=d2l.get_dataloader_workers())
Using a Multi-GPU Training Model
We train the ResNet-18 model described in Section 7.6 on the CIFAR-10 dataset. We will also apply
the methods described in Section 12.6 and use a multi-GPU training model.
Next, we define the training function to train and evaluate the model using multiple GPUs.
#@save
def train_batch_ch13(net, features, labels, loss, trainer, devices,
split_f=d2l.split_batch):
X_shards, y_shards = split_f(features, labels, devices)
with autograd.record():
pred_shards = [net(X_shard) for X_shard in X_shards]
ls = [loss(pred_shard, y_shard) for pred_shard, y_shard
in zip(pred_shards, y_shards)]
for l in ls:
l.backward()
# The True flag allows parameters with stale gradients, which is useful
# later (e.g., in fine-tuning BERT)
trainer.step(labels.shape[0], ignore_stale_grad=True)
train_loss_sum = sum([float(l.sum()) for l in ls])
train_acc_sum = sum(d2l.accuracy(pred_shard, y_shard)
for pred_shard, y_shard in zip(pred_shards, y_shards))
return train_loss_sum, train_acc_sum
#@save
def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus(), split_f=d2l.split_batch):
num_batches, timer = len(train_iter), d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs], ylim=[0, 1],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
# Store training_loss, training_accuracy, num_examples, num_features
metric = d2l.Accumulator(4)
(continues on next page)
13.1. Image Augmentation 543
(continued from previous page)
for i, (features, labels) in enumerate(train_iter):
timer.start()
l, acc = train_batch_ch13(
net, features, labels, loss, trainer, devices, split_f)
metric.add(l, acc, labels.shape[0], labels.size)
timer.stop()
if (i + 1) % (num_batches // 5) == 0:
animator.add(epoch + i / num_batches,
(metric[0] / metric[2], metric[1] / metric[3],
None))
test_acc = d2l.evaluate_accuracy_gpus(net, test_iter, split_f)
animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {metric[0] / metric[2]:.3f}, train acc '
f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
f'{str(devices)}')
Now, we can define the train_with_data_aug function to use image augmentation to train the
model. This function obtains all available GPUs and uses Adam as the optimization algorithm
for training. It then applies image augmentation to the training dataset, and finally calls the
train_ch13 function just defined to train and evaluate the model.
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10)
net.initialize(init=init.Xavier(), ctx=devices)
def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
train_iter = load_cifar10(True, train_augs, batch_size)
test_iter = load_cifar10(False, test_augs, batch_size)
loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': lr})
train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
Now we train the model using image augmentation of random flipping left and right.
train_with_data_aug(train_augs, test_augs, net)
loss 0.168, train acc 0.942, test acc 0.851
4811.5 examples/sec on [gpu(0), gpu(1)]
544 Chapter 13. Computer Vision
Summary
� Image augmentation generates random images based on existing training data to cope with
overfitting.
� In order to obtain definitive results during prediction, we usually only apply image augmentation
to the training example, and do not use image augmentation with random operations
during prediction.
� We can obtain classes related to image augmentation from Gluon?s transforms module.
Exercises
1. Train the model without using image augmentation: train_with_data_aug(no_aug,
no_aug). Compare training and testing accuracy when using and not using image augmentation.
Can this comparative experiment support the argument that image augmentation can
mitigate overfitting? Why?
2. Add different image augmentation methods in model training based on the CIFAR-10
dataset. Observe the implementation results.
3. With reference to the MXNet documentation, what other image augmentation methods are
provided in Gluon?s transforms module?
Discussions179
179 https://discuss.d2l.ai/t/367
13.1. Image Augmentation 545
13.2 Fine-Tuning
In earlier chapters, we discussed how to train models on the Fashion-MNIST training dataset,
which only has 60,000 images. We also described ImageNet, the most widely used large-scale
image dataset in the academic world, with more than 10 million images and objects of over 1000
categories. However, the size of datasets that we often deal with is usually larger than the first, but
smaller than the second.
Assume we want to identify different kinds of chairs in images and then push the purchase link
to the user. One possible method is to first find a hundred common chairs, take one thousand
different images with different angles for each chair, and then train a classification model on the
collected image dataset. Although this dataset may be larger than Fashion-MNIST, the number of
examples is still less than one tenth of ImageNet. This may result in the overfitting of the complicated
model applicable to ImageNet on this dataset. At the same time, because of the limited
amount of data, the accuracy of the final trained model may not meet the practical requirements.
In order to deal with the above problems, an obvious solution is to collect more data. However,
collecting and labeling data can consume a lot of time and money. For example, in order to collect
the ImageNet datasets, researchers have spent millions of dollars of research funding. Although,
recently, data collection costs have dropped significantly, the costs still cannot be ignored.
Another solution is to apply transfer learning to migrate the knowledge learned from the source
dataset to the target dataset. For example, although the images in ImageNet are mostly unrelated
to chairs, models trained on this dataset can extract more general image features that can help
identify edges, textures, shapes, and object composition. These similar features may be equally
effective for recognizing a chair.
In this section, we introduce a common technique in transfer learning: fine tuning. As shown in
Fig. 13.2.1, fine tuning consists of the following four steps:
1. Pre-train a neural network model, i.e., the source model, on a source dataset (e.g., the ImageNet
dataset).
2. Create a new neural network model, i.e., the target model. This replicates all model designs
and their parameters on the source model, except the output layer. We assume that these
model parameters contain the knowledge learned from the source dataset and this knowledge
will be equally applicable to the target dataset. We also assume that the output layer
of the source model is closely related to the labels of the source dataset and is therefore not
used in the target model.
3. Add an output layer whose output size is the number of target dataset categories to the target
model, and randomly initialize the model parameters of this layer.
4. Train the target model on a target dataset, such as a chair dataset. We will train the output
layer from scratch, while the parameters of all remaining layers are fine-tuned based on the
parameters of the source model.
546 Chapter 13. Computer Vision
Fig. 13.2.1: Fine tuning.
13.2.1 Hot Dog Recognition
Next, we will use a specific example for practice: hot dog recognition. We will fine-tune the ResNet
model trained on the ImageNet dataset based on a small dataset. This small dataset contains thousands
of images, some of which contain hot dogs. We will use the model obtained by fine tuning
to identify whether an image contains a hot dog.
First, import the packages and modules required for the experiment. Gluon?s model_zoo package
provides a common pre-trained model. If you want to get more pre-trained models for computer
vision, you can use the GluonCV Toolkit180.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn
import os
npx.set_np()
Obtaining the Dataset
The hot dog dataset we use was taken from online images and contains 1; 400 positive images containing
hot dogs and the same number of negative images containing other foods. 1; 000 images
of various classes are used for training and the rest are used for testing.
We first download the compressed dataset and get two folders hotdog/train and hotdog/test.
Both folders have hotdog and not-hotdog category subfolders, each of which has corresponding
image files.
180 https://gluon-cv.mxnet.io
13.2. Fine-Tuning 547
#@save
d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL+'hotdog.zip',
'fba480ffa8aa7e0febbb511d181409f899b9baa5')
data_dir = d2l.download_extract('hotdog')
Downloading ../data/hotdog.zip from http://d2l-data.s3-accelerate.amazonaws.com/hotdog.zip...
We create two ImageFolderDataset instances to read all the image files in the training dataset and
testing dataset, respectively.
train_imgs = gluon.data.vision.ImageFolderDataset(
os.path.join(data_dir, 'train'))
test_imgs = gluon.data.vision.ImageFolderDataset(
os.path.join(data_dir, 'test'))
The first 8 positive examples and the last 8 negative images are shown below. As you can see, the
images vary in size and aspect ratio.
hotdogs = [train_imgs[i][0] for i in range(8)]
not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]
d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);
During training, we first crop a random area with random size and random aspect ratio from the
image and then scale the area to an input with a height and width of 224 pixels. During testing,
we scale the height and width of images to 256 pixels, and then crop the center area with height
and width of 224 pixels to use as the input. In addition, we normalize the values of the three RGB
(red, green, and blue) color channels. The average of all values of the channel is subtracted from
each value and then the result is divided by the standard deviation of all values of the channel to
produce the output.
# We specify the mean and variance of the three RGB channels to normalize the
# image channel
normalize = gluon.data.vision.transforms.Normalize(
[0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
train_augs = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.RandomResizedCrop(224),
gluon.data.vision.transforms.RandomFlipLeftRight(),
gluon.data.vision.transforms.ToTensor(),
(continues on next page)
548 Chapter 13. Computer Vision
(continued from previous page)
normalize])
test_augs = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.Resize(256),
gluon.data.vision.transforms.CenterCrop(224),
gluon.data.vision.transforms.ToTensor(),
normalize])
Defining and Initializing the Model
We use ResNet-18, which was pre-trained on the ImageNet dataset, as the source model. Here, we
specify pretrained=True to automatically download and load the pre-trained model parameters.
The first time they are used, the model parameters need to be downloaded from the Internet.
pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)
The pre-trained source model instance contains two member variables: features and output.
The former contains all layers of the model, except the output layer, and the latter is the output
layer of the model. The main purpose of this division is to facilitate the fine tuning of the model
parameters of all layers except the output layer. The member variable output of source model is
given below. As a fully connected layer, it transforms ResNet?s final global average pooling layer
output into 1000 class output on the ImageNet dataset.
pretrained_net.output
Dense(512 -> 1000, linear)
We then build a new neural network to use as the target model. It is defined in the same way as the
pre-trained source model, but the final number of outputs is equal to the number of categories in
the target dataset. In the code below, the model parameters in the member variable features of
the target model instance finetune_net are initialized to model parameters of the corresponding
layer of the source model. Because the model parameters in features are obtained by pre-training
on the ImageNet dataset, it is good enough. Therefore, we generally only need to use small learning
rates to �fine-tune� these parameters. In contrast, model parameters in the member variable
output are randomly initialized and generally require a larger learning rate to learn from scratch.
Assume the learning rate in the Trainer instance is  and use a learning rate of 10 to update the
model parameters in the member variable output.
finetune_net = gluon.model_zoo.vision.resnet18_v2(classes=2)
finetune_net.features = pretrained_net.features
finetune_net.output.initialize(init.Xavier())
# The model parameters in output will be updated using a learning rate ten
# times greater
finetune_net.output.collect_params().setattr('lr_mult', 10)
13.2. Fine-Tuning 549
Fine Tuning the Model
We first define a training function train_fine_tuning that uses fine tuning so it can be called
multiple times.
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5):
train_iter = gluon.data.DataLoader(
train_imgs.transform_first(train_augs), batch_size, shuffle=True)
test_iter = gluon.data.DataLoader(
test_imgs.transform_first(test_augs), batch_size)
devices = d2l.try_all_gpus()
net.collect_params().reset_ctx(devices)
net.hybridize()
loss = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {
'learning_rate': learning_rate, 'wd': 0.001})
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices)
We set the learning rate in the Trainer instance to a smaller value, such as 0.01, in order to
fine-tune the model parameters obtained in pretraining. Based on the previous settings, we will
train the output layer parameters of the target model from scratch using a learning rate ten times
greater.
train_fine_tuning(finetune_net, 0.01)
loss 0.307, train acc 0.912, test acc 0.940
668.6 examples/sec on [gpu(0), gpu(1)]
For comparison, we define an identical model, but initialize all of its model parameters to random
values. Since the entire model needs to be trained from scratch, we can use a larger learning rate.
scratch_net = gluon.model_zoo.vision.resnet18_v2(classes=2)
scratch_net.initialize(init=init.Xavier())
train_fine_tuning(scratch_net, 0.1)
550 Chapter 13. Computer Vision
loss 0.390, train acc 0.828, test acc 0.861
706.3 examples/sec on [gpu(0), gpu(1)]
As you can see, the fine-tuned model tends to achieve higher precision in the same epoch because
the initial values of the parameters are better.
Summary
� Transfer learning migrates the knowledge learned from the source dataset to the target
dataset. Fine tuning is a common technique for transfer learning.
� The target model replicates all model designs and their parameters on the source model,
except the output layer, and fine-tunes these parameters based on the target dataset. In
contrast, the output layer of the target model needs to be trained from scratch.
� Generally, fine tuning parameters use a smaller learning rate, while training the output layer
from scratch can use a larger learning rate.
Exercises
1. Keep increasing the learning rate of finetune_net. How does the precision of the model
change?
2. Further tune the hyperparameters of finetune_net and scratch_net in the comparative experiment.
Do they still have different precisions?
3. Set the parameters in finetune_net.features to the parameters of the source model and do
not update them during training. What will happen? You can use the following code.
finetune_net.features.collect_params().setattr('grad_req', 'null')
4. In fact, there is also a �hotdog� class in the ImageNet dataset. Its corresponding weight parameter
at the output layer can be obtained by using the following code. How can we use
this parameter?
13.2. Fine-Tuning 551
weight = pretrained_net.output.weight
hotdog_w = np.split(weight.data(), 1000, axis=0)[713]
hotdog_w.shape
(1, 512)
Discussions181
13.3 Object Detection and Bounding Boxes
In the previous section, we introduced many models for image classification. In image classification
tasks, we assume that there is only one main target in the image and we only focus on how to
identify the target category. However, in many situations, there are multiple targets in the image
that we are interested in. We not only want to classify them, but also want to obtain their specific
positions in the image. In computer vision, we refer to such tasks as object detection (or object
recognition).
Object detection is widely used in many fields. For example, in self-driving technology, we need
to plan routes by identifying the locations of vehicles, pedestrians, roads, and obstacles in the
captured video image. Robots often perform this type of task to detect targets of interest. Systems
in the security field need to detect abnormal targets, such as intruders or bombs.
In the next few sections, we will introduce multiple deep learning models used for object detection.
Before that, we should discuss the concept of target location. First, import the packages and
modules required for the experiment.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import image, npx
npx.set_np()
Next, we will load the sample images that will be used in this section. We can see there is a dog
on the left side of the image and a cat on the right. They are the two main targets in this image.
d2l.set_figsize()
img = image.imread('../img/catdog.jpg').asnumpy()
d2l.plt.imshow(img);
181 https://discuss.d2l.ai/t/368
552 Chapter 13. Computer Vision
13.3.1 Bounding Box
In object detection, we usually use a bounding box to describe the target location. The bounding
box is a rectangular box that can be determined by the x and y axis coordinates in the upper-left
corner and the x and y axis coordinates in the lower-right corner of the rectangle. We will define
the bounding boxes of the dog and the cat in the image based on the coordinate information in the
above image. The origin of the coordinates in the above image is the upper left corner of the image,
and to the right and down are the positive directions of the x axis and the y axis, respectively.
# bbox is the abbreviation for bounding box
dog_bbox, cat_bbox = [60, 45, 378, 516], [400, 112, 655, 493]
We can draw the bounding box in the image to check if it is accurate. Before drawing the box, we
will define a helper function bbox_to_rect. It represents the bounding box in the bounding box
format of matplotlib.
#@save
def bbox_to_rect(bbox, color):
"""Convert bounding box to matplotlib format."""
# Convert the bounding box (top-left x, top-left y, bottom-right x,
# bottom-right y) format to matplotlib format: ((upper-left x,
# upper-left y), width, height)
return d2l.plt.Rectangle(
xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
fill=False, edgecolor=color, linewidth=2)
After loading the bounding box on the image, we can see that the main outline of the target is
basically inside the box.
fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));
13.3. Object Detection and Bounding Boxes 553
Summary
� In object detection, we not only need to identify all the objects of interest in the image, but
also their positions. The positions are generally represented by a rectangular bounding box.
Exercises
1. Find some images and try to label a bounding box that contains the target. Compare the
difference between the time it takes to label the bounding box and label the category.
Discussions182
13.4 Anchor Boxes
Object detection algorithms usually sample a large number of regions in the input image, determine
whether these regions contain objects of interest, and adjust the edges of the regions so as
to predict the ground-truth bounding box of the target more accurately. Different models may
use different region sampling methods. Here, we introduce one such method: it generates multiple
bounding boxes with different sizes and aspect ratios while centering on each pixel. These
bounding boxes are called anchor boxes. We will practice object detection based on anchor boxes
in the following sections.
First, import the packages or modules required for this section. Here, we have modified the printing
accuracy of NumPy. Because printing tensors actually calls the print function of NumPy, the
floating-point numbers in tensors printed in this section are more concise.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx
np.set_printoptions(2)
npx.set_np()
182 https://discuss.d2l.ai/t/369
554 Chapter 13. Computer Vision
13.4.1 Generating Multiple Anchor Boxes
Assume that the input image has a height of h and width of w. We generate anchor boxes with
different shapes centered on each pixel of the image. Assume the size is s 2 (0; 1], the aspect ratio
is r > 0, and the width and height of the anchor box are ws
p
r and hs/
p
r, respectively. When the
center position is given, an anchor box with known width and height is determined.
Below we set a set of sizes s1; : : : ; sn and a set of aspect ratios r1; : : : ; rm. If we use a combination
of all sizes and aspect ratios with each pixel as the center, the input image will have a total of
whnm anchor boxes. Although these anchor boxes may cover all ground-truth bounding boxes,
the computational complexity is often excessive. Therefore, we are usually only interested in a
combination containing s1 or r1 sizes and aspect ratios, that is:
(s1; r1); (s1; r2); : : : ; (s1; rm); (s2; r1); (s3; r1); : : : ; (sn; r1): (13.4.1)
That is, the number of anchor boxes centered on the same pixel is n+m??1. For the entire input
image, we will generate a total of wh(n + m ?? 1) anchor boxes.
The above method of generating anchor boxes has been implemented in the multibox_prior function.
We specify the input, a set of sizes, and a set of aspect ratios, and this function will return
all the anchor boxes entered.
img = image.imread('../img/catdog.jpg').asnumpy()
h, w = img.shape[0:2]
print(h, w)
X = np.random.uniform(size=(1, 3, h, w)) # Construct input data
Y = npx.multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
Y.shape
561 728
(1, 2042040, 4)
We can see that the shape of the returned anchor box variable y is (batch size, number of anchor
boxes, 4). After changing the shape of the anchor box variable y to (image height, image
width, number of anchor boxes centered on the same pixel, 4), we can obtain all the anchor boxes
centered on a specified pixel position. In the following example, we access the first anchor box
centered on (250, 250). It has four elements: the x; y axis coordinates in the upper-left corner and
the x; y axis coordinates in the lower-right corner of the anchor box. The coordinate values of the
x and y axis are divided by the width and height of the image, respectively, so the value range is
between 0 and 1.
boxes = Y.reshape(h, w, 5, 4)
boxes[250, 250, 0, :]
array([0.06, 0.07, 0.63, 0.82])
In order to describe all anchor boxes centered on one pixel in the image, we first define the
show_bboxes function to draw multiple bounding boxes on the image.
13.4. Anchor Boxes 555
#@save
def show_bboxes(axes, bboxes, labels=None, colors=None):
"""Show bounding boxes."""
def _make_list(obj, default_values=None):
if obj is None:
obj = default_values
elif not isinstance(obj, (list, tuple)):
obj = [obj]
return obj
labels = _make_list(labels)
colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)]
rect = d2l.bbox_to_rect(bbox.asnumpy(), color)
axes.add_patch(rect)
if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w'
axes.text(rect.xy[0], rect.xy[1], labels[i],
va='center', ha='center', fontsize=9, color=text_color,
bbox=dict(facecolor=color, lw=0))
As we just saw, the coordinate values of the x and y axis in the variable boxes have been divided
by the width and height of the image, respectively. When drawing images, we need to restore
the original coordinate values of the anchor boxes and therefore define the variable bbox_scale.
Now, we can draw all the anchor boxes centered on (250, 250) in the image. As you can see, the
blue anchor box with a size of 0.75 and an aspect ratio of 1 covers the dog in the image well.
d2l.set_figsize()
bbox_scale = np.array((w, h, w, h))
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
's=0.75, r=0.5'])
556 Chapter 13. Computer Vision
13.4.2 Intersection over Union
We just mentioned that the anchor box covers the dog in the image well. If the ground-truth
bounding box of the target is known, how can �well� here be quantified? An intuitive method
is to measure the similarity between anchor boxes and the ground-truth bounding box. We know
that the Jaccard index can measure the similarity between two sets. Given sets A and B, their
Jaccard index is the size of their intersection divided by the size of their union:
J(A; B) =
jA \ Bj
jA [ Bj : (13.4.2)
In fact, we can consider the pixel area of a bounding box as a collection of pixels. In this way,
we can measure the similarity of the two bounding boxes by the Jaccard index of their pixel sets.
When we measure the similarity of two bounding boxes, we usually refer the Jaccard index as
intersection over union (IoU), which is the ratio of the intersecting area to the union area of the
two bounding boxes, as shown in Fig. 13.4.1. The value range of IoU is between 0 and 1: 0 means
that there are no overlapping pixels between the two bounding boxes, while 1 indicates that the
two bounding boxes are equal.
Fig. 13.4.1: IoU is the ratio of the intersecting area to the union area of two bounding boxes.
For the remainder of this section, we will use IoU to measure the similarity between anchor boxes
and ground-truth bounding boxes, and between different anchor boxes.
13.4.3 Labeling Training Set Anchor Boxes
In the training set, we consider each anchor box as a training example. In order to train the object
detection model, we need to mark two types of labels for each anchor box: first, the category of the
target contained in the anchor box (category) and, second, the offset of the ground-truth bounding
box relative to the anchor box (offset). In object detection, we first generate multiple anchor boxes,
predict the categories and offsets for each anchor box, adjust the anchor box position according
to the predicted offset to obtain the bounding boxes to be used for prediction, and finally filter out
the prediction bounding boxes that need to be output.
We know that, in the object detection training set, each image is labelled with the location of the
ground-truth bounding box and the category of the target contained. After the anchor boxes are
generated, we primarily label anchor boxes based on the location and category information of
the ground-truth bounding boxes similar to the anchor boxes. So how do we assign ground-truth
bounding boxes to anchor boxes similar to them?
Assume that the anchor boxes in the image are A1;A2; : : : ;Ana and the ground-truth bounding
boxes are B1;B2; : : : ;Bnb and na  nb. Define matrix X 2 Rnanb , where element xij in the ith row
13.4. Anchor Boxes 557
and jth column is the IoU of the anchor box Ai to the ground-truth bounding box Bj . First, we find
the largest element in the matrix X and record the row index and column index of the element
as i1; j1. We assign the ground-truth bounding box Bj1 to the anchor box Ai1 . Obviously, anchor
box Ai1 and ground-truth bounding box Bj1 have the highest similarity among all the �anchor
box�ground-truth bounding box� pairings. Next, discard all elements in the i1th row and the j1th
column in the matrix X. Find the largest remaining element in the matrix X and record the row
index and column index of the element as i2; j2. We assign ground-truth bounding box Bj2 to
anchor box Ai2 and then discard all elements in the i2th row and the j2th column in the matrix X.
At this point, elements in two rows and two columns in the matrix X have been discarded.
We proceed until all elements in the nb column in the matrix X are discarded. At this time, we have
assigned a ground-truth bounding box to each of the nb anchor boxes. Next, we only traverse the
remaining na ?? nb anchor boxes. Given anchor box Ai, find the bounding box Bj with the largest
IoU with Ai according to the ith row of the matrix X, and only assign ground-truth bounding box
Bj to anchor box Ai when the IoU is greater than the predetermined threshold.
As shown in Fig. 13.4.2 (left), assuming that the maximum value in the matrix X is x23, we will
assign ground-truth bounding box B3 to anchor box A2. Then, we discard all the elements in row
2 and column 3 of the matrix, find the largest element x71 of the remaining shaded area, and assign
ground-truth bounding box B1 to anchor box A7. Then, as shown in :numref:fig_anchor_label
(middle), discard all the elements in row 7 and column 1 of the matrix, find the largest element x54
of the remaining shaded area, and assign ground-truth bounding boxB4 to anchor boxA5. Finally,
as shown in :numref:fig_anchor_label (right), discard all the elements in row 5 and column 4 of
the matrix, find the largest element x92 of the remaining shaded area, and assign ground-truth
bounding box B2 to anchor box A9. After that, we only need to traverse the remaining anchor
boxes of A1;A3;A4;A6;A8 and determine whether to assign ground-truth bounding boxes to the
remaining anchor boxes according to the threshold.
Fig. 13.4.2: Assign ground-truth bounding boxes to anchor boxes.
Now we can label the categories and offsets of the anchor boxes. If an anchor box A is assigned
ground-truth bounding box B, the category of the anchor box A is set to the category of B. And
the offset of the anchor box A is set according to the relative position of the central coordinates of
558 Chapter 13. Computer Vision
B and A and the relative sizes of the two boxes. Because the positions and sizes of various boxes
in the dataset may vary, these relative positions and relative sizes usually require some special
transformations to make the offset distribution more uniform and easier to fit. Assume the center
coordinates of anchor box A and its assigned ground-truth bounding box B are (xa; ya); (xb; yb),
the widths of A and B are wa;wb, and their heights are ha; hb, respectively. In this case, a common
technique is to label the offset of A as
(
xb??xa
wa
?? x
x
;
yb??ya
ha
?? y
y
;
log wb
wa
?? w
w
;
log hb
ha
?? h
h
)
; (13.4.3)
The default values of the constant are x = y = w = h = 0; x = y = 0:1; andw = h = 0:2.
If an anchor box is not assigned a ground-truth bounding box, we only need to set the category of
the anchor box to background. Anchor boxes whose category is background are often referred to
as negative anchor boxes, and the rest are referred to as positive anchor boxes.
Below we demonstrate a detailed example. We define ground-truth bounding boxes for the cat and
dog in the read image, where the first element is category (0 for dog, 1 for cat) and the remaining
four elements are the x; y axis coordinates at top-left corner and x; y axis coordinates at lowerright
corner (the value range is between 0 and 1). Here, we construct five anchor boxes to be
labeled by the coordinates of the upper-left corner and the lower-right corner, which are recorded
as A0; : : : ;A4, respectively (the index in the program starts from 0). First, draw the positions of
these anchor boxes and the ground-truth bounding boxes in the image.
ground_truth = np.array([[0, 0.1, 0.08, 0.52, 0.92],
[1, 0.55, 0.2, 0.9, 0.88]])
anchors = np.array([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
[0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
[0.57, 0.3, 0.92, 0.9]])
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);
We can label categories and offsets for anchor boxes by using the multibox_target function. This
function sets the background category to 0 and increments the integer index of the target category
from zero by 1 (1 for dog and 2 for cat). We add example dimensions to the anchor boxes and
ground-truth bounding boxes and construct random predicted results with a shape of (batch size,
number of categories including background, number of anchor boxes) by using the expand_dims
function.
13.4. Anchor Boxes 559
labels = npx.multibox_target(np.expand_dims(anchors, axis=0),
np.expand_dims(ground_truth, axis=0),
np.zeros((1, 3, 5)))
There are three items in the returned result, all of which are in the tensor format. The third item
is represented by the category labeled for the anchor box.
labels[2]
array([[0., 1., 2., 0., 2.]])
We analyze these labelled categories based on positions of anchor boxes and ground-truth bounding
boxes in the image. First, in all �anchor box�ground-truth bounding box� pairs, the IoU of
anchor box A4 to the ground-truth bounding box of the cat is the largest, so the category of anchor
box A4 is labeled as cat. Without considering anchor box A4 or the ground-truth bounding
box of the cat, in the remaining �anchor box�ground-truth bounding box� pairs, the pair with the
largest IoU is anchor box A1 and the ground-truth bounding box of the dog, so the category of
anchor box A1 is labeled as dog. Next, traverse the remaining three unlabeled anchor boxes. The
category of the ground-truth bounding box with the largest IoU with anchor box A0 is dog, but
the IoU is smaller than the threshold (the default is 0.5), so the category is labeled as background;
the category of the ground-truth bounding box with the largest IoU with anchor box A2 is cat and
the IoU is greater than the threshold, so the category is labeled as cat; the category of the groundtruth
bounding box with the largest IoU with anchor box A3 is cat, but the IoU is smaller than the
threshold, so the category is labeled as background.
The second item of the return value is a mask variable, with the shape of (batch size, four times
the number of anchor boxes). The elements in the mask variable correspond one-to-one with the
four offset values of each anchor box. Because we do not care about background detection, offsets
of the negative class should not affect the target function. By multiplying by element, the 0 in the
mask variable can filter out negative class offsets before calculating target function.
labels[1]
array([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
1., 1., 1., 1.]])
The first item returned is the four offset values labeled for each anchor box, with the offsets of
negative class anchor boxes labeled as 0.
labels[0]
array([[ 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.40e+00, 1.00e+01,
2.59e+00, 7.18e+00, -1.20e+00, 2.69e-01, 1.68e+00, -1.57e+00,
0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, -5.71e-01, -1.00e+00,
-8.94e-07, 6.26e-01]])
560 Chapter 13. Computer Vision
13.4.4 Bounding Boxes for Prediction
During model prediction phase, we first generate multiple anchor boxes for the image and then
predict categories and offsets for these anchor boxes one by one. Then, we obtain prediction
bounding boxes based on anchor boxes and their predicted offsets. When there are many anchor
boxes, many similar prediction bounding boxes may be output for the same target. To simplify
the results, we can remove similar prediction bounding boxes. A commonly used method is called
non-maximum suppression (NMS).
Let us take a look at how NMS works. For a prediction bounding box B, the model calculates
the predicted probability for each category. Assume the largest predicted probability is p, the
category corresponding to this probability is the predicted category of B. We also refer to p as
the confidence level of prediction bounding box B. On the same image, we sort the prediction
bounding boxes with predicted categories other than background by confidence level from high
to low, and obtain the list L. Select the prediction bounding box B1 with highest confidence level
from L as a baseline and remove all non-benchmark prediction bounding boxes with an IoU with
B1 greater than a certain threshold from L. The threshold here is a preset hyperparameter. At this
point, L retains the prediction bounding box with the highest confidence level and removes other
prediction bounding boxes similar to it. Next, select the prediction bounding box B2 with the
second highest confidence level from L as a baseline, and remove all non-benchmark prediction
bounding boxes with an IoU with B2 greater than a certain threshold from L. Repeat this process
until all prediction bounding boxes in L have been used as a baseline. At this time, the IoU of any
pair of prediction bounding boxes in L is less than the threshold. Finally, output all prediction
bounding boxes in the list L.
Next, we will look at a detailed example. First, construct four anchor boxes. For the sake of simplicity,
we assume that predicted offsets are all 0. This means that the prediction bounding boxes
are anchor boxes. Finally, we construct a predicted probability for each category.
anchors = np.array([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
[0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
offset_preds = np.array([0] * anchors.size)
cls_probs = np.array([[0] * 4, # Predicted probability for background
[0.9, 0.8, 0.7, 0.1], # Predicted probability for dog
[0.1, 0.2, 0.3, 0.9]]) # Predicted probability for cat
Print prediction bounding boxes and their confidence levels on the image.
fig = d2l.plt.imshow(img)
show_bboxes(fig.axes, anchors * bbox_scale,
['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])
13.4. Anchor Boxes 561
We use the multibox_detection function to perform NMS and set the threshold to 0.5. This adds an
example dimension to the tensor input. We can see that the shape of the returned result is (batch
size, number of anchor boxes, 6). The 6 elements of each row represent the output information
for the same prediction bounding box. The first element is the predicted category index, which
starts from 0 (0 is dog, 1 is cat). The value -1 indicates background or removal in NMS. The second
element is the confidence level of prediction bounding box. The remaining four elements are the
x; y axis coordinates of the upper-left corner and the x; y axis coordinates of the lower-right corner
of the prediction bounding box (the value range is between 0 and 1).
output = npx.multibox_detection(
np.expand_dims(cls_probs, axis=0),
np.expand_dims(offset_preds, axis=0),
np.expand_dims(anchors, axis=0),
nms_threshold=0.5)
output
array([[[ 0. , 0.9 , 0.1 , 0.08, 0.52, 0.92],
[ 1. , 0.9 , 0.55, 0.2 , 0.9 , 0.88],
[-1. , 0.8 , 0.08, 0.2 , 0.56, 0.95],
[-1. , 0.7 , 0.15, 0.3 , 0.62, 0.91]]])
We remove the prediction bounding boxes of category -1 and visualize the results retained by NMS.
fig = d2l.plt.imshow(img)
for i in output[0].asnumpy():
if i[0] == -1:
continue
label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
show_bboxes(fig.axes, [np.array(i[2:]) * bbox_scale], label)
562 Chapter 13. Computer Vision
In practice, we can remove prediction bounding boxes with lower confidence levels before performing
NMS, thereby reducing the amount of computation for NMS. We can also filter the output
of NMS, for example, by only retaining results with higher confidence levels as the final output.
Summary
� We generate multiple anchor boxes with different sizes and aspect ratios, centered on each
pixel.
� IoU, also called Jaccard index, measures the similarity of two bounding boxes. It is the ratio
of the intersecting area to the union area of two bounding boxes.
� In the training set, we mark two types of labels for each anchor box: one is the category of the
target contained in the anchor box and the other is the offset of the ground-truth bounding
box relative to the anchor box.
� When predicting, we can use non-maximum suppression (NMS) to remove similar prediction
bounding boxes, thereby simplifying the results.
Exercises
1. Change the sizes and ratios values in the multibox_prior function and observe the changes
to the generated anchor boxes.
2. Construct two bounding boxes with an IoU of 0.5, and observe their coincidence.
3. Verify the output of offset labels[0] by marking the anchor box offsets as defined in this
section (the constant is the default value).
4. Modify the variable anchors in the �Labeling Training Set Anchor Boxes� and �Output Bounding
Boxes for Prediction� sections. How do the results change?
Discussions183
183 https://discuss.d2l.ai/t/370
13.4. Anchor Boxes 563
13.5 Multiscale Object Detection
In Section 13.4, we generated multiple anchor boxes centered on each pixel of the input image.
These anchor boxes are used to sample different regions of the input image. However, if anchor
boxes are generated centered on each pixel of the image, soon there will be too many anchor boxes
for us to compute. For example, we assume that the input image has a height and a width of 561
and 728 pixels respectively. If five different shapes of anchor boxes are generated centered on
each pixel, over two million anchor boxes (5617285) need to be predicted and labeled on the
image.
It is not difficult to reduce the number of anchor boxes. An easy way is to apply uniform sampling
on a small portion of pixels from the input image and generate anchor boxes centered on the sampled
pixels. In addition, we can generate anchor boxes of varied numbers and sizes on multiple
scales. Notice that smaller objects are more likely to be positioned on the image than larger ones.
Here, we will use a simple example: Objects with shapes of 1  1, 1  2, and 2  2 may have 4, 2,
and 1 possible position(s) on an image with the shape 22. Therefore, when using smaller anchor
boxes to detect smaller objects, we can sample more regions; when using larger anchor boxes to
detect larger objects, we can sample fewer regions.
To demonstrate how to generate anchor boxes on multiple scales, let us read an image first. It has
a height and width of 561  728 pixels.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import image, np, npx
npx.set_np()
img = image.imread('../img/catdog.jpg')
h, w = img.shape[0:2]
h, w
(561, 728)
In Section 6.2, the 2D array output of the convolutional neural network (CNN) is called a feature
map. We can determine the midpoints of anchor boxes uniformly sampled on any image by defining
the shape of the feature map.
The function display_anchors is defined below. We are going to generate anchor boxes anchors
centered on each unit (pixel) on the feature map fmap. Since the coordinates of axes x and y in
anchor boxes anchors have been divided by the width and height of the feature map fmap, values
between 0 and 1 can be used to represent relative positions of anchor boxes in the feature map.
Since the midpoints of anchor boxes anchors overlap with all the units on feature map fmap, the
relative spatial positions of the midpoints of the anchors on any image must have a uniform distribution.
Specifically, when the width and height of the feature map are set to fmap_w and fmap_h
respectively, the function will conduct uniform sampling for fmap_h rows and fmap_w columns of
pixels and use them as midpoints to generate anchor boxes with size s (we assume that the length
of list s is 1) and different aspect ratios (ratios).
def display_anchors(fmap_w, fmap_h, s):
d2l.set_figsize()
# The values from the first two dimensions will not affect the output
(continues on next page)
564 Chapter 13. Computer Vision
(continued from previous page)
fmap = np.zeros((1, 10, fmap_w, fmap_h))
anchors = npx.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])
bbox_scale = np.array((w, h, w, h))
d2l.show_bboxes(d2l.plt.imshow(img.asnumpy()).axes,
anchors[0] * bbox_scale)
We will first focus on the detection of small objects. In order to make it easier to distinguish upon
display, the anchor boxes with different midpoints here do not overlap. We assume that the size
of the anchor boxes is 0.15 and the height and width of the feature map are 4. We can see that the
midpoints of anchor boxes from the 4 rows and 4 columns on the image are uniformly distributed.
display_anchors(fmap_w=4, fmap_h=4, s=[0.15])
We are going to reduce the height and width of the feature map by half and use a larger anchor
box to detect larger objects. When the size is set to 0.4, overlaps will occur between regions of
some anchor boxes.
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])
Finally, we are going to reduce the height and width of the feature map by half and increase the
anchor box size to 0.8. Now the midpoint of the anchor box is the center of the image.
13.5. Multiscale Object Detection 565
display_anchors(fmap_w=1, fmap_h=1, s=[0.8])
Since we have generated anchor boxes of different sizes on multiple scales, we will use them to
detect objects of various sizes at different scales. Now we are going to introduce a method based
on convolutional neural networks (CNNs).
At a certain scale, suppose we generate hw sets of anchor boxes with different midpoints based
on ci feature maps with the shape h  w and the number of anchor boxes in each set is a. For
example, for the first scale of the experiment, we generate 16 sets of anchor boxes with different
midpoints based on 10 (number of channels) feature maps with a shape of 4  4, and each set
contains 3 anchor boxes. Next, each anchor box is labeled with a category and offset based on
the classification and position of the ground-truth bounding box. At the current scale, the object
detection model needs to predict the category and offset of hw sets of anchor boxes with different
midpoints based on the input image.
We assume that the ci feature maps are the intermediate output of the CNN based on the input
image. Since each feature map has hw different spatial positions, the same position will have ci
units. According to the definition of receptive field in the Section 6.2, the ci units of the feature map
at the same spatial position have the same receptive field on the input image. Thus, they represent
the information of the input image in this same receptive field. Therefore, we can transform the ci
units of the feature map at the same spatial position into the categories and offsets of the a anchor
boxes generated using that position as a midpoint. It is not hard to see that, in essence, we use the
information of the input image in a certain receptive field to predict the category and offset of the
anchor boxes close to the field on the input image.
When the feature maps of different layers have receptive fields of different sizes on the input image,
they are used to detect objects of different sizes. For example, we can design a network to
have a wider receptive field for each unit in the feature map that is closer to the output layer, to
detect objects with larger sizes in the input image.
We will implement a multiscale object detection model in the following section.
566 Chapter 13. Computer Vision
Summary
� We can generate anchor boxes with different numbers and sizes on multiple scales to detect
objects of different sizes on multiple scales.
� The shape of the feature map can be used to determine the midpoint of the anchor boxes
that uniformly sample any image.
� We use the information for the input image from a certain receptive field to predict the category
and offset of the anchor boxes close to that field on the image.
Exercises
1. Given an input image, assume 1ci hw to be the shape of the feature map while ci; h;w
are the number, height, and width of the feature map. What methods can you think of to
convert this variable into the anchor box?s category and offset? What is the shape of the
output?
Discussions184
13.6 The Object Detection Dataset
There are no small datasets, like MNIST or Fashion-MNIST, in the object detection field. In order
to quickly test models, we are going to assemble a small dataset. First, we generate 1000 banana
images of different angles and sizes using free bananas from our office. Then, we collect a series
of background images and place a banana image at a random position on each image. We use the
im2rec tool185 provided by MXNet to convert the images to binary RecordIO format[1]. This format
can reduce the storage overhead of the dataset on the disk and improve the reading efficiency. If
you want to learn more about how to read images, refer to the documentation for the GluonCV
Toolkit186.
13.6.1 Downloading the Dataset
The banana detection dataset in RecordIO format can be downloaded directly from the Internet.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx
import os
npx.set_np()
#@save
d2l.DATA_HUB['bananas'] = (d2l.DATA_URL + 'bananas.zip',
'aadfd1c4c5d7178616799dd1801c9a234ccdaf19')
184 https://discuss.d2l.ai/t/371
185 https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py
186 https://gluon-cv.mxnet.io/
13.6. The Object Detection Dataset 567
13.6.2 Reading the Dataset
We are going to read the object detection dataset by creating the instance ImageDetIter. The �Det�
in the name refers to Detection. We will read the training dataset in random order. Since the format
of the dataset is RecordIO, we need the image index file 'train.idx' to read random minibatches.
In addition, for each image of the training set, we will use random cropping and require
the cropped image to cover at least 95% of each object. Since the cropping is random, this requirement
is not always satisfied. We preset the maximum number of random cropping attempts
to 200. If none of them meets the requirement, the image will not be cropped. To ensure the certainty
of the output, we will not randomly crop the images in the test dataset. We also do not need
to read the test dataset in random order.
#@save
def load_data_bananas(batch_size, edge_size=256):
"""Load the bananas dataset."""
data_dir = d2l.download_extract('bananas')
train_iter = image.ImageDetIter(
path_imgrec=os.path.join(data_dir, 'train.rec'),
path_imgidx=os.path.join(data_dir, 'train.idx'),
batch_size=batch_size,
data_shape=(3, edge_size, edge_size), # The shape of the output image
shuffle=True, # Read the dataset in random order
rand_crop=1, # The probability of random cropping is 1
min_object_covered=0.95, max_attempts=200)
val_iter = image.ImageDetIter(
path_imgrec=os.path.join(data_dir, 'val.rec'), batch_size=batch_size,
data_shape=(3, edge_size, edge_size), shuffle=False)
return train_iter, val_iter
Below, we read a minibatch and print the shape of the image and label. The shape of the image
is the same as in the previous experiment (batch size, number of channels, height, width). The
shape of the label is (batch size, m, 5), where m is equal to the maximum number of bounding
boxes contained in a single image in the dataset. Although computation for the minibatch is very
efficient, it requires each image to contain the same number of bounding boxes so that they can be
placed in the same batch. Since each image may have a different number of bounding boxes, we
can add illegal bounding boxes to images that have less than m bounding boxes until each image
containsmbounding boxes. Thus, we can read a minibatch of images each time. The label of each
bounding box in the image is represented by an array of length 5. The first element in the array is
the category of the object contained in the bounding box. When the value is -1, the bounding box
is an illegal bounding box for filling purpose. The remaining four elements of the array represent
the x; y axis coordinates of the upper-left corner of the bounding box and the x; y axis coordinates
of the lower-right corner of the bounding box (the value range is between 0 and 1). The banana
dataset here has only one bounding box per image, so m = 1.
batch_size, edge_size = 32, 256
train_iter, _ = load_data_bananas(batch_size, edge_size)
batch = train_iter.next()
batch.data[0].shape, batch.label[0].shape
Downloading ../data/bananas.zip from http://d2l-data.s3-accelerate.amazonaws.com/bananas.zip.
,!..
568 Chapter 13. Computer Vision
((32, 3, 256, 256), (32, 1, 5))
13.6.3 Demonstration
We have ten images with bounding boxes on them. We can see that the angle, size, and position of
banana are different in each image. Of course, this is a simple artificial dataset. In actual practice,
the data are usually much more complicated.
imgs = (batch.data[0][0:10].transpose(0, 2, 3, 1)) / 255
axes = d2l.show_images(imgs, 2, 5, scale=2)
for ax, label in zip(axes, batch.label[0][0:10]):
d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
Summary
� The banana detection dataset we synthesized can be used to test object detection models.
� The data reading for object detection is similar to that for image classification. However,
after we introduce bounding boxes, the label shape and image augmentation (e.g., random
cropping) are changed.
Exercises
1. Referring to the MXNet documentation, what are the parameters for the constructors of the
image.ImageDetIter and image.CreateDetAugmenter classes? What is their significance?
Discussions187
187 https://discuss.d2l.ai/t/372
13.6. The Object Detection Dataset 569
13.7 Single Shot Multibox Detection (SSD)
In the previous few sections, we have introduced bounding boxes, anchor boxes, multiscale object
detection, and datasets. Now, we will use this background knowledge to construct an object
detection model: single shot multibox detection (SSD) (Liu et al., 2016). This quick and easy model
is already widely used. Some of the design concepts and implementation details of this model are
also applicable to other object detection models.
13.7.1 Model
Fig. 13.7.1 shows the design of an SSD model. The model?s main components are a base network
block and several multiscale feature blocks connected in a series. Here, the base network block is
used to extract features of original images, and it generally takes the form of a deep convolutional
neural network. The paper on SSDs chooses to place a truncated VGG before the classification
layer (Liu et al., 2016), but this is now commonly replaced by ResNet. We can design the base
network so that it outputs larger heights and widths. In this way, more anchor boxes are generated
based on this feature map, allowing us to detect smaller objects. Next, each multiscale
feature block reduces the height and width of the feature map provided by the previous layer (for
example, it may reduce the sizes by half). The blocks then use each element in the feature map to
expand the receptive field on the input image. In this way, the closer a multiscale feature block
is to the top of Fig. 13.7.1 the smaller its output feature map, and the fewer the anchor boxes that
are generated based on the feature map. In addition, the closer a feature block is to the top, the
larger the receptive field of each element in the feature map and the better suited it is to detect
larger objects. As the SSD generates different numbers of anchor boxes of different sizes based
on the base network block and each multiscale feature block and then predicts the categories and
offsets (i.e., predicted bounding boxes) of the anchor boxes in order to detect objects of different
sizes, SSD is a multiscale object detection model.
Fig. 13.7.1: The SSD is composed of a base network block and several multiscale feature blocks
connected in a series.
Next, we will describe the implementation of the modules in Fig. 13.7.1. First, we need to discuss
570 Chapter 13. Computer Vision
the implementation of category prediction and bounding box prediction.
Category Prediction Layer
Set the number of object categories to q. In this case, the number of anchor box categories is
q + 1, with 0 indicating an anchor box that only contains background. For a certain scale, set
the height and width of the feature map to h and w, respectively. If we use each element as the
center to generate a anchor boxes, we need to classify a total of hwa anchor boxes. If we use a
fully connected layer (FCN) for the output, this will likely result in an excessive number of model
parameters. Recall how we used convolutional layer channels to output category predictions in
Section 7.3. SSD uses the same method to reduce the model complexity.
Specifically, the category prediction layer uses a convolutional layer that maintains the input
height and width. Thus, the output and input have a one-to-one correspondence to the spatial
coordinates along the width and height of the feature map. Assuming that the output and input
have the same spatial coordinates (x; y), the channel for the coordinates (x; y) on the output feature
map contains the category predictions for all anchor boxes generated using the input feature
map coordinates (x; y) as the center. Therefore, there are a(q + 1) output channels, with the output
channels indexed as i(q+1)+j (0  j  q) representing the predictions of the category index
j for the anchor box index i.
Now, we will define a category prediction layer of this type. After we specify the parameters a and
q, it uses a 3  3 convolutional layer with a padding of 1. The heights and widths of the input and
output of this convolutional layer remain unchanged.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, image, init, np, npx
from mxnet.gluon import nn
npx.set_np()
def cls_predictor(num_anchors, num_classes):
return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,
padding=1)
Bounding Box Prediction Layer
The design of the bounding box prediction layer is similar to that of the category prediction layer.
The only difference is that, here, we need to predict 4 offsets for each anchor box, rather than q+1
categories.
def bbox_predictor(num_anchors):
return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)
13.7. Single Shot Multibox Detection (SSD) 571
Concatenating Predictions for Multiple Scales
As we mentioned, SSD uses feature maps based on multiple scales to generate anchor boxes and
predict their categories and offsets. Because the shapes and number of anchor boxes centered on
the same element differ for the feature maps of different scales, the prediction outputs at different
scales may have different shapes.
In the following example, we use the same batch of data to construct feature maps of two different
scales, Y1 and Y2. Here, Y2 has half the height and half the width of Y1. Using category prediction
as an example, we assume that each element in the Y1 and Y2 feature maps generates five (Y1) or
three (Y2) anchor boxes. When there are 10 object categories, the number of category prediction
output channels is either 5  (10 + 1) = 55 or 3  (10 + 1) = 33. The format of the prediction
output is (batch size, number of channels, height, width). As you can see, except for the batch
size, the sizes of the other dimensions are different. Therefore, we must transform them into a
consistent format and concatenate the predictions of the multiple scales to facilitate subsequent
computation.
def forward(x, block):
block.initialize()
return block(x)
Y1 = forward(np.zeros((2, 8, 20, 20)), cls_predictor(5, 10))
Y2 = forward(np.zeros((2, 16, 10, 10)), cls_predictor(3, 10))
(Y1.shape, Y2.shape)
((2, 55, 20, 20), (2, 33, 10, 10))
The channel dimension contains the predictions for all anchor boxes with the same center. We
first move the channel dimension to the final dimension. Because the batch size is the same for
all scales, we can convert the prediction results to binary format (batch size, height  width 
number of channels) to facilitate subsequent concatenation on the 1st dimension.
def flatten_pred(pred):
return npx.batch_flatten(pred.transpose(0, 2, 3, 1))
def concat_preds(preds):
return np.concatenate([flatten_pred(p) for p in preds], axis=1)
Thus, regardless of the different shapes of Y1 and Y2, we can still concatenate the prediction results
for the two different scales of the same batch.
concat_preds([Y1, Y2]).shape
(2, 25300)
572 Chapter 13. Computer Vision
Height and Width Downsample Block
For multiscale object detection, we define the following down_sample_blk block, which reduces
the height and width by 50%. This block consists of two 33 convolutional layers with a padding
of 1 and a 22 maximum pooling layer with a stride of 2 connected in a series. As we know, 33
convolutional layers with a padding of 1 do not change the shape of feature maps. However, the
subsequent pooling layer directly reduces the size of the feature map by half. Because 1  2 +
(3 ?? 1) + (3 ?? 1) = 6, each element in the output feature map has a receptive field on the input
feature map of the shape 6  6. As you can see, the height and width downsample block enlarges
the receptive field of each element in the output feature map.
def down_sample_blk(num_channels):
blk = nn.Sequential()
for _ in range(2):
blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1),
nn.BatchNorm(in_channels=num_channels),
nn.Activation('relu'))
blk.add(nn.MaxPool2D(2))
return blk
By testing forward computation in the height and width downsample block, we can see that it
changes the number of input channels and halves the height and width.
forward(np.zeros((2, 3, 20, 20)), down_sample_blk(10)).shape
(2, 10, 10, 10)
Base Network Block
The base network block is used to extract features from original images. To simplify the computation,
we will construct a small base network. This network consists of three height and width
downsample blocks connected in a series, so it doubles the number of channels at each step.
When we input an original image with the shape 256  256, the base network block outputs a
feature map with the shape 32  32.
def base_net():
blk = nn.Sequential()
for num_filters in [16, 32, 64]:
blk.add(down_sample_blk(num_filters))
return blk
forward(np.zeros((2, 3, 256, 256)), base_net()).shape
(2, 64, 32, 32)
13.7. Single Shot Multibox Detection (SSD) 573
The Complete Model
The SSD model contains a total of five modules. Each module outputs a feature map used to generate
anchor boxes and predict the categories and offsets of these anchor boxes. The first module
is the base network block, modules two to four are height and width downsample blocks, and the
fifth module is a global maximum pooling layer that reduces the height and width to 1. Therefore,
modules two to five are all multiscale feature blocks shown in Fig. 13.7.1.
def get_blk(i):
if i == 0:
blk = base_net()
elif i == 4:
blk = nn.GlobalMaxPool2D()
else:
blk = down_sample_blk(128)
return blk
Now, we will define the forward computation process for each module. In contrast to the
previously-described convolutional neural networks, this module not only returns feature map
Y output by convolutional computation, but also the anchor boxes of the current scale generated
from Y and their predicted categories and offsets.
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):
Y = blk(X)
anchors = npx.multibox_prior(Y, sizes=size, ratios=ratio)
cls_preds = cls_predictor(Y)
bbox_preds = bbox_predictor(Y)
return (Y, anchors, cls_preds, bbox_preds)
As we mentioned, the closer a multiscale feature block is to the top in Fig. 13.7.1, the larger the
objects it detects and the larger the anchor boxes it must generate. Here, we first divide the interval
from 0.2 to 1.05 into five equal parts to determine the sizes of smaller anchor boxes at different
scales: 0.2, 0.37, 0.54, etc. Then, according to
p
0:2  0:37 = 0:272,
p
0:37  0:54 = 0:447, and
similar formulas, we determine the sizes of larger anchor boxes at the different scales.
sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],
[0.88, 0.961]]
ratios = [[1, 2, 0.5]] * 5
num_anchors = len(sizes[0]) + len(ratios[0]) - 1
Now, we can define the complete model, TinySSD.
class TinySSD(nn.Block):
def __init__(self, num_classes, **kwargs):
super(TinySSD, self).__init__(**kwargs)
self.num_classes = num_classes
for i in range(5):
# The assignment statement is self.blk_i = get_blk(i)
setattr(self, f'blk_{i}', get_blk(i))
setattr(self, f'cls_{i}', cls_predictor(num_anchors, num_classes))
setattr(self, f'bbox_{i}', bbox_predictor(num_anchors))
def forward(self, X):
(continues on next page)
574 Chapter 13. Computer Vision
(continued from previous page)
anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
for i in range(5):
# getattr(self, 'blk_%d' % i) accesses self.blk_i
X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(
X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],
getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))
# In the reshape function, 0 indicates that the batch size remains
# unchanged
anchors = np.concatenate(anchors, axis=1)
cls_preds = concat_preds(cls_preds)
cls_preds = cls_preds.reshape(
cls_preds.shape[0], -1, self.num_classes + 1)
bbox_preds = concat_preds(bbox_preds)
return anchors, cls_preds, bbox_preds
We now create an SSD model instance and use it to perform forward computation on image minibatch
X, which has a height and width of 256 pixels. As we verified previously, the first module
outputs a feature map with the shape 32  32. Because modules two to four are height and width
downsample blocks, module five is a global pooling layer, and each element in the feature map is
used as the center for 4 anchor boxes, a total of (322 + 162 + 82 + 42 + 1)  4 = 5444 anchor boxes
are generated for each image at the five scales.
net = TinySSD(num_classes=1)
net.initialize()
X = np.zeros((32, 3, 256, 256))
anchors, cls_preds, bbox_preds = net(X)
print('output anchors:', anchors.shape)
print('output class preds:', cls_preds.shape)
print('output bbox preds:', bbox_preds.shape)
output anchors: (1, 5444, 4)
output class preds: (32, 5444, 2)
output bbox preds: (32, 21776)
13.7.2 Training
Now, we will explain, step by step, how to train the SSD model for object detection.
Data Reading and Initialization
We read the banana detection dataset we created in the previous section.
batch_size = 32
train_iter, _ = d2l.load_data_bananas(batch_size)
There is 1 category in the banana detection dataset. After defining the module, we need to initialize
the model parameters and define the optimization algorithm.
13.7. Single Shot Multibox Detection (SSD) 575
device, net = d2l.try_gpu(), TinySSD(num_classes=1)
net.initialize(init=init.Xavier(), ctx=device)
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': 0.2, 'wd': 5e-4})
Defining Loss and Evaluation Functions
Object detection is subject to two types of losses. The first is anchor box category loss. For this, we
can simply reuse the cross-entropy loss function we used in image classification. The second loss
is positive anchor box offset loss. Offset prediction is a normalization problem. However, here,
we do not use the squared loss introduced previously. Rather, we use the L1 norm loss, which
is the absolute value of the difference between the predicted value and the ground-truth value.
The mask variable bbox_masks removes negative anchor boxes and padding anchor boxes from
the loss calculation. Finally, we add the anchor box category and offset losses to find the final loss
function for the model.
cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()
bbox_loss = gluon.loss.L1Loss()
def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
cls = cls_loss(cls_preds, cls_labels)
bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)
return cls + bbox
We can use the accuracy rate to evaluate the classification results. As we use the L1 norm loss, we
will use the average absolute error to evaluate the bounding box prediction results.
def cls_eval(cls_preds, cls_labels):
# Because the category prediction results are placed in the final
# dimension, argmax must specify this dimension
return float((cls_preds.argmax(axis=-1).astype(
cls_labels.dtype) == cls_labels).sum())
def bbox_eval(bbox_preds, bbox_labels, bbox_masks):
return float((np.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())
Training the Model
During model training, we must generate multiscale anchor boxes (anchors) in the model?s forward
computation process and predict the category (cls_preds) and offset (bbox_preds) for each
anchor box. Afterwards, we label the category (cls_labels) and offset (bbox_labels) of each generated
anchor box based on the label information Y. Finally, we calculate the loss function using
the predicted and labeled category and offset values. To simplify the code, we do not evaluate the
training dataset here.
num_epochs, timer = 20, d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=['class error', 'bbox mae'])
for epoch in range(num_epochs):
(continues on next page)
576 Chapter 13. Computer Vision
(continued from previous page)
# accuracy_sum, mae_sum, num_examples, num_labels
metric = d2l.Accumulator(4)
train_iter.reset() # Read data from the start.
for batch in train_iter:
timer.start()
X = batch.data[0].as_in_ctx(device)
Y = batch.label[0].as_in_ctx(device)
with autograd.record():
# Generate multiscale anchor boxes and predict the category and
# offset of each
anchors, cls_preds, bbox_preds = net(X)
# Label the category and offset of each anchor box
bbox_labels, bbox_masks, cls_labels = npx.multibox_target(
anchors, Y, cls_preds.transpose(0, 2, 1))
# Calculate the loss function using the predicted and labeled
# category and offset values
l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
bbox_masks)
l.backward()
trainer.step(batch_size)
metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,
bbox_eval(bbox_preds, bbox_labels, bbox_masks),
bbox_labels.size)
cls_err, bbox_mae = 1-metric[0]/metric[1], metric[2]/metric[3]
animator.add(epoch+1, (cls_err, bbox_mae))
print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')
print(f'{train_iter.num_image/timer.stop():.1f} examples/sec on '
f'{str(device)}')
class err 5.07e-03, bbox mae 5.51e-03
4775.3 examples/sec on gpu(0)
13.7. Single Shot Multibox Detection (SSD) 577
13.7.3 Prediction
In the prediction stage, we want to detect all objects of interest in the image. Below, we read the
test image and transform its size. Then, we convert it to the four-dimensional format required by
the convolutional layer.
img = image.imread('../img/banana.jpg')
feature = image.imresize(img, 256, 256).astype('float32')
X = np.expand_dims(feature.transpose(2, 0, 1), axis=0)
Using the MultiBoxDetection function, we predict the bounding boxes based on the anchor boxes
and their predicted offsets. Then, we use non-maximum suppression to remove similar bounding
boxes.
def predict(X):
anchors, cls_preds, bbox_preds = net(X.as_in_ctx(device))
cls_probs = npx.softmax(cls_preds).transpose(0, 2, 1)
output = npx.multibox_detection(cls_probs, bbox_preds, anchors)
idx = [i for i, row in enumerate(output[0]) if row[0] != -1]
return output[0, idx]
output = predict(X)
Finally, we take all the bounding boxes with a confidence level of at least 0.3 and display them as
the final output.
def display(img, output, threshold):
d2l.set_figsize((5, 5))
fig = d2l.plt.imshow(img.asnumpy())
for row in output:
score = float(row[1])
if score < threshold:
continue
h, w = img.shape[0:2]
bbox = [row[2:6] * np.array((w, h, w, h), ctx=row.ctx)]
d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')
display(img, output, threshold=0.9)
578 Chapter 13. Computer Vision
Summary
� SSD is a multiscale object detection model. This model generates different numbers of anchor
boxes of different sizes based on the base network block and each multiscale feature
block and predicts the categories and offsets of the anchor boxes to detect objects of different
sizes.
� During SSD model training, the loss function is calculated using the predicted and labeled
category and offset values.
Exercises
1. Due to space limitations, we have ignored some of the implementation details of the SSD
model in this experiment. Can you further improve the model in the following areas?
Loss Function
A. For the predicted offsets, replace L1 norm loss with L1 regularization loss. This loss function
uses a square function around zero for greater smoothness. This is the regularized area controlled
by the hyperparameter :
f(x) =
{
(x)2/2; if jxj < 1/2
jxj ?? 0:5/2; otherwise
(13.7.1)
When  is large, this loss is similar to the L1 norm loss. When the value is small, the loss function
is smoother.
13.7. Single Shot Multibox Detection (SSD) 579
sigmas = [10, 1, 0.5]
lines = ['-', '--', '-.']
x = np.arange(-2, 2, 0.1)
d2l.set_figsize()
for l, s in zip(lines, sigmas):
y = npx.smooth_l1(x, scalar=s)
d2l.plt.plot(x.asnumpy(), y.asnumpy(), l, label='sigma=%.1f' % s)
d2l.plt.legend();
In the experiment, we used cross-entropy loss for category prediction. Now, assume that the prediction
probability of the actual category j is pj and the cross-entropy loss is ??log pj . We can also
use the focal loss (Lin et al., 2017). Given the positive hyperparameters 
 and , this loss is defined
as:
??(1 ?? pj)
 log pj : (13.7.2)
As you can see, by increasing 
, we can effectively reduce the loss when the probability of predicting
the correct category is high.
def focal_loss(gamma, x):
return -(1 - x) ** gamma * np.log(x)
x = np.arange(0.01, 1, 0.01)
for l, gamma in zip(lines, [0, 1, 5]):
y = d2l.plt.plot(x.asnumpy(), focal_loss(gamma, x).asnumpy(), l,
label='gamma=%.1f' % gamma)
d2l.plt.legend();
580 Chapter 13. Computer Vision
Training and Prediction
B. When an object is relatively large compared to the image, the model normally adopts a larger
input image size.
C. This generally produces a large number of negative anchor boxes when labeling anchor box
categories. We can sample the negative anchor boxes to better balance the data categories. To do
this, we can set the MultiBoxTarget function?s negative_mining_ratio parameter.
D. Assign hyperparameters with different weights to the anchor box category loss and positive
anchor box offset loss in the loss function.
E. Refer to the SSD paper. What methods can be used to evaluate the precision of object detection
models (Liu et al., 2016)?
Discussions188
13.8 Region-based CNNs (R-CNNs)
Region-based convolutional neural networks or regions with CNN features (R-CNNs) are a pioneering
approach that applies deep models to object detection (Girshick et al., 2014). In this section,
we will discuss R-CNNs and a series of improvements made to them: Fast R-CNN (Girshick,
2015), Faster R-CNN (Ren et al., 2015), and Mask R-CNN (He et al., 2017a). Due to space limitations,
we will confine our discussion to the designs of these models.
13.8.1 R-CNNs
R-CNN models first select several proposed regions from an image (for example, anchor boxes are
one type of selection method) and then label their categories and bounding boxes (e.g., offsets).
Then, they use a CNN to perform forward computation to extract features from each proposed
area. Afterwards, we use the features of each proposed region to predict their categories and
bounding boxes. Fig. 13.8.1 shows an R-CNN model.
188 https://discuss.d2l.ai/t/373
13.8. Region-based CNNs (R-CNNs) 581
Fig. 13.8.1: R-CNN model.
Specifically, R-CNNs are composed of four main parts:
1. Selective search is performed on the input image to select multiple high-quality proposed
regions (Uijlings et al., 2013). These proposed regions are generally selected on multiple
scales and have different shapes and sizes. The category and ground-truth bounding box of
each proposed region is labeled.
2. A pre-trained CNN is selected and placed, in truncated form, before the output layer. It
transforms each proposed region into the input dimensions required by the network and
uses forward computation to output the features extracted from the proposed regions.
3. The features and labeled category of each proposed region are combined as an example to
train multiple support vector machines for object classification. Here, each support vector
machine is used to determine whether an example belongs to a certain category.
4. The features and labeled bounding box of each proposed region are combined as an example
to train a linear regression model for ground-truth bounding box prediction.
Although R-CNN models use pre-trained CNNs to effectively extract image features, the main
downside is the slow speed. As you can imagine, we can select thousands of proposed regions
from a single image, requiring thousands of forward computations from the CNN to perform object
detection. This massive computing load means that R-CNNs are not widely used in actual
applications.
13.8.2 Fast R-CNN
The main performance bottleneck of an R-CNN model is the need to independently extract features
for each proposed region. As these regions have a high degree of overlap, independent
feature extraction results in a high volume of repetitive computations. Fast R-CNN improves on
the R-CNN by only performing CNN forward computation on the image as a whole.
582 Chapter 13. Computer Vision
Fig. 13.8.2: Fast R-CNN model.
Fig. 13.8.2 shows a Fast R-CNN model. It is primary computation steps are described below:
1. Compared to an R-CNN model, a Fast R-CNN model uses the entire image as the CNN input
for feature extraction, rather than each proposed region. Moreover, this network is generally
trained to update the model parameters. As the input is an entire image, the CNN output
shape is 1  c  h1  w1.
2. Assuming selective search generates n proposed regions, their different shapes indicate regions
of interests (RoIs) of different shapes on the CNN output. Features of the same shapes
must be extracted from these RoIs (here we assume that the height is h2 and the width is
w2). Fast R-CNN introduces RoI pooling, which uses the CNN output and RoIs as input to
output a concatenation of the features extracted from each proposed region with the shape
n  c  h2  w2.
3. A fully connected layer is used to transform the output shape to nd, where d is determined
by the model design.
4. During category prediction, the shape of the fully connected layer output is again transformed
to n  q and we use softmax regression (q is the number of categories). During
bounding box prediction, the shape of the fully connected layer output is again transformed
to n4. This means that we predict the category and bounding box for each proposed region.
The RoI pooling layer in Fast R-CNN is somewhat different from the pooling layers we have discussed
before. In a normal pooling layer, we set the pooling window, padding, and stride to control
the output shape. In an RoI pooling layer, we can directly specify the output shape of each
region, such as specifying the height and width of each region as h2;w2. Assuming that the height
and width of the RoI window are h and w, this window is divided into a grid of sub-windows with
the shape h2w2. The size of each sub-window is about (h/h2)(w/w2). The sub-window height
and width must always be integers and the largest element is used as the output for a given subwindow.
This allows the RoI pooling layer to extract features of the same shape from RoIs of
different shapes.
In Fig. 13.8.3, we select an 33 region as an RoI of the 44 input. For this RoI, we use a 22 RoI
pooling layer to obtain a single 2  2 output. When we divide the region into four sub-windows,
they respectively contain the elements 0, 1, 4, and 5 (5 is the largest); 2 and 6 (6 is the largest); 8
13.8. Region-based CNNs (R-CNNs) 583
and 9 (9 is the largest); and 10.
Fig. 13.8.3: 2  2 RoI pooling layer.
We use the ROIPooling function to demonstrate the RoI pooling layer computation. Assume that
the CNN extracts the feature X with both a height and width of 4 and only a single channel.
from mxnet import np, npx
npx.set_np()
X = np.arange(16).reshape(1, 1, 4, 4)
X
array([[[[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.]]]])
Assume that the height and width of the image are both 40 pixels and that selective search generates
two proposed regions on the image. Each region is expressed as five elements: the region?s
object category and the x; y coordinates of its upper-left and bottom-right corners.
rois = np.array([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])
Because the height and width of X are 1/10 of the height and width of the image, the coordinates
of the two proposed regions are multiplied by 0.1 according to the spatial_scale, and then the
RoIs are labeled on X as X[:, :, 0:3, 0:3] and X[:, :, 1:4, 0:4], respectively. Finally, we
divide the two RoIs into a sub-window grid and extract features with a height and width of 2.
npx.roi_pooling(X, rois, pooled_size=(2, 2), spatial_scale=0.1)
array([[[[ 5., 6.],
[ 9., 10.]]],
[[[ 9., 11.],
[13., 15.]]]])
584 Chapter 13. Computer Vision
13.8.3 Faster R-CNN
In order to obtain precise object detection results, Fast R-CNN generally requires that many proposed
regions be generated in selective search. Faster R-CNN replaces selective search with a region
proposal network. This reduces the number of proposed regions generated, while ensuring
precise object detection.
Fig. 13.8.4: Faster R-CNN model.
Fig. 13.8.4 shows a Faster R-CNN model. Compared to Fast R-CNN, Faster R-CNN only changes the
method for generating proposed regions from selective search to region proposal network. The
other parts of the model remain unchanged. The detailed region proposal network computation
process is described below:
1. We use a 3  3 convolutional layer with a padding of 1 to transform the CNN output and set
the number of output channels to c. This way, each element in the feature map the CNN
extracts from the image is a new feature with a length of c.
2. We use each element in the feature map as a center to generate multiple anchor boxes of
different sizes and aspect ratios and then label them.
3. We use the features of the elements of length c at the center on the anchor boxes to predict
the binary category (object or background) and bounding box for their respective anchor
boxes.
4. Then, we use non-maximum suppression to remove similar bounding box results that correspond
to category predictions of �object�. Finally, we output the predicted bounding boxes
as the proposed regions required by the RoI pooling layer.
It is worth noting that, as a part of the Faster R-CNN model, the region proposal network is trained
together with the rest of the model. In addition, the Faster R-CNN object functions include the
category and bounding box predictions in object detection, as well as the binary category and
bounding box predictions for the anchor boxes in the region proposal network. Finally, the region
proposal network can learn how to generate high-quality proposed regions, which reduces the
number of proposed regions while maintaining the precision of object detection.
13.8. Region-based CNNs (R-CNNs) 585
13.8.4 Mask R-CNN
If training data is labeled with the pixel-level positions of each object in an image, a Mask R-CNN
model can effectively use these detailed labels to further improve the precision of object detection.
Fig. 13.8.5: Mask R-CNN model.
As shown in Fig. 13.8.5, Mask R-CNN is a modification to the Faster R-CNN model. Mask R-CNN
models replace the RoI pooling layer with an RoI alignment layer. This allows the use of bilinear
interpolation to retain spatial information on feature maps, making Mask R-CNN better suited
for pixel-level predictions. The RoI alignment layer outputs feature maps of the same shape for
all RoIs. This not only predicts the categories and bounding boxes of RoIs, but allows us to use
an additional fully convolutional network to predict the pixel-level positions of objects. We will
describe how to use fully convolutional networks to predict pixel-level semantics in images later
in this chapter.
Summary
� An R-CNN model selects several proposed regions and uses a CNN to perform forward computation
and extract the features from each proposed region. It then uses these features to
predict the categories and bounding boxes of proposed regions.
� Fast R-CNN improves on the R-CNN by only performing CNN forward computation on the
image as a whole. It introduces an RoI pooling layer to extract features of the same shape
from RoIs of different shapes.
� Faster R-CNN replaces the selective search used in Fast R-CNN with a region proposal network.
This reduces the number of proposed regions generated, while ensuring precise object
detection.
� Mask R-CNN uses the same basic structure as Faster R-CNN, but adds a fully convolution
layer to help locate objects at the pixel level and further improve the precision of object
detection.
586 Chapter 13. Computer Vision
Exercises
1. Study the implementation of each model in the GluonCV toolkit189 related to this section.
Discussions190
13.9 Semantic Segmentation and the Dataset
In our discussion of object detection issues in the previous sections, we only used rectangular
bounding boxes to label and predict objects in images. In this section, we will look at semantic
segmentation, which attempts to segment images into regions with different semantic categories.
These semantic regions label and predict objects at the pixel level. Fig. 13.9.1 shows a
semantically-segmented image, with areas labeled �dog�, �cat�, and �background�. As you can
see, compared to object detection, semantic segmentation labels areas with pixel-level borders,
for significantly greater precision.
Fig. 13.9.1: Semantically-segmented image, with areas labeled �dog�, �cat�, and �background�.
13.9.1 Image Segmentation and Instance Segmentation
In the computer vision field, there are two important methods related to semantic segmentation:
image segmentation and instance segmentation. Here, we will distinguish these concepts from
semantic segmentation as follows:
� Image segmentation divides an image into several constituent regions. This method generally
uses the correlations between pixels in an image. During training, labels are not
needed for image pixels. However, during prediction, this method cannot ensure that the
segmented regions have the semantics we want. If we input the image in 9.10, image segmentation
might divide the dog into two regions, one covering the dog?s mouth and eyes
where black is the prominent color and the other covering the rest of the dog where yellow
is the prominent color.
� Instance segmentation is also called simultaneous detection and segmentation. This
method attempts to identify the pixel-level regions of each object instance in an image. In
contrast to semantic segmentation, instance segmentation not only distinguishes semantics,
but also different object instances. If an image contains two dogs, instance segmentation
will distinguish which pixels belong to which dog.
189 https://github.com/dmlc/gluon-cv/
190 https://discuss.d2l.ai/t/374
13.9. Semantic Segmentation and the Dataset 587
13.9.2 The Pascal VOC2012 Semantic Segmentation Dataset
In the semantic segmentation field, one important dataset is Pascal VOC2012191. To better understand
this dataset, we must first import the package or module needed for the experiment.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, np, npx
import os
npx.set_np()
The original site might be unstable, so we download the data from a mirror site. The archive is
about 2 GB, so it will take some time to download. After you decompress the archive, the dataset
is located in the ../data/VOCdevkit/VOC2012 path.
#@save
d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
Go to ../data/VOCdevkit/VOC2012 to see the different parts of the dataset. The ImageSets/
Segmentation path contains text files that specify the training and testing examples. The JPEGImages
and SegmentationClass paths contain the example input images and labels, respectively.
These labels are also in image format, with the same dimensions as the input images to which
they correspond. In the labels, pixels with the same color belong to the same semantic category.
The read_voc_images function defined below reads all input images and labels to the memory.
#@save
def read_voc_images(voc_dir, is_train=True):
"""Read all VOC feature and label images."""
txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
'train.txt' if is_train else 'val.txt')
with open(txt_fname, 'r') as f:
images = f.read().split()
features, labels = [], []
for i, fname in enumerate(images):
features.append(image.imread(os.path.join(
voc_dir, 'JPEGImages', f'{fname}.jpg')))
labels.append(image.imread(os.path.join(
voc_dir, 'SegmentationClass', f'{fname}.png')))
return features, labels
train_features, train_labels = read_voc_images(voc_dir, True)
We draw the first five input images and their labels. In the label images, white represents borders
and black represents the background. Other colors correspond to different categories.
n = 5
imgs = train_features[0:n] + train_labels[0:n]
d2l.show_images(imgs, 2, n);
191 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/
588 Chapter 13. Computer Vision
Next, we list each RGB color value in the labels and the categories they label.
#@save
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
[0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
[64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
[64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
[0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
[0, 64, 128]]
#@save
VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
'diningtable', 'dog', 'horse', 'motorbike', 'person',
'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']
After defining the two constants above, we can easily find the category index for each pixel in the
labels.
#@save
def build_colormap2label():
"""Build an RGB color to label mapping for segmentation."""
colormap2label = np.zeros(256 ** 3)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[(colormap[0]*256 + colormap[1])*256 + colormap[2]] = i
return colormap2label
#@save
def voc_label_indices(colormap, colormap2label):
"""Map an RGB color to a label."""
colormap = colormap.astype(np.int32)
idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256
+ colormap[:, :, 2])
return colormap2label[idx]
For example, in the first example image, the category index for the front part of the airplane is 1
and the index for the background is 0.
y = voc_label_indices(train_labels[0], build_colormap2label())
y[105:115, 130:140], VOC_CLASSES[1]
13.9. Semantic Segmentation and the Dataset 589
(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
[0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],
[0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],
[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],
[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],
[0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],
[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],
[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],
[0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],
[0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]]),
'aeroplane')
Data Preprocessing
In the preceding chapters, we scaled images to make them fit the input shape of the model. In
semantic segmentation, this method would require us to re-map the predicted pixel categories
back to the original-size input image. It would be very difficult to do this precisely, especially in
segmented regions with different semantics. To avoid this problem, we crop the images to set
dimensions and do not scale them. Specifically, we use the random cropping method used in
image augmentation to crop the same region from input images and their labels.
#@save
def voc_rand_crop(feature, label, height, width):
"""Randomly crop for both feature and label images."""
feature, rect = image.random_crop(feature, (width, height))
label = image.fixed_crop(label, *rect)
return feature, label
imgs = []
for _ in range(n):
imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)
d2l.show_images(imgs[::2] + imgs[1::2], 2, n);
590 Chapter 13. Computer Vision
Dataset Classes for Custom Semantic Segmentation
We use the inherited Dataset class provided by Gluon to customize the semantic segmentation
dataset class VOCSegDataset. By implementing the __getitem__ function, we can arbitrarily access
the input image with the index idx and the category indexes for each of its pixels from the dataset.
As some images in the dataset may be smaller than the output dimensions specified for random
cropping, we must remove these example by using a custom filter function. In addition, we
define the normalize_image function to normalize each of the three RGB channels of the input
images.
#@save
class VOCSegDataset(gluon.data.Dataset):
"""A customized dataset to load VOC dataset."""
def __init__(self, is_train, crop_size, voc_dir):
self.rgb_mean = np.array([0.485, 0.456, 0.406])
self.rgb_std = np.array([0.229, 0.224, 0.225])
self.crop_size = crop_size
features, labels = read_voc_images(voc_dir, is_train=is_train)
self.features = [self.normalize_image(feature)
for feature in self.filter(features)]
self.labels = self.filter(labels)
self.colormap2label = build_colormap2label()
print('read ' + str(len(self.features)) + ' examples')
def normalize_image(self, img):
return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std
def filter(self, imgs):
return [img for img in imgs if (
img.shape[0] >= self.crop_size[0] and
img.shape[1] >= self.crop_size[1])]
def __getitem__(self, idx):
feature, label = voc_rand_crop(self.features[idx], self.labels[idx],
*self.crop_size)
return (feature.transpose(2, 0, 1),
voc_label_indices(label, self.colormap2label))
def __len__(self):
return len(self.features)
Reading the Dataset
Using the custom VOCSegDataset class, we create the training set and testing set instances. We
assume the random cropping operation output images in the shape 320  480. Below, we can see
the number of examples retained in the training and testing sets.
crop_size = (320, 480)
voc_train = VOCSegDataset(True, crop_size, voc_dir)
voc_test = VOCSegDataset(False, crop_size, voc_dir)
13.9. Semantic Segmentation and the Dataset 591
read 1114 examples
read 1078 examples
We set the batch size to 64 and define the iterators for the training and testing sets. Print the shape
of the first minibatch. In contrast to image classification and object recognition, labels here are
three-dimensional arrays.
batch_size = 64
train_iter = gluon.data.DataLoader(voc_train, batch_size, shuffle=True,
last_batch='discard',
num_workers=d2l.get_dataloader_workers())
for X, Y in train_iter:
print(X.shape)
print(Y.shape)
break
(64, 3, 320, 480)
(64, 320, 480)
Putting All Things Together
Finally, we define a function load_data_voc that downloads and loads this dataset, and then returns
the data iterators.
#@save
def load_data_voc(batch_size, crop_size):
"""Download and load the VOC2012 semantic dataset."""
voc_dir = d2l.download_extract('voc2012', os.path.join(
'VOCdevkit', 'VOC2012'))
num_workers = d2l.get_dataloader_workers()
train_iter = gluon.data.DataLoader(
VOCSegDataset(True, crop_size, voc_dir), batch_size,
shuffle=True, last_batch='discard', num_workers=num_workers)
test_iter = gluon.data.DataLoader(
VOCSegDataset(False, crop_size, voc_dir), batch_size,
last_batch='discard', num_workers=num_workers)
return train_iter, test_iter
Summary
� Semantic segmentation looks at how images can be segmented into regions with different
semantic categories.
� In the semantic segmentation field, one important dataset is Pascal VOC2012.
� Because the input images and labels in semantic segmentation have a one-to-one correspondence
at the pixel level, we randomly crop them to a fixed size, rather than scaling them.
592 Chapter 13. Computer Vision
Exercises
1. Recall the content we covered in Section 13.1. Which of the image augmentation methods
used in image classification would be hard to use in semantic segmentation?
Discussions192
13.10 Transposed Convolution
The layers we introduced so far for convolutional neural networks, including convolutional layers
(Section 6.2) and pooling layers (Section 6.5), often reduce the input width and height, or
keep them unchanged. Applications such as semantic segmentation (Section 13.9) and generative
adversarial networks (Section 17.2), however, require to predict values for each pixel and therefore
needs to increase input width and height. Transposed convolution, also named fractionallystrided
convolution (Dumoulin & Visin, 2016) or deconvolution (Long et al., 2015), serves this purpose.
from mxnet import np, npx, init
from mxnet.gluon import nn
from d2l import mxnet as d2l
npx.set_np()
13.10.1 Basic 2D Transposed Convolution
Let us consider a basic case that both input and output channels are 1, with 0 padding and 1 stride.
Fig. 13.10.1 illustrates how transposed convolution with a 2  2 kernel is computed on the 2  2
input matrix.
Fig. 13.10.1: Transposed convolution layer with a 2  2 kernel.
We can implement this operation by giving matrix kernel K and matrix input X.
def trans_conv(X, K):
h, w = K.shape
Y = np.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))
for i in range(X.shape[0]):
for j in range(X.shape[1]):
Y[i: i + h, j: j + w] += X[i, j] * K
return Y
192 https://discuss.d2l.ai/t/375
13.10. Transposed Convolution 593
Remember the convolution computes results by Y[i, j] = (X[i: i + h, j: j + w] * K).
sum() (refer to corr2d in Section 6.2), which summarizes input values through the kernel. While
the transposed convolution broadcasts input values through the kernel, which results in a larger
output shape.
Verify the results in Fig. 13.10.1.
X = np.array([[0, 1], [2, 3]])
K = np.array([[0, 1], [2, 3]])
trans_conv(X, K)
array([[ 0., 0., 1.],
[ 0., 4., 6.],
[ 4., 12., 9.]])
Or we can use nn.Conv2DTranspose to obtain the same results. As nn.Conv2D, both input and kernel
should be 4-D tensors.
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)
tconv = nn.Conv2DTranspose(1, kernel_size=2)
tconv.initialize(init.Constant(K))
tconv(X)
array([[[[ 0., 0., 1.],
[ 0., 4., 6.],
[ 4., 12., 9.]]]])
13.10.2 Padding, Strides, and Channels
We apply padding elements to the input in convolution, while they are applied to the output in
transposed convolution. A 1  1 padding means we first compute the output as normal, then
remove the first/last rows and columns.
tconv = nn.Conv2DTranspose(1, kernel_size=2, padding=1)
tconv.initialize(init.Constant(K))
tconv(X)
array([[[[4.]]]])
Similarly, strides are applied to outputs as well.
tconv = nn.Conv2DTranspose(1, kernel_size=2, strides=2)
tconv.initialize(init.Constant(K))
tconv(X)
array([[[[0., 0., 0., 1.],
[0., 0., 2., 3.],
[0., 2., 0., 3.],
[4., 6., 6., 9.]]]])
594 Chapter 13. Computer Vision
The multi-channel extension of the transposed convolution is the same as the convolution. When
the input has multiple channels, denoted by ci, the transposed convolution assigns a khkw kernel
matrix to each input channel. If the output has a channel size co, then we have a cikhkw kernel
for each output channel.
As a result, if we feedX into a convolutional layer f to compute Y = f(X) and create a transposed
convolution layer g with the same hyperparameters as f except for the output channel set to be
the channel size of X, then g(Y ) should has the same shape as X. Let us verify this statement.
X = np.random.uniform(size=(1, 10, 16, 16))
conv = nn.Conv2D(20, kernel_size=5, padding=2, strides=3)
tconv = nn.Conv2DTranspose(10, kernel_size=5, padding=2, strides=3)
conv.initialize()
tconv.initialize()
tconv(conv(X)).shape == X.shape
True
13.10.3 Analogy to Matrix Transposition
The transposed convolution takes its name from the matrix transposition. In fact, convolution
operations can also be achieved by matrix multiplication. In the example below, we define a 3
input X with a 2  2 kernel K, and then use corr2d to compute the convolution output.
X = np.arange(9).reshape(3, 3)
K = np.array([[0, 1], [2, 3]])
Y = d2l.corr2d(X, K)
Y
array([[19., 25.],
[37., 43.]])
Next, we rewrite convolution kernel K as a matrix W. Its shape will be (4; 9), where the ith row
present applying the kernel to the input to generate the ith output element.
def kernel2matrix(K):
k, W = np.zeros(5), np.zeros((4, 9))
k[:2], k[3:5] = K[0, :], K[1, :]
W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k
return W
W = kernel2matrix(K)
W
array([[0., 1., 0., 2., 3., 0., 0., 0., 0.],
[0., 0., 1., 0., 2., 3., 0., 0., 0.],
[0., 0., 0., 0., 1., 0., 2., 3., 0.],
[0., 0., 0., 0., 0., 1., 0., 2., 3.]])
Then the convolution operator can be implemented by matrix multiplication with proper reshaping.
13.10. Transposed Convolution 595
Y == np.dot(W, X.reshape(-1)).reshape(2, 2)
array([[ True, True],
[ True, True]])
We can implement transposed convolution as a matrix multiplication as well by reusing kernel2matrix.
To reuse the generated W, we construct a 2  2 input, so the corresponding weight
matrix will have a shape (9; 4), which isW?. Let us verify the results.
X = np.array([[0, 1], [2, 3]])
Y = trans_conv(X, K)
Y == np.dot(W.T, X.reshape(-1)).reshape(3, 3)
array([[ True, True, True],
[ True, True, True],
[ True, True, True]])
Summary
� Compared to convolutions that reduce inputs through kernels, transposed convolutions
broadcast inputs.
� If a convolution layer reduces the input width and height by nw and hh time, respectively.
Then a transposed convolution layer with the same kernel sizes, padding and strides will
increase the input width and height by nw and nh, respectively.
� We can implement convolution operations by the matrix multiplication, the corresponding
transposed convolutions can be done by transposed matrix multiplication.
Exercises
1. Is it efficient to use matrix multiplication to implement convolution operations? Why?
Discussions193
13.11 Fully Convolutional Networks (FCN)
We previously discussed semantic segmentation using each pixel in an image for category prediction.
A fully convolutional network (FCN) (Long et al., 2015) uses a convolutional neural network
to transform image pixels to pixel categories. Unlike the convolutional neural networks previously
introduced, an FCN transforms the height and width of the intermediate layer feature map back
to the size of input image through the transposed convolution layer, so that the predictions have a
one-to-one correspondence with input image in spatial dimension (height and width). Given a position
on the spatial dimension, the output of the channel dimension will be a category prediction
of the pixel corresponding to the location.
193 https://discuss.d2l.ai/t/376
596 Chapter 13. Computer Vision
We will first import the package or module needed for the experiment and then explain the transposed
convolution layer.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import gluon, image, init, np, npx
from mxnet.gluon import nn
npx.set_np()
13.11.1 Constructing a Model
Here, we demonstrate the most basic design of a fully convolutional network model. As shown in
Fig. 13.11.1, the fully convolutional network first uses the convolutional neural network to extract
image features, then transforms the number of channels into the number of categories through
the 1  1 convolution layer, and finally transforms the height and width of the feature map to
the size of the input image by using the transposed convolution layer Section 13.10. The model
output has the same height and width as the input image and has a one-to-one correspondence
in spatial positions. The final output channel contains the category prediction of the pixel of the
corresponding spatial position.
Fig. 13.11.1: Fully convolutional network.
Below, we use a ResNet-18 model pre-trained on the ImageNet dataset to extract image features
and record the network instance as pretrained_net. As you can see, the last two layers of the
model member variable features are the global maximum pooling layer GlobalAvgPool2D and
example flattening layer Flatten. The output module contains the fully connected layer used for
output. These layers are not required for a fully convolutional network.
13.11. Fully Convolutional Networks (FCN) 597
pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)
pretrained_net.features[-4:], pretrained_net.output
(HybridSequential(
(0): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False,?
,!in_channels=512)
(1): Activation(relu)
(2): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_
,!pool=True, pool_type=avg, layout=NCHW)
(3): Flatten
),
Dense(512 -> 1000, linear))
Next, we create the fully convolutional network instance net. It duplicates all the neural layers
except the last two layers of the instance member variable features of pretrained_net and the
model parameters obtained after pre-training.
net = nn.HybridSequential()
for layer in pretrained_net.features[:-2]:
net.add(layer)
Given an input of a height and width of 320 and 480 respectively, the forward computation of net
will reduce the height and width of the input to 1/32 of the original, i.e., 10 and 15.
X = np.random.uniform(size=(1, 3, 320, 480))
net(X).shape
(1, 512, 10, 15)
Next, we transform the number of output channels to the number of categories of Pascal VOC2012
(21) through the 1  1 convolution layer. Finally, we need to magnify the height and width of the
feature map by a factor of 32 to change them back to the height and width of the input image.
Recall the calculation method for the convolution layer output shape described in Section 6.3.
Because (320 ?? 64 + 16  2 + 32)/32 = 10 and (480 ?? 64 + 16  2 + 32)/32 = 15, we construct a
transposed convolution layer with a stride of 32 and set the height and width of the convolution
kernel to 64 and the padding to 16. It is not difficult to see that, if the stride is s, the padding is
s/2 (assuming s/2 is an integer), and the height and width of the convolution kernel are 2s, the
transposed convolution kernel will magnify both the height and width of the input by a factor of
s.
num_classes = 21
net.add(nn.Conv2D(num_classes, kernel_size=1),
nn.Conv2DTranspose(
num_classes, kernel_size=64, padding=16, strides=32))
598 Chapter 13. Computer Vision
13.11.2 Initializing the Transposed Convolution Layer
We already know that the transposed convolution layer can magnify a feature map. In image processing,
sometimes we need to magnify the image, i.e., upsampling. There are many methods
for upsampling, and one common method is bilinear interpolation. Simply speaking, in order to
get the pixel of the output image at the coordinates (x; y), the coordinates are first mapped to the
coordinates of the input image (x?; y?). This can be done based on the ratio of the size of three
input to the size of the output. The mapped values x? and y? are usually real numbers. Then,
we find the four pixels closest to the coordinate (x?; y?) on the input image. Finally, the pixels
of the output image at coordinates (x; y) are calculated based on these four pixels on the input
image and their relative distances to (x?; y?). Upsampling by bilinear interpolation can be implemented
by transposed convolution layer of the convolution kernel constructed using the following
bilinear_kernel function. Due to space limitations, we only give the implementation of the bilinear_
kernel function and will not discuss the principles of the algorithm.
def bilinear_kernel(in_channels, out_channels, kernel_size):
factor = (kernel_size + 1) // 2
if kernel_size % 2 == 1:
center = factor - 1
else:
center = factor - 0.5
og = (np.arange(kernel_size).reshape(-1, 1),
np.arange(kernel_size).reshape(1, -1))
filt = (1 - np.abs(og[0] - center) / factor) * \
(1 - np.abs(og[1] - center) / factor)
weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size))
weight[range(in_channels), range(out_channels), :, :] = filt
return np.array(weight)
Now, we will experiment with bilinear interpolation upsampling implemented by transposed convolution
layers. Construct a transposed convolution layer that magnifies height and width of input
by a factor of 2 and initialize its convolution kernel with the bilinear_kernel function.
conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)
conv_trans.initialize(init.Constant(bilinear_kernel(3, 3, 4)))
Read the image X and record the result of upsampling as Y. In order to print the image, we need
to adjust the position of the channel dimension.
img = image.imread('../img/catdog.jpg')
X = np.expand_dims(img.astype('float32').transpose(2, 0, 1), axis=0) / 255
Y = conv_trans(X)
out_img = Y[0].transpose(1, 2, 0)
As you can see, the transposed convolution layer magnifies both the height and width of the image
by a factor of 2. It is worth mentioning that, besides to the difference in coordinate scale, the image
magnified by bilinear interpolation and original image printed in Section 13.3 look the same.
d2l.set_figsize()
print('input image shape:', img.shape)
d2l.plt.imshow(img.asnumpy());
print('output image shape:', out_img.shape)
d2l.plt.imshow(out_img.asnumpy());
13.11. Fully Convolutional Networks (FCN) 599
input image shape: (561, 728, 3)
output image shape: (1122, 1456, 3)
In a fully convolutional network, we initialize the transposed convolution layer for upsampled
bilinear interpolation. For a 1  1 convolution layer, we use Xavier for randomly initialization.
W = bilinear_kernel(num_classes, num_classes, 64)
net[-1].initialize(init.Constant(W))
net[-2].initialize(init=init.Xavier())
13.11.3 Reading the Dataset
We read the dataset using the method described in the previous section. Here, we specify shape
of the randomly cropped output image as 320  480, so both the height and width are divisible by
32.
batch_size, crop_size = 32, (320, 480)
train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)
Downloading ../data/VOCtrainval_11-May-2012.tar from http://d2l-data.s3-accelerate.amazonaws.
,!com/VOCtrainval_11-May-2012.tar...
read 1114 examples
read 1078 examples
13.11.4 Training
Now we can start training the model. The loss function and accuracy calculation here are not
substantially different from those used in image classification. Because we use the channel of the
transposed convolution layer to predict pixel categories, the axis=1 (channel dimension) option
is specified in SoftmaxCrossEntropyLoss. In addition, the model calculates the accuracy based on
whether the prediction category of each pixel is correct.
600 Chapter 13. Computer Vision
num_epochs, lr, wd, devices = 5, 0.1, 1e-3, d2l.try_all_gpus()
loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)
net.collect_params().reset_ctx(devices)
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': lr, 'wd': wd})
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.328, train acc 0.891, test acc 0.853
297.5 examples/sec on [gpu(0), gpu(1)]
13.11.5 Prediction
During predicting, we need to standardize the input image in each channel and transform them
into the four-dimensional input format required by the convolutional neural network.
def predict(img):
X = test_iter._dataset.normalize_image(img)
X = np.expand_dims(X.transpose(2, 0, 1), axis=0)
pred = net(X.as_in_ctx(devices[0])).argmax(axis=1)
return pred.reshape(pred.shape[1], pred.shape[2])
To visualize the predicted categories for each pixel, we map the predicted categories back to their
labeled colors in the dataset.
def label2image(pred):
colormap = np.array(d2l.VOC_COLORMAP, ctx=devices[0], dtype='uint8')
X = pred.astype('int32')
return colormap[X, :]
The size and shape of the images in the test dataset vary. Because the model uses a transposed
convolution layer with a stride of 32, when the height or width of the input image is not divisible
by 32, the height or width of the transposed convolution layer output deviates from the size of the
input image. In order to solve this problem, we can crop multiple rectangular areas in the image
with heights and widths as integer multiples of 32, and then perform forward computation on the
pixels in these areas. When combined, these areas must completely cover the input image. When
a pixel is covered by multiple areas, the average of the transposed convolution layer output in the
13.11. Fully Convolutional Networks (FCN) 601
forward computation of the different areas can be used as an input for the softmax operation to
predict the category.
For the sake of simplicity, we only read a few large test images and crop an area with a shape of
320480 from the top-left corner of the image. Only this area is used for prediction. For the input
image, we print the cropped area first, then print the predicted result, and finally print the labeled
category.
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
test_images, test_labels = d2l.read_voc_images(voc_dir, False)
n, imgs = 4, []
for i in range(n):
crop_rect = (0, 0, 480, 320)
X = image.fixed_crop(test_images[i], *crop_rect)
pred = label2image(predict(X))
imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]
d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);
Summary
� The fully convolutional network first uses the convolutional neural network to extract image
features, then transforms the number of channels into the number of categories through the
1  1 convolution layer, and finally transforms the height and width of the feature map to
the size of the input image by using the transposed convolution layer to output the category
of each pixel.
� In a fully convolutional network, we initialize the transposed convolution layer for upsampled
bilinear interpolation.
602 Chapter 13. Computer Vision
Exercises
1. If we use Xavier to randomly initialize the transposed convolution layer, what will happen
to the result?
2. Can you further improve the accuracy of the model by tuning the hyperparameters?
3. Predict the categories of all pixels in the test image.
4. The outputs of some intermediate layers of the convolutional neural network are also used
in the paper on fully convolutional networks[1]. Try to implement this idea.
Discussions194
13.12 Neural Style Transfer
If you use social sharing apps or happen to be an amateur photographer, you are familiar with
filters. Filters can alter the color styles of photos to make the background sharper or people?s
faces whiter. However, a filter generally can only change one aspect of a photo. To create the
ideal photo, you often need to try many different filter combinations. This process is as complex
as tuning the hyperparameters of a model.
In this section, we will discuss how we can use convolution neural networks (CNNs) to automatically
apply the style of one image to another image, an operation known as style transfer (Gatys
et al., 2016). Here, we need two input images, one content image and one style image. We use
a neural network to alter the content image so that its style mirrors that of the style image. In
Fig. 13.12.1, the content image is a landscape photo the author took in Mount Rainier National
Part near Seattle. The style image is an oil painting of oak trees in autumn. The output composite
image retains the overall shapes of the objects in the content image, but applies the oil painting
brushwork of the style image and makes the overall color more vivid.
Fig. 13.12.1: Content and style input images and composite image produced by style transfer.
194 https://discuss.d2l.ai/t/377
13.12. Neural Style Transfer 603
13.12.1 Technique
The CNN-based style transfer model is shown in Fig. 13.12.2. First, we initialize the composite
image. For example, we can initialize it as the content image. This composite image is the only
variable that needs to be updated in the style transfer process, i.e., the model parameter to be
updated in style transfer. Then, we select a pre-trained CNN to extract image features. These
model parameters do not need to be updated during training. The deep CNN uses multiple neural
layers that successively extract image features. We can select the output of certain layers to
use as content features or style features. If we use the structure in Fig. 13.12.2, the pre-trained
neural network contains three convolutional layers. The second layer outputs the image content
features, while the outputs of the first and third layers are used as style features. Next, we use forward
propagation (in the direction of the solid lines) to compute the style transfer loss function
and backward propagation (in the direction of the dotted lines) to update the model parameter,
constantly updating the composite image. The loss functions used in style transfer generally have
three parts: 1. Content loss is used to make the composite image approximate the content image
as regards content features. 2. Style loss is used to make the composite image approximate
the style image in terms of style features. 3. Total variation loss helps reduce the noise in the
composite image. Finally, after we finish training the model, we output the style transfer model
parameters to obtain the final composite image.
Fig. 13.12.2: CNN-based style transfer process. Solid lines show the direction of forward propagation
and dotted lines show backward propagation.
Next, we will perform an experiment to help us better understand the technical details of style
transfer.
604 Chapter 13. Computer Vision
13.12.2 Reading the Content and Style Images
First, we read the content and style images. By printing out the image coordinate axes, we can see
that they have different dimensions.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, image, init, np, npx
from mxnet.gluon import nn
npx.set_np()
d2l.set_figsize()
content_img = image.imread('../img/rainier.jpg')
d2l.plt.imshow(content_img.asnumpy());
style_img = image.imread('../img/autumn_oak.jpg')
d2l.plt.imshow(style_img.asnumpy());
13.12. Neural Style Transfer 605
13.12.3 Preprocessing and Postprocessing
Below, we define the functions for image preprocessing and postprocessing. The preprocess function
normalizes each of the three RGB channels of the input images and transforms the results to
a format that can be input to the CNN. The postprocess function restores the pixel values in the
output image to their original values before normalization. Because the image printing function
requires that each pixel has a floating point value from 0 to 1, we use the clip function to replace
values smaller than 0 or greater than 1 with 0 or 1, respectively.
rgb_mean = np.array([0.485, 0.456, 0.406])
rgb_std = np.array([0.229, 0.224, 0.225])
def preprocess(img, image_shape):
img = image.imresize(img, *image_shape)
img = (img.astype('float32') / 255 - rgb_mean) / rgb_std
return np.expand_dims(img.transpose(2, 0, 1), axis=0)
def postprocess(img):
img = img[0].as_in_ctx(rgb_std.ctx)
return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)
13.12.4 Extracting Features
We use the VGG-19 model pre-trained on the ImageNet dataset to extract image features[1].
pretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)
To extract image content and style features, we can select the outputs of certain layers in the VGG
network. In general, the closer an output is to the input layer, the easier it is to extract image
detail information. The farther away an output is, the easier it is to extract global information. To
prevent the composite image from retaining too many details from the content image, we select
a VGG network layer near the output layer to output the image content features. This layer is
called the content layer. We also select the outputs of different layers from the VGG network for
matching local and global styles. These are called the style layers. As we mentioned in Section 7.2,
VGG networks have five convolutional blocks. In this experiment, we select the last convolutional
layer of the fourth convolutional block as the content layer and the first layer of each block as style
layers. We can obtain the indexes for these layers by printing the pretrained_net instance.
style_layers, content_layers = [0, 5, 10, 19, 28], [25]
During feature extraction, we only need to use all the VGG layers from the input layer to the content
or style layer nearest the output layer. Below, we build a new network, net, which only retains the
layers in the VGG network we need to use. We then use net to extract features.
net = nn.Sequential()
for i in range(max(content_layers + style_layers) + 1):
net.add(pretrained_net.features[i])
Given input X, if we simply call the forward computation net(X), we can only obtain the output of
the last layer. Because we also need the outputs of the intermediate layers, we need to perform
layer-by-layer computation and retain the content and style layer outputs.
606 Chapter 13. Computer Vision
def extract_features(X, content_layers, style_layers):
contents = []
styles = []
for i in range(len(net)):
X = net[i](X)
if i in style_layers:
styles.append(X)
if i in content_layers:
contents.append(X)
return contents, styles
Next, we define two functions: The get_contents function obtains the content features extracted
from the content image, while the get_styles function obtains the style features extracted from
the style image. Because we do not need to change the parameters of the pre-trained VGG model
during training, we can extract the content features from the content image and style features
from the style image before the start of training. As the composite image is the model parameter
that must be updated during style transfer, we can only call the extract_features function during
training to extract the content and style features of the composite image.
def get_contents(image_shape, device):
content_X = preprocess(content_img, image_shape).copyto(device)
contents_Y, _ = extract_features(content_X, content_layers, style_layers)
return content_X, contents_Y
def get_styles(image_shape, device):
style_X = preprocess(style_img, image_shape).copyto(device)
_, styles_Y = extract_features(style_X, content_layers, style_layers)
return style_X, styles_Y
13.12.5 Defining the Loss Function
Next, we will look at the loss function used for style transfer. The loss function includes the content
loss, style loss, and total variation loss.
Content Loss
Similar to the loss function used in linear regression, content loss uses a square error function
to measure the difference in content features between the composite image and content image.
The two inputs of the square error function are both content layer outputs obtained from the extract_
features function.
def content_loss(Y_hat, Y):
return np.square(Y_hat - Y).mean()
13.12. Neural Style Transfer 607
Style Loss
Style loss, similar to content loss, uses a square error function to measure the difference in style
between the composite image and style image. To express the styles output by the style layers,
we first use the extract_features function to compute the style layer output. Assuming that the
output has 1 example, c channels, and a height and width of h and w, we can transform the output
into the matrix X, which has c rows and hw columns. You can think of matrix X as the combination
of the c vectors x1; : : : ; xc, which have a length of hw. Here, the vector xi represents the style
feature of channel i. In the Gram matrix of these vectors XX? 2 Rcc, element xij in row i column
j is the inner product of vectors xi and xj . It represents the correlation of the style features of
channels i and j. We use this type of Gram matrix to represent the style output by the style layers.
You must note that, when the hw value is large, this often leads to large values in the Gram matrix.
In addition, the height and width of the Gram matrix are both the number of channels c. To ensure
that the style loss is not affected by the size of these values, we define the gram function below to
divide the Gram matrix by the number of its elements, i.e., c  h  w.
def gram(X):
num_channels, n = X.shape[1], X.size // X.shape[1]
X = X.reshape(num_channels, n)
return np.dot(X, X.T) / (num_channels * n)
Naturally, the two Gram matrix inputs of the square error function for style loss are taken from
the composite image and style image style layer outputs. Here, we assume that the Gram matrix
of the style image, gram_Y, has been computed in advance.
def style_loss(Y_hat, gram_Y):
return np.square(gram(Y_hat) - gram_Y).mean()
Total Variance Loss
Sometimes, the composite images we learn have a lot of high-frequency noise, particularly bright
or dark pixels. One common noise reduction method is total variation denoising. We assume that
xi;j represents the pixel value at the coordinate (i; j), so the total variance loss is:
?
i;j
jxi;j ?? xi+1;j j + jxi;j ?? xi;j+1j : (13.12.1)
We try to make the values of neighboring pixels as similar as possible.
def tv_loss(Y_hat):
return 0.5 * (np.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +
np.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())
608 Chapter 13. Computer Vision
The Loss Function
The loss function for style transfer is the weighted sum of the content loss, style loss, and total
variance loss. By adjusting these weight hyperparameters, we can balance the retained content,
transferred style, and noise reduction in the composite image according to their relative importance.
content_weight, style_weight, tv_weight = 1, 1e3, 10
def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):
# Calculate the content, style, and total variance losses respectively
contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(
contents_Y_hat, contents_Y)]
styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(
styles_Y_hat, styles_Y_gram)]
tv_l = tv_loss(X) * tv_weight
# Add up all the losses
l = sum(styles_l + contents_l + [tv_l])
return contents_l, styles_l, tv_l, l
13.12.6 Creating and Initializing the Composite Image
In style transfer, the composite image is the only variable that needs to be updated. Therefore, we
can define a simple model, GeneratedImage, and treat the composite image as a model parameter.
In the model, forward computation only returns the model parameter.
class GeneratedImage(nn.Block):
def __init__(self, img_shape, **kwargs):
super(GeneratedImage, self).__init__(**kwargs)
self.weight = self.params.get('weight', shape=img_shape)
def forward(self):
return self.weight.data()
Next, we define the get_inits function. This function creates a composite image model instance
and initializes it to the image X. The Gram matrix for the various style layers of the style image,
styles_Y_gram, is computed prior to training.
def get_inits(X, device, lr, styles_Y):
gen_img = GeneratedImage(X.shape)
gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True)
trainer = gluon.Trainer(gen_img.collect_params(), 'adam',
{'learning_rate': lr})
styles_Y_gram = [gram(Y) for Y in styles_Y]
return gen_img(), styles_Y_gram, trainer
13.12. Neural Style Transfer 609
13.12.7 Training
During model training, we constantly extract the content and style features of the composite image
and calculate the loss function. Recall our discussion of how synchronization functions force
the front end to wait for computation results in Section 12.2. Because we only call the asscalar
synchronization function every 50 epochs, the process may occupy a great deal of memory. Therefore,
we call the waitall synchronization function during every epoch.
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[1, num_epochs],
legend=['content', 'style', 'TV'],
ncols=2, figsize=(7, 2.5))
for epoch in range(1, num_epochs+1):
with autograd.record():
contents_Y_hat, styles_Y_hat = extract_features(
X, content_layers, style_layers)
contents_l, styles_l, tv_l, l = compute_loss(
X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)
l.backward()
trainer.step(1)
npx.waitall()
if epoch % lr_decay_epoch == 0:
trainer.set_learning_rate(trainer.learning_rate * 0.1)
if epoch % 10 == 0:
animator.axes[1].imshow(postprocess(X).asnumpy())
animator.add(epoch, [float(sum(contents_l)),
float(sum(styles_l)),
float(tv_l)])
return X
Next, we start to train the model. First, we set the height and width of the content and style images
to 150 by 225 pixels. We use the content image to initialize the composite image.
device, image_shape = d2l.try_gpu(), (225, 150)
net.collect_params().reset_ctx(device)
content_X, contents_Y = get_contents(image_shape, device)
_, styles_Y = get_styles(image_shape, device)
output = train(content_X, contents_Y, styles_Y, device, 0.01, 500, 200)
610 Chapter 13. Computer Vision
As you can see, the composite image retains the scenery and objects of the content image, while
introducing the color of the style image. Because the image is relatively small, the details are a bit
fuzzy.
To obtain a clearer composite image, we train the model using a larger image size: 900  600. We
increase the height and width of the image used before by a factor of four and initialize a larger
composite image.
image_shape = (900, 600)
_, content_Y = get_contents(image_shape, device)
_, style_Y = get_styles(image_shape, device)
X = preprocess(postprocess(output) * 255, image_shape)
output = train(X, content_Y, style_Y, device, 0.01, 300, 100)
d2l.plt.imsave('../img/neural-style.jpg', postprocess(output).asnumpy())
As you can see, each epoch takes more time due to the larger image size. As shown in Fig. 13.12.3,
the composite image produced retains more detail due to its larger size. The composite image not
only has large blocks of color like the style image, but these blocks even have the subtle texture of
brush strokes.
13.12. Neural Style Transfer 611
Fig. 13.12.3: 900  600 composite image.
Summary
� The loss functions used in style transfer generally have three parts: 1. Content loss is used
to make the composite image approximate the content image as regards content features. 2.
Style loss is used to make the composite image approximate the style image in terms of style
features. 3. Total variation loss helps reduce the noise in the composite image.
� We can use a pre-trained CNN to extract image features and minimize the loss function to
continuously update the composite image.
� We use a Gram matrix to represent the style output by the style layers.
Exercises
1. How does the output change when you select different content and style layers?
2. Adjust the weight hyperparameters in the loss function. Does the output retain more content
or have less noise?
3. Use different content and style images. Can you create more interesting composite images?
Discussions195
195 https://discuss.d2l.ai/t/378
612 Chapter 13. Computer Vision
13.13 Image Classification (CIFAR-10) on Kaggle
So far, we have been using Gluon?s data package to directly obtain image datasets in the tensor format.
In practice, however, image datasets often exist in the format of image files. In this section,
we will start with the original image files and organize, read, and convert the files to the tensor
format step by step.
We performed an experiment on the CIFAR-10 dataset in Section 13.1. This is an important data set
in the computer vision field. Now, we will apply the knowledge we learned in the previous sections
in order to participate in the Kaggle competition, which addresses CIFAR-10 image classification
problems. The competition?s web address is
https://www.kaggle.com/c/cifar-10
Fig. 13.13.1 shows the information on the competition?s webpage. In order to submit the results,
please register an account on the Kaggle website first.
Fig. 13.13.1: CIFAR-10 image classification competition webpage information. The dataset for the
competition can be accessed by clicking the �Data� tab.
First, import the packages or modules required for the competition.
import collections
from d2l import mxnet as d2l
import math
from mxnet import autograd, gluon, init, npx
from mxnet.gluon import nn
import os
import pandas as pd
import shutil
import time
npx.set_np()
13.13. Image Classification (CIFAR-10) on Kaggle 613
13.13.1 Obtaining and Organizing the Dataset
The competition data is divided into a training set and testing set. The training set contains 50; 000
images. The testing set contains 300; 000 images, of which 10; 000 images are used for scoring,
while the other 290; 000 non-scoring images are included to prevent the manual labeling of the
testing set and the submission of labeling results. The image formats in both datasets are PNG,
with heights and widths of 32 pixels and three color channels (RGB). The images cover 10 categories:
planes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks. The upper-left corner
of Figure 9.16 shows some images of planes, cars, and birds in the dataset.
Downloading the Dataset
After logging in to Kaggle, we can click on the �Data� tab on the CIFAR-10 image classification
competition webpage shown in Fig. 13.13.1 and download the dataset by clicking the �Download
All� button. After unzipping the downloaded file in ../data, and unzipping train.7z and test.7z
inside it, you will find the entire dataset in the following paths:
� ../data/cifar-10/train/[1-50000].png
� ../data/cifar-10/test/[1-300000].png
� ../data/cifar-10/trainLabels.csv
� ../data/cifar-10/sampleSubmission.csv
Here folders train and test contain the training and testing images respectively, trainLabels.csv
has labels for the training images, and sample_submission.csv is a sample of submission.
To make it easier to get started, we provide a small-scale sample of the dataset: it contains the first
1000 training images and 5 random testing images. To use the full dataset of the Kaggle competition,
you need to set the following demo variable to False.
#@save
d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')
# If you use the full dataset downloaded for the Kaggle competition, set
# `demo` to False
demo = True
if demo:
data_dir = d2l.download_extract('cifar10_tiny')
else:
data_dir = '../data/cifar-10/'
Downloading ../data/kaggle_cifar10_tiny.zip from http://d2l-data.s3-accelerate.amazonaws.com/
,!kaggle_cifar10_tiny.zip...
614 Chapter 13. Computer Vision
Organizing the Dataset
We need to organize datasets to facilitate model training and testing. Let us first read the labels
from the csv file. The following function returns a dictionary that maps the filename without
extension to its label.
#@save
def read_csv_labels(fname):
"""Read fname to return a name to label dictionary."""
with open(fname, 'r') as f:
# Skip the file header line (column name)
lines = f.readlines()[1:]
tokens = [l.rstrip().split(',') for l in lines]
return dict(((name, label) for name, label in tokens))
labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
print('# training examples:', len(labels))
print('# classes:', len(set(labels.values())))
# training examples: 1000
# classes: 10
Next, we define the reorg_train_valid function to segment the validation set from the original
training set. The argument valid_ratio in this function is the ratio of the number of examples
in the validation set to the number of examples in the original training set. In particular, let n
be the number of images of the class with the least examples, and r be the ratio, then we will
use max(?nr?; 1) images for each class as the validation set. Let us use valid_ratio=0.1 as an
example. Since the original training set has 50; 000 images, there will be 45; 000 images used for
training and stored in the path �train_valid_test/train� when tuning hyperparameters, while
the other 5; 000 images will be stored as validation set in the path �train_valid_test/valid�. After
organizing the data, images of the same class will be placed under the same folder so that we can
read them later.
#@save
def copyfile(filename, target_dir):
"""Copy a file into a target directory."""
d2l.mkdir_if_not_exist(target_dir)
shutil.copy(filename, target_dir)
#@save
def reorg_train_valid(data_dir, labels, valid_ratio):
# The number of examples of the class with the least examples in the
# training dataset
n = collections.Counter(labels.values()).most_common()[-1][1]
# The number of examples per class for the validation set
n_valid_per_label = max(1, math.floor(n * valid_ratio))
label_count = {}
for train_file in os.listdir(os.path.join(data_dir, 'train')):
label = labels[train_file.split('.')[0]]
fname = os.path.join(data_dir, 'train', train_file)
# Copy to train_valid_test/train_valid with a subfolder per class
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'train_valid', label))
(continues on next page)
13.13. Image Classification (CIFAR-10) on Kaggle 615
(continued from previous page)
if label not in label_count or label_count[label] < n_valid_per_label:
# Copy to train_valid_test/valid
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'valid', label))
label_count[label] = label_count.get(label, 0) + 1
else:
# Copy to train_valid_test/train
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'train', label))
return n_valid_per_label
The reorg_test function below is used to organize the testing set to facilitate the reading during
prediction.
#@save
def reorg_test(data_dir):
for test_file in os.listdir(os.path.join(data_dir, 'test')):
copyfile(os.path.join(data_dir, 'test', test_file),
os.path.join(data_dir, 'train_valid_test', 'test',
'unknown'))
Finally, we use a function to call the previously defined read_csv_labels, reorg_train_valid, and
reorg_test functions.
def reorg_cifar10_data(data_dir, valid_ratio):
labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
reorg_train_valid(data_dir, labels, valid_ratio)
reorg_test(data_dir)
We only set the batch size to 4 for the demo dataset. During actual training and testing, the complete
dataset of the Kaggle competition should be used and batch_size should be set to a larger
integer, such as 128. We use 10% of the training examples as the validation set for tuning hyperparameters.
batch_size = 4 if demo else 128
valid_ratio = 0.1
reorg_cifar10_data(data_dir, valid_ratio)
13.13.2 Image Augmentation
To cope with overfitting, we use image augmentation. For example, by adding transforms.
RandomFlipLeftRight(), the images can be flipped at random. We can also perform normalization
for the three RGB channels of color images using transforms.Normalize(). Below, we list some
of these operations that you can choose to use or modify depending on requirements.
transform_train = gluon.data.vision.transforms.Compose([
# Magnify the image to a square of 40 pixels in both height and width
gluon.data.vision.transforms.Resize(40),
# Randomly crop a square image of 40 pixels in both height and width to
# produce a small square of 0.64 to 1 times the area of the original
(continues on next page)
616 Chapter 13. Computer Vision
(continued from previous page)
# image, and then shrink it to a square of 32 pixels in both height and
# width
gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
ratio=(1.0, 1.0)),
gluon.data.vision.transforms.RandomFlipLeftRight(),
gluon.data.vision.transforms.ToTensor(),
# Normalize each channel of the image
gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],
[0.2023, 0.1994, 0.2010])])
In order to ensure the certainty of the output during testing, we only perform normalization on
the image.
transform_test = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.ToTensor(),
gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],
[0.2023, 0.1994, 0.2010])])
13.13.3 Reading the Dataset
Next, we can create the ImageFolderDataset instance to read the organized dataset containing the
original image files, where each example includes the image and label.
train_ds, valid_ds, train_valid_ds, test_ds = [
gluon.data.vision.ImageFolderDataset(
os.path.join(data_dir, 'train_valid_test', folder))
for folder in ['train', 'valid', 'train_valid', 'test']]
We specify the defined image augmentation operation in DataLoader. During training, we only use
the validation set to evaluate the model, so we need to ensure the certainty of the output. During
prediction, we will train the model on the combined training set and validation set to make full
use of all labelled data.
train_iter, train_valid_iter = [gluon.data.DataLoader(
dataset.transform_first(transform_train), batch_size, shuffle=True,
last_batch='discard') for dataset in (train_ds, train_valid_ds)]
valid_iter = gluon.data.DataLoader(
valid_ds.transform_first(transform_test), batch_size, shuffle=False,
last_batch='discard')
test_iter = gluon.data.DataLoader(
test_ds.transform_first(transform_test), batch_size, shuffle=False,
last_batch='keep')
13.13. Image Classification (CIFAR-10) on Kaggle 617
13.13.4 Defining the Model
Here, we build the residual blocks based on the HybridBlock class, which is slightly different than
the implementation described in Section 7.6. This is done to improve execution efficiency.
class Residual(nn.HybridBlock):
def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
super(Residual, self).__init__(**kwargs)
self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
strides=strides)
self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
if use_1x1conv:
self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
strides=strides)
else:
self.conv3 = None
self.bn1 = nn.BatchNorm()
self.bn2 = nn.BatchNorm()
def hybrid_forward(self, F, X):
Y = F.npx.relu(self.bn1(self.conv1(X)))
Y = self.bn2(self.conv2(Y))
if self.conv3:
X = self.conv3(X)
return F.npx.relu(Y + X)
Next, we define the ResNet-18 model.
def resnet18(num_classes):
net = nn.HybridSequential()
net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
nn.BatchNorm(), nn.Activation('relu'))
def resnet_block(num_channels, num_residuals, first_block=False):
blk = nn.HybridSequential()
for i in range(num_residuals):
if i == 0 and not first_block:
blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
else:
blk.add(Residual(num_channels))
return blk
net.add(resnet_block(64, 2, first_block=True),
resnet_block(128, 2),
resnet_block(256, 2),
resnet_block(512, 2))
net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
return net
The CIFAR-10 image classification challenge uses 10 categories. We will perform Xavier random
initialization on the model before training begins.
def get_net(devices):
num_classes = 10
net = resnet18(num_classes)
(continues on next page)
618 Chapter 13. Computer Vision
(continued from previous page)
net.initialize(ctx=devices, init=init.Xavier())
return net
loss = gluon.loss.SoftmaxCrossEntropyLoss()
13.13.5 Defining the Training Functions
We will select the model and tune hyperparameters according to the model?s performance on the
validation set. Next, we define the model training function train. We record the training time of
each epoch, which helps us compare the time costs of different models.
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay):
trainer = gluon.Trainer(net.collect_params(), 'sgd',
{'learning_rate': lr, 'momentum': 0.9, 'wd': wd})
num_batches, timer = len(train_iter), d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
legend=['train loss', 'train acc', 'valid acc'])
for epoch in range(num_epochs):
metric = d2l.Accumulator(3)
if epoch > 0 and epoch % lr_period == 0:
trainer.set_learning_rate(trainer.learning_rate * lr_decay)
for i, (features, labels) in enumerate(train_iter):
timer.start()
l, acc = d2l.train_batch_ch13(
net, features, labels.astype('float32'), loss, trainer,
devices, d2l.split_batch)
metric.add(l, acc, labels.shape[0])
timer.stop()
if (i + 1) % (num_batches // 5) == 0:
animator.add(epoch + i / num_batches,
(metric[0] / metric[2], metric[1] / metric[2],
None))
if valid_iter is not None:
valid_acc = d2l.evaluate_accuracy_gpus(net, valid_iter, d2l.split_batch)
animator.add(epoch + 1, (None, None, valid_acc))
if valid_iter is not None:
print(f'loss {metric[0] / metric[2]:.3f}, '
f'train acc {metric[1] / metric[2]:.3f}, '
f'valid acc {valid_acc:.3f}')
else:
print(f'loss {metric[0] / metric[2]:.3f}, '
f'train acc {metric[1] / metric[2]:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(devices)}')
13.13. Image Classification (CIFAR-10) on Kaggle 619
13.13.6 Training and Validating the Model
Now, we can train and validate the model. The following hyperparameters can be tuned. For
example, we can increase the number of epochs. Because lr_period and lr_decay are set to 80
and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after
every 80 epochs. For simplicity, we only train one epoch here.
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 5, 0.1, 5e-4
lr_period, lr_decay, net = 50, 0.1, get_net(devices)
net.hybridize()
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay)
loss 2.342, train acc 0.113, valid acc 0.113
144.8 examples/sec on [gpu(0), gpu(1)]
13.13.7 Classifying the Testing Set and Submitting Results on Kaggle
After obtaining a satisfactory model design and hyperparameters, we use all training datasets (including
validation sets) to retrain the model and classify the testing set.
net, preds = get_net(devices), []
net.hybridize()
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
lr_decay)
for X, _ in test_iter:
y_hat = net(X.as_in_ctx(devices[0]))
preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())
sorted_ids = list(range(1, len(test_ds) + 1))
sorted_ids.sort(key=lambda x: str(x))
df = pd.DataFrame({'id': sorted_ids, 'label': preds})
df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])
df.to_csv('submission.csv', index=False)
620 Chapter 13. Computer Vision
loss 2.316, train acc 0.128
147.2 examples/sec on [gpu(0), gpu(1)]
After executing the above code, we will get a �submission.csv� file. The format of this file is consistent
with the Kaggle competition requirements. The method for submitting results is similar
to method in Section 4.10.
Summary
� We can create an ImageFolderDataset instance to read the dataset containing the original
image files.
� We can use convolutional neural networks, image augmentation, and hybrid programming
to take part in an image classification competition.
Exercises
1. Use the complete CIFAR-10 dataset for the Kaggle competition. Change the batch_size and
number of epochs num_epochs to 128 and 100, respectively. See what accuracy and ranking
you can achieve in this competition.
2. What accuracy can you achieve when not using image augmentation?
3. Scan the QR code to access the relevant discussions and exchange ideas about the methods
used and the results obtained with the community. Can you come up with any better
techniques?
Discussions196
196 https://discuss.d2l.ai/t/379
13.13. Image Classification (CIFAR-10) on Kaggle 621
13.14 Dog Breed Identification (ImageNet Dogs) on Kaggle
In this section, we will tackle the dog breed identification challenge in the Kaggle Competition.
The competition?s web address is
https://www.kaggle.com/c/dog-breed-identification
In this competition, we attempt to identify 120 different breeds of dogs. The dataset used in this
competition is actually a subset of the famous ImageNet dataset. Different from the images in the
CIFAR-10 dataset used in the previous section, the images in the ImageNet dataset are higher and
wider and their dimensions are inconsistent.
Fig. 13.14.1 shows the information on the competition?s webpage. In order to submit the results,
please register an account on the Kaggle website first.
Fig. 13.14.1: Dog breed identification competition website. The dataset for the competition can
be accessed by clicking the �Data� tab.
First, import the packages or modules required for the competition.
import collections
from d2l import mxnet as d2l
import math
from mxnet import autograd, gluon, init, npx
from mxnet.gluon import nn
import os
import time
npx.set_np()
622 Chapter 13. Computer Vision
13.14.1 Obtaining and Organizing the Dataset
The competition data is divided into a training set and testing set. The training set contains 10; 222
images and the testing set contains 10; 357 images. The images in both sets are in JPEG format.
These images contain three RGB channels (color) and they have different heights and widths.
There are 120 breeds of dogs in the training set, including Labradors, Poodles, Dachshunds,
Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers.
Downloading the Dataset
After logging in to Kaggle, we can click on the �Data� tab on the dog breed identification competition
webpage shown in Fig. 13.14.1 and download the dataset by clicking the �Download All�
button. After unzipping the downloaded file in ../data, you will find the entire dataset in the
following paths:
� ../data/dog-breed-identification/labels.csv
� ../data/dog-breed-identification/sample_submission.csv
� ../data/dog-breed-identification/train
� ../data/dog-breed-identification/test
You may have noticed that the above structure is quite similar to that of the CIFAR-10 competition
in Section 13.13, where folders train/ and test/ contain training and testing dog images respectively,
and labels.csv has the labels for the training images.
Similarly, to make it easier to get started, we provide a small-scale sample of the dataset mentioned
above, �train_valid_test_tiny.zip�. If you are going to use the full dataset for the Kaggle
competition, you will also need to change the demo variable below to False.
#@save
d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')
# If you use the full dataset downloaded for the Kaggle competition, change
# the variable below to False
demo = True
if demo:
data_dir = d2l.download_extract('dog_tiny')
else:
data_dir = os.path.join('..', 'data', 'dog-breed-identification')
Downloading ../data/kaggle_dog_tiny.zip from http://d2l-data.s3-accelerate.amazonaws.com/
,!kaggle_dog_tiny.zip...
13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle 623
Organizing the Dataset
We can organize the dataset similarly to what we did in Section 13.13, namely separating a validation
set from the training set, and moving images into subfolders grouped by labels.
The reorg_dog_data function below is used to read the training data labels, segment the validation
set, and organize the training set.
def reorg_dog_data(data_dir, valid_ratio):
labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))
d2l.reorg_train_valid(data_dir, labels, valid_ratio)
d2l.reorg_test(data_dir)
batch_size = 4 if demo else 128
valid_ratio = 0.1
reorg_dog_data(data_dir, valid_ratio)
13.14.2 Image Augmentation
The size of the images in this section are larger than the images in the previous section. Here are
some more image augmentation operations that might be useful.
transform_train = gluon.data.vision.transforms.Compose([
# Randomly crop the image to obtain an image with an area of 0.08 to 1 of
# the original area and height to width ratio between 3/4 and 4/3. Then,
# scale the image to create a new image with a height and width of 224
# pixels each
gluon.data.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),
ratio=(3.0/4.0, 4.0/3.0)),
gluon.data.vision.transforms.RandomFlipLeftRight(),
# Randomly change the brightness, contrast, and saturation
gluon.data.vision.transforms.RandomColorJitter(brightness=0.4,
contrast=0.4,
saturation=0.4),
# Add random noise
gluon.data.vision.transforms.RandomLighting(0.1),
gluon.data.vision.transforms.ToTensor(),
# Standardize each channel of the image
gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],
[0.229, 0.224, 0.225])])
During testing, we only use definite image preprocessing operations.
transform_test = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.Resize(256),
# Crop a square of 224 by 224 from the center of the image
gluon.data.vision.transforms.CenterCrop(224),
gluon.data.vision.transforms.ToTensor(),
gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],
[0.229, 0.224, 0.225])])
624 Chapter 13. Computer Vision
13.14.3 Reading the Dataset
As in the previous section, we can create an ImageFolderDataset instance to read the dataset containing
the original image files.
train_ds, valid_ds, train_valid_ds, test_ds = [
gluon.data.vision.ImageFolderDataset(
os.path.join(data_dir, 'train_valid_test', folder))
for folder in ('train', 'valid', 'train_valid', 'test')]
Here, we create DataLoader instances, just like in Section 13.13.
train_iter, train_valid_iter = [gluon.data.DataLoader(
dataset.transform_first(transform_train), batch_size, shuffle=True,
last_batch='discard') for dataset in (train_ds, train_valid_ds)]
valid_iter = gluon.data.DataLoader(
valid_ds.transform_first(transform_test), batch_size, shuffle=False,
last_batch='discard')
test_iter = gluon.data.DataLoader(
test_ds.transform_first(transform_test), batch_size, shuffle=False,
last_batch='keep')
13.14.4 Defining the Model
The dataset for this competition is a subset of the ImageNet data set. Therefore, we can use the
approach discussed in Section 13.2 to select a model pre-trained on the entire ImageNet dataset
and use it to extract image features to be input in the custom small-scale output network. Gluon
provides a wide range of pre-trained models. Here, we will use the pre-trained ResNet-34 model.
Because the competition dataset is a subset of the pre-training dataset, we simply reuse the input
of the pre-trained model?s output layer, i.e., the extracted features. Then, we can replace the
original output layer with a small custom output network that can be trained, such as two fully
connected layers in a series. Different from the experiment in Section 13.2, here, we do not retrain
the pre-trained model used for feature extraction. This reduces the training time and the
memory required to store model parameter gradients.
You must note that, during image augmentation, we use the mean values and standard deviations
of the three RGB channels for the entire ImageNet dataset for normalization. This is consistent
with the normalization of the pre-trained model.
def get_net(devices):
finetune_net = gluon.model_zoo.vision.resnet34_v2(pretrained=True)
# Define a new output network
finetune_net.output_new = nn.HybridSequential(prefix='')
finetune_net.output_new.add(nn.Dense(256, activation='relu'))
# There are 120 output categories
finetune_net.output_new.add(nn.Dense(120))
# Initialize the output network
finetune_net.output_new.initialize(init.Xavier(), ctx=devices)
# Distribute the model parameters to the CPUs or GPUs used for computation
finetune_net.collect_params().reset_ctx(devices)
return finetune_net
13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle 625
When calculating the loss, we first use the member variable features to obtain the input of the
pre-trained model?s output layer, i.e., the extracted feature. Then, we use this feature as the input
for our small custom output network and compute the output.
loss = gluon.loss.SoftmaxCrossEntropyLoss()
def evaluate_loss(data_iter, net, devices):
l_sum, n = 0.0, 0
for features, labels in data_iter:
X_shards, y_shards = d2l.split_batch(features, labels, devices)
output_features = [net.features(X_shard) for X_shard in X_shards]
outputs = [net.output_new(feature) for feature in output_features]
ls = [loss(output, y_shard).sum() for output, y_shard
in zip(outputs, y_shards)]
l_sum += sum([float(l.sum()) for l in ls])
n += labels.size
return l_sum / n
13.14.5 Defining the Training Functions
We will select the model and tune hyperparameters according to the model?s performance on the
validation set. The model training function train only trains the small custom output network.
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay):
# Only train the small custom output network
trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',
{'learning_rate': lr, 'momentum': 0.9, 'wd': wd})
num_batches, timer = len(train_iter), d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
legend=['train loss', 'valid loss'])
for epoch in range(num_epochs):
metric = d2l.Accumulator(2)
if epoch > 0 and epoch % lr_period == 0:
trainer.set_learning_rate(trainer.learning_rate * lr_decay)
for i, (features, labels) in enumerate(train_iter):
timer.start()
X_shards, y_shards = d2l.split_batch(features, labels, devices)
output_features = [net.features(X_shard) for X_shard in X_shards]
with autograd.record():
outputs = [net.output_new(feature) for feature in output_features]
ls = [loss(output, y_shard).sum() for output, y_shard
in zip(outputs, y_shards)]
for l in ls:
l.backward()
trainer.step(batch_size)
metric.add(sum([float(l.sum()) for l in ls]), labels.shape[0])
timer.stop()
if (i + 1) % (num_batches // 5) == 0:
animator.add(epoch + i / num_batches,
(metric[0] / metric[1], None))
if valid_iter is not None:
valid_loss = evaluate_loss(valid_iter, net, devices)
animator.add(epoch + 1, (None, valid_loss))
(continues on next page)
626 Chapter 13. Computer Vision
(continued from previous page)
if valid_iter is not None:
print(f'train loss {metric[0] / metric[1]:.3f}, '
f'valid loss {valid_loss:.3f}')
else:
print(f'train loss {metric[0] / metric[1]:.3f}')
print(f'{metric[1] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(devices)}')
13.14.6 Training and Validating the Model
Now, we can train and validate the model. The following hyperparameters can be tuned. For
example, we can increase the number of epochs. Because lr_period and lr_decay are set to 10
and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after
every 10 epochs.
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 5, 0.01, 1e-4
lr_period, lr_decay, net = 10, 0.1, get_net(devices)
net.hybridize()
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
lr_decay)
train loss 2.338, valid loss 2.340
269.6 examples/sec on [gpu(0), gpu(1)]
13.14.7 Classifying the Testing Set and Submitting Results on Kaggle
After obtaining a satisfactory model design and hyperparameters, we use all training datasets (including
validation sets) to retrain the model and then classify the testing set. Note that predictions
are made by the output network we just trained.
net = get_net(devices)
net.hybridize()
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
(continues on next page)
13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle 627
(continued from previous page)
lr_decay)
preds = []
for data, label in test_iter:
output_features = net.features(data.as_in_ctx(devices[0]))
output = npx.softmax(net.output_new(output_features))
preds.extend(output.asnumpy())
ids = sorted(os.listdir(
os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))
with open('submission.csv', 'w') as f:
f.write('id,' + ','.join(train_valid_ds.synsets) + '\n')
for i, output in zip(ids, preds):
f.write(i.split('.')[0] + ',' + ','.join(
[str(num) for num in output]) + '\n')
train loss 2.446
273.0 examples/sec on [gpu(0), gpu(1)]
After executing the above code, we will generate a �submission.csv� file. The format of this file
is consistent with the Kaggle competition requirements. The method for submitting results is
similar to method in Section 4.10.
628 Chapter 13. Computer Vision
Summary
� We can use a model pre-trained on the ImageNet dataset to extract features and only train a
small custom output network. This will allow us to classify a subset of the ImageNet dataset
with lower computing and storage overhead.
Exercises
1. When using the entire Kaggle dataset, what kind of results do you get when you increase the
batch_size (batch size) and num_epochs (number of epochs)?
2. Do you get better results if you use a deeper pre-trained model?
3. Scan the QR code to access the relevant discussions and exchange ideas about the methods
used and the results obtained with the community. Can you come up with any better
techniques?
Discussions197
197 https://discuss.d2l.ai/t/380
13.14. Dog Breed Identification (ImageNet Dogs) on Kaggle 629
630 Chapter 13. Computer Vision
14 | Natural Language Processing: Pretraining
Humans need to communicate. Out of this basic need of the human condition, a vast amount of
written text has been generated on an everyday basis. Given rich text in social media, chat apps,
emails, product reviews, news articles, research papers, and books, it becomes vital to enable
computers to understand them to offer assistance or make decisions based on human languages.
Natural language processing studies interactions between computers and humans using natural
languages. In practice, it is very common to use natural language processing techniques to process
and analyze text (human natural language) data, such as language models in Section 8.3 and
machine translation models in Section 9.5.
To understand text, we can begin with its representation, such as treating each word or subword
as an individual text token. As we will see in this chapter, the representation of each token can
be pretrained on a large corpus, using word2vec, GloVe, or subword embedding models. After
pretraining, representation of each token can be a vector, however, it remains the same no matter
what the context is. For instance, the vector representation of �bank� is the same in both �go
to the bank to deposit some money� and �go to the bank to sit down�. Thus, many more recent
pretraining models adapt representation of the same token to different contexts. Among them is
BERT, a much deeper model based on the Transformer encoder. In this chapter, we will focus on
how to pretrain such representations for text, as highlighted in Fig. 14.1.
Fig. 14.1: Pretrained text representations can be fed to various deep learning architectures for
different downstream natural language processing applications. This chapter focuses on the upstream
text representation pretraining.
631
As shown in Fig. 14.1, the pretrained text representations can be fed to a variety of deep learning
architectures for different downstream natural language processing applications. We will cover
them in Chapter 15.
14.1 Word Embedding (word2vec)
A natural language is a complex system that we use to express meanings. In this system, words
are the basic unit of linguistic meaning. As its name implies, a word vector is a vector used to
represent a word. It can also be thought of as the feature vector of a word. The technique of
mapping words to vectors of real numbers is also known as word embedding. Over the last few
years, word embedding has gradually become basic knowledge in natural language processing.
14.1.1 Why Not Use One-hot Vectors?
We used one-hot vectors to represent words (characters are words) in Section 8.5 . Recall that
when we assume the number of different words in a dictionary (the dictionary size) is N, each
word can correspond one-to-one with consecutive integers from 0 to N ?? 1. These integers that
correspond to words are called the indices of the words. We assume that the index of a word is i.
In order to get the one-hot vector representation of the word, we create a vector of all 0s with a
length of N and set element i to 1. In this way, each word is represented as a vector of length N
that can be used directly by the neural network.
Although one-hot word vectors are easy to construct, they are usually not a good choice. One of the
major reasons is that the one-hot word vectors cannot accurately express the similarity between
different words, such as the cosine similarity that we commonly use. For the vectors x; y 2 Rd,
their cosine similarities are the cosines of the angles between them:
x?y
?x??y?
2 [??1; 1]: (14.1.1)
Since the cosine similarity between the one-hot vectors of any two different words is 0, it is difficult
to use the one-hot vector to accurately represent the similarity between multiple different words.
Word2vec198 is a tool that we came up with to solve the problem above. It represents each word
with a fixed-length vector and uses these vectors to better indicate the similarity and analogy relationships
between different words. The Word2vec tool contains two models: skip-gram (Mikolov
et al., 2013b) and continuous bag of words (CBOW) (Mikolov et al., 2013a). Next, we will take a
look at the two models and their training methods.
14.1.2 The Skip-Gram Model
The skip-gram model assumes that a word can be used to generate the words that surround it in a
text sequence. For example, we assume that the text sequence is �the�, �man�, �loves�, �his�, and
�son�. We use �loves� as the central target word and set the context window size to 2. As shown
in Fig. 14.1.1, given the central target word �loves�, the skip-gram model is concerned with the
conditional probability for generating the context words, �the�, �man�, �his� and �son�, that are
within a distance of no more than 2 words, which is
P("the"; "man"; "his"; "son" j "loves"): (14.1.2)
198 https://code.google.com/archive/p/word2vec/
632 Chapter 14. Natural Language Processing: Pretraining
We assume that, given the central target word, the context words are generated independently of
each other. In this case, the formula above can be rewritten as
P("the" j "loves")  P("man" j "loves")  P("his" j "loves")  P("son" j "loves"): (14.1.3)
Fig. 14.1.1: The skip-gram model cares about the conditional probability of generating context
words for a given central target word.
In the skip-gram model, each word is represented as two d-dimension vectors, which are used to
compute the conditional probability. We assume that the word is indexed as i in the dictionary, its
vector is represented as vi 2 Rd when it is the central target word, and ui 2 Rd when it is a context
word. Let the central target word wc and context word wo be indexed as c and o respectively in the
dictionary. The conditional probability of generating the context word for the given central target
word can be obtained by performing a softmax operation on the vector inner product:
P(wo j wc) =
exp(u?
o vc) ?
i2V exp(u?
i vc)
; (14.1.4)
where vocabulary index set V = f0; 1; : : : ; jVj??1g. Assume that a text sequence of length T is given,
where the word at timestep t is denoted as w(t). Assume that context words are independently
generated given center words. When context window size is m, the likelihood function of the
skip-gram model is the joint probability of generating all the context words given any center word
?T
t=1
?
??mjm; j?=0
P(w(t+j) j w(t)); (14.1.5)
Here, any timestep that is less than 1 or greater than T can be ignored.
Skip-Gram Model Training
The skip-gram model parameters are the central target word vector and context word vector for
each individual word. In the training process, we are going to learn the model parameters by
maximizing the likelihood function, which is also known as maximum likelihood estimation. This
is equivalent to minimizing the following loss function:
??
?T
t=1
?
??mjm; j?=0
log P(w(t+j) j w(t)): (14.1.6)
If we use the SGD, in each iteration we are going to pick a shorter subsequence through random
sampling to compute the loss for that subsequence, and then compute the gradient to update the
14.1. Word Embedding (word2vec) 633
model parameters. The key of gradient computation is to compute the gradient of the logarithmic
conditional probability for the central word vector and the context word vector. By definition, we
first have
log P(wo j wc) = u?
o vc ?? log
(
?
i2V
exp(u?
i vc)
)
: (14.1.7)
Through differentiation, we can get the gradient vc from the formula above.
@log P(wo j wc)
@vc
= uo ??
?
j2V exp(u?
j vc)uj ?
i2V exp(u?
i vc)
= uo ??
?
j2V
(
exp(u?
j vc)
?
i2V exp(u?
i vc)
)
uj
= uo ??
?
j2V
P(wj j wc)uj :
(14.1.8)
Its computation obtains the conditional probability for all the words in the dictionary given the
central target word wc. We then use the same method to obtain the gradients for other word vectors.
After the training, for any word in the dictionary with index i, we are going to get its two word
vector sets vi and ui. In applications of natural language processing, the central target word vector
in the skip-gram model is generally used as the representation vector of a word.
14.1.3 The Continuous Bag of Words (CBOW) Model
The continuous bag of words (CBOW) model is similar to the skip-gram model. The biggest difference
is that the CBOW model assumes that the central target word is generated based on the
context words before and after it in the text sequence. With the same text sequence �the�, �man�,
�loves�, �his� and �son�, in which �loves� is the central target word, given a context window size
of 2, the CBOW model is concerned with the conditional probability of generating the target word
�loves� based on the context words �the�, �man�, �his� and �son�(as shown in Fig. 14.1.2), such as
P("loves" j "the"; "man"; "his"; "son"): (14.1.9)
Fig. 14.1.2: The CBOW model cares about the conditional probability of generating the central
target word from given context words.
Since there are multiple context words in the CBOW model, we will average their word vectors
and then use the same method as the skip-gram model to compute the conditional probability.
634 Chapter 14. Natural Language Processing: Pretraining
We assume that vi 2 Rd and ui 2 Rd are the context word vector and central target word vector
of the word with index i in the dictionary (notice that the symbols are opposite to the ones in the
skip-gram model). Let central target word wc be indexed as c, and context words wo1 ; : : : ;wo2m be
indexed as o1; : : : ; o2m in the dictionary. Thus, the conditional probability of generating a central
target word from the given context word is
P(wc j wo1 ; : : : ;wo2m) =
exp
( 1
2mu?
c (vo1 + : : : ;+vo2m)
)
?
i2V exp
( 1
2mu?
i (vo1 + : : : ;+vo2m)
): (14.1.10)
For brevity, denoteWo = fwo1 ; : : : ;wo2m
g, and vo = (vo1 + : : : ;+vo2m) /(2m). The equation above
can be simplified as
P(wc j Wo) =
exp
(
u?
c vo
)
?
i2V exp
(
u?
i vo
): (14.1.11)
Given a text sequence of length T, we assume that the word at timestep t is w(t), and the context
window size ism. The likelihood function of the CBOW model is the probability of generating any
central target word from the context words.
?T
t=1
P(w(t) j w(t??m); : : : ;w(t??1);w(t+1); : : : ;w(t+m)): (14.1.12)
CBOW Model Training
CBOW model training is quite similar to skip-gram model training. The maximum likelihood estimation
of the CBOW model is equivalent to minimizing the loss function.
??
?T
t=1
log P(w(t) j w(t??m); : : : ;w(t??1);w(t+1); : : : ;w(t+m)): (14.1.13)
Notice that
log P(wc j Wo) = u?
c vo ?? log
(
?
i2V
exp
(
u?
i vo
))
: (14.1.14)
Through differentiation, we can compute the logarithm of the conditional probability of the gradient
of any context word vector voi (i = 1; : : : ; 2m) in the formula above.
@ log P(wc j Wo)
@voi
=
1
2m
0
@uc ??
?
j2V
exp(u?
j vo)uj ?
i2V exp(u?
i vo)
1
A =
1
2m
0
@uc ??
?
j2V
P(wj j Wo)uj
1
A:
(14.1.15)
We then use the same method to obtain the gradients for other word vectors. Unlike the skip-gram
model, we usually use the context word vector as the representation vector for a word in the CBOW
model.
14.1. Word Embedding (word2vec) 635
Summary
� A word vector is a vector used to represent a word. The technique of mapping words to
vectors of real numbers is also known as word embedding.
� Word2vec includes both the continuous bag of words (CBOW) and skip-gram models. The
skip-gram model assumes that context words are generated based on the central target word.
The CBOW model assumes that the central target word is generated based on the context
words.
Exercises
1. What is the computational complexity of each gradient? If the dictionary contains a large
volume of words, what problems will this cause?
2. There are some fixed phrases in the English language which consist of multiple words, such
as �new york�. How can you train their word vectors? Hint: See section 4 in the Word2vec
paper[2].
3. Use the skip-gram model as an example to think about the design of a word2vec model. What
is the relationship between the inner product of two word vectors and the cosine similarity
in the skip-gram model? For a pair of words with close semantical meaning, why it is likely
for their word vector cosine similarity to be high?
Discussions199
14.2 Approximate Training
Recall content of the last section. The core feature of the skip-gram model is the use of softmax
operations to compute the conditional probability of generating context word wo based on the
given central target word wc.
P(wo j wc) =
exp(u?
o vc) ?
i2V exp(u?
i vc)
: (14.2.1)
The logarithmic loss corresponding to the conditional probability is given as
??log P(wo j wc) = ??u?
o vc + log
(
?
i2V
exp(u?
i vc)
)
: (14.2.2)
Because the softmax operation has considered that the context word could be any word in the
dictionary V, the loss mentioned above actually includes the sum of the number of items in the
dictionary size. From the last section, we know that for both the skip-gram model and CBOW
model, because they both get the conditional probability using a softmax operation, the gradient
computation for each step contains the sum of the number of items in the dictionary size. For
larger dictionaries with hundreds of thousands or even millions of words, the overhead for computing
each gradient may be too high. In order to reduce such computational complexity, we will
introduce two approximate training methods in this section: negative sampling and hierarchical
softmax. Since there is no major difference between the skip-gram model and the CBOW model,
we will only use the skip-gram model as an example to introduce these two training methods in
this section.
199 https://discuss.d2l.ai/t/381
636 Chapter 14. Natural Language Processing: Pretraining
14.2.1 Negative Sampling
Negative sampling modifies the original objective function. Given a context window for the central
target word wc, we will treat it as an event for context word wo to appear in the context window
and compute the probability of this event from
P(D = 1 j wc;wo) = (u?
o vc); (14.2.3)
Here, the  function has the same definition as the sigmoid activation function:
(x) =
1
1 + exp(??x)
: (14.2.4)
We will first consider training the word vector by maximizing the joint probability of all events in
the text sequence. Given a text sequence of length T, we assume that the word at timestep t is w(t)
and the context window size is m. Now we consider maximizing the joint probability
?T
t=1
?
??mjm; j?=0
P(D = 1 j w(t);w(t+j)): (14.2.5)
However, the events included in the model only consider positive examples. In this case, only
when all the word vectors are equal and their values approach infinity can the joint probability
above be maximized to 1. Obviously, such word vectors are meaningless. Negative sampling
makes the objective function more meaningful by sampling with an addition of negative examples.
Assume that event P occurs when context word wo appears in the context window of central
target word wc, and we sample K words that do not appear in the context window according to
the distribution P(w) to act as noise words. We assume the event for noise word wk(k = 1; : : : ;K)
to not appear in the context window of central target word wc is Nk. Suppose that events P and
N1; : : : ;NK for both positive and negative examples are independent of each other. By considering
negative sampling, we can rewrite the joint probability above, which only considers the positive
examples, as
?T
t=1
?
??mjm; j?=0
P(w(t+j) j w(t)); (14.2.6)
Here, the conditional probability is approximated to be
P(w(t+j) j w(t)) = P(D = 1 j w(t);w(t+j))
K?
k=1; wkP(w)
P(D = 0 j w(t);wk): (14.2.7)
Let the text sequence index of word w(t) at timestep t be it and hk for noise word wk in the dictionary.
The logarithmic loss for the conditional probability above is
??log P(w(t+j) j w(t)) = ?? log P(D = 1 j w(t);w(t+j)) ??
?K
k=1; wkP(w)
log P(D = 0 j w(t);wk)
= ?? log 
(
u?
it+j vit
)
??
?K
k=1; wkP(w)
log
(
1 ?? 
(
u?
hk vit
))
= ?? log 
(
u?
it+j vit
)
??
?K
k=1; wkP(w)
log 
(
??u?
hk vit
)
:
(14.2.8)
Here, the gradient computation in each step of the training is no longer related to the dictionary
size, but linearly related toK. WhenK takes a smaller constant, the negative sampling has a lower
computational overhead for each step.
14.2. Approximate Training 637
14.2.2 Hierarchical Softmax
Hierarchical softmax is another type of approximate training method. It uses a binary tree for
data structure as illustrated in Fig. 14.2.1, with the leaf nodes of the tree representing every word
in the dictionary V.
Fig. 14.2.1: Hierarchical Softmax. Each leaf node of the tree represents a word in the dictionary.
We assume that L(w) is the number of nodes on the path (including the root and leaf nodes) from
the root node of the binary tree to the leaf node of word w. Let n(w; j) be the jth node on this path,
with the context word vector un(w;j). We use Figure 10.3 as an example, so L(w3) = 4. Hierarchical
softmax will approximate the conditional probability in the skip-gram model as
P(wo j wc) =
L(w?o)??1
j=1

(
[[n(wo; j + 1) = leftChild(n(wo; j))]]  u?
n(wo;j)vc
)
; (14.2.9)
Here the  function has the same definition as the sigmoid activation function, and leftChild(n)
is the left child node of node n. If x is true, [[x]] = 1; otherwise [[x]] = ??1. Now, we will compute
the conditional probability of generating word w3 based on the given word wc in Figure 10.3. We
need to find the inner product of word vector vc (for word wc) and each non-leaf node vector on
the path from the root node to w3. Because, in the binary tree, the path from the root node to leaf
node w3 needs to be traversed left, right, and left again (the path with the bold line in Figure 10.3),
we get
P(w3 j wc) = (u?
n(w3;1)vc)  (??u?
n(w3;2)vc)  (u?
n(w3;3)vc): (14.2.10)
Because (x) + (??x) = 1, the condition that the sum of the conditional probability of any word
generated based on the given central target word wc in dictionary V be 1 will also suffice:
?
w2V
P(w j wc) = 1: (14.2.11)
In addition, because the order of magnitude for L(wo)??1 isO(log2
jVj), when the size of dictionary
V is large, the computational overhead for each step in the hierarchical softmax training is greatly
reduced compared to situations where we do not use approximate training.
638 Chapter 14. Natural Language Processing: Pretraining
Summary
� Negative sampling constructs the loss function by considering independent events that contain
both positive and negative examples. The gradient computational overhead for each
step in the training process is linearly related to the number of noise words we sample.
� Hierarchical softmax uses a binary tree and constructs the loss function based on the path
from the root node to the leaf node. The gradient computational overhead for each step in
the training process is related to the logarithm of the dictionary size.
Exercises
1. Before reading the next section, think about how we should sample noise words in negative
sampling.
2. What makes the last formula in this section hold?
3. How can we apply negative sampling and hierarchical softmax in the skip-gram model?
Discussions200
14.3 The Dataset for Pretraining Word Embedding
In this section, we will introduce how to preprocess a dataset with negative sampling Section 14.2
and load into minibatches for word2vec training. The dataset we use is Penn Tree Bank (PTB)201,
which is a small but commonly-used corpus. It takes samples from Wall Street Journal articles
and includes training sets, validation sets, and test sets.
First, import the packages and modules required for the experiment.
from d2l import mxnet as d2l
import math
from mxnet import gluon, np
import os
import random
14.3.1 Reading and Preprocessing the Dataset
This dataset has already been preprocessed. Each line of the dataset acts as a sentence. All the
words in a sentence are separated by spaces. In the word embedding task, each word is a token.
#@save
d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')
#@save
def read_ptb():
data_dir = d2l.download_extract('ptb')
(continues on next page)
200 https://discuss.d2l.ai/t/382
201 https://catalog.ldc.upenn.edu/LDC99T42
14.3. The Dataset for Pretraining Word Embedding 639
(continued from previous page)
with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
raw_text = f.read()
return [line.split() for line in raw_text.split('\n')]
sentences = read_ptb()
f'# sentences: {len(sentences)}'
Downloading ../data/ptb.zip from http://d2l-data.s3-accelerate.amazonaws.com/ptb.zip...
'# sentences: 42069'
Next we build a vocabulary with words appeared not greater than 10 times mapped into a �<unk>�
token. Note that the preprocessed PTB data also contains �<unk>� tokens presenting rare words.
vocab = d2l.Vocab(sentences, min_freq=10)
f'vocab size: {len(vocab)}'
'vocab size: 6719'
14.3.2 Subsampling
In text data, there are generally some words that appear at high frequencies, such �the�, �a�, and
�in� in English. Generally speaking, in a context window, it is better to train the word embedding
model when a word (such as �chip�) and a lower-frequency word (such as �microprocessor�) appear
at the same time, rather than when a word appears with a higher-frequency word (such as
�the�). Therefore, when training the word embedding model, we can perform subsampling[2] on
the words. Specifically, each indexed word wi in the dataset will drop out at a certain probability.
The dropout probability is given as:
P(wi) = max
(
1 ??
v
t
f(wi)
; 0
)
; (14.3.1)
Here, f(wi) is the ratio of the instances of word wi to the total number of words in the dataset,
and the constant t is a hyperparameter (set to 10??4 in this experiment). As we can see, it is only
possible to drop out the word wi in subsampling when f(wi) > t. The higher the word?s frequency,
the higher its dropout probability.
#@save
def subsampling(sentences, vocab):
# Map low frequency words into <unk>
sentences = [[vocab.idx_to_token[vocab[tk]] for tk in line]
for line in sentences]
# Count the frequency for each word
counter = d2l.count_corpus(sentences)
num_tokens = sum(counter.values())
# Return True if to keep this token during subsampling
def keep(token):
(continues on next page)
640 Chapter 14. Natural Language Processing: Pretraining
(continued from previous page)
return(random.uniform(0, 1) <
math.sqrt(1e-4 / counter[token] * num_tokens))
# Now do the subsampling
return [[tk for tk in line if keep(tk)] for line in sentences]
subsampled = subsampling(sentences, vocab)
Compare the sequence lengths before and after sampling, we can see subsampling significantly
reduced the sequence length.
d2l.set_figsize()
d2l.plt.hist([[len(line) for line in sentences],
[len(line) for line in subsampled]])
d2l.plt.xlabel('# tokens per sentence')
d2l.plt.ylabel('count')
d2l.plt.legend(['origin', 'subsampled']);
For individual tokens, the sampling rate of the high-frequency word �the� is less than 1/20.
def compare_counts(token):
return (f'# of "{token}": '
f'before={sum([line.count(token) for line in sentences])}, '
f'after={sum([line.count(token) for line in subsampled])}')
compare_counts('the')
'# of "the": before=50770, after=2083'
But the low-frequency word �join� is completely preserved.
compare_counts('join')
'# of "join": before=45, after=45'
Last, we map each token into an index to construct the corpus.
14.3. The Dataset for Pretraining Word Embedding 641
corpus = [vocab[line] for line in subsampled]
corpus[0:3]
[[0], [392, 2132, 406], [5464, 3080, 1595, 95]]
14.3.3 Loading the Dataset
Next we read the corpus with token indicies into data batches for training.
Extracting Central Target Words and Context Words
We use words with a distance from the central target word not exceeding the context window size
as the context words of the given center target word. The following definition function extracts all
the central target words and their context words. It uniformly and randomly samples an integer to
be used as the context window size between integer 1 and the max_window_size (maximum context
window).
#@save
def get_centers_and_contexts(corpus, max_window_size):
centers, contexts = [], []
for line in corpus:
# Each sentence needs at least 2 words to form a "central target word
# - context word" pair
if len(line) < 2:
continue
centers += line
for i in range(len(line)): # Context window centered at i
window_size = random.randint(1, max_window_size)
indices = list(range(max(0, i - window_size),
min(len(line), i + 1 + window_size)))
# Exclude the central target word from the context words
indices.remove(i)
contexts.append([line[idx] for idx in indices])
return centers, contexts
Next, we create an artificial dataset containing two sentences of 7 and 3 words, respectively. Assume
the maximum context window is 2 and print all the central target words and their context
words.
tiny_dataset = [list(range(7)), list(range(7, 10))]
print('dataset', tiny_dataset)
for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):
print('center', center, 'has contexts', context)
dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]
center 0 has contexts [1, 2]
center 1 has contexts [0, 2, 3]
center 2 has contexts [1, 3]
center 3 has contexts [1, 2, 4, 5]
(continues on next page)
642 Chapter 14. Natural Language Processing: Pretraining
(continued from previous page)
center 4 has contexts [3, 5]
center 5 has contexts [3, 4, 6]
center 6 has contexts [4, 5]
center 7 has contexts [8]
center 8 has contexts [7, 9]
center 9 has contexts [7, 8]
We set the maximum context window size to 5. The following extracts all the central target words
and their context words in the dataset.
all_centers, all_contexts = get_centers_and_contexts(corpus, 5)
f'# center-context pairs: {len(all_centers)}'
'# center-context pairs: 353005'
Negative Sampling
We use negative sampling for approximate training. For a central and context word pair, we randomly
sample K noise words (K = 5 in the experiment). According to the suggestion in the
Word2vec paper, the noise word sampling probability P(w) is the ratio of the word frequency of
w to the total word frequency raised to the power of 0.75 [2].
We first define a class to draw a candidate according to the sampling weights. It caches a 10000
size random number bank instead of calling random.choices every time.
#@save
class RandomGenerator:
"""Draw a random int in [0, n] according to n sampling weights."""
def __init__(self, sampling_weights):
self.population = list(range(len(sampling_weights)))
self.sampling_weights = sampling_weights
self.candidates = []
self.i = 0
def draw(self):
if self.i == len(self.candidates):
self.candidates = random.choices(
self.population, self.sampling_weights, k=10000)
self.i = 0
self.i += 1
return self.candidates[self.i-1]
generator = RandomGenerator([2, 3, 4])
[generator.draw() for _ in range(10)]
[2, 1, 1, 0, 0, 2, 0, 0, 2, 1]
#@save
def get_negatives(all_contexts, corpus, K):
(continues on next page)
14.3. The Dataset for Pretraining Word Embedding 643
(continued from previous page)
counter = d2l.count_corpus(corpus)
sampling_weights = [counter[i]**0.75 for i in range(len(counter))]
all_negatives, generator = [], RandomGenerator(sampling_weights)
for contexts in all_contexts:
negatives = []
while len(negatives) < len(contexts) * K:
neg = generator.draw()
# Noise words cannot be context words
if neg not in contexts:
negatives.append(neg)
all_negatives.append(negatives)
return all_negatives
all_negatives = get_negatives(all_contexts, corpus, 5)
Reading into Batches
We extract all central target words all_centers, and the context words all_contexts and noise
words all_negatives of each central target word from the dataset. We will read them in random
minibatches.
In a minibatch of data, the ith example includes a central word and its corresponding ni context
words and mi noise words. Since the context window size of each example may be different,
the sum of context words and noise words, ni + mi, will be different. When constructing a
minibatch, we concatenate the context words and noise words of each example, and add 0s for
padding until the length of the concatenations are the same, that is, the length of all concatenations
is maxi ni + mi(max_len). In order to avoid the effect of padding on the loss function
calculation, we construct the mask variable masks, each element of which corresponds to an element
in the concatenation of context and noise words, contexts_negatives. When an element
in the variable contexts_negatives is a padding, the element in the mask variable masks at the
same position will be 0. Otherwise, it takes the value 1. In order to distinguish between positive
and negative examples, we also need to distinguish the context words from the noise words in the
contexts_negatives variable. Based on the construction of the mask variable, we only need to
create a label variable labels with the same shape as the contexts_negatives variable and set the
elements corresponding to context words (positive examples) to 1, and the rest to 0.
Next, we will implement the minibatch reading function batchify. Its minibatch input data is a
list whose length is the batch size, each element of which contains central target words center,
context words context, and noise words negative. The minibatch data returned by this function
conforms to the format we need, for example, it includes the mask variable.
#@save
def batchify(data):
max_len = max(len(c) + len(n) for _, c, n in data)
centers, contexts_negatives, masks, labels = [], [], [], []
for center, context, negative in data:
cur_len = len(context) + len(negative)
centers += [center]
contexts_negatives += [context + negative + [0] * (max_len - cur_len)]
masks += [[1] * cur_len + [0] * (max_len - cur_len)]
labels += [[1] * len(context) + [0] * (max_len - len(context))]
(continues on next page)
644 Chapter 14. Natural Language Processing: Pretraining
(continued from previous page)
return (np.array(centers).reshape(-1, 1), np.array(contexts_negatives),
np.array(masks), np.array(labels))
Construct two simple examples:
x_1 = (1, [2, 2], [3, 3, 3, 3])
x_2 = (1, [2, 2, 2], [3, 3])
batch = batchify((x_1, x_2))
names = ['centers', 'contexts_negatives', 'masks', 'labels']
for name, data in zip(names, batch):
print(name, '=', data)
centers = [[1.]
[1.]]
contexts_negatives = [[2. 2. 3. 3. 3. 3.]
[2. 2. 2. 3. 3. 0.]]
masks = [[1. 1. 1. 1. 1. 1.]
[1. 1. 1. 1. 1. 0.]]
labels = [[1. 1. 0. 0. 0. 0.]
[1. 1. 1. 0. 0. 0.]]
We use the batchify function just defined to specify the minibatch reading method in the DataLoader
instance.
14.3.4 Putting All Things Together
Last, we define the load_data_ptb function that read the PTB dataset and return the data iterator.
#@save
def load_data_ptb(batch_size, max_window_size, num_noise_words):
num_workers = d2l.get_dataloader_workers()
sentences = read_ptb()
vocab = d2l.Vocab(sentences, min_freq=10)
subsampled = subsampling(sentences, vocab)
corpus = [vocab[line] for line in subsampled]
all_centers, all_contexts = get_centers_and_contexts(
corpus, max_window_size)
all_negatives = get_negatives(all_contexts, corpus, num_noise_words)
dataset = gluon.data.ArrayDataset(
all_centers, all_contexts, all_negatives)
data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True,
batchify_fn=batchify,
num_workers=num_workers)
return data_iter, vocab
Let us print the first minibatch of the data iterator.
data_iter, vocab = load_data_ptb(512, 5, 5)
for batch in data_iter:
for name, data in zip(names, batch):
(continues on next page)
14.3. The Dataset for Pretraining Word Embedding 645
(continued from previous page)
print(name, 'shape:', data.shape)
break
centers shape: (512, 1)
contexts_negatives shape: (512, 60)
masks shape: (512, 60)
labels shape: (512, 60)
Summary
� Subsampling attempts to minimize the impact of high-frequency words on the training of a
word embedding model.
� We can pad examples of different lengths to create minibatches with examples of all the
same length and use mask variables to distinguish between padding and non-padding elements,
so that only non-padding elements participate in the calculation of the loss function.
Exercises
1. We use the batchify function to specify the minibatch reading method in the DataLoader
instance and print the shape of each variable in the first batch read. How should these shapes
be calculated?
Discussions202
14.4 Pretraining word2vec
In this section, we will train a skip-gram model defined in Section 14.1.
First, import the packages and modules required for the experiment, and load the PTB dataset.
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
npx.set_np()
batch_size, max_window_size, num_noise_words = 512, 5, 5
data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
num_noise_words)
202 https://discuss.d2l.ai/t/383
646 Chapter 14. Natural Language Processing: Pretraining
14.4.1 The Skip-Gram Model
We will implement the skip-gram model by using embedding layers and minibatch multiplication.
These methods are also often used to implement other natural language processing applications.
Embedding Layer
The layer in which the obtained word is embedded is called the embedding layer, which can be
obtained by creating an nn.Embedding instance in Gluon. The weight of the embedding layer is a
matrix whose number of rows is the dictionary size (input_dim) and whose number of columns
is the dimension of each word vector (output_dim). We set the dictionary size to 20 and the word
vector dimension to 4.
embed = nn.Embedding(input_dim=20, output_dim=4)
embed.initialize()
embed.weight
Parameter embedding0_weight (shape=(20, 4), dtype=float32)
The input of the embedding layer is the index of the word. When we enter the index i of a word,
the embedding layer returns the ith row of the weight matrix as its word vector. Below we enter
an index of shape (2, 3) into the embedding layer. Because the dimension of the word vector is 4,
we obtain a word vector of shape (2, 3, 4).
x = np.array([[1, 2, 3], [4, 5, 6]])
embed(x)
array([[[ 0.01438687, 0.05011239, 0.00628365, 0.04861524],
[-0.01068833, 0.01729892, 0.02042518, -0.01618656],
[-0.00873779, -0.02834515, 0.05484822, -0.06206018]],
[[ 0.06491279, -0.03182812, -0.01631819, -0.00312688],
[ 0.0408415 , 0.04370362, 0.00404529, -0.0028032 ],
[ 0.00952624, -0.01501013, 0.05958354, 0.04705103]]])
Minibatch Multiplication
We can multiply the matrices in two minibatches one by one, by the minibatch multiplication
operation batch_dot. Suppose the first batch contains n matrices X1; : : : ; Xn with a shape of ab,
and the second batch contains n matrices Y1; : : : ; Yn with a shape of b  c. The output of matrix
multiplication on these two batches are n matrices X1Y1; : : : ; XnYn with a shape of ac. Therefore,
given two tensors of shape (n, a, b) and (n, b, c), the shape of the minibatch multiplication output
is (n, a, c).
X = np.ones((2, 1, 4))
Y = np.ones((2, 4, 6))
npx.batch_dot(X, Y).shape
14.4. Pretraining word2vec 647
(2, 1, 6)
Skip-gram Model Forward Calculation
In forward calculation, the input of the skip-gram model contains the central target word index
center and the concatenated context and noise word index contexts_and_negatives. In which,
the center variable has the shape (batch size, 1), while the contexts_and_negatives variable has
the shape (batch size, max_len). These two variables are first transformed from word indexes to
word vectors by the word embedding layer, and then the output of shape (batch size, 1, max_len)
is obtained by minibatch multiplication. Each element in the output is the inner product of the
central target word vector and the context word vector or noise word vector.
def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
v = embed_v(center)
u = embed_u(contexts_and_negatives)
pred = npx.batch_dot(v, u.swapaxes(1, 2))
return pred
Verify that the output shape should be (batch size, 1, max_len).
skip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape
(2, 1, 4)
14.4.2 Training
Before training the word embedding model, we need to define the loss function of the model.
Binary Cross Entropy Loss Function
According to the definition of the loss function in negative sampling, we can directly use Gluon?s
binary cross-entropy loss function SigmoidBinaryCrossEntropyLoss.
loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()
It is worth mentioning that we can use the mask variable to specify the partial predicted value
and label that participate in loss function calculation in the minibatch: when the mask is 1, the
predicted value and label of the corresponding position will participate in the calculation of the
loss function; When the mask is 0, the predicted value and label of the corresponding position do
not participate in the calculation of the loss function. As we mentioned earlier, mask variables
can be used to avoid the effect of padding on loss function calculations.
Given two identical examples, different masks lead to different loss values.
pred = np.array([[.5]*4]*2)
label = np.array([[1, 0, 1, 0]]*2)
mask = np.array([[1, 1, 1, 1], [1, 1, 0, 0]])
loss(pred, label, mask)
648 Chapter 14. Natural Language Processing: Pretraining
array([0.724077 , 0.3620385])
We can normalize the loss in each example due to various lengths in each example.
loss(pred, label, mask) / mask.sum(axis=1) * mask.shape[1]
array([0.724077, 0.724077])
Initializing Model Parameters
We construct the embedding layers of the central and context words, respectively, and set the
hyperparameter word vector dimension embed_size to 100.
embed_size = 100
net = nn.Sequential()
net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),
nn.Embedding(input_dim=len(vocab), output_dim=embed_size))
Training
The training function is defined below. Because of the existence of padding, the calculation of the
loss function is slightly different compared to the previous training functions.
def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):
net.initialize(ctx=device, force_reinit=True)
trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': lr})
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs])
for epoch in range(num_epochs):
timer = d2l.Timer()
metric = d2l.Accumulator(2) # Sum of losses, no. of tokens
for i, batch in enumerate(data_iter):
center, context_negative, mask, label = [
data.as_in_ctx(device) for data in batch]
with autograd.record():
pred = skip_gram(center, context_negative, net[0], net[1])
l = (loss(pred.reshape(label.shape), label, mask)
/ mask.sum(axis=1) * mask.shape[1])
l.backward()
trainer.step(batch_size)
metric.add(l.sum(), l.size)
if (i+1) % 50 == 0:
animator.add(epoch+(i+1)/len(data_iter),
(metric[0]/metric[1],))
print(f'loss {metric[0] / metric[1]:.3f}, '
f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')
Now, we can train a skip-gram model using negative sampling.
14.4. Pretraining word2vec 649
lr, num_epochs = 0.01, 5
train(net, data_iter, lr, num_epochs)
loss 0.331, 19659.4 tokens/sec on gpu(0)
14.4.3 Applying the Word Embedding Model
After training the word embedding model, we can represent similarity in meaning between words
based on the cosine similarity of two word vectors. As we can see, when using the trained word
embedding model, the words closest in meaning to the word �chip� are mostly related to chips.
def get_similar_tokens(query_token, k, embed):
W = embed.weight.data()
x = W[vocab[query_token]]
# Compute the cosine similarity. Add 1e-9 for numerical stability
cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)
topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')
for i in topk[1:]: # Remove the input words
print(f'cosine sim={float(cos[i]):.3f}: {vocab.idx_to_token[i]}')
get_similar_tokens('chip', 3, net[0])
cosine sim=0.572: intel
cosine sim=0.453: microprocessor
cosine sim=0.442: bugs
650 Chapter 14. Natural Language Processing: Pretraining
Summary
� We can pretrain a skip-gram model through negative sampling.
Exercises
1. Set sparse_grad=True when creating an instance of nn.Embedding. Does it accelerate training?
Look up MXNet documentation to learn the meaning of this argument.
2. Try to find synonyms for other words.
3. Tune the hyperparameters and observe and analyze the experimental results.
4. When the dataset is large, we usually sample the context words and the noise words for the
central target word in the current minibatch only when updating the model parameters. In
other words, the same central target word may have different context words or noise words
in different epochs. What are the benefits of this sort of training? Try to implement this
training method.
Discussions203
14.5 Word Embedding with Global Vectors (GloVe)
First, we should review the skip-gram model in word2vec. The conditional probability P(wj j wi)
expressed in the skip-gram model using the softmax operation will be recorded as qij , that is:
qij =
exp(u?
j vi)
?
k2V exp(u?
k vi)
; (14.5.1)
where vi and ui are the vector representations of word wi of index i as the center word and context
word respectively, and V = f0; 1; : : : ; jVj ?? 1g is the vocabulary index set.
For word wi, it may appear in the dataset for multiple times. We collect all the context words
every time when wi is a center word and keep duplicates, denoted as multiset Ci. The number
of an element in a multiset is called the multiplicity of the element. For instance, suppose that
word wi appears twice in the dataset: the context windows when these two wi become center
words in the text sequence contain context word indices 2; 1; 5; 2 and 2; 3; 2; 1. Then, multiset
Ci = f1; 1; 2; 2; 2; 2; 3; 5g, where multiplicity of element 1 is 2, multiplicity of element 2 is 4, and
multiplicities of elements 3 and 5 are both 1. Denote multiplicity of element j in multiset Ci as xij :
it is the number of word wj in all the context windows for center word wi in the entire dataset. As
a result, the loss function of the skip-gram model can be expressed in a different way:
??
?
i2V
?
j2V
xij log qij : (14.5.2)
We add up the number of all the context words for the central target word wi to get xi, and record
the conditional probability xij/xi for generating context word wj based on central target word wi
as pij . We can rewrite the loss function of the skip-gram model as
??
?
i2V
xi
?
j2V
pij log qij : (14.5.3)
203 https://discuss.d2l.ai/t/384
14.5. Word Embedding with Global Vectors (GloVe) 651
In the formula above,
?
j2V pij log qij computes the conditional probability distribution pij for
context word generation based on the central target word wi and the cross-entropy of conditional
probability distribution qij predicted by the model. The loss function is weighted using the sum
of the number of context words with the central target word wi. If we minimize the loss function
from the formula above, we will be able to allow the predicted conditional probability distribution
to approach as close as possible to the true conditional probability distribution.
However, although the most common type of loss function, the cross-entropy loss function is
sometimes not a good choice. On the one hand, as we mentioned in Section 14.2 the cost of letting
the model prediction qij become the legal probability distribution has the sum of all items in the
entire dictionary in its denominator. This can easily lead to excessive computational overhead.
On the other hand, there are often a lot of uncommon words in the dictionary, and they appear
rarely in the dataset. In the cross-entropy loss function, the final prediction of the conditional
probability distribution on a large number of uncommon words is likely to be inaccurate.
14.5.1 The GloVe Model
To address this, GloVe (Pennington et al., 2014), a word embedding model that came after
word2vec, adopts square loss and makes three changes to the skip-gram model based on this loss.
1. Here, we use the non-probability distribution variables p?
ij = xij and q?
ij = exp(u?
j vi) and
take their logs. Therefore, we get the square loss
(
log p?
ij
?? log q?
ij
)2
=
(
u?
j vi ?? log xij
)2
.
2. We add two scalar model parameters for each word wi: the bias terms bi (for central target
words) and ci( for context words).
3. Replace the weight of each loss with the function h(xij). The weight function h(x) is a monotone
increasing function with the range [0; 1].
Therefore, the goal of GloVe is to minimize the loss function.
?
i2V
?
j2V
h(xij)
(
u?
j vi + bi + cj ?? log xij
)2
: (14.5.4)
Here, we have a suggestion for the choice of weight function h(x): when x < c (e.g c = 100), make
h(x) = (x/c) (e.g  = 0:75), otherwise make h(x) = 1. Because h(0) = 0, the squared loss term
for xij = 0 can be simply ignored. When we use minibatch SGD for training, we conduct random
sampling to get a non-zero minibatch xij from each timestep and compute the gradient to update
the model parameters. These non-zero xij are computed in advance based on the entire dataset
and they contain global statistics for the dataset. Therefore, the name GloVe is taken from �Global
Vectors�.
Notice that if word wi appears in the context window of word wj , then word wj will also appear in
the context window of word wi. Therefore, xij = xji. Unlike word2vec, GloVe fits the symmetric
log xij in lieu of the asymmetric conditional probability pij . Therefore, the central target word
vector and context word vector of any word are equivalent in GloVe. However, the two sets of word
vectors that are learned by the same word may be different in the end due to different initialization
values. After learning all the word vectors, GloVe will use the sum of the central target word vector
and the context word vector as the final word vector for the word.
652 Chapter 14. Natural Language Processing: Pretraining
14.5.2 Understanding GloVe from Conditional Probability Ratios
We can also try to understand GloVe word embedding from another perspective. We will continue
the use of symbols from earlier in this section, P(wj j wi) represents the conditional probability
of generating context word wj with central target word wi in the dataset, and it will be recorded as
pij . From a real example from a large corpus, here we have the following two sets of conditional
probabilities with �ice� and �steam� as the central target words and the ratio between them:
wk= �solid� �gas� �water� �fashion�
p1 = P(wk j �ice� ) 0.00019 0.000066 0.003 0.000017
p2 = P(wk j �steam� ) 0.000022 0.00078 0.0022 0.000018
p1/p2 8.9 0.085 1.36 0.96
We will be able to observe phenomena such as:
� For a word wk that is related to �ice� but not to �steam�, such as wk =�solid�, we would expect
a larger conditional probability ratio, like the value 8.9 in the last row of the table above.
� For a word wk that is related to �steam� but not to �ice�, such as wk =�gas�, we would expect
a smaller conditional probability ratio, like the value 0.085 in the last row of the table above.
� For a word wk that is related to both �ice� and �steam�, such as wk =�water�, we would expect
a conditional probability ratio close to 1, like the value 1.36 in the last row of the table above.
� For a word wk that is related to neither �ice� or �steam�, such as wk =�fashion�, we would
expect a conditional probability ratio close to 1, like the value 0.96 in the last row of the table
above.
We can see that the conditional probability ratio can represent the relationship between different
words more intuitively. We can construct a word vector function to fit the conditional probability
ratio more effectively. As we know, to obtain any ratio of this type requires three words wi, wj ,
and wk. The conditional probability ratio with wi as the central target word is pij/pik. We can find
a function that uses word vectors to fit this conditional probability ratio.
f(uj ; uk; vi)  pij
pik
: (14.5.5)
The possible design of function f here will not be unique. We only need to consider a more reasonable
possibility. Notice that the conditional probability ratio is a scalar, we can limit f to be a scalar
function: f(uj ; uk; vi) = f
(
(uj ?? uk)?vi
)
. After exchanging index j with k, we will be able to see
that function f satisfies the condition f(x)f(??x) = 1, so one possibility could be f(x) = exp(x).
Thus:
f(uj ; uk; vi) =
exp
(
u?
j vi
)
exp
(
u?
k vi
)  pij
pik
: (14.5.6)
One possibility that satisfies the right side of the approximation sign is exp
(
u?
j vi
)
 pij , where
 is a constant. Considering that pij = xij/xi, after taking the logarithm we get u?
j vi  log  +
log xij??log xi. We use additional bias terms to fit??log +log xi, such as the central target word
bias term bi and context word bias term cj :
u?
j vi + bi + cj  log(xij): (14.5.7)
By taking the square error and weighting the left and right sides of the formula above, we can get
the loss function of GloVe.
14.5. Word Embedding with Global Vectors (GloVe) 653
Summary
� In some cases, the cross-entropy loss function may have a disadvantage. GloVe uses squared
loss and the word vector to fit global statistics computed in advance based on the entire
dataset.
� The central target word vector and context word vector of any word are equivalent in GloVe.
Exercises
1. If a word appears in the context window of another word, how can we use the distance between
them in the text sequence to redesign the method for computing the conditional probability
pij ? Hint: See section 4.2 from the paper GloVe (Pennington et al., 2014).
2. For any word, will its central target word bias term and context word bias term be equivalent
to each other in GloVe? Why?
Discussions204
14.6 Subword Embedding
English words usually have internal structures and formation methods. For example, we can deduce
the relationship between �dog�, �dogs�, and �dogcatcher� by their spelling. All these words
have the same root, �dog�, but they use different suffixes to change the meaning of the word.
Moreover, this association can be extended to other words. For example, the relationship between
�dog� and �dogs� is just like the relationship between �cat� and �cats�. The relationship
between �boy� and �boyfriend� is just like the relationship between �girl� and �girlfriend�. This
characteristic is not unique to English. In French and Spanish, a lot of verbs can have more than
40 different forms depending on the context. In Finnish, a noun may have more than 15 forms. In
fact, morphology, which is an important branch of linguistics, studies the internal structure and
formation of words.
14.6.1 fastText
In word2vec, we did not directly use morphology information. In both the skip-gram model and
continuous bag-of-words model, we use different vectors to represent words with different forms.
For example, �dog� and �dogs� are represented by two different vectors, while the relationship
between these two vectors is not directly represented in the model. In view of this, fastText (Bojanowski
et al., 2017) proposes the method of subword embedding, thereby attempting to introduce
morphological information in the skip-gram model in word2vec.
In fastText, each central word is represented as a collection of subwords. Below we use the word
�where� as an example to understand how subwords are formed. First, we add the special characters
�<� and �>� at the beginning and end of the word to distinguish the subwords used as prefixes
and suffixes. Then, we treat the word as a sequence of characters to extract the n-grams. For
example, when n = 3, we can get all subwords with a length of 3:
"<wh"; "whe"; "her"; "ere"; "re>"; (14.6.1)
204 https://discuss.d2l.ai/t/385
654 Chapter 14. Natural Language Processing: Pretraining
and the special subword "<where>".
In fastText, for a word w, we record the union of all its subwords with length of 3 to 6 and special
subwords as Gw. Thus, the dictionary is the union of the collection of subwords of all words.
Assume the vector of the subword g in the dictionary is zg. Then, the central word vector uw for
the word w in the skip-gram model can be expressed as
uw =
?
g2Gw
zg: (14.6.2)
The rest of the fastText process is consistent with the skip-gram model, so it is not repeated here.
As we can see, compared with the skip-gram model, the dictionary in fastText is larger, resulting
in more model parameters. Also, the vector of one word requires the summation of all subword
vectors, which results in higher computation complexity. However, we can obtain better vectors
for more uncommon complex words, even words not existing in the dictionary, by looking at other
words with similar structures.
14.6.2 Byte Pair Encoding
In fastText, all the extracted subwords have to be of the specified lengths, such as 3 to 6, thus the
vocabulary size cannot be predefined. To allow for variable-length subwords in a fixed-size vocabulary,
we can apply a compression algorithm called byte pair encoding (BPE) to extract subwords
(Sennrich et al., 2015).
Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols
within a word, such as consecutive characters of arbitrary length. Starting from symbols of
length 1, byte pair encoding iteratively merges the most frequent pair of consecutive symbols to
produce new longer symbols. Note that for efficiency, pairs crossing word boundaries are not considered.
In the end, we can use such symbols as subwords to segment words. Byte pair encoding
and its variants has been used for input representations in popular natural language processing
pretraining models such as GPT-2 (Radford et al., 2019) and RoBERTa (Liu et al., 2019). In the
following, we will illustrate how byte pair encoding works.
First, we initialize the vocabulary of symbols as all the English lowercase characters, a special
end-of-word symbol '_', and a special unknown symbol '[UNK]'.
import collections
symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
'_', '[UNK]']
Since we do not consider symbol pairs that cross boundaries of words, we only need a dictionary
raw_token_freqs that maps words to their frequencies (number of occurrences) in a dataset. Note
that the special symbol '_' is appended to each word so that we can easily recover a word sequence
(e.g., �a taller man�) from a sequence of output symbols ( e.g., �a_ tall er_ man�). Since we start
the merging process from a vocabulary of only single characters and special symbols, space is
inserted between every pair of consecutive characters within each word (keys of the dictionary
token_freqs). In other words, space is the delimiter between symbols within a word.
raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}
token_freqs = {}
(continues on next page)
14.6. Subword Embedding 655
(continued from previous page)
for token, freq in raw_token_freqs.items():
token_freqs[' '.join(list(token))] = raw_token_freqs[token]
token_freqs
{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}
We define the following get_max_freq_pair function that returns the most frequent pair of consecutive
symbols within a word, where words come from keys of the input dictionary token_freqs.
def get_max_freq_pair(token_freqs):
pairs = collections.defaultdict(int)
for token, freq in token_freqs.items():
symbols = token.split()
for i in range(len(symbols) - 1):
# Key of `pairs` is a tuple of two consecutive symbols
pairs[symbols[i], symbols[i + 1]] += freq
return max(pairs, key=pairs.get) # Key of `pairs` with the max value
As a greedy approach based on frequency of consecutive symbols, byte pair encoding will use
the following merge_symbols function to merge the most frequent pair of consecutive symbols to
produce new symbols.
def merge_symbols(max_freq_pair, token_freqs, symbols):
symbols.append(''.join(max_freq_pair))
new_token_freqs = dict()
for token, freq in token_freqs.items():
new_token = token.replace(' '.join(max_freq_pair),
''.join(max_freq_pair))
new_token_freqs[new_token] = token_freqs[token]
return new_token_freqs
Now we iteratively perform the byte pair encoding algorithm over the keys of the dictionary token_
freqs. In the first iteration, the most frequent pair of consecutive symbols are 't' and 'a',
thus byte pair encoding merges them to produce a new symbol 'ta'. In the second iteration, byte
pair encoding continues to merge 'ta' and 'l' to result in another new symbol 'tal'.
num_merges = 10
for i in range(num_merges):
max_freq_pair = get_max_freq_pair(token_freqs)
token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)
print(f'merge #{i + 1}:', max_freq_pair)
merge #1: ('t', 'a')
merge #2: ('ta', 'l')
merge #3: ('tal', 'l')
merge #4: ('f', 'a')
merge #5: ('fa', 's')
merge #6: ('fas', 't')
merge #7: ('e', 'r')
merge #8: ('er', '_')
merge #9: ('tall', '_')
merge #10: ('fast', '_')
656 Chapter 14. Natural Language Processing: Pretraining
After 10 iterations of byte pair encoding, we can see that list symbols now contains 10 more symbols
that are iteratively merged from other symbols.
print(symbols)
['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's
,!', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast
,!', 'er', 'er_', 'tall_', 'fast_']
For the same dataset specified in the keys of the dictionary raw_token_freqs, each word in the
dataset is now segmented by subwords �fast_�, �fast�, �er_�, �tall_�, and �tall� as a result of the byte
pair encoding algorithm. For instance, words �faster_� and �taller_� are segmented as �fast er_�
and �tall er_�, respectively.
print(list(token_freqs.keys()))
['fast_', 'fast er_', 'tall_', 'tall er_']
Note that the result of byte pair encoding depends on the dataset being used. We can also use the
subwords learned from one dataset to segment words of another dataset. As a greedy approach,
the following segment_BPE function tries to break words into the longest possible subwords from
the input argument symbols.
def segment_BPE(tokens, symbols):
outputs = []
for token in tokens:
start, end = 0, len(token)
cur_output = []
# Segment token with the longest possible subwords from symbols
while start < len(token) and start < end:
if token[start: end] in symbols:
cur_output.append(token[start: end])
start = end
end = len(token)
else:
end -= 1
if start < len(token):
cur_output.append('[UNK]')
outputs.append(' '.join(cur_output))
return outputs
In the following, we use the subwords in list symbols, which is learned from the aforementioned
dataset, to segment tokens that represent another dataset.
tokens = ['tallest_', 'fatter_']
print(segment_BPE(tokens, symbols))
['tall e s t _', 'fa t t er_']
14.6. Subword Embedding 657
Summary
� FastText proposes a subword embedding method. Based on the skip-gram model in
word2vec, it represents the central word vector as the sum of the subword vectors of the
word.
� Subword embedding utilizes the principles of morphology, which usually improves the quality
of representations of uncommon words.
� Byte pair encoding performs a statistical analysis of the training dataset to discover common
symbols within a word. As a greedy approach, byte pair encoding iteratively merges the most
frequent pair of consecutive symbols.
Exercises
1. When there are too many subwords (for example, 6 words in English result in about 3  108
combinations), what problems arise? Can you think of any methods to solve them? Hint:
Refer to the end of section 3.2 of the fastText paper[1].
2. How can you design a subword embedding model based on the continuous bag-of-words
model?
3. To get a vocabulary of size m, how many merging operations are needed when the initial
symbol vocabulary size is n?
4. How can we extend the idea of byte pair encoding to extract phrases?
Discussions205
14.7 Finding Synonyms and Analogies
In Section 14.4 we trained a word2vec word embedding model on a small-scale dataset and
searched for synonyms using the cosine similarity of word vectors. In practice, word vectors pretrained
on a large-scale corpus can often be applied to downstream natural language processing
tasks. This section will demonstrate how to use these pretrained word vectors to find synonyms
and analogies. We will continue to apply pretrained word vectors in subsequent sections.
from d2l import mxnet as d2l
from mxnet import np, npx
import os
npx.set_np()
205 https://discuss.d2l.ai/t/386
658 Chapter 14. Natural Language Processing: Pretraining
14.7.1 Using Pretrained Word Vectors
Below lists pretrained GloVe embeddings of dimensions 50, 100, and 300, which can be downloaded
from the GloVe website206. The pretrained fastText embeddings are available in multiple
languages. Here we consider one English version (300-dimensional �wiki.en�) that can be downloaded
from the fastText website207.
#@save
d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d')
#@save
d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')
#@save
d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
'b5116e234e9eb9076672cfeabf5469f3eec904fa')
#@save
d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
'c1816da3821ae9f43899be655002f6c723e91b88')
We define the following TokenEmbedding class to load the above pretrained Glove and fastText embeddings.
#@save
class TokenEmbedding:
"""Token Embedding."""
def __init__(self, embedding_name):
self.idx_to_token, self.idx_to_vec = self._load_embedding(
embedding_name)
self.unknown_idx = 0
self.token_to_idx = {token: idx for idx, token in
enumerate(self.idx_to_token)}
def _load_embedding(self, embedding_name):
idx_to_token, idx_to_vec = ['<unk>'], []
data_dir = d2l.download_extract(embedding_name)
# GloVe website: https://nlp.stanford.edu/projects/glove/
# fastText website: https://fasttext.cc/
with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:
for line in f:
elems = line.rstrip().split(' ')
token, elems = elems[0], [float(elem) for elem in elems[1:]]
# Skip header information, such as the top row in fastText
if len(elems) > 1:
idx_to_token.append(token)
idx_to_vec.append(elems)
idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec
return idx_to_token, np.array(idx_to_vec)
(continues on next page)
206 https://nlp.stanford.edu/projects/glove/
207 https://fasttext.cc/
14.7. Finding Synonyms and Analogies 659
(continued from previous page)
def __getitem__(self, tokens):
indices = [self.token_to_idx.get(token, self.unknown_idx)
for token in tokens]
vecs = self.idx_to_vec[np.array(indices)]
return vecs
def __len__(self):
return len(self.idx_to_token)
Next, we use 50-dimensional GloVe embeddings pretrained on a subset of the Wikipedia. The
corresponding word embedding is automatically downloaded the first time we create a pretrained
word embedding instance.
glove_6b50d = TokenEmbedding('glove.6b.50d')
Downloading ../data/glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.
,!6B.50d.zip...
Output the dictionary size. The dictionary contains 400; 000 words and a special unknown token.
len(glove_6b50d)
400001
We can use a word to get its index in the dictionary, or we can get the word from its index.
glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]
(3367, 'beautiful')
14.7.2 Applying Pretrained Word Vectors
Below, we demonstrate the application of pretrained word vectors, using GloVe as an example.
Finding Synonyms
Here, we re-implement the algorithm used to search for synonyms by cosine similarity introduced
in Section 14.1
In order to reuse the logic for seeking the k nearest neighbors when seeking analogies, we encapsulate
this part of the logic separately in the knn (k-nearest neighbors) function.
def knn(W, x, k):
# The added 1e-9 is for numerical stability
cos = np.dot(W, x.reshape(-1,)) / (
np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt((x * x).sum()))
topk = npx.topk(cos, k=k, ret_typ='indices')
return topk, [cos[int(i)] for i in topk]
660 Chapter 14. Natural Language Processing: Pretraining
Then, we search for synonyms by pre-training the word vector instance embed.
def get_similar_tokens(query_token, k, embed):
topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)
for i, c in zip(topk[1:], cos[1:]): # Remove input words
print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')
The dictionary of pretrained word vector instance glove_6b50d already created contains 400,000
words and a special unknown token. Excluding input words and unknown words, we search for
the three words that are the most similar in meaning to �chip�.
get_similar_tokens('chip', 3, glove_6b50d)
cosine sim=0.856: chips
cosine sim=0.749: intel
cosine sim=0.749: electronics
Next, we search for the synonyms of �baby� and �beautiful�.
get_similar_tokens('baby', 3, glove_6b50d)
cosine sim=0.839: babies
cosine sim=0.800: boy
cosine sim=0.792: girl
get_similar_tokens('beautiful', 3, glove_6b50d)
cosine sim=0.921: lovely
cosine sim=0.893: gorgeous
cosine sim=0.830: wonderful
Finding Analogies
In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies
between words. For example, �man�:�woman�::�son�:�daughter� is an example of analogy,
�man� is to �woman� as �son� is to �daughter�. The problem of seeking analogies can be defined
as follows: for four words in the analogical relationship a : b :: c : d, given the first three words,
a, b and c, we want to find d. Assume the word vector for the word w is vec(w). To solve the
analogy problem, we need to find the word vector that is most similar to the result vector of
vec(c) + vec(b) ?? vec(a).
def get_analogy(token_a, token_b, token_c, embed):
vecs = embed[[token_a, token_b, token_c]]
x = vecs[1] - vecs[0] + vecs[2]
topk, cos = knn(embed.idx_to_vec, x, 1)
return embed.idx_to_token[int(topk[0])] # Remove unknown words
Verify the �male-female� analogy.
14.7. Finding Synonyms and Analogies 661
get_analogy('man', 'woman', 'son', glove_6b50d)
'daughter'
�Capital-country� analogy: �beijing� is to �china� as �tokyo� is to what? The answer should be
�japan�.
get_analogy('beijing', 'china', 'tokyo', glove_6b50d)
'japan'
�Adjective-superlative adjective� analogy: �bad� is to �worst� as �big� is to what? The answer
should be �biggest�.
get_analogy('bad', 'worst', 'big', glove_6b50d)
'biggest'
�Present tense verb-past tense verb� analogy: �do� is to �did� as �go� is to what? The answer should
be �went�.
get_analogy('do', 'did', 'go', glove_6b50d)
'went'
Summary
� Word vectors pre-trained on a large-scale corpus can often be applied to downstream natural
language processing tasks.
� We can use pre-trained word vectors to seek synonyms and analogies.
Exercises
1. Test the fastText results using TokenEmbedding('wiki.en').
2. If the dictionary is extremely large, how can we accelerate finding synonyms and analogies?
Discussions208
208 https://discuss.d2l.ai/t/387
662 Chapter 14. Natural Language Processing: Pretraining
14.8 Bidirectional Encoder Representations from Transformers (BERT)
We have introduced several word embedding models for natural language understanding. After
pretraining, the output can be thought of as a matrix where each row is a vector that represents
a word of a predefined vocabulary. In fact, these word embedding models are all contextindependent.
Let us begin by illustrating this property.
14.8.1 From Context-Independent to Context-Sensitive
Recall the experiments in Section 14.4 and Section 14.7. For instance, word2vec and GloVe both
assign the same pretrained vector to the same word regardless of the context of the word (if any).
Formally, a context-independent representation of any token x is a function f(x) that only takes
x as its input. Given the abundance of polysemy and complex semantics in natural languages,
context-independent representations have obvious limitations. For instance, the word �crane� in
contexts �a crane is flying� and �a crane driver came� has completely different meanings; thus,
the same word may be assigned different representations depending on contexts.
This motivates the development of context-sensitive word representations, where representations
of words depend on their contexts. Hence, a context-sensitive representation of token x is a function
f(x; c(x)) depending on both x and its context c(x). Popular context-sensitive representations
include TagLM (language-model-augmented sequence tagger) (Peters et al., 2017b), CoVe (Context
Vectors) (McCann et al., 2017), and ELMo (Embeddings from Language Models) (Peters et al.,
2018).
For example, by taking the entire sequence as the input, ELMo is a function that assigns a representation
to each word from the input sequence. Specifically, ELMo combines all the intermediate
layer representations from pretrained bidirectional LSTM as the output representation.
Then the ELMo representation will be added to a downstream task?s existing supervised model as
additional features, such as by concatenating ELMo representation and the original representation
(e.g., GloVe) of tokens in the existing model. On one hand, all the weights in the pretrained
bidirectional LSTM model are frozen after ELMo representations are added. On the other hand,
the existing supervised model is specifically customized for a given task. Leveraging different
best models for different tasks at that time, adding ELMo improved the state of the art across six
natural language processing tasks: sentiment analysis, natural language inference, semantic role
labeling, coreference resolution, named entity recognition, and question answering.
14.8.2 From Task-Specific to Task-Agnostic
Although ELMo has significantly improved solutions to a diverse set of natural language processing
tasks, each solution still hinges on a task-specific architecture. However, it is practically nontrivial
to craft a specific architecture for every natural language processing task. The GPT (Generative
Pre-Training) model represents an effort in designing a general task-agnostic model for
context-sensitive representations (Radford et al., 2018). Built on a Transformer decoder, GPT pretrains
a language model that will be used to represent text sequences. When applying GPT to a
downstream task, the output of the language model will be fed into an added linear output layer to
predict the label of the task. In sharp contrast to ELMo that freezes parameters of the pretrained
model, GPT fine-tunes all the parameters in the pretrained Transformer decoder during supervised
learning of the downstream task. GPT was evaluated on twelve tasks of natural language
inference, question answering, sentence similarity, and classification, and improved the state of
the art in nine of them with minimal changes to the model architecture.
14.8. Bidirectional Encoder Representations from Transformers (BERT) 663
However, due to the autoregressive nature of language models, GPT only looks forward (left-toright).
In contexts �i went to the bank to deposit cash� and �i went to the bank to sit down�, as
�bank� is sensitive to the context to its left, GPT will return the same representation for �bank�,
though it has different meanings.
14.8.3 BERT: Combining the Best of Both Worlds
As we have seen, ELMo encodes context bidirectionally but uses task-specific architectures; while
GPT is task-agnostic but encodes context left-to-right. Combining the best of both worlds, BERT
(Bidirectional Encoder Representations from Transformers) encodes context bidirectionally and
requires minimal architecture changes for a wide range of natural language processing tasks (Devlin
et al., 2018). Using a pretrained Transformer encoder, BERT is able to represent any token
based on its bidirectional context. During supervised learning of downstream tasks, BERT is similar
to GPT in two aspects. First, BERT representations will be fed into an added output layer,
with minimal changes to the model architecture depending on nature of tasks, such as predicting
for every token vs. predicting for the entire sequence. Second, all the parameters of the pretrained
Transformer encoder are fine-tuned, while the additional output layer will be trained from
scratch. Fig. 14.8.1 depicts the differences among ELMo, GPT, and BERT.
Fig. 14.8.1: A comparison of ELMo, GPT, and BERT.
BERT further improved the state of the art on eleven natural language processing tasks under
broad categories of i) single text classification (e.g., sentiment analysis), ii) text pair classification
(e.g., natural language inference), iii) question answering, iv) text tagging (e.g., named entity
recognition). All proposed in 2018, from context-sensitive ELMo to task-agnostic GPT and BERT,
conceptually simple yet empirically powerful pretraining of deep representations for natural languages
have revolutionized solutions to various natural language processing tasks.
In the rest of this chapter, we will dive into the pretraining of BERT. When natural language processing
applications are explained in Chapter 15, we will illustrate fine-tuning of BERT for downstream
applications.
664 Chapter 14. Natural Language Processing: Pretraining
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
from mxnet.gluon import nn
npx.set_np()
14.8.4 Input Representation
In natural language processing, some tasks (e.g., sentiment analysis) take single text as the input,
while in some other tasks (e.g., natural language inference), the input is a pair of text sequences.
The BERT input sequence unambiguously represents both single text and text pairs. In the former,
the BERT input sequence is the concatenation of the special classification token �<cls>�, tokens of
a text sequence, and the special separation token �<sep>�. In the latter, the BERT input sequence
is the concatenation of �<cls>�, tokens of the first text sequence, �<sep>�, tokens of the second text
sequence, and �<sep>�. We will consistently distinguish the terminology �BERT input sequence�
from other types of �sequences�. For instance, one BERT input sequence may include either one
text sequence or two text sequences.
To distinguish text pairs, the learned segment embeddings eA and eB are added to the token embeddings
of the first sequence and the second sequence, respectively. For single text inputs, only
eA is used.
The following get_tokens_and_segments takes either one sentence or two sentences as the input,
then returns tokens of the BERT input sequence and their corresponding segment IDs.
#@save
def get_tokens_and_segments(tokens_a, tokens_b=None):
tokens = ['<cls>'] + tokens_a + ['<sep>']
# 0 and 1 are marking segment A and B, respectively
segments = [0] * (len(tokens_a) + 2)
if tokens_b is not None:
tokens += tokens_b + ['<sep>']
segments += [1] * (len(tokens_b) + 1)
return tokens, segments
BERT chooses the Transformer encoder as its bidirectional architecture. Common in the Transformer
encoder, positional embeddings are added at every position of the BERT input sequence.
However, different from the original Transformer encoder, BERT uses learnable positional embeddings.
To sum up, Fig. 14.8.2 shows that the embeddings of the BERT input sequence are the
sum of the token embeddings, segment embeddings, and positional embeddings.
14.8. Bidirectional Encoder Representations from Transformers (BERT) 665
Fig. 14.8.2: The embeddings of the BERT input sequence are the sum of the token embeddings,
segment embeddings, and positional embeddings.
The following BERTEncoder class is similar to the TransformerEncoder class as implemented in Section
10.3. Different from TransformerEncoder, BERTEncoder uses segment embeddings and learnable
positional embeddings.
#@save
class BERTEncoder(nn.Block):
def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000, **kwargs):
super(BERTEncoder, self).__init__(**kwargs)
self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
self.segment_embedding = nn.Embedding(2, num_hiddens)
self.blks = nn.Sequential()
for _ in range(num_layers):
self.blks.add(d2l.EncoderBlock(
num_hiddens, ffn_num_hiddens, num_heads, dropout, True))
# In BERT, positional embeddings are learnable, thus we create a
# parameter of positional embeddings that are long enough
self.pos_embedding = self.params.get('pos_embedding',
shape=(1, max_len, num_hiddens))
def forward(self, tokens, segments, valid_lens):
# Shape of `X` remains unchanged in the following code snippet:
# (batch size, max sequence length, `num_hiddens`)
X = self.token_embedding(tokens) + self.segment_embedding(segments)
X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :]
for blk in self.blks:
X = blk(X, valid_lens)
return X
Suppose that the vocabulary size is 10,000. To demonstrate forward inference of BERTEncoder, let
us create an instance of it and initialize its parameters.
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
num_layers, dropout = 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout)
encoder.initialize()
We define tokens to be 2 BERT input sequences of length 8, where each token is an index of the vo-
666 Chapter 14. Natural Language Processing: Pretraining
cabulary. The forward inference of BERTEncoder with the input tokens returns the encoded result
where each token is represented by a vector whose length is predefined by the hyperparameter
num_hiddens. This hyperparameter is usually referred to as the hidden size (number of hidden
units) of the Transformer encoder.
tokens = np.random.randint(0, vocab_size, (2, 8))
segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape
(2, 8, 768)
14.8.5 Pretraining Tasks
The forward inference of BERTEncoder gives the BERT representation of each token of the input
text and the inserted special tokens �<cls>� and �<seq>�. Next, we will use these representations
to compute the loss function for pretraining BERT. The pretraining is composed of the following
two tasks: masked language modeling and next sentence prediction.
Masked Language Modeling
As illustrated in Section 8.3, a language model predicts a token using the context on its left. To
encode context bidirectionally for representing each token, BERT randomly masks tokens and
uses tokens from the bidirectional context to predict the masked tokens. This task is referred to
as a masked language model.
In this pretraining task, 15% of tokens will be selected at random as the masked tokens for prediction.
To predict a masked token without cheating by using the label, one straightforward approach
is to always replace it with a special �<mask>� token in the BERT input sequence. However, the
artificial special token �<mask>� will never appear in fine-tuning. To avoid such a mismatch between
pretraining and fine-tuning, if a token is masked for prediction (e.g., �great� is selected to
be masked and predicted in �this movie is great�), in the input it will be replaced with:
� a special �<mask>� token for 80% of the time (e.g., �this movie is great� becomes �this movie
is <mask>�);
� a random token for 10% of the time (e.g., �this movie is great� becomes �this movie is drink�);
� the unchanged label token for 10% of the time (e.g., �this movie is great� becomes �this
movie is great�).
Note that for 10% of 15% time a random token is inserted. This occasional noise encourages BERT
to be less biased towards the masked token (especially when the label token remains unchanged)
in its bidirectional context encoding.
We implement the following MaskLM class to predict masked tokens in the masked language model
task of BERT pretraining. The prediction uses a one-hidden-layer MLP (self.mlp). In forward
inference, it takes two inputs: the encoded result of BERTEncoder and the token positions for prediction.
The output is the prediction results at these positions.
14.8. Bidirectional Encoder Representations from Transformers (BERT) 667
#@save
class MaskLM(nn.Block):
def __init__(self, vocab_size, num_hiddens, **kwargs):
super(MaskLM, self).__init__(**kwargs)
self.mlp = nn.Sequential()
self.mlp.add(
nn.Dense(num_hiddens, flatten=False, activation='relu'))
self.mlp.add(nn.LayerNorm())
self.mlp.add(nn.Dense(vocab_size, flatten=False))
def forward(self, X, pred_positions):
num_pred_positions = pred_positions.shape[1]
pred_positions = pred_positions.reshape(-1)
batch_size = X.shape[0]
batch_idx = np.arange(0, batch_size)
# Suppose that `batch_size` = 2, `num_pred_positions` = 3, then
# `batch_idx` is `np.array([0, 0, 0, 1, 1, 1])`
batch_idx = np.repeat(batch_idx, num_pred_positions)
masked_X = X[batch_idx, pred_positions]
masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
mlm_Y_hat = self.mlp(masked_X)
return mlm_Y_hat
To demonstrate the forward inference of MaskLM, we create its instance mlm and initialize it. Recall
that encoded_X from the forward inference of BERTEncoder represents 2 BERT input sequences.
We define mlm_positions as the 3 indices to predict in either BERT input sequence of encoded_X.
The forward inference of mlm returns prediction results mlm_Y_hat at all the masked positions
mlm_positions of encoded_X. For each prediction, the size of the result is equal to the vocabulary
size.
mlm = MaskLM(vocab_size, num_hiddens)
mlm.initialize()
mlm_positions = np.array([[1, 5, 2], [6, 1, 5]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
(2, 3, 10000)
With the ground truth labels mlm_Y of the predicted tokens mlm_Y_hat under masks, we can calculate
the cross entropy loss of the masked language model task in BERT pretraining.
mlm_Y = np.array([[7, 8, 9], [10, 20, 30]])
loss = gluon.loss.SoftmaxCrossEntropyLoss()
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape
(6,)
668 Chapter 14. Natural Language Processing: Pretraining
Next Sentence Prediction
Although masked language modeling is able to encode bidirectional context for representing
words, it does not explicitly model the logical relationship between text pairs. To help understand
the relationship between two text sequences, BERT considers a binary classification task,
next sentence prediction, in its pretraining. When generating sentence pairs for pretraining, for
half of the time they are indeed consecutive sentences with the label �True�; while for the other
half of the time the second sentence is randomly sampled from the corpus with the label �False�.
The following NextSentencePred class uses a one-hidden-layer MLP to predict whether the second
sentence is the next sentence of the first in the BERT input sequence. Due to self-attention in the
Transformer encoder, the BERT representation of the special token �<cls>� encodes both the two
sentences from the input. Hence, the output layer (self.output) of the MLP classifier takes X as
the input, where X is the output of the MLP hidden layer whose input is the encoded �<cls>� token.
#@save
class NextSentencePred(nn.Block):
def __init__(self, **kwargs):
super(NextSentencePred, self).__init__(**kwargs)
self.output = nn.Dense(2)
def forward(self, X):
# `X` shape: (batch size, `num_hiddens`)
return self.output(X)
We can see that the forward inference of an NextSentencePred instance returns binary predictions
for each BERT input sequence.
nsp = NextSentencePred()
nsp.initialize()
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
(2, 2)
The cross-entropy loss of the 2 binary classifications can also be computed.
nsp_y = np.array([0, 1])
nsp_l = loss(nsp_Y_hat, nsp_y)
nsp_l.shape
(2,)
It is noteworthy that all the labels in both the aforementioned pretraining tasks can be trivially
obtained from the pretraining corpus without manual labeling effort. The original BERT has been
pretrained on the concatenation of BookCorpus (Zhu et al., 2015) and English Wikipedia. These
two text corpora are huge: they have 800 million words and 2.5 billion words, respectively.
14.8. Bidirectional Encoder Representations from Transformers (BERT) 669
14.8.6 Putting All Things Together
When pretraining BERT, the final loss function is a linear combination of both the loss functions
for masked language modeling and next sentence prediction. Now we can define the BERTModel
class by instantiating the three classes BERTEncoder, MaskLM, and NextSentencePred. The forward
inference returns the encoded BERT representations encoded_X, predictions of masked language
modeling mlm_Y_hat, and next sentence predictions nsp_Y_hat.
#@save
class BERTModel(nn.Block):
def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000):
super(BERTModel, self).__init__()
self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout, max_len)
self.hidden = nn.Dense(num_hiddens, activation='tanh')
self.mlm = MaskLM(vocab_size, num_hiddens)
self.nsp = NextSentencePred()
def forward(self, tokens, segments, valid_lens=None, pred_positions=None):
encoded_X = self.encoder(tokens, segments, valid_lens)
if pred_positions is not None:
mlm_Y_hat = self.mlm(encoded_X, pred_positions)
else:
mlm_Y_hat = None
# The hidden layer of the MLP classifier for next sentence prediction.
# 0 is the index of the '<cls>' token
nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
return encoded_X, mlm_Y_hat, nsp_Y_hat
Summary
� Word embedding models such as word2vec and GloVe are context-independent. They assign
the same pretrained vector to the same word regardless of the context of the word (if any).
It is hard for them to handle well polysemy or complex semantics in natural languages.
� For context-sensitive word representations such as ELMo and GPT, representations of words
depend on their contexts.
� ELMo encodes context bidirectionally but uses task-specific architectures (however, it is
practically non-trivial to craft a specific architecture for every natural language processing
task); while GPT is task-agnostic but encodes context left-to-right.
� BERT combines the best of both worlds: it encodes context bidirectionally and requires minimal
architecture changes for a wide range of natural language processing tasks.
� The embeddings of the BERT input sequence are the sum of the token embeddings, segment
embeddings, and positional embeddings.
� Pretraining BERT is composed of two tasks: masked language modeling and next sentence
prediction. The former is able to encode bidirectional context for representing words, while
the later explicitly models the logical relationship between text pairs.
670 Chapter 14. Natural Language Processing: Pretraining
Exercises
1. Why does BERT succeed?
2. All other things being equal, will a masked language model require more or fewer pretraining
steps to converge than a left-to-right language model? Why?
3. In the original implementation of BERT, the position-wise feed-forward network in BERTEncoder
(via d2l.EncoderBlock) and the fully-connected layer in MaskLM both use the Gaussian
error linear unit (GELU) (Hendrycks & Gimpel, 2016) as the activation function. Research
into the difference between GELU and ReLU.
Discussions209
14.9 The Dataset for Pretraining BERT
To pretrain the BERT model as implemented in Section 14.8, we need to generate the dataset in the
ideal format to facilitate the two pretraining tasks: masked language modeling and next sentence
prediction. On one hand, the original BERT model is pretrained on the concatenation of two huge
corpora BookCorpus and English Wikipedia (see Section 14.8.5), making it hard to run for most
readers of this book. On the other hand, the off-the-shelf pretrained BERT model may not fit for
applications from specific domains like medicine. Thus, it is getting popular to pretrain BERT on a
customized dataset. To facilitate the demonstration of BERT pretraining, we use a smaller corpus
WikiText-2 (Merity et al., 2016).
Comparing with the PTB dataset used for pretraining word2vec in Section 14.3, WikiText-2 i) retains
the original punctuation, making it suitable for next sentence prediction; ii) retrains the
original case and numbers; iii) is over twice larger.
import collections
from d2l import mxnet as d2l
import mxnet as mx
from mxnet import autograd, gluon, init, np, npx
import os
import random
import time
import zipfile
npx.set_np()
In the WikiText-2 dataset, each line represents a paragraph where space is inserted between any
punctuation and its preceding token. Paragraphs with at least two sentences are retained. To split
sentences, we only use the period as the delimiter for simplicity. We leave discussions of more
complex sentence splitting techniques in the exercises at the end of this section.
#@save
d2l.DATA_HUB['wikitext-2'] = (
'https://s3.amazonaws.com/research.metamind.io/wikitext/'
'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')
#@save
(continues on next page)
209 https://discuss.d2l.ai/t/388
14.9. The Dataset for Pretraining BERT 671
(continued from previous page)
def _read_wiki(data_dir):
file_name = os.path.join(data_dir, 'wiki.train.tokens')
with open(file_name, 'r') as f:
lines = f.readlines()
# Uppercase letters are converted to lowercase ones
paragraphs = [line.strip().lower().split(' . ')
for line in lines if len(line.split(' . ')) >= 2]
random.shuffle(paragraphs)
return paragraphs
14.9.1 Defining Helper Functions for Pretraining Tasks
In the following, we begin by implementing helper functions for the two BERT pretraining tasks:
next sentence prediction and masked language modeling. These helper functions will be invoked
later when transforming the raw text corpus into the dataset of the ideal format to pretrain BERT.
Generating the Next Sentence Prediction Task
According to descriptions of .. _subsec_nsp:, the _get_next_sentence function generates a training
example for the binary classification task.
#@save
def _get_next_sentence(sentence, next_sentence, paragraphs):
if random.random() < 0.5:
is_next = True
else:
# `paragraphs` is a list of lists of lists
next_sentence = random.choice(random.choice(paragraphs))
is_next = False
return sentence, next_sentence, is_next
The following function generates training examples for next sentence prediction from the input
paragraph by invoking the _get_next_sentence function. Here paragraph is a list of sentences,
where each sentence is a list of tokens. The argument max_len specifies the maximum length of a
BERT input sequence during pretraining.
#@save
def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
nsp_data_from_paragraph = []
for i in range(len(paragraph) - 1):
tokens_a, tokens_b, is_next = _get_next_sentence(
paragraph[i], paragraph[i + 1], paragraphs)
# Consider 1 '<cls>' token and 2 '<sep>' tokens
if len(tokens_a) + len(tokens_b) + 3 > max_len:
continue
tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
nsp_data_from_paragraph.append((tokens, segments, is_next))
return nsp_data_from_paragraph
672 Chapter 14. Natural Language Processing: Pretraining
Generating the Masked Language Modeling Task
In order to generate training examples for the masked language modeling task from a BERT input
sequence, we define the following _replace_mlm_tokens function. In its inputs, tokens is a list of
tokens representing a BERT input sequence, candidate_pred_positions is a list of token indices
of the BERT input sequence excluding those of special tokens (special tokens are not predicted
in the masked language modeling task), and num_mlm_preds indicates the number of predictions
(recall 15% random tokens to predict). Following the definition of the masked language modeling
task in Section 14.8.5, at each prediction position, the input may be replaced by a special �<mask>�
token or a random token, or remain unchanged. In the end, the function returns the input tokens
after possible replacement, the token indices where predictions take place and labels for these
predictions.
#@save
def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
vocab):
# Make a new copy of tokens for the input of a masked language model,
# where the input may contain replaced '<mask>' or random tokens
mlm_input_tokens = [token for token in tokens]
pred_positions_and_labels = []
# Shuffle for getting 15% random tokens for prediction in the masked
# language modeling task
random.shuffle(candidate_pred_positions)
for mlm_pred_position in candidate_pred_positions:
if len(pred_positions_and_labels) >= num_mlm_preds:
break
masked_token = None
# 80% of the time: replace the word with the '<mask>' token
if random.random() < 0.8:
masked_token = '<mask>'
else:
# 10% of the time: keep the word unchanged
if random.random() < 0.5:
masked_token = tokens[mlm_pred_position]
# 10% of the time: replace the word with a random word
else:
masked_token = random.randint(0, len(vocab) - 1)
mlm_input_tokens[mlm_pred_position] = masked_token
pred_positions_and_labels.append(
(mlm_pred_position, tokens[mlm_pred_position]))
return mlm_input_tokens, pred_positions_and_labels
By invoking the aforementioned _replace_mlm_tokens function, the following function takes a
BERT input sequence (tokens) as an input and returns indices of the input tokens (after possible
token replacement as described in Section 14.8.5), the token indices where predictions take place,
and label indices for these predictions.
#@save
def _get_mlm_data_from_tokens(tokens, vocab):
candidate_pred_positions = []
# `tokens` is a list of strings
for i, token in enumerate(tokens):
# Special tokens are not predicted in the masked language modeling
# task
(continues on next page)
14.9. The Dataset for Pretraining BERT 673
(continued from previous page)
if token in ['<cls>', '<sep>']:
continue
candidate_pred_positions.append(i)
# 15% of random tokens are predicted in the masked language modeling task
num_mlm_preds = max(1, round(len(tokens) * 0.15))
mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
tokens, candidate_pred_positions, num_mlm_preds, vocab)
pred_positions_and_labels = sorted(pred_positions_and_labels,
key=lambda x: x[0])
pred_positions = [v[0] for v in pred_positions_and_labels]
mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]
14.9.2 Transforming Text into the Pretraining Dataset
Now we are almost ready to customize a Dataset class for pretraining BERT. Before that, we
still need to define a helper function _pad_bert_inputs to append the special �<mask>� tokens
to the inputs. Its argument examples contain the outputs from the helper functions
_get_nsp_data_from_paragraph and _get_mlm_data_from_tokens for the two pretraining tasks.
#@save
def _pad_bert_inputs(examples, max_len, vocab):
max_num_mlm_preds = round(max_len * 0.15)
all_token_ids, all_segments, valid_lens, = [], [], []
all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
nsp_labels = []
for (token_ids, pred_positions, mlm_pred_label_ids, segments,
is_next) in examples:
all_token_ids.append(np.array(token_ids + [vocab['<pad>']] * (
max_len - len(token_ids)), dtype='int32'))
all_segments.append(np.array(segments + [0] * (
max_len - len(segments)), dtype='int32'))
# `valid_lens` excludes count of '<pad>' tokens
valid_lens.append(np.array(len(token_ids), dtype='float32'))
all_pred_positions.append(np.array(pred_positions + [0] * (
max_num_mlm_preds - len(pred_positions)), dtype='int32'))
# Predictions of padded tokens will be filtered out in the loss via
# multiplication of 0 weights
all_mlm_weights.append(
np.array([1.0] * len(mlm_pred_label_ids) + [0.0] * (
max_num_mlm_preds - len(pred_positions)), dtype='float32'))
all_mlm_labels.append(np.array(mlm_pred_label_ids + [0] * (
max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'))
nsp_labels.append(np.array(is_next))
return (all_token_ids, all_segments, valid_lens, all_pred_positions,
all_mlm_weights, all_mlm_labels, nsp_labels)
Putting the helper functions for generating training examples of the two pretraining tasks, and the
helper function for padding inputs together, we customize the following _WikiTextDataset class
as the WikiText-2 dataset for pretraining BERT. By implementing the __getitem__function, we
can arbitrarily access the pretraining (masked language modeling and next sentence prediction)
examples generated from a pair of sentences from the WikiText-2 corpus.
674 Chapter 14. Natural Language Processing: Pretraining
The original BERT model uses WordPiece embeddings whose vocabulary size is 30,000 (Wu et al.,
2016). The tokenization method of WordPiece is a slight modification of the original byte pair
encoding algorithm in Section 14.6.2. For simplicity, we use the d2l.tokenize function for tokenization.
Infrequent tokens that appear less than five times are filtered out.
#@save
class _WikiTextDataset(gluon.data.Dataset):
def __init__(self, paragraphs, max_len):
# Input `paragraphs[i]` is a list of sentence strings representing a
# paragraph; while output `paragraphs[i]` is a list of sentences
# representing a paragraph, where each sentence is a list of tokens
paragraphs = [d2l.tokenize(
paragraph, token='word') for paragraph in paragraphs]
sentences = [sentence for paragraph in paragraphs
for sentence in paragraph]
self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
'<pad>', '<mask>', '<cls>', '<sep>'])
# Get data for the next sentence prediction task
examples = []
for paragraph in paragraphs:
examples.extend(_get_nsp_data_from_paragraph(
paragraph, paragraphs, self.vocab, max_len))
# Get data for the masked language model task
examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
+ (segments, is_next))
for tokens, segments, is_next in examples]
# Pad inputs
(self.all_token_ids, self.all_segments, self.valid_lens,
self.all_pred_positions, self.all_mlm_weights,
self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
examples, max_len, self.vocab)
def __getitem__(self, idx):
return (self.all_token_ids[idx], self.all_segments[idx],
self.valid_lens[idx], self.all_pred_positions[idx],
self.all_mlm_weights[idx], self.all_mlm_labels[idx],
self.nsp_labels[idx])
def __len__(self):
return len(self.all_token_ids)
By using the _read_wiki function and the _WikiTextDataset class, we define the following
load_data_wiki to download and WikiText-2 dataset and generate pretraining examples from it.
#@save
def load_data_wiki(batch_size, max_len):
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')
paragraphs = _read_wiki(data_dir)
train_set = _WikiTextDataset(paragraphs, max_len)
train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers)
return train_iter, train_set.vocab
Setting the batch size to 512 and the maximum length of a BERT input sequence to be 64, we
print out the shapes of a minibatch of BERT pretraining examples. Note that in each BERT input
14.9. The Dataset for Pretraining BERT 675
sequence, 10 (64  0:15) positions are predicted for the masked language modeling task.
batch_size, max_len = 512, 64
train_iter, vocab = load_data_wiki(batch_size, max_len)
for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
mlm_Y, nsp_y) in train_iter:
print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,
pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,
nsp_y.shape)
break
Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/
,!wikitext/wikitext-2-v1.zip...
(512, 64) (512, 64) (512,) (512, 10) (512, 10) (512, 10) (512,)
In the end, let us take a look at the vocabulary size. Even after filtering out infrequent tokens, it is
still over twice larger than that of the PTB dataset.
len(vocab)
20256
Summary
� Comparing with the PTB dataset, the WikiText-2 dateset retains the original punctuation,
case and numbers, and is over twice larger.
� We can arbitrarily access the pretraining (masked language modeling and next sentence
prediction) examples generated from a pair of sentences from the WikiText-2 corpus.
Exercises
1. For simplicity, the period is used as the only delimiter for splitting sentences. Try other sentence
splitting techniques, such as the spaCy and NLTK. Take NLTK as an example. You need
to install NLTK first: pip install nltk. In the code, first import nltk. Then, download the
Punkt sentence tokenizer: nltk.download('punkt'). To split sentences such as sentences
= 'This is great ! Why not ?', invoking nltk.tokenize.sent_tokenize(sentences) will
return a list of two sentence strings: ['This is great !', 'Why not ?'].
2. What is the vocabulary size if we do not filter out any infrequent token?
Discussions210
210 https://discuss.d2l.ai/t/389
676 Chapter 14. Natural Language Processing: Pretraining
14.10 Pretraining BERT
With the BERT model implemented in Section 14.8 and the pretraining examples generated from
the WikiText-2 dataset in Section 14.9, we will pretrain BERT on the WikiText-2 dataset in this
section.
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
npx.set_np()
To start, we load the WikiText-2 dataset as minibatches of pretraining examples for masked language
modeling and next sentence prediction. The batch size is 512 and the maximum length of
a BERT input sequence is 64. Note that in the original BERT model, the maximum length is 512.
batch_size, max_len = 512, 64
train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)
14.10.1 Pretraining BERT
The original BERT has two versions of different model sizes (Devlin et al., 2018). The base model
(BERTBASE) uses 12 layers (Transformer encoder blocks) with 768 hidden units (hidden size) and
12 self-attention heads. The large model (BERTLARGE) uses 24 layers with 1024 hidden units and
16 self-attention heads. Notably, the former has 110 million parameters while the latter has 340
million parameters. For demonstration with ease, we define a small BERT, using 2 layers, 128
hidden units, and 2 self-attention heads.
net = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_hiddens=256,
num_heads=2, num_layers=2, dropout=0.2)
devices = d2l.try_all_gpus()
net.initialize(init.Xavier(), ctx=devices)
loss = gluon.loss.SoftmaxCELoss()
Before defining the training loop, we define a helper function _get_batch_loss_bert. Given the
shard of training examples, this function computes the loss for both the masked language modeling
and next sentence prediction tasks. Note that the final loss of BERT pretraining is just the sum
of both the masked language modeling loss and the next sentence prediction loss.
#@save
def _get_batch_loss_bert(net, loss, vocab_size, tokens_X_shards,
segments_X_shards, valid_lens_x_shards,
pred_positions_X_shards, mlm_weights_X_shards,
mlm_Y_shards, nsp_y_shards):
mlm_ls, nsp_ls, ls = [], [], []
for (tokens_X_shard, segments_X_shard, valid_lens_x_shard,
pred_positions_X_shard, mlm_weights_X_shard, mlm_Y_shard,
nsp_y_shard) in zip(
tokens_X_shards, segments_X_shards, valid_lens_x_shards,
pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards,
nsp_y_shards):
# Forward pass
(continues on next page)
14.10. Pretraining BERT 677
(continued from previous page)
_, mlm_Y_hat, nsp_Y_hat = net(
tokens_X_shard, segments_X_shard, valid_lens_x_shard.reshape(-1),
pred_positions_X_shard)
# Compute masked language model loss
mlm_l = loss(
mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y_shard.reshape(-1),
mlm_weights_X_shard.reshape((-1, 1)))
mlm_l = mlm_l.sum() / (mlm_weights_X_shard.sum() + 1e-8)
# Compute next sentence prediction loss
nsp_l = loss(nsp_Y_hat, nsp_y_shard)
nsp_l = nsp_l.mean()
mlm_ls.append(mlm_l)
nsp_ls.append(nsp_l)
ls.append(mlm_l + nsp_l)
npx.waitall()
return mlm_ls, nsp_ls, ls
Invoking the two aforementioned helper functions, the following train_bert function defines the
procedure to pretrain BERT (net) on the WikiText-2 (train_iter) dataset. Training BERT can take
very long. Instead of specifying the number of epochs for training as in the train_ch13 function
(see Section 13.1), the input num_steps of the following function specifies the number of iteration
steps for training.
#@save
def train_bert(train_iter, net, loss, vocab_size, devices, log_interval,
num_steps):
trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': 1e-3})
step, timer = 0, d2l.Timer()
animator = d2l.Animator(xlabel='step', ylabel='loss',
xlim=[1, num_steps], legend=['mlm', 'nsp'])
# Sum of masked language modeling losses, sum of next sentence prediction
# losses, no. of sentence pairs, count
metric = d2l.Accumulator(4)
num_steps_reached = False
while step < num_steps and not num_steps_reached:
for batch in train_iter:
(tokens_X_shards, segments_X_shards, valid_lens_x_shards,
pred_positions_X_shards, mlm_weights_X_shards,
mlm_Y_shards, nsp_y_shards) = [gluon.utils.split_and_load(
elem, devices, even_split=False) for elem in batch]
timer.start()
with autograd.record():
mlm_ls, nsp_ls, ls = _get_batch_loss_bert(
net, loss, vocab_size, tokens_X_shards, segments_X_shards,
valid_lens_x_shards, pred_positions_X_shards,
mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards)
for l in ls:
l.backward()
trainer.step(1)
mlm_l_mean = sum([float(l) for l in mlm_ls]) / len(mlm_ls)
nsp_l_mean = sum([float(l) for l in nsp_ls]) / len(nsp_ls)
metric.add(mlm_l_mean, nsp_l_mean, batch[0].shape[0], 1)
timer.stop()
(continues on next page)
678 Chapter 14. Natural Language Processing: Pretraining
(continued from previous page)
if (step + 1) % log_interval == 0:
animator.add(step + 1,
(metric[0] / metric[3], metric[1] / metric[3]))
step += 1
if step == num_steps:
num_steps_reached = True
break
print(f'MLM loss {metric[0] / metric[3]:.3f}, '
f'NSP loss {metric[1] / metric[3]:.3f}')
print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
f'{str(devices)}')
We can plot both the masked language modeling loss and the next sentence prediction loss during
BERT pretraining.
train_bert(train_iter, net, loss, len(vocab), devices, 1, 50)
MLM loss 7.906, NSP loss 0.732
8803.2 sentence pairs/sec on [gpu(0), gpu(1)]
14.10.2 Representing Text with BERT
After pretraining BERT, we can use it to represent single text, text pairs, or any token in them. The
following function returns the BERT (net) representations for all tokens in tokens_a and tokens_b.
def get_bert_encoding(net, tokens_a, tokens_b=None):
tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
token_ids = np.expand_dims(np.array(vocab[tokens], ctx=devices[0]),
axis=0)
segments = np.expand_dims(np.array(segments, ctx=devices[0]), axis=0)
valid_len = np.expand_dims(np.array(len(tokens), ctx=devices[0]), axis=0)
encoded_X, _, _ = net(token_ids, segments, valid_len)
return encoded_X
14.10. Pretraining BERT 679
Consider the sentence �a crane is flying�. Recall the input representation of BERT as discussed in
Section 14.8.4. After inserting special tokens �<cls>� (used for classification) and �<sep>� (used for
separation), the BERT input sequence has a length of six. Since zero is the index of the �<cls>� token,
encoded_text[:, 0, :] is the BERT representation of the entire input sentence. To evaluate
the polysemy token �crane�, we also print out the first three elements of the BERT representation
of the token.
tokens_a = ['a', 'crane', 'is', 'flying']
encoded_text = get_bert_encoding(net, tokens_a)
# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'
encoded_text_cls = encoded_text[:, 0, :]
encoded_text_crane = encoded_text[:, 2, :]
encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]
((1, 6, 128),
(1, 128),
array([ 0.8918758, 1.0964735, -1.0611488], ctx=gpu(0)))
Now consider a sentence pair �a crane driver came� and �he just left�. Similarly, encoded_pair[:,
0, :] is the encoded result of the entire sentence pair from the pretrained BERT. Note that the
first three elements of the polysemy token �crane� are different from those when the context is
different. This supports that BERT representations are context-sensitive.
tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']
encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)
# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',
# 'left', '<sep>'
encoded_pair_cls = encoded_pair[:, 0, :]
encoded_pair_crane = encoded_pair[:, 2, :]
encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]
((1, 10, 128),
(1, 128),
array([ 0.91807 , 1.1650459, -1.0900582], ctx=gpu(0)))
In Chapter 15, we will fine-tune a pretrained BERT model for downstream natural language processing
applications.
Summary
� The original BERT has two versions, where the base model has 110 million parameters and
the large model has 340 million parameters.
� After pretraining BERT, we can use it to represent single text, text pairs, or any token in them.
� In the experiment, the same token has different BERT representation when their contexts
are different. This supports that BERT representations are context-sensitive.
680 Chapter 14. Natural Language Processing: Pretraining
Exercises
1. In the experiment, we can see that the masked language modeling loss is significantly higher
than the next sentence prediction loss. Why?
2. Set the maximum length of a BERT input sequence to be 512 (same as the original BERT
model). Use the configurations of the original BERT model such as BERTLARGE. Do you encounter
any error when running this section? Why?
Discussions211
211 https://discuss.d2l.ai/t/390
14.10. Pretraining BERT 681
682 Chapter 14. Natural Language Processing: Pretraining
15 | Natural Language Processing: Applications
We have seen how to represent text tokens and train their representations in Chapter 14. Such
pretrained text representations can be fed to various models for different downstream natural
language processing tasks.
This book does not intend to cover natural language processing applications in a comprehensive
manner. Our focus is on how to apply (deep) representation learning of languages to addressing natural
language processing problems. Nonetheless, we have already discussed several natural language
processing applications without pretraining in earlier chapters, just for explaining deep learning
architectures. For instance, in Chapter 8, we have relied on RNNs to design language models to
generate novella-like text. In Chapter 9 and Chapter 10, we have also designed models based on
RNNs and attention mechanisms for machine translation. Given pretrained text representations,
in this chapter, we will consider two more downstream natural language processing tasks: sentiment
analysis and natural language inference. These are popular and representative natural
language processing applications: the former analyzes single text and the latter analyzes relationships
of text pairs.
Fig. 15.1: Pretrained text representations can be fed to various deep learning architectures for
different downstream natural language processing applications. This chapter focuses on how to
design models for different downstream natural language processing applications.
As depicted in Fig. 15.1, this chapter focuses on describing the basic ideas of designing natural
language processing models using different types of deep learning architectures, such as MLPs,
CNNs, RNNs, and attention. Though it is possible to combine any pretrained text representations
683
with any architecture for either downstream natural language processing task in Fig. 15.1, we
select a few representative combinations. Specifically, we will explore popular architectures based
on RNNs and CNNs for sentiment analysis. For natural language inference, we choose attention
and MLPs to demonstrate how to analyze text pairs. In the end, we introduce how to fine-tune
a pretrained BERT model for a wide range of natural language processing applications, such as
on a sequence level (single text classification and text pair classification) and a token level (text
tagging and question answering). As a concrete empirical case, we will fine-tune BERT for natural
language processing.
As we have introduced in Section 14.8, BERT requires minimal architecture changes for a wide
range of natural language processing applications. However, this benefit comes at the cost of finetuning
a huge number of BERT parameters for the downstream applications. When space or time
is limited, those crafted models based on MLPs, CNNs, RNNs, and attention are more feasible. In
the following, we start by the sentiment analysis application and illustrate the model design based
on RNNs and CNNs, respectively.
15.1 Sentiment Analysis and the Dataset
Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. It is similar to the image classification, the most
frequently used application in this book, e.g., Section 18.9. The only difference is that, rather than
an image, text classification?s example is a text sentence.
This section will focus on loading data for one of the sub-questions in this field: using text sentiment
classification to analyze the emotions of the text?s author. This problem is also called sentiment
analysis and has a wide range of applications. For example, we can analyze user reviews of
products to obtain user satisfaction statistics, or analyze user sentiments about market conditions
and use it to predict future trends.
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
import os
npx.set_np()
15.1.1 The Sentiment Analysis Dataset
We use Stanford?s Large Movie Review Dataset212 as the dataset for sentiment analysis. This
dataset is divided into two datasets for training and testing purposes, each containing 25,000 movie
reviews downloaded from IMDb. In each dataset, the number of comments labeled as �positive�
and �negative� is equal.
212 https://ai.stanford.edu/~amaas/data/sentiment/
684 Chapter 15. Natural Language Processing: Applications
Reading the Dataset
We first download this dataset to the �../data� path and extract it to �../data/aclImdb�.
#@save
d2l.DATA_HUB['aclImdb'] = (
'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
'01ada507287d82875905620988597833ad4e0903')
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
Downloading ../data/aclImdb_v1.tar.gz from http://ai.stanford.edu/~amaas/data/sentiment/
,!aclImdb_v1.tar.gz...
Next, read the training and test datasets. Each example is a review and its corresponding label: 1
indicates �positive� and 0 indicates �negative�.
#@save
def read_imdb(data_dir, is_train):
data, labels = [], []
for label in ('pos', 'neg'):
folder_name = os.path.join(data_dir, 'train' if is_train else 'test',
label)
for file in os.listdir(folder_name):
with open(os.path.join(folder_name, file), 'rb') as f:
review = f.read().decode('utf-8').replace('\n', '')
data.append(review)
labels.append(1 if label == 'pos' else 0)
return data, labels
train_data = read_imdb(data_dir, is_train=True)
print('# trainings:', len(train_data[0]))
for x, y in zip(train_data[0][:3], train_data[1][:3]):
print('label:', y, 'review:', x[0:60])
# trainings: 25000
label: 1 review: Normally the best way to annoy me in a film is to include so
label: 1 review: The Bible teaches us that the love of money is the root of a
label: 1 review: Being someone who lists Night of the Living Dead at number t
Tokenization and Vocabulary
We use a word as a token, and then create a dictionary based on the training dataset.
train_tokens = d2l.tokenize(train_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])
d2l.set_figsize()
d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));
15.1. Sentiment Analysis and the Dataset 685
Padding to the Same Length
Because the reviews have different lengths, so they cannot be directly combined into minibatches.
Here we fix the length of each comment to 500 by truncating or adding �<unk>� indices.
num_steps = 500 # sequence length
train_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])
train_features.shape
(25000, 500)
Creating the Data Iterator
Now, we will create a data iterator. Each iteration will return a minibatch of data.
train_iter = d2l.load_array((train_features, train_data[1]), 64)
for X, y in train_iter:
print('X', X.shape, 'y', y.shape)
break
'# batches:', len(train_iter)
X (64, 500) y (64,)
('# batches:', 391)
686 Chapter 15. Natural Language Processing: Applications
15.1.2 Putting All Things Together
Last, we will save a function load_data_imdb into d2l, which returns the vocabulary and data iterators.
#@save
def load_data_imdb(batch_size, num_steps=500):
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
train_data = read_imdb(data_dir, True)
test_data = read_imdb(data_dir, False)
train_tokens = d2l.tokenize(train_data[0], token='word')
test_tokens = d2l.tokenize(test_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5)
train_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab.unk) for line in train_tokens])
test_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab.unk) for line in test_tokens])
train_iter = d2l.load_array((train_features, train_data[1]), batch_size)
test_iter = d2l.load_array((test_features, test_data[1]), batch_size,
is_train=False)
return train_iter, test_iter, vocab
Summary
� Text classification can classify a text sequence into a category.
� To classify a text sentiment, we load an IMDb dataset and tokenize its words. Then we pad
the text sequence for short reviews and create a data iterator.
Exercises
1. Discover a different natural language dataset (such as Amazon reviews213) and build a similar
data_loader function as load_data_imdb.
Discussions214
15.2 Sentiment Analysis: Using Recurrent Neural Networks
Similar to search synonyms and analogies, text classification is also a downstream application of
word embedding. In this section, we will apply pre-trained word vectors (GloVe) and bidirectional
recurrent neural networks with multiple hidden layers (Maas et al., 2011), as shown in Fig. 15.2.1.
We will use the model to determine whether a text sequence of indefinite length contains positive
or negative emotion.
213 https://snap.stanford.edu/data/web-Amazon.html
214 https://discuss.d2l.ai/t/391
15.2. Sentiment Analysis: Using Recurrent Neural Networks 687
Fig. 15.2.1: This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn, rnn
npx.set_np()
batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
15.2.1 Using a Recurrent Neural Network Model
In this model, each word first obtains a feature vector from the embedding layer. Then, we further
encode the feature sequence using a bidirectional recurrent neural network to obtain sequence
information. Finally, we transform the encoded sequence information to output through the fully
connected layer. Specifically, we can concatenate hidden states of bidirectional long-short term
memory in the initial timestep and final timestep and pass it to the output layer classification
as encoded feature sequence information. In the BiRNN class implemented below, the Embedding
instance is the embedding layer, the LSTM instance is the hidden layer for sequence encoding, and
the Dense instance is the output layer for generated classification results.
class BiRNN(nn.Block):
def __init__(self, vocab_size, embed_size, num_hiddens,
num_layers, **kwargs):
super(BiRNN, self).__init__(**kwargs)
self.embedding = nn.Embedding(vocab_size, embed_size)
# Set `bidirectional` to True to get a bidirectional recurrent neural
# network
self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,
bidirectional=True, input_size=embed_size)
self.decoder = nn.Dense(2)
def forward(self, inputs):
# The shape of `inputs` is (batch size, no. of words). Because LSTM
(continues on next page)
688 Chapter 15. Natural Language Processing: Applications
(continued from previous page)
# needs to use sequence as the first dimension, the input is
# transformed and the word feature is then extracted. The output shape
# is (no. of words, batch size, word vector dimension).
embeddings = self.embedding(inputs.T)
# Since the input (embeddings) is the only argument passed into
# rnn.LSTM, it only returns the hidden states of the last hidden layer
# at different timestep (outputs). The shape of `outputs` is
# (no. of words, batch size, 2 * no. of hidden units).
outputs = self.encoder(embeddings)
# Concatenate the hidden states of the initial timestep and final
# timestep to use as the input of the fully connected layer. Its
# shape is (batch size, 4 * no. of hidden units)
encoding = np.concatenate((outputs[0], outputs[-1]), axis=1)
outs = self.decoder(encoding)
return outs
Create a bidirectional recurrent neural network with two hidden layers.
embed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()
net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)
net.initialize(init.Xavier(), ctx=devices)
Loading Pre-trained Word Vectors
Because the training dataset for sentiment classification is not very large, in order to deal with
overfitting, we will directly use word vectors pre-trained on a larger corpus as the feature vectors
of all words. Here, we load a 100-dimensional GloVe word vector for each word in the dictionary
vocab.
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
Query the word vectors that in our vocabulary.
embeds = glove_embedding[vocab.idx_to_token]
embeds.shape
(49339, 100)
Then, we will use these word vectors as feature vectors for each word in the reviews. Note that
the dimensions of the pre-trained word vectors need to be consistent with the embedding layer
output size embed_size in the created model. In addition, we no longer update these word vectors
during training.
net.embedding.weight.set_data(embeds)
net.embedding.collect_params().setattr('grad_req', 'null')
15.2. Sentiment Analysis: Using Recurrent Neural Networks 689
Training and Evaluating the Model
Now, we can start training.
lr, num_epochs = 0.01, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.268, train acc 0.888, test acc 0.858
574.1 examples/sec on [gpu(0), gpu(1)]
Finally, define the prediction function.
#@save
def predict_sentiment(net, vocab, sentence):
sentence = np.array(vocab[sentence.split()], ctx=d2l.try_gpu())
label = np.argmax(net(sentence.reshape(1, -1)), axis=1)
return 'positive' if label == 1 else 'negative'
Then, use the trained model to classify the sentiments of two simple sentences.
predict_sentiment(net, vocab, 'this movie is so great')
'positive'
predict_sentiment(net, vocab, 'this movie is so bad')
'negative'
690 Chapter 15. Natural Language Processing: Applications
Summary
� Text classification transforms a sequence of text of indefinite length into a category of text.
This is a downstream application of word embedding.
� We can apply pre-trained word vectors and recurrent neural networks to classify the emotions
in a text.
Exercises
1. Increase the number of epochs. What accuracy rate can you achieve on the training and
testing datasets? What about trying to re-tune other hyperparameters?
2. Will using larger pre-trained word vectors, such as 300-dimensional GloVe word vectors, improve
classification accuracy?
3. Can we improve the classification accuracy by using the spaCy word tokenization tool? You
need to install spaCy: pip install spacy and install the English package: python -m spacy
download en. In the code, first import spacy: import spacy. Then, load the spacy English
package: spacy_en = spacy.load('en'). Finally, define the function def tokenizer(text):
return [tok.text for tok in spacy_en.tokenizer(text)] and replace the original tokenizer
function. It should be noted that GloVe?s word vector uses �-� to connect each word
when storing noun phrases. For example, the phrase �new york� is represented as �newyork�
in GloVe. After using spaCy tokenization, �new york� may be stored as �new york�.
Discussions215
15.3 Sentiment Analysis: Using Convolutional Neural Networks
In Chapter 6, we explored how to process two-dimensional image data with two-dimensional convolutional
neural networks. In the previous language models and text classification tasks, we
treated text data as a time series with only one dimension, and naturally, we used recurrent neural
networks to process such data. In fact, we can also treat text as a one-dimensional image, so
that we can use one-dimensional convolutional neural networks to capture associations between
adjacent words. As described in .. _fig_nlp-map-sa-cnn: This section describes a groundbreaking
approach to applying convolutional neural networks to sentiment analysis: textCNN (Kim, 2014).
215 https://discuss.d2l.ai/t/392
15.3. Sentiment Analysis: Using Convolutional Neural Networks 691
Fig. 15.3.1: This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.
First, import the packages and modules required for the experiment.
from d2l import mxnet as d2l
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
batch_size = 64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)
15.3.1 One-Dimensional Convolutional Layer
Before introducing the model, let us explain how a one-dimensional convolutional layer works.
Like a two-dimensional convolutional layer, a one-dimensional convolutional layer uses a onedimensional
cross-correlation operation. In the one-dimensional cross-correlation operation, the
convolution window starts from the leftmost side of the input array and slides on the input array
from left to right successively. When the convolution window slides to a certain position, the
input subarray in the window and kernel array are multiplied and summed by element to get the
element at the corresponding location in the output array. As shown in Fig. 15.3.2, the input is a
one-dimensional array with a width of 7 and the width of the kernel array is 2. As we can see, the
output width is 7 ?? 2 + 1 = 6 and the first element is obtained by performing multiplication by
element on the leftmost input subarray with a width of 2 and kernel array and then summing the
results.
Fig. 15.3.2: One-dimensional cross-correlation operation. The shaded parts are the first output
element as well as the input and kernel array elements used in its calculation: 0  1 + 1  2 = 2.
692 Chapter 15. Natural Language Processing: Applications
Next, we implement one-dimensional cross-correlation in the corr1d function. It accepts the input
array X and kernel array K and outputs the array Y.
def corr1d(X, K):
w = K.shape[0]
Y = np.zeros((X.shape[0] - w + 1))
for i in range(Y.shape[0]):
Y[i] = (X[i: i + w] * K).sum()
return Y
Now, we will reproduce the results of the one-dimensional cross-correlation operation in Fig.
15.3.2.
X, K = np.array([0, 1, 2, 3, 4, 5, 6]), np.array([1, 2])
corr1d(X, K)
array([ 2., 5., 8., 11., 14., 17.])
The one-dimensional cross-correlation operation for multiple input channels is also similar to
the two-dimensional cross-correlation operation for multiple input channels. On each channel,
it performs the one-dimensional cross-correlation operation on the kernel and its corresponding
input and adds the results of the channels to get the output. Fig. 15.3.3 shows a one-dimensional
cross-correlation operation with three input channels.
Fig. 15.3.3: One-dimensional cross-correlation operation with three input channels. The shaded
parts are the first output element as well as the input and kernel array elements used in its calculation:
0  1 + 1  2 + 1  3 + 2  4 + 2  (??1) + 3  (??3) = 2.
Now, we reproduce the results of the one-dimensional cross-correlation operation with multiinput
channel in Fig. 15.3.3.
def corr1d_multi_in(X, K):
# First, we traverse along the 0th dimension (channel dimension) of `X`
# and `K`. Then, we add them together by using * to turn the result list
# into a positional argument of the `add_n` function
return sum(corr1d(x, k) for x, k in zip(X, K))
X = np.array([[0, 1, 2, 3, 4, 5, 6],
[1, 2, 3, 4, 5, 6, 7],
[2, 3, 4, 5, 6, 7, 8]])
K = np.array([[1, 2], [3, 4], [-1, -3]])
corr1d_multi_in(X, K)
15.3. Sentiment Analysis: Using Convolutional Neural Networks 693
array([ 2., 8., 14., 20., 26., 32.])
The definition of a two-dimensional cross-correlation operation tells us that a one-dimensional
cross-correlation operation with multiple input channels can be regarded as a two-dimensional
cross-correlation operation with a single input channel. As shown in Fig. 15.3.4, we can also
present the one-dimensional cross-correlation operation with multiple input channels in Fig.
15.3.3 as the equivalent two-dimensional cross-correlation operation with a single input channel.
Here, the height of the kernel is equal to the height of the input.
Fig. 15.3.4: Two-dimensional cross-correlation operation with a single input channel. The highlighted
parts are the first output element and the input and kernel array elements used in its calculation:
2  (??1) + 3  (??3) + 1  3 + 2  4 + 0  1 + 1  2 = 2.
Both the outputs in Fig. 15.3.2 and Fig. 15.3.3 have only one channel. We discussed how to specify
multiple output channels in a two-dimensional convolutional layer in Section 6.4. Similarly, we
can also specify multiple output channels in the one-dimensional convolutional layer to extend
the model parameters in the convolutional layer.
15.3.2 Max-Over-Time Pooling Layer
Similarly, we have a one-dimensional pooling layer. The max-over-time pooling layer used in
TextCNN actually corresponds to a one-dimensional global maximum pooling layer. Assuming
that the input contains multiple channels, and each channel consists of values on different
timesteps, the output of each channel will be the largest value of all timesteps in the channel.
Therefore, the input of the max-over-time pooling layer can have different timesteps on each channel.
To improve computing performance, we often combine timing examples of different lengths into
a minibatch and make the lengths of each timing example in the batch consistent by appending
special characters (such as 0) to the end of shorter examples. Naturally, the added special characters
have no intrinsic meaning. Because the main purpose of the max-over-time pooling layer
is to capture the most important features of timing, it usually allows the model to be unaffected
by the manually added characters.
694 Chapter 15. Natural Language Processing: Applications
15.3.3 The TextCNN Model
TextCNN mainly uses a one-dimensional convolutional layer and max-over-time pooling layer.
Suppose the input text sequence consists of n words, and each word is represented by a d-
dimension word vector. Then the input example has a width of n, a height of 1, and d input
channels. The calculation of textCNN can be mainly divided into the following steps:
1. Define multiple one-dimensional convolution kernels and use them to perform convolution
calculations on the inputs. Convolution kernels with different widths may capture the correlation
of different numbers of adjacent words.
2. Perform max-over-time pooling on all output channels, and then concatenate the pooling
output values of these channels in a vector.
3. The concatenated vector is transformed into the output for each category through the fully
connected layer. A dropout layer can be used in this step to deal with overfitting.
Fig. 15.3.5: TextCNN design.
Fig. 15.3.5 gives an example to illustrate the textCNN. The input here is a sentence with 11 words,
with each word represented by a 6-dimensional word vector. Therefore, the input sequence has a
width of 11 and 6 input channels. We assume there are two one-dimensional convolution kernels
with widths of 2 and 4, and 4 and 5 output channels, respectively. Therefore, after one-dimensional
convolution calculation, the width of the four output channels is 11 ?? 2 + 1 = 10, while the width
of the other five channels is 11??4+1 = 8. Even though the width of each channel is different, we
can still perform max-over-time pooling for each channel and concatenate the pooling outputs of
the 9 channels into a 9-dimensional vector. Finally, we use a fully connected layer to transform
15.3. Sentiment Analysis: Using Convolutional Neural Networks 695
the 9-dimensional vector into a 2-dimensional output: positive sentiment and negative sentiment
predictions.
Next, we will implement a textCNN model. Compared with the previous section, in addition to
replacing the recurrent neural network with a one-dimensional convolutional layer, here we use
two embedding layers, one with a fixed weight and another that participates in training.
class TextCNN(nn.Block):
def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,
**kwargs):
super(TextCNN, self).__init__(**kwargs)
self.embedding = nn.Embedding(vocab_size, embed_size)
# The embedding layer does not participate in training
self.constant_embedding = nn.Embedding(vocab_size, embed_size)
self.dropout = nn.Dropout(0.5)
self.decoder = nn.Dense(2)
# The max-over-time pooling layer has no weight, so it can share an
# instance
self.pool = nn.GlobalMaxPool1D()
# Create multiple one-dimensional convolutional layers
self.convs = nn.Sequential()
for c, k in zip(num_channels, kernel_sizes):
self.convs.add(nn.Conv1D(c, k, activation='relu'))
def forward(self, inputs):
# Concatenate the output of two embedding layers with shape of
# (batch size, no. of words, word vector dimension) by word vector
embeddings = np.concatenate((
self.embedding(inputs), self.constant_embedding(inputs)), axis=2)
# According to the input format required by Conv1D, the word vector
# dimension, that is, the channel dimension of the one-dimensional
# convolutional layer, is transformed into the previous dimension
embeddings = embeddings.transpose(0, 2, 1)
# For each one-dimensional convolutional layer, after max-over-time
# pooling, an ndarray with the shape of (batch size, channel size, 1)
# can be obtained. Use the flatten function to remove the last
# dimension and then concatenate on the channel dimension
encoding = np.concatenate([
np.squeeze(self.pool(conv(embeddings)), axis=-1)
for conv in self.convs], axis=1)
# After applying the dropout method, use a fully connected layer to
# obtain the output
outputs = self.decoder(self.dropout(encoding))
return outputs
Create a TextCNN instance. It has 3 convolutional layers with kernel widths of 3, 4, and 5, all with
100 output channels.
embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]
devices = d2l.try_all_gpus()
net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)
net.initialize(init.Xavier(), ctx=devices)
696 Chapter 15. Natural Language Processing: Applications
Load Pre-trained Word Vectors
As in the previous section, load pre-trained 100-dimensional GloVe word vectors and initialize the
embedding layers embedding and constant_embedding. Here, the former participates in training
while the latter has a fixed weight.
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
net.embedding.weight.set_data(embeds)
net.constant_embedding.weight.set_data(embeds)
net.constant_embedding.collect_params().setattr('grad_req', 'null')
Train and Evaluate the Model
Now we can train the model.
lr, num_epochs = 0.001, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.091, train acc 0.969, test acc 0.860
3768.8 examples/sec on [gpu(0), gpu(1)]
Below, we use the trained model to classify sentiments of two simple sentences.
d2l.predict_sentiment(net, vocab, 'this movie is so great')
'negative'
d2l.predict_sentiment(net, vocab, 'this movie is so bad')
15.3. Sentiment Analysis: Using Convolutional Neural Networks 697
'negative'
Summary
� We can use one-dimensional convolution to process and analyze timing data.
� A one-dimensional cross-correlation operation with multiple input channels can be regarded
as a two-dimensional cross-correlation operation with a single input channel.
� The input of the max-over-time pooling layer can have different numbers of timesteps on
each channel.
� TextCNN mainly uses a one-dimensional convolutional layer and max-over-time pooling
layer.
Exercises
1. Tune the hyperparameters and compare the two sentiment analysis methods, using recurrent
neural networks and using convolutional neural networks, as regards accuracy and operational
efficiency.
2. Can you further improve the accuracy of the model on the test set by using the three methods
introduced in the previous section: tuning hyperparameters, using larger pre-trained word
vectors, and using the spaCy word tokenization tool?
3. What other natural language processing tasks can you use textCNN for?
Discussions216
15.4 Natural Language Inference and the Dataset
In Section 15.1, we discussed the problem of sentiment analysis. This task aims to classify a single
text sequence into predefined categories, such as a set of sentiment polarities. However, when
there is a need to decide whether one sentence can be inferred form another, or eliminate redundancy
by identifying sentences that are semantically equivalent, knowing how to classify one text
sequence is insufficient. Instead, we need to be able to reason over pairs of text sequences.
15.4.1 Natural Language Inference
Natural language inference studies whether a hypothesis can be inferred from a premise, where both
are a text sequence. In other words, natural language inference determines the logical relationship
between a pair of text sequences. Such relationships usually fall into three types:
� Entailment: the hypothesis can be inferred from the premise.
� Contradiction: the negation of the hypothesis can be inferred from the premise.
� Neutral: all the other cases.
216 https://discuss.d2l.ai/t/393
698 Chapter 15. Natural Language Processing: Applications
Natural language inference is also known as the recognizing textual entailment task. For example,
the following pair will be labeled as entailment because �showing affection� in the hypothesis can
be inferred from �hugging one another� in the premise.
Premise: Two women are hugging each other.
Hypothesis: Two women are showing affection.
The following is an example of contradiction as �running the coding example� indicates �not sleeping�
rather than �sleeping�.
Premise: A man is running the coding example from Dive into Deep Learning.
Hypothesis: The man is sleeping.
The third example shows a neutrality relationship because neither �famous� nor �not famous� can
be inferred from the fact that �are performing for us�.
Premise: The musicians are performing for us.
Hypothesis: The musicians are famous.
Natural language inference has been a central topic for understanding natural language. It enjoys
wide applications ranging from information retrieval to open-domain question answering.
To study this problem, we will begin by investigating a popular natural language inference benchmark
dataset.
15.4.2 The Stanford Natural Language Inference (SNLI) Dataset
Stanford Natural Language Inference (SNLI) Corpus is a collection of over 500; 000 labeled English
sentence pairs (Bowman et al., 2015). We download and store the extracted SNLI dataset in the
path ../data/snli_1.0.
import collections
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
import os
import re
import zipfile
npx.set_np()
#@save
d2l.DATA_HUB['SNLI'] = (
'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
'9fcde07509c7e87ec61c640c1b2753d9041758e4')
data_dir = d2l.download_extract('SNLI')
Downloading ../data/snli_1.0.zip from https://nlp.stanford.edu/projects/snli/snli_1.0.zip...
15.4. Natural Language Inference and the Dataset 699
Reading the Dataset
The original SNLI dataset contains much richer information than what we really need in our experiments.
Thus, we define a function read_snli to only extract part of the dataset, then return
lists of premises, hypotheses, and their labels.
#@save
def read_snli(data_dir, is_train):
"""Read the SNLI dataset into premises, hypotheses, and labels."""
def extract_text(s):
# Remove information that will not be used by us
s = re.sub('\\(', '', s)
s = re.sub('\\)', '', s)
# Substitute two or more consecutive whitespace with space
s = re.sub('\\s{2,}', ' ', s)
return s.strip()
label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
if is_train else 'snli_1.0_test.txt')
with open(file_name, 'r') as f:
rows = [row.split('\t') for row in f.readlines()[1:]]
premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]
labels = [label_set[row[0]] for row in rows if row[0] in label_set]
return premises, hypotheses, labels
Now let us print the first 3 pairs of premise and hypothesis, as well as their labels (�0�, �1�, and �2�
correspond to �entailment�, �contradiction�, and �neutral�, respectively ).
train_data = read_snli(data_dir, is_train=True)
for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):
print('premise:', x0)
print('hypothesis:', x1)
print('label:', y)
premise: A person on a horse jumps over a broken down airplane .
hypothesis: A person is training his horse for a competition .
label: 2
premise: A person on a horse jumps over a broken down airplane .
hypothesis: A person is at a diner , ordering an omelette .
label: 1
premise: A person on a horse jumps over a broken down airplane .
hypothesis: A person is outdoors , on a horse .
label: 0
The training set has about 550; 000 pairs, and the testing set has about 10; 000 pairs. The following
shows that the three labels �entailment�, �contradiction�, and �neutral� are balanced in both the
training set and the testing set.
test_data = read_snli(data_dir, is_train=False)
for data in [train_data, test_data]:
print([[row for row in data[2]].count(i) for i in range(3)])
700 Chapter 15. Natural Language Processing: Applications
[183416, 183187, 182764]
[3368, 3237, 3219]
Defining a Class for Loading the Dataset
Below we define a class for loading the SNLI dataset by inheriting from the Dataset class in Gluon.
The argument num_steps in the class constructor specifies the length of a text sequence so that
each minibatch of sequences will have the same shape. In other words, tokens after the first
num_steps ones in longer sequence are trimmed, while special tokens �<pad>� will be appended
to shorter sequences until their length becomes num_steps. By implementing the __getitem__
function, we can arbitrarily access the premise, hypothesis, and label with the index idx.
#@save
class SNLIDataset(gluon.data.Dataset):
"""A customized dataset to load the SNLI dataset."""
def __init__(self, dataset, num_steps, vocab=None):
self.num_steps = num_steps
all_premise_tokens = d2l.tokenize(dataset[0])
all_hypothesis_tokens = d2l.tokenize(dataset[1])
if vocab is None:
self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,
min_freq=5, reserved_tokens=['<pad>'])
else:
self.vocab = vocab
self.premises = self._pad(all_premise_tokens)
self.hypotheses = self._pad(all_hypothesis_tokens)
self.labels = np.array(dataset[2])
print('read ' + str(len(self.premises)) + ' examples')
def _pad(self, lines):
return np.array([d2l.truncate_pad(
self.vocab[line], self.num_steps, self.vocab['<pad>'])
for line in lines])
def __getitem__(self, idx):
return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]
def __len__(self):
return len(self.premises)
Putting All Things Together
Now we can invoke the read_snli function and the SNLIDataset class to download the SNLI dataset
and return DataLoader instances for both training and testing sets, together with the vocabulary
of the training set. It is noteworthy that we must use the vocabulary constructed from the training
set as that of the testing set. As a result, any new token from the testing set will be unknown to
the model trained on the training set.
#@save
def load_data_snli(batch_size, num_steps=50):
(continues on next page)
15.4. Natural Language Inference and the Dataset 701
(continued from previous page)
"""Download the SNLI dataset and return data iterators and vocabulary."""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_data = read_snli(data_dir, True)
test_data = read_snli(data_dir, False)
train_set = SNLIDataset(train_data, num_steps)
test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers)
test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,
num_workers=num_workers)
return train_iter, test_iter, train_set.vocab
Here we set the batch size to 128 and sequence length to 50, and invoke the load_data_snli function
to get the data iterators and vocabulary. Then we print the vocabulary size.
train_iter, test_iter, vocab = load_data_snli(128, 50)
len(vocab)
read 549367 examples
read 9824 examples
18678
Now we print the shape of the first minibatch. Contrary to sentiment analysis, we have 2 inputs
X[0] and X[1] representing pairs of premises and hypotheses.
for X, Y in train_iter:
print(X[0].shape)
print(X[1].shape)
print(Y.shape)
break
(128, 50)
(128, 50)
(128,)
Summary
� Natural language inference studies whether a hypothesis can be inferred from a premise,
where both are a text sequence.
� In natural language inference, relationships between premises and hypotheses include entailment,
contradiction, and neutral.
� Stanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of natural
language inference.
702 Chapter 15. Natural Language Processing: Applications
Exercises
1. Machine translation has long been evaluated based on superficial n-gram matching between
an output translation and a ground-truth translation. Can you design a measure for evaluating
machine translation results by using natural language inference?
2. How can we change hyperparameters to reduce the vocabulary size?
Discussions217
15.5 Natural Language Inference: Using Attention
We introduced the natural language inference task and the SNLI dataset in Section 15.4. In view of
many models that are based on complex and deep architectures, Parikh et al. proposed to address
natural language inference with attention mechanisms and called it a �decomposable attention
model� (Parikh et al., 2016). This results in a model without recurrent or convolutional layers,
achieving the best result at the time on the SNLI dataset with much fewer parameters. In this
section, we will describe and implement this attention-based method (with MLPs) for natural language
inference, as depicted in Fig. 15.5.1.
Fig. 15.5.1: This section feeds pretrained GloVe to an architecture based on attention and MLPs
for natural language inference.
15.5.1 The Model
Simpler than preserving the order of words in premises and hypotheses, we can just align words
in one text sequence to every word in the other, and vice versa, then compare and aggregate such
information to predict the logical relationships between premises and hypotheses. Similar to
alignment of words between source and target sentences in machine translation, the alignment of
words between premises and hypotheses can be neatly accomplished by attention mechanisms.
217 https://discuss.d2l.ai/t/394
15.5. Natural Language Inference: Using Attention 703
Fig. 15.5.2: Natural language inference using attention mechanisms.
Fig. 15.5.2 depicts the natural language inference method using attention mechanisms. At a high
level, it consists of three jointly trained steps: attending, comparing, and aggregating. We will
illustrate them step by step in the following.
from d2l import mxnet as d2l
import mxnet as mx
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
Attending
The first step is to align words in one text sequence to each word in the other sequence. Suppose
that the premise is �i do need sleep� and the hypothesis is �i am tired�. Due to semantical similarity,
we may wish to align �i� in the hypothesis with �i� in the premise, and align �tired� in the
hypothesis with �sleep� in the premise. Likewise, we may wish to align �i� in the premise with
�i� in the hypothesis, and align �need� and �sleep� in the premise with �tired� in the hypothesis.
Note that such alignment is soft using weighted average, where ideally large weights are associated
with the words to be aligned. For ease of demonstration, Fig. 15.5.2 shows such alignment in a
hard way.
Now we describe the soft alignment using attention mechanisms in more detail. Denote by A =
(a1; : : : ; am) and B = (b1; : : : ; bn) the premise and hypothesis, whose number of words are m and
n, respectively, where ai; bj 2 Rd (i = 1; : : : ; m; j = 1; : : : ; n) is a d-dimensional word embedding
vector. For soft alignment, we compute the attention weights eij 2 R as
eij = f(ai)
?
f(bj); (15.5.1)
where the function f is a multilayer perceptron defined in the following mlp function. The output
dimension of f is specified by the num_hiddens argument of mlp.
704 Chapter 15. Natural Language Processing: Applications
def mlp(num_hiddens, flatten):
net = nn.Sequential()
net.add(nn.Dropout(0.2))
net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))
net.add(nn.Dropout(0.2))
net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))
return net
It should be highlighted that, in (15.5.1) f takes inputs ai and bj separately rather than takes a pair
of them together as the input. This decomposition trick leads to only m + n applications (linear
complexity) of f rather than mn applications (quadratic complexity).
Normalizing the attention weights in (15.5.1), we compute the weighted average of all the word
embeddings in the hypothesis to obtain representation of the hypothesis that is softly aligned with
the word indexed by i in the premise:
i =
?n
j=1
? exp(eij) n
k=1 exp(eik)
bj : (15.5.2)
Likewise, we compute soft alignment of premise words for each word indexed by j in the hypothesis:
j =
?m
i=1
? exp(eij) m
k=1 exp(ekj)
ai: (15.5.3)
Below we define the Attend class to compute the soft alignment of hypotheses (beta) with input
premises A and soft alignment of premises (alpha) with input hypotheses B.
class Attend(nn.Block):
def __init__(self, num_hiddens, **kwargs):
super(Attend, self).__init__(**kwargs)
self.f = mlp(num_hiddens=num_hiddens, flatten=False)
def forward(self, A, B):
# Shape of `A`/`B`: (b`atch_size`, no. of words in sequence A/B,
# `embed_size`)
# Shape of `f_A`/`f_B`: (`batch_size`, no. of words in sequence A/B,
# `num_hiddens`)
f_A = self.f(A)
f_B = self.f(B)
# Shape of `e`: (`batch_size`, no. of words in sequence A,
# no. of words in sequence B)
e = npx.batch_dot(f_A, f_B, transpose_b=True)
# Shape of `beta`: (`batch_size`, no. of words in sequence A,
# `embed_size`), where sequence B is softly aligned with each word
# (axis 1 of `beta`) in sequence A
beta = npx.batch_dot(npx.softmax(e), B)
# Shape of `alpha`: (`batch_size`, no. of words in sequence B,
# `embed_size`), where sequence A is softly aligned with each word
# (axis 1 of `alpha`) in sequence B
alpha = npx.batch_dot(npx.softmax(e.transpose(0, 2, 1)), A)
return beta, alpha
15.5. Natural Language Inference: Using Attention 705
Comparing
In the next step, we compare a word in one sequence with the other sequence that is softly aligned
with that word. Note that in soft alignment, all the words from one sequence, though with probably
different attention weights, will be compared with a word in the other sequence. For easy of
demonstration, Fig. 15.5.2 pairs words with aligned words in a hard way. For example, suppose
that the attending step determines that �need� and �sleep� in the premise are both aligned with
�tired� in the hypothesis, the pair �tired�need sleep� will be compared.
In the comparing step, we feed the concatenation (operator [; ]) of words from one sequence and
aligned words from the other sequence into a function g (a multilayer perceptron):
vA;i = g([ai; i]); i = 1; : : : ;m
vB;j = g([bj ;j ]); j = 1; : : : ; n:
(15.5.4)
In (15.5.4), vA;i is the comparison between word i in the premise and all the hypothesis words that
are softly aligned with word i; while vB;j is the comparison between word j in the hypothesis and
all the premise words that are softly aligned with word j. The following Compare class defines such
as comparing step.
class Compare(nn.Block):
def __init__(self, num_hiddens, **kwargs):
super(Compare, self).__init__(**kwargs)
self.g = mlp(num_hiddens=num_hiddens, flatten=False)
def forward(self, A, B, beta, alpha):
V_A = self.g(np.concatenate([A, beta], axis=2))
V_B = self.g(np.concatenate([B, alpha], axis=2))
return V_A, V_B
Aggregating
With two sets of comparison vectors vA;i (i = 1; : : : ;m) and vB;j (j = 1; : : : ; n) on hand, in the last
step we will aggregate such information to infer the logical relationship. We begin by summing
up both sets:
vA =
?m
i=1
vA;i; vB =
?n
j=1
vB;j : (15.5.5)
Next we feed the concatenation of both summarization results into function h (a multilayer perceptron)
to obtain the classification result of the logical relationship:
^y = h([vA; vB]): (15.5.6)
The aggregation step is defined in the following Aggregate class.
class Aggregate(nn.Block):
def __init__(self, num_hiddens, num_outputs, **kwargs):
super(Aggregate, self).__init__(**kwargs)
self.h = mlp(num_hiddens=num_hiddens, flatten=True)
self.h.add(nn.Dense(num_outputs))
(continues on next page)
706 Chapter 15. Natural Language Processing: Applications
(continued from previous page)
def forward(self, V_A, V_B):
# Sum up both sets of comparison vectors
V_A = V_A.sum(axis=1)
V_B = V_B.sum(axis=1)
# Feed the concatenation of both summarization results into an MLP
Y_hat = self.h(np.concatenate([V_A, V_B], axis=1))
return Y_hat
Putting All Things Together
By putting the attending, comparing, and aggregating steps together, we define the decomposable
attention model to jointly train these three steps.
class DecomposableAttention(nn.Block):
def __init__(self, vocab, embed_size, num_hiddens, **kwargs):
super(DecomposableAttention, self).__init__(**kwargs)
self.embedding = nn.Embedding(len(vocab), embed_size)
self.attend = Attend(num_hiddens)
self.compare = Compare(num_hiddens)
# There are 3 possible outputs: entailment, contradiction, and neutral
self.aggregate = Aggregate(num_hiddens, 3)
def forward(self, X):
premises, hypotheses = X
A = self.embedding(premises)
B = self.embedding(hypotheses)
beta, alpha = self.attend(A, B)
V_A, V_B = self.compare(A, B, beta, alpha)
Y_hat = self.aggregate(V_A, V_B)
return Y_hat
15.5.2 Training and Evaluating the Model
Now we will train and evaluate the defined decomposable attention model on the SNLI dataset.
We begin by reading the dataset.
Reading the dataset
We download and read the SNLI dataset using the function defined in Section 15.4. The batch size
and sequence length are set to 256 and 50, respectively.
batch_size, num_steps = 256, 50
train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)
read 549367 examples
read 9824 examples
15.5. Natural Language Inference: Using Attention 707
Creating the Model
We use the pretrained 100-dimensional GloVe embedding to represent the input tokens. Thus, we
predefine the dimension of vectors ai and bj in :eqref:eq_nli_e as 100. The output dimension of
functions f in (15.5.1) and g in :eqref:eq_nli_v_ab is set to 200. Then we create a model instance,
initialize its parameters, and load the GloVe embedding to initialize vectors of input tokens.
embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()
net = DecomposableAttention(vocab, embed_size, num_hiddens)
net.initialize(init.Xavier(), ctx=devices)
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')
embeds = glove_embedding[vocab.idx_to_token]
net.embedding.weight.set_data(embeds)
Downloading ../data/glove.6B.100d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.
,!6B.100d.zip...
Training and Evaluating the Model
In contrast to the split_batch function in Section 12.5 that takes single inputs such as text sequences
(or images), we define a split_batch_multi_inputs function to take multiple inputs such
as premises and hypotheses in minibatches.
#@save
def split_batch_multi_inputs(X, y, devices):
"""Split multi-input `X` and `y` into multiple devices."""
X = list(zip(*[gluon.utils.split_and_load(
feature, devices, even_split=False) for feature in X]))
return (X, gluon.utils.split_and_load(y, devices, even_split=False))
Now we can train and evaluate the model on the SNLI dataset.
lr, num_epochs = 0.001, 4
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,
split_batch_multi_inputs)
loss 0.518, train acc 0.795, test acc 0.819
10522.2 examples/sec on [gpu(0), gpu(1)]
708 Chapter 15. Natural Language Processing: Applications
Using the Model
Finally, define the prediction function to output the logical relationship between a pair of premise
and hypothesis.
#@save
def predict_snli(net, vocab, premise, hypothesis):
premise = np.array(vocab[premise], ctx=d2l.try_gpu())
hypothesis = np.array(vocab[hypothesis], ctx=d2l.try_gpu())
label = np.argmax(net([premise.reshape((1, -1)),
hypothesis.reshape((1, -1))]), axis=1)
return 'entailment' if label == 0 else 'contradiction' if label == 1 \
else 'neutral'
We can use the trained model to obtain the natural language inference result for a sample pair of
sentences.
predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])
'contradiction'
Summary
� The decomposable attention model consists of three steps for predicting the logical relationships
between premises and hypotheses: attending, comparing, and aggregating.
� With attention mechanisms, we can align words in one text sequence to every word in the
other, and vice versa. Such alignment is soft using weighted average, where ideally large
weights are associated with the words to be aligned.
� The decomposition trick leads to a more desirable linear complexity than quadratic complexity
when computing attention weights.
� We can use pretrained word embedding as the input representation for downstream natural
language processing task such as natural language inference.
15.5. Natural Language Inference: Using Attention 709
Exercises
1. Train the model with other combinations of hyperparameters. Can you get better accuracy
on the test set?
2. What are major drawbacks of the decomposable attention model for natural language inference?
3. Suppose that we want to get the level of semantical similarity (e.g., a continuous value between
0 and 1) for any pair of sentences. How shall we collect and label the dataset? Can you
design a model with attention mechanisms?
Discussions218
15.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications
In the previous sections of this chapter, we have designed different models for natural language
processing applications, such as based on RNNs, CNNs, attention, and MLPs. These models are
helpful when there is space or time constraint, however, crafting a specific model for every natural
language processing task is practically infeasible. In Section 14.8, we introduced a pretraining
model, BERT, that requires minimal architecture changes for a wide range of natural language
processing tasks. One one hand, at the time of its proposal, BERT improved the state of the art
on various natural language processing tasks. On the other hand, as noted in Section 14.10, the
two versions of the original BERT model come with 110 million and 340 million parameters. Thus,
when there are sufficient computational resources, we may consider fine-tuning BERT for downstream
natural language processing applications.
In the following, we generalize a subset of natural language processing applications as sequencelevel
and token-level. On the sequence level, we introduce how to transform the BERT representation
of the text input to the output label in single text classification and text pair classification
or regression. On the token level, we will briefly introduce new applications such as text tagging
and question answering and shed light on how BERT can represent their inputs and get transformed
into output labels. During fine-tuning, the �minimal architecture changes� required by
BERT across different applications are the extra fully-connected layers. During supervised learning
of a downstream application, parameters of the extra layers are learned from scratch while all
the parameters in the pretrained BERT model are fine-tuned.
15.6.1 Single Text Classification
Single text classification takes a single text sequence as the input and outputs its classification result.
Besides sentiment analysis that we have studied in this chapter, the Corpus of Linguistic
Acceptability (CoLA) is also a dataset for single text classification, judging whether a given sentence
is grammatically acceptable or not (Warstadt et al., 2019). For instance, �I should study.� is
acceptable but �I should studying.� is not.
218 https://discuss.d2l.ai/t/395
710 Chapter 15. Natural Language Processing: Applications
Fig. 15.6.1: Fine-tuning BERT for single text classification applications, such as sentiment analysis
and testing linguistic acceptability. Suppose that the input single text has six tokens.
Section 14.8 describes the input representation of BERT. The BERT input sequence unambiguously
represents both single text and text pairs, where the special classification token �<cls>� is used for
sequence classification and the special classification token �<sep>� marks the end of single text
or separates a pair of text. As shown in Fig. 15.6.1, in single text classification applications, the
BERT representation of the special classification token �<cls>� encodes the information of the
entire input text sequence. As the representation of the input single text, it will be fed into a small
MLP consisting of fully-connected (dense) layers to output the distribution of all the discrete label
values.
15.6.2 Text Pair Classification or Regression
We have also examined natural language inference in this chapter. It belongs to text pair classification,
a type of application classifying a pair of text.
Taking a pair of text as the input but outputting a continuous value, semantic textual similarity
is a popular text pair regression task. This task measures semantic similarity of sentences. For
instance, in the Semantic Textual Similarity Benchmark dataset, the similarity score of a pair of
sentences is an ordinal scale ranging from 0 (no meaning overlap) to 5 (meaning equivalence) (Cer
et al., 2017). The goal is to predict these scores. Examples from the Semantic Textual Similarity
Benchmark dataset include (sentence 1, sentence 2, similarity score):
� �A plane is taking off.�, �An air plane is taking off.�, 5.000;
� �A woman is eating something.�, �A woman is eating meat.�, 3.000;
� �A woman is dancing.�, �A man is talking.�, 0.000.
15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications 711
Fig. 15.6.2: Fine-tuning BERT for text pair classification or regression applications, such as natural
language inference and semantic textual similarity. Suppose that the input text pair has two and
three tokens.
Comparing with single text classification in Fig. 15.6.1, fine-tuning BERT for text pair classification
in Fig. 15.6.2 is different in the input representation. For text pair regression tasks such as
semantic textual similarity, trivial changes can be applied such as outputting a continuous label
value and using the mean squared loss: they are common for regression.
15.6.3 Text Tagging
Now let us consider token-level tasks, such as text tagging, where each token is assigned a label.
Among text tagging tasks, part-of-speech tagging assigns each word a part-of-speech tag (e.g., adjective
and determiner) according to the role of the word in the sentence. For example, according
to the Penn Treebank II tag set, the sentence �John Smith ?s car is new� should be tagged as �NNP
(noun, proper singular) NNP POS (possessive ending) NN (noun, singular or mass) VB (verb, base
form) JJ (adjective)�.
712 Chapter 15. Natural Language Processing: Applications
Fig. 15.6.3: Fine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose
that the input single text has six tokens.
Fine-tuning BERT for text tagging applications is illustrated in Fig. 15.6.3. Comparing with Fig.
15.6.1, the only distinction lies in that in text tagging, the BERT representation of every token of the
input text is fed into the same extra fully-connected layers to output the label of the token, such
as a part-of-speech tag.
15.6.4 Question Answering
As another token-level application, question answering reflects capabilities of reading comprehension.
For example, the Stanford Question Answering Dataset (SQuAD v1.1) consists of reading
passages and questions, where the answer to every question is just a segment of text (text span)
from the passage that the question is about (Rajpurkar et al., 2016). To explain, consider a passage
�Some experts report that a mask?s efficacy is inconclusive. However, mask makers insist that
their products, such as N95 respirator masks, can guard against the virus.� and a question �Who
say that N95 respirator masks can guard against the virus?�. The answer should be the text span
�mask makers� in the passage. Thus, the goal in SQuAD v1.1 is to predict the start and end of the
text span in the passage given a pair of question and passage.
15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications 713
Fig. 15.6.4: Fine-tuning BERT for question answering. Suppose that the input text pair has two
and three tokens.
To fine-tune BERT for question answering, the question and passage are packed as the first and
second text sequence, respectively, in the input of BERT. To predict the position of the start of
the text span, the same additional fully-connected layer will transform the BERT representation
of any token from the passage of position i into a scalar score si. Such scores of all the passage
tokens are further transformed by the softmax operation into a probability distribution, so that
each token position i in the passage is assigned a probability pi of being the start of the text span.
Predicting the end of the text span is the same as above, except that parameters in its additional
fully-connected layer are independent from those for predicting the start. When predicting the
end, any passage token of position i is transformed by the same fully-connected layer into a scalar
score ei. :numref:fig_bert-qa depicts fine-tuning BERT for question answering.
For question answering, the supervised learning?s training objective is as straightforward as maximizing
the log-likelihoods of the ground-truth start and end positions. When predicting the span,
we can compute the score si + ej for a valid span from position i to position j (i  j), and output
the span with the highest score.
Summary
� BERT requires minimal architecture changes (extra fully-connected layers) for sequencelevel
and token-level natural language processing applications, such as single text classification
(e.g., sentiment analysis and testing linguistic acceptability), text pair classification
or regression (e.g., natural language inference and semantic textual similarity), text tagging
(e.g., part-of-speech tagging), and question answering.
� During supervised learning of a downstream application, parameters of the extra layers are
learned from scratch while all the parameters in the pretrained BERT model are fine-tuned.
714 Chapter 15. Natural Language Processing: Applications
Exercises
1. Let us design a search engine algorithm for news articles. When the system receives an
query (e.g., �oil industry during the coronavirus outbreak�), it should return a ranked list
of news articles that are most relevant to the query. Suppose that we have a huge pool of
news articles and a large number of queries. To simplify the problem, suppose that the
most relevant article has been labeled for each query. How can we apply negative sampling
(see Section 14.2.1) and BERT in the algorithm design?
2. How can we leverage BERT in training language models?
3. Can we leverage BERT in machine translation?
Discussions219
15.7 Natural Language Inference: Fine-Tuning BERT
In earlier sections of this chapter, we have designed an attention-based architecture (in Section
15.5) for the natural language inference task on the SNLI dataset (as described in Section 15.4).
Now we revisit this task by fine-tuning BERT. As discussed in Section 15.6, natural language inference
is a sequence-level text pair classification problem, and fine-tuning BERT only requires an
additional MLP-based architecture, as illustrated in Fig. 15.7.1.
Fig. 15.7.1: This section feeds pretrained BERT to an MLP-based architecture for natural language
inference.
In this section, we will download a pretrained small version of BERT, then fine-tune it for natural
language inference on the SNLI dataset.
from d2l import mxnet as d2l
import json
import multiprocessing
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
(continues on next page)
219 https://discuss.d2l.ai/t/396
15.7. Natural Language Inference: Fine-Tuning BERT 715
(continued from previous page)
import os
npx.set_np()
15.7.1 Loading Pretrained BERT
We have explained how to pretrain BERT on the WikiText-2 dataset in Section 14.9 and Section
14.10 (note that the original BERT model is pretrained on much bigger corpora). As discussed in
Section 14.10, the original BERT model has hundreds of millions of parameters. In the following,
we provide two versions of pretrained BERT: �bert.base� is about as big as the original BERT base
model that requires a lot of computational resources to fine-tune, while �bert.small� is a small
version to facilitate demonstration.
d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.zip',
'7b3820b35da691042e5d34c0971ac3edbd80d3f4')
d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.zip',
'a4e718a47137ccd1809c9107ab4f5edd317bae2c')
Either pretrained BERT model contains a �vocab.json� file that defines the vocabulary set
and a �pretrained.params� file of the pretrained parameters. We implement the following
load_pretrained_model function to load pretrained BERT parameters.
def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout, max_len, devices):
data_dir = d2l.download_extract(pretrained_model)
# Define an empty vocabulary to load the predefined vocabulary
vocab = d2l.Vocab([])
vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))
vocab.token_to_idx = {token: idx for idx, token in enumerate(
vocab.idx_to_token)}
bert = d2l.BERTModel(len(vocab), num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len)
# Load pretrained BERT parameters
bert.load_parameters(os.path.join(data_dir, 'pretrained.params'),
ctx=devices)
return bert, vocab
To facilitate demonstration on most of machines, we will load and fine-tune the small version
(�bert.small�) of the pretrained BERT in this section. In the exercise, we will show how to finetune
the much larger �bert.base� to significantly improve the testing accuracy.
devices = d2l.try_all_gpus()
bert, vocab = load_pretrained_model(
'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,
num_layers=2, dropout=0.1, max_len=512, devices=devices)
Downloading ../data/bert.small.zip from http://d2l-data.s3-accelerate.amazonaws.com/bert.
,!small.zip...
716 Chapter 15. Natural Language Processing: Applications
15.7.2 The Dataset for Fine-Tuning BERT
For the downstream task natural language inference on the SNLI dataset, we define a customized
dataset class SNLIBERTDataset. In each example, the premise and hypothesis form a pair of text
sequence and is packed into one BERT input sequence as depicted in Fig. 15.6.2. Recall Section
14.8.4 that segment IDs are used to distinguish the premise and the hypothesis in a BERT input sequence.
With the predefined maximum length of a BERT input sequence (max_len), the last token
of the longer of the input text pair keeps getting removed until max_len is met. To accelerate generation
of the SNLI dataset for fine-tuning BERT, we use 4 worker processes to generate training
or testing examples in parallel.
class SNLIBERTDataset(gluon.data.Dataset):
def __init__(self, dataset, max_len, vocab=None):
all_premise_hypothesis_tokens = [[
p_tokens, h_tokens] for p_tokens, h_tokens in zip(
*[d2l.tokenize([s.lower() for s in sentences])
for sentences in dataset[:2]])]
self.labels = np.array(dataset[2])
self.vocab = vocab
self.max_len = max_len
(self.all_token_ids, self.all_segments,
self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)
print('read ' + str(len(self.all_token_ids)) + ' examples')
def _preprocess(self, all_premise_hypothesis_tokens):
pool = multiprocessing.Pool(4) # Use 4 worker processes
out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)
all_token_ids = [
token_ids for token_ids, segments, valid_len in out]
all_segments = [segments for token_ids, segments, valid_len in out]
valid_lens = [valid_len for token_ids, segments, valid_len in out]
return (np.array(all_token_ids, dtype='int32'),
np.array(all_segments, dtype='int32'),
np.array(valid_lens))
def _mp_worker(self, premise_hypothesis_tokens):
p_tokens, h_tokens = premise_hypothesis_tokens
self._truncate_pair_of_tokens(p_tokens, h_tokens)
tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)
token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \
* (self.max_len - len(tokens))
segments = segments + [0] * (self.max_len - len(segments))
valid_len = len(tokens)
return token_ids, segments, valid_len
def _truncate_pair_of_tokens(self, p_tokens, h_tokens):
# Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT
# input
while len(p_tokens) + len(h_tokens) > self.max_len - 3:
if len(p_tokens) > len(h_tokens):
p_tokens.pop()
else:
h_tokens.pop()
(continues on next page)
15.7. Natural Language Inference: Fine-Tuning BERT 717
(continued from previous page)
def __getitem__(self, idx):
return (self.all_token_ids[idx], self.all_segments[idx],
self.valid_lens[idx]), self.labels[idx]
def __len__(self):
return len(self.all_token_ids)
After downloading the SNLI dataset, we generate training and testing examples by instantiating
the SNLIBERTDataset class. Such examples will be read in minibatches during training and testing
of natural language inference.
# Reduce `batch_size` if there is an out of memory error. In the original BERT
# model, `max_len` = 512
batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)
test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)
train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers)
test_iter = gluon.data.DataLoader(test_set, batch_size,
num_workers=num_workers)
read 549367 examples
read 9824 examples
15.7.3 Fine-Tuning BERT
As Fig. 15.6.2 indicates, fine-tuning BERT for natural language inference requires only an extra
MLP consisting of two fully-connected layers (see self.hidden and self.output in the following
BERTClassifier class). This MLP transforms the BERT representation of the special �<cls>� token,
which encodes the information of both the premise and the hypothesis, into three outputs of
natural language inference: entailment, contradiction, and neutral.
class BERTClassifier(nn.Block):
def __init__(self, bert):
super(BERTClassifier, self).__init__()
self.encoder = bert.encoder
self.hidden = bert.hidden
self.output = nn.Dense(3)
def forward(self, inputs):
tokens_X, segments_X, valid_lens_x = inputs
encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)
return self.output(self.hidden(encoded_X[:, 0, :]))
In the following, the pretrained BERT model bert is fed into the BERTClassifier instance net for
the downstream application. In common implementations of BERT fine-tuning, only the parameters
of the output layer of the additional MLP (net.output) will be learned from scratch. All the
parameters of the pretrained BERT encoder (net.encoder) and the hidden layer of the additional
MLP (net.hidden) will be fine-tuned.
718 Chapter 15. Natural Language Processing: Applications
net = BERTClassifier(bert)
net.output.initialize(ctx=devices)
Recall that in Section 14.8 both the MaskLM class and the NextSentencePred class have parameters
in their employed MLPs. These parameters are part of those in the pretrained BERT model bert,
and thus part of parameters in net. However, such parameters are only for computing the masked
language modeling loss and the next sentence prediction loss during pretraining. These two loss
functions are irrelevant to fine-tuning downstream applications, thus the parameters of the employed
MLPs in MaskLM and NextSentencePred are not updated (staled) when BERT is fine-tuned.
To allow parameters with stale gradients, the flag ignore_stale_grad=True is set in the step function
of d2l.train_batch_ch13. We use this function to train and evaluate the model net using the
training set (train_iter) and the testing set (test_iter) of SNLI. Due to the limited computational
resources, the training and testing accuracy can be further improved: we leave its discussions in
the exercises.
lr, num_epochs = 1e-4, 5
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})
loss = gluon.loss.SoftmaxCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,
d2l.split_batch_multi_inputs)
loss 0.594, train acc 0.744, test acc 0.711
8312.2 examples/sec on [gpu(0), gpu(1)]
Summary
� We can fine-tune the pretrained BERT model for downstream applications, such as natural
language inference on the SNLI dataset.
� During fine-tuning, the BERT model becomes part of the model for the downstream application.
Parameters that are only related to pretraining loss will not be updated during finetuning.
15.7. Natural Language Inference: Fine-Tuning BERT 719
Exercises
1. Fine-tune a much larger pretrained BERT model that is about as big as the original BERT base
model if your computational resource allows. Set arguments in the load_pretrained_model
function as: replacing ?bert.small? with ?bert.base?, increasing values of num_hiddens=256,
ffn_num_hiddens=512, num_heads=4, num_layers=2 to 768, 3072, 12, 12, respectively. By increasing
fine-tuning epochs (and possibly tuning other hyperparameters), can you get a testing
accuracy higher than 0.86?
2. How to truncate a pair of sequences according to their ratio of length? Compare this pair
truncation method and the one used in the SNLIBERTDataset class. What are their pros and
cons?
Discussions220
220 https://discuss.d2l.ai/t/397
720 Chapter 15. Natural Language Processing: Applications
16 | Recommender Systems
Shuai Zhang (Amazon), Aston Zhang (Amazon), and Yi Tay (Google)
Recommender systems are widely employed in industry and are ubiquitous in our daily lives.
These systems are utilized in a number of areas such as online shopping sites (e.g., amazon.com),
music/movie services site (e.g., Netflix and Spotify), mobile application stores (e.g., IOS app store
and google play), online advertising, just to name a few.
The major goal of recommender systems is to help users discover relevant items such as movies
to watch, text to read or products to buy, so as to create a delightful user experience. Moreover,
recommender systems are among the most powerful machine learning systems that online retailers
implement in order to drive incremental revenue. Recommender systems are replacements
of search engines by reducing the efforts in proactive searches and surprising users with offers
they never searched for. Many companies managed to position themselves ahead of their competitors
with the help of more effective recommender systems. As such, recommender systems
are central to not only our everyday lives but also highly indispensable in some industries.
In this chapter, we will cover the fundamentals and advancements of recommender systems,
along with exploring some common fundamental techniques for building recommender systems
with different data sources available and their implementations. Specifically, you will learn how
to predict the rating a user might give to a prospective item, how to generate a recommendation
list of items and how to predict the click-through rate from abundant features. These tasks are
commonplace in real-world applications. By studying this chapter, you will get hands-on experience
pertaining to solving real world recommendation problems with not only classical methods
but the more advanced deep learning based models as well.
16.1 Overview of Recommender Systems
In the last decade, the Internet has evolved into a platform for large-scale online services, which
profoundly changed the way we communicate, read news, buy products, and watch movies. In
the meanwhile, the unprecedented number of items (we use the term item to refer to movies,
news, books, and products.) offered online requires a system that can help us discover items that
we preferred. Recommender systems are therefore powerful information filtering tools that can
facilitate personalized services and provide tailored experience to individual users. In short, recommender
systems play a pivotal role in utilizing the wealth of data available to make choices
manageable. Nowadays, recommender systems are at the core of a number of online services
providers such as Amazon, Netflix, and YouTube. Recall the example of Deep learning books recommended
by Amazon in Fig. 1.3.3. The benefits of employing recommender systems are twofolds:
On the one hand, it can largely reduce users? effort in finding items and alleviate the issue of
information overload. On the other hand, it can add business value to online service providers and
is an important source of revenue. This chapter will introduce the fundamental concepts, classic
721
models and recent advances with deep learning in the field of recommender systems, together
with implemented examples.
Fig. 16.1.1: Illustration of the Recommendation Process
16.1.1 Collaborative Filtering
We start the journey with the important concept in recommender systems�collaborative filtering
(CF), which was first coined by the Tapestry system (Goldberg et al., 1992), referring to �people
collaborate to help one another perform the filtering process in order to handle the large amounts
of email and messages posted to newsgroups�. This term has been enriched with more senses. In
a broad sense, it is the process of filtering for information or patterns using techniques involving
collaboration among multiple users, agents, and data sources. CF has many forms and numerous
CF methods proposed since its advent.
Overall, CF techniques can be categorized into: memory-based CF, model-based CF, and their
hybrid (Su & Khoshgoftaar, 2009). Representative memory-based CF techniques are nearest
neighbor-based CF such as user-based CF and item-based CF (Sarwar et al., 2001). Latent factor
models such as matrix factorization are examples of model-based CF. Memory-based CF has limitations
in dealing with sparse and large-scale data since it computes the similarity values based
on common items. Model-based methods become more popular with its better capability in dealing
with sparsity and scalability. Many model-based CF approaches can be extended with neural
networks, leading to more flexible and scalable models with the computation acceleration
in deep learning (Zhang et al., 2019). In general, CF only uses the user-item interaction data to
make predictions and recommendations. Besides CF, content-based and context-based recommender
systems are also useful in incorporating the content descriptions of items/users and contextual
signals such as timestamps and locations. Obviously, we may need to adjust the model
types/structures when different input data is available.
722 Chapter 16. Recommender Systems
16.1.2 Explicit Feedback and Implicit Feedback
To learn the preference of users, the system shall collect feedback from them. The feedback can be
either explicit or implicit (Hu et al., 2008). For example, IMDB221 collects star ratings ranging from
one to ten stars for movies. YouTube provides the thumbs-up and thumbs-down buttons for users
to show their preferences. It is apparent that gathering explicit feedback requires users to indicate
their interests proactively. Nonetheless, explicit feedback is not always readily available as many
users may be reluctant to rate products. Relatively speaking, implicit feedback is often readily
available since it is mainly concerned with modeling implicit behavior such user clicks. As such,
many recommender systems are centered on implicit feedback which indirectly reflects user?s
opinion through observing user behavior. There are diverse forms of implicit feedback including
purchase history, browsing history, watches and even mouse movements. For example, a user that
purchased many books by the same author probably likes that author. Note that implicit feedback
is inherently noisy. We can only guess their preferences and true motives. A user watched a movie
does not necessarily indicate a positive view of that movie.
16.1.3 Recommendation Tasks
A number of recommendation tasks have been investigated in the past decades. Based on the
domain of applications, there are movies recommendation, news recommendations, point-ofinterest
recommendation (Ye et al., 2011) and so forth. It is also possible to differentiate the tasks
based on the types of feedback and input data, for example, the rating prediction task aims to
predict the explicit ratings. Top-n recommendation (item ranking) ranks all items for each user
personally based on the implicit feedback. If time-stamp information is also included, we can
build sequence-aware recommendation (Quadrana et al., 2018). Another popular task is called
click-through rate prediction, which is also based on implicit feedback, but various categorical
features can be utilized. Recommending for new users and recommending new items to existing
users are called cold-start recommendation (Schein et al., 2002).
Summary
� Recommender systems are important for individual users and industries. Collaborative filtering
is a key concept in recommendation.
� There are two types of feedbacks: implicit feedback and explicit feedback. A number of
recommendation tasks have been explored during the last decade.
Exercises
1. Can you explain how recommender systems influence your daily life?
2. What interesting recommendation tasks do you think can be investigated?
Discussions222
221 https://www.imdb.com/
222 https://discuss.d2l.ai/t/398
16.1. Overview of Recommender Systems 723
16.2 The MovieLens Dataset
There are a number of datasets that are available for recommendation research. Amongst
them, the MovieLens223 dataset is probably one of the more popular ones. MovieLens is a noncommercial
web-based movie recommender system. It is created in 1997 and run by GroupLens,
a research lab at the University of Minnesota, in order to gather movie rating data for research
purposes. MovieLens data has been critical for several research studies including personalized
recommendation and social psychology.
16.2.1 Getting the Data
The MovieLens dataset is hosted by the GroupLens224 website. Several versions are available. We
will use the MovieLens 100K dataset (Herlocker et al., 1999). This dataset is comprised of 100; 000
ratings, ranging from 1 to 5 stars, from 943 users on 1682 movies. It has been cleaned up so that
each user has rated at least 20 movies. Some simple demographic information such as age, gender,
genres for the users and items are also available. We can download the ml-100k.zip225 and extract
the u.data file, which contains all the 100; 000 ratings in the csv format. There are many other
files in the folder, a detailed description for each file can be found in the README226 file of the
dataset.
To begin with, let us import the packages required to run this section?s experiments.
from d2l import mxnet as d2l
from mxnet import gluon, np
import os
import pandas as pd
Then, we download the MovieLens 100k dataset and load the interactions as DataFrame.
#@save
d2l.DATA_HUB['ml-100k'] = (
'http://files.grouplens.org/datasets/movielens/ml-100k.zip',
'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')
#@save
def read_data_ml100k():
data_dir = d2l.download_extract('ml-100k')
names = ['user_id', 'item_id', 'rating', 'timestamp']
data = pd.read_csv(os.path.join(data_dir, 'u.data'), '\t', names=names,
engine='python')
num_users = data.user_id.unique().shape[0]
num_items = data.item_id.unique().shape[0]
return data, num_users, num_items
223 https://movielens.org/
224 https://grouplens.org/datasets/movielens/
225 http://files.grouplens.org/datasets/movielens/ml-100k.zip
226 http://files.grouplens.org/datasets/movielens/ml-100k-README.txt
724 Chapter 16. Recommender Systems
16.2.2 Statistics of the Dataset
Let us load up the data and inspect the first five records manually. It is an effective way to learn
the data structure and verify that they have been loaded properly.
data, num_users, num_items = read_data_ml100k()
sparsity = 1 - len(data) / (num_users * num_items)
print(f'number of users: {num_users}, number of items: {num_items}')
print(f'matrix sparsity: {sparsity:f}')
print(data.head(5))
number of users: 943, number of items: 1682
matrix sparsity: 0.936953
user_id item_id rating timestamp
0 196 242 3 881250949
1 186 302 3 891717742
2 22 377 1 878887116
3 244 51 2 880606923
4 166 346 1 886397596
We can see that each line consists of four columns, including �user id� 1-943, �item id� 1-1682,
�rating� 1-5 and �timestamp�. We can construct an interaction matrix of size n  m, where n and
m are the number of users and the number of items respectively. This dataset only records the
existing ratings, so we can also call it rating matrix and we will use interaction matrix and rating
matrix interchangeably in case that the values of this matrix represent exact ratings. Most of the
values in the rating matrix are unknown as users have not rated the majority of movies. We also
show the sparsity of this dataset. The sparsity is defined as 1 - number of nonzero entries / (
number of users * number of items). Clearly, the interaction matrix is extremely sparse (i.e.,
sparsity = 93.695%). Real world datasets may suffer from a greater extent of sparsity and has been
a long-standing challenge in building recommender systems. A viable solution is to use additional
side information such as user/item features to alleviate the sparsity.
We then plot the distribution of the count of different ratings. As expected, it appears to be a
normal distribution, with most ratings centered at 3-4.
d2l.plt.hist(data['rating'], bins=5, ec='black')
d2l.plt.xlabel('Rating')
d2l.plt.ylabel('Count')
d2l.plt.title('Distribution of Ratings in MovieLens 100K')
d2l.plt.show()
16.2. The MovieLens Dataset 725
16.2.3 Splitting the dataset
We split the dataset into training and test sets. The following function provides two split modes
including random and seq-aware. In the random mode, the function splits the 100k interactions
randomly without considering timestamp and uses the 90% of the data as training samples and the
rest 10% as test samples by default. In the seq-aware mode, we leave out the item that a user rated
most recently for test, and users? historical interactions as training set. User historical interactions
are sorted from oldest to newest based on timestamp. This mode will be used in the sequenceaware
recommendation section.
#@save
def split_data_ml100k(data, num_users, num_items,
split_mode='random', test_ratio=0.1):
"""Split the dataset in random mode or seq-aware mode."""
if split_mode == 'seq-aware':
train_items, test_items, train_list = {}, {}, []
for line in data.itertuples():
u, i, rating, time = line[1], line[2], line[3], line[4]
train_items.setdefault(u, []).append((u, i, rating, time))
if u not in test_items or test_items[u][-1] < time:
test_items[u] = (i, rating, time)
for u in range(1, num_users + 1):
train_list.extend(sorted(train_items[u], key=lambda k: k[3]))
test_data = [(key, *value) for key, value in test_items.items()]
train_data = [item for item in train_list if item not in test_data]
train_data = pd.DataFrame(train_data)
test_data = pd.DataFrame(test_data)
else:
mask = [True if x == 1 else False for x in np.random.uniform(
0, 1, (len(data))) < 1 - test_ratio]
(continues on next page)
726 Chapter 16. Recommender Systems
(continued from previous page)
neg_mask = [not x for x in mask]
train_data, test_data = data[mask], data[neg_mask]
return train_data, test_data
Note that it is good practice to use a validation set in practice, apart from only a test set. However,
we omit that for the sake of brevity. In this case, our test set can be regarded as our held-out
validation set.
16.2.4 Loading the data
After dataset splitting, we will convert the training set and test set into lists and dictionaries/matrix
for the sake of convenience. The following function reads the dataframe line by line and enumerates
the index of users/items start from zero. The function then returns lists of users, items,
ratings and a dictionary/matrix that records the interactions. We can specify the type of feedback
to either explicit or implicit.
#@save
def load_data_ml100k(data, num_users, num_items, feedback='explicit'):
users, items, scores = [], [], []
inter = np.zeros((num_items, num_users)) if feedback == 'explicit' else {}
for line in data.itertuples():
user_index, item_index = int(line[1] - 1), int(line[2] - 1)
score = int(line[3]) if feedback == 'explicit' else 1
users.append(user_index)
items.append(item_index)
scores.append(score)
if feedback == 'implicit':
inter.setdefault(user_index, []).append(item_index)
else:
inter[item_index, user_index] = score
return users, items, scores, inter
Afterwards, we put the above steps together and it will be used in the next section. The results are
wrapped with Dataset and DataLoader. Note that the last_batch of DataLoader for training data
is set to the rollover mode (The remaining samples are rolled over to the next epoch.) and orders
are shuffled.
#@save
def split_and_load_ml100k(split_mode='seq-aware', feedback='explicit',
test_ratio=0.1, batch_size=256):
data, num_users, num_items = read_data_ml100k()
train_data, test_data = split_data_ml100k(
data, num_users, num_items, split_mode, test_ratio)
train_u, train_i, train_r, _ = load_data_ml100k(
train_data, num_users, num_items, feedback)
test_u, test_i, test_r, _ = load_data_ml100k(
test_data, num_users, num_items, feedback)
train_set = gluon.data.ArrayDataset(
np.array(train_u), np.array(train_i), np.array(train_r))
test_set = gluon.data.ArrayDataset(
np.array(test_u), np.array(test_i), np.array(test_r))
(continues on next page)
16.2. The MovieLens Dataset 727
(continued from previous page)
train_iter = gluon.data.DataLoader(
train_set, shuffle=True, last_batch='rollover',
batch_size=batch_size)
test_iter = gluon.data.DataLoader(
test_set, batch_size=batch_size)
return num_users, num_items, train_iter, test_iter
Summary
� MovieLens datasets are widely used for recommendation research. It is public available and
free to use.
� We define functions to download and preprocess the MovieLens 100k dataset for further use
in later sections.
Exercises
� What other similar recommendation datasets can you find?
� Go through the https://movielens.org/ site for more information about MovieLens.
Discussions227
16.3 Matrix Factorization
Matrix Factorization (Koren et al., 2009) is a well-established algorithm in the recommender systems
literature. The first version of matrix factorization model is proposed by Simon Funk in a
famous blog post228 in which he described the idea of factorizing the interaction matrix. It then
became widely known due to the Netflix contest which was held in 2006. At that time, Netflix, a
media-streaming and video-rental company, announced a contest to improve its recommender
system performance. The best team that can improve on the Netflix baseline, i.e., Cinematch),
by 10 percent would win a one million USD prize. As such, this contest attracted a lot of attention
to the field of recommender system research. Subsequently, the grand prize was won by the
BellKor?s Pragmatic Chaos team, a combined team of BellKor, Pragmatic Theory, and BigChaos
(you do not need to worry about these algorithms now). Although the final score was the result
of an ensemble solution (i.e., a combination of many algorithms), the matrix factorization algorithm
played a critical role in the final blend. The technical report the Netflix Grand Prize solution
(Toscher et al., 2009) provides a detailed introduction to the adopted model. In this section, we
will dive into the details of the matrix factorization model and its implementation.
227 https://discuss.d2l.ai/t/399
228 https://sifter.org/~simon/journal/20061211.html
728 Chapter 16. Recommender Systems
16.3.1 The Matrix Factorization Model
Matrix factorization is a class of collaborative filtering models. Specifically, the model factorizes
the user-item interaction matrix (e.g., rating matrix) into the product of two lower-rank matrices,
capturing the low-rank structure of the user-item interactions.
Let R 2 Rmn denote the interaction matrix with m users and n items, and the values of R
represent explicit ratings. The user-item interaction will be factorized into a user latent matrix
P 2 Rmk and an item latent matrix Q 2 Rnk, where k ? m; n, is the latent factor size. Let pu
denote the uth row of P and qi denote the ith row of Q. For a given item i, the elements of qi measure
the extent to which the item possesses those characteristics such as the genres and languages
of a movie. For a given user u, the elements of pu measure the extent of interest the user has in
items? corresponding characteristics. These latent factors might measure obvious dimensions as
mentioned in those examples or are completely uninterpretable. The predicted ratings can be
estimated by
^R = PQ? (16.3.1)
where ^R 2 Rmn is the predicted rating matrix which has the same shape as R. One major problem
of this prediction rule is that users/items biases can not be modeled. For example, some users
tend to give higher ratings or some items always get lower ratings due to poorer quality. These
biases are commonplace in real-world applications. To capture these biases, user specific and
item specific bias terms are introduced. Specifically, the predicted rating user u gives to item i is
calculated by
^Rui = puq?
i + bu + bi (16.3.2)
Then, we train the matrix factorization model by minimizing the mean squared error between
predicted rating scores and real rating scores. The objective function is defined as follows:
argmin
P;Q;b
?
(u;i)2K
?Rui ?? ^Rui?2 + (?P?2
F + ?Q?2
F + b2
u + b2i
) (16.3.3)
where  denotes the regularization rate. The regularizing term (?P?2
F + ?Q?2
F + b2
u + b2i
) is used
to avoid over-fitting by penalizing the magnitude of the parameters. The (u; i) pairs for which Rui
is known are stored in the set K = f(u; i) j Rui is knowng. The model parameters can be learned
with an optimization algorithm, such as Stochastic Gradient Descent and Adam.
An intuitive illustration of the matrix factorization model is shown below:
Fig. 16.3.1: Illustration of matrix factorization model
16.3. Matrix Factorization 729
In the rest of this section, we will explain the implementation of matrix factorization and train the
model on the MovieLens dataset.
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
npx.set_np()
16.3.2 Model Implementation
First, we implement the matrix factorization model described above. The user and item latent
factors can be created with the nn.Embedding. The input_dim is the number of items/users and
the (output_dim) is the dimension of the latent factors (k). We can also use nn.Embedding to create
the user/item biases by setting the output_dim to one. In the forward function, user and item ids
are used to look up the embeddings.
class MF(nn.Block):
def __init__(self, num_factors, num_users, num_items, **kwargs):
super(MF, self).__init__(**kwargs)
self.P = nn.Embedding(input_dim=num_users, output_dim=num_factors)
self.Q = nn.Embedding(input_dim=num_items, output_dim=num_factors)
self.user_bias = nn.Embedding(num_users, 1)
self.item_bias = nn.Embedding(num_items, 1)
def forward(self, user_id, item_id):
P_u = self.P(user_id)
Q_i = self.Q(item_id)
b_u = self.user_bias(user_id)
b_i = self.item_bias(item_id)
outputs = (P_u * Q_i).sum(axis=1) + np.squeeze(b_u) + np.squeeze(b_i)
return outputs.flatten()
16.3.3 Evaluation Measures
We then implement the RMSE (root-mean-square error) measure, which is commonly used to
measure the differences between rating scores predicted by the model and the actually observed
ratings (ground truth) (Gunawardana & Shani, 2015). RMSE is defined as:
RMSE =
vuut
1
jT j
?
(u;i)2T
(Rui ?? ^Rui)2 (16.3.4)
where T is the set consisting of pairs of users and items that you want to evaluate on. jT j is the
size of this set. We can use the RMSE function provided by mx.metric.
def evaluator(net, test_iter, devices):
rmse = mx.metric.RMSE() # Get the RMSE
rmse_list = []
for idx, (users, items, ratings) in enumerate(test_iter):
u = gluon.utils.split_and_load(users, devices, even_split=False)
(continues on next page)
730 Chapter 16. Recommender Systems
(continued from previous page)
i = gluon.utils.split_and_load(items, devices, even_split=False)
r_ui = gluon.utils.split_and_load(ratings, devices, even_split=False)
r_hat = [net(u, i) for u, i in zip(u, i)]
rmse.update(labels=r_ui, preds=r_hat)
rmse_list.append(rmse.get()[1])
return float(np.mean(np.array(rmse_list)))
16.3.4 Training and Evaluating the Model
In the training function, we adopt the L2 loss with weight decay. The weight decay mechanism
has the same effect as the L2 regularization.
#@save
def train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus(), evaluator=None,
**kwargs):
timer = d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 2],
legend=['train loss', 'test RMSE'])
for epoch in range(num_epochs):
metric, l = d2l.Accumulator(3), 0.
for i, values in enumerate(train_iter):
timer.start()
input_data = []
values = values if isinstance(values, list) else [values]
for v in values:
input_data.append(gluon.utils.split_and_load(v, devices))
train_feat = input_data[0:-1] if len(values) > 1 else input_data
train_label = input_data[-1]
with autograd.record():
preds = [net(*t) for t in zip(*train_feat)]
ls = [loss(p, s) for p, s in zip(preds, train_label)]
[l.backward() for l in ls]
l += sum([l.asnumpy() for l in ls]).mean() / len(devices)
trainer.step(values[0].shape[0])
metric.add(l, values[0].shape[0], values[0].size)
timer.stop()
if len(kwargs) > 0: # It will be used in section AutoRec
test_rmse = evaluator(net, test_iter, kwargs['inter_mat'],
devices)
else:
test_rmse = evaluator(net, test_iter, devices)
train_l = l / (i + 1)
animator.add(epoch + 1, (train_l, test_rmse))
print(f'train loss {metric[0] / metric[1]:.3f}, '
f'test RMSE {test_rmse:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(devices)}')
Finally, let us put all things together and train the model. Here, we set the latent factor dimension
to 30.
16.3. Matrix Factorization 731
devices = d2l.try_all_gpus()
num_users, num_items, train_iter, test_iter = d2l.split_and_load_ml100k(
test_ratio=0.1, batch_size=512)
net = MF(30, num_users, num_items)
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
lr, num_epochs, wd, optimizer = 0.002, 20, 1e-5, 'adam'
loss = gluon.loss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
{"learning_rate": lr, 'wd': wd})
train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,
devices, evaluator)
train loss 0.065, test RMSE 1.057
75096.4 examples/sec on [gpu(0), gpu(1)]
Below, we use the trained model to predict the rating that a user (ID 20) might give to an item (ID
30).
scores = net(np.array([20], dtype='int', ctx=devices[0]),
np.array([30], dtype='int', ctx=devices[0]))
scores
array([3.0418706], ctx=gpu(0))
Summary
� The matrix factorization model is widely used in recommender systems. It can be used to
predict ratings that a user might give to an item.
� We can implement and train matrix factorization for recommender systems.
732 Chapter 16. Recommender Systems
Exercises
� Vary the size of latent factors. How does the size of latent factors influence the model performance?
� Try different optimizers, learning rates, and weight decay rates.
� Check the predicted rating scores of other users for a specific movie.
Discussions229
16.4 AutoRec: Rating Prediction with Autoencoders
Although the matrix factorization model achieves decent performance on the rating prediction
task, it is essentially a linear model. Thus, such models are not capable of capturing complex
nonlinear and intricate relationships that may be predictive of users? preferences. In this section,
we introduce a nonlinear neural network collaborative filtering model, AutoRec (Sedhain et al.,
2015). It identifies collaborative filtering (CF) with an autoencoder architecture and aims to integrate
nonlinear transformations into CF on the basis of explicit feedback. Neural networks have
been proven to be capable of approximating any continuous function, making it suitable to address
the limitation of matrix factorization and enrich the expressiveness of matrix factorization.
On one hand, AutoRec has the same structure as an autoencoder which consists of an input layer,
a hidden layer, and a reconstruction (output) layer. An autoencoder is a neural network that
learns to copy its input to its output in order to code the inputs into the hidden (and usually
low-dimensional) representations. In AutoRec, instead of explicitly embedding users/items into
low-dimensional space, it uses the column/row of the interaction matrix as the input, then reconstructs
the interaction matrix in the output layer.
On the other hand, AutoRec differs from a traditional autoencoder: rather than learning the hidden
representations, AutoRec focuses on learning/reconstructing the output layer. It uses a partially
observed interaction matrix as the input, aiming to reconstruct a completed rating matrix.
In the meantime, the missing entries of the input are filled in the output layer via reconstruction
for the purpose of recommendation.
There are two variants of AutoRec: user-based and item-based. For brevity, here we only introduce
the item-based AutoRec. User-based AutoRec can be derived accordingly.
16.4.1 Model
Let Ri denote the ith column of the rating matrix, where unknown ratings are set to zeros by
default. The neural architecture is defined as:
h(Ri) = f(W  g(VRi + ) + b) (16.4.1)
where f() and g() represent activation functions, W and V are weight matrices,  and b are biases.
Let h() denote the whole network of AutoRec. The output h(Ri) is the reconstruction of the ith
column of the rating matrix.
229 https://discuss.d2l.ai/t/
16.4. AutoRec: Rating Prediction with Autoencoders 733
The following objective function aims to minimize the reconstruction error:
argmin
W;V;;b
M?
i=1
? Ri ?? h(Ri) ?2
O + (?W?2
F + ?V?2
F ) (16.4.2)
where ?  ?O means only the contribution of observed ratings are considered, that is, only weights
that are associated with observed inputs are updated during back-propagation.
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
import sys
npx.set_np()
16.4.2 Implementing the Model
A typical autoencoder consists of an encoder and a decoder. The encoder projects the input to
hidden representations and the decoder maps the hidden layer to the reconstruction layer. We
follow this practice and create the encoder and decoder with dense layers. The activation of encoder
is set to sigmoid by default and no activation is applied for decoder. Dropout is included
after the encoding transformation to reduce over-fitting. The gradients of unobserved inputs are
masked out to ensure that only observed ratings contribute to the model learning process.
class AutoRec(nn.Block):
def __init__(self, num_hidden, num_users, dropout=0.05):
super(AutoRec, self).__init__()
self.encoder = nn.Dense(num_hidden, activation='sigmoid',
use_bias=True)
self.decoder = nn.Dense(num_users, use_bias=True)
self.dropout = nn.Dropout(dropout)
def forward(self, input):
hidden = self.dropout(self.encoder(input))
pred = self.decoder(hidden)
if autograd.is_training(): # Mask the gradient during training
return pred * np.sign(input)
else:
return pred
16.4.3 Reimplementing the Evaluator
Since the input and output have been changed, we need to reimplement the evaluation function,
while we still use RMSE as the accuracy measure.
def evaluator(network, inter_matrix, test_data, devices):
scores = []
for values in inter_matrix:
feat = gluon.utils.split_and_load(values, devices, even_split=False)
scores.extend([network(i).asnumpy() for i in feat])
(continues on next page)
734 Chapter 16. Recommender Systems
(continued from previous page)
recons = np.array([item for sublist in scores for item in sublist])
# Calculate the test RMSE
rmse = np.sqrt(np.sum(np.square(test_data - np.sign(test_data) * recons))
/ np.sum(np.sign(test_data)))
return float(rmse)
16.4.4 Training and Evaluating the Model
Now, let us train and evaluate AutoRec on the MovieLens dataset. We can clearly see that the
test RMSE is lower than the matrix factorization model, confirming the effectiveness of neural
networks in the rating prediction task.
devices = d2l.try_all_gpus()
# Load the MovieLens 100K dataset
df, num_users, num_items = d2l.read_data_ml100k()
train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items)
_, _, _, train_inter_mat = d2l.load_data_ml100k(train_data, num_users,
num_items)
_, _, _, test_inter_mat = d2l.load_data_ml100k(test_data, num_users,
num_items)
train_iter = gluon.data.DataLoader(train_inter_mat, shuffle=True,
last_batch="rollover", batch_size=256,
num_workers=d2l.get_dataloader_workers())
test_iter = gluon.data.DataLoader(np.array(train_inter_mat), shuffle=False,
last_batch="keep", batch_size=1024,
num_workers=d2l.get_dataloader_workers())
# Model initialization, training, and evaluation
net = AutoRec(500, num_users)
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
lr, num_epochs, wd, optimizer = 0.002, 25, 1e-5, 'adam'
loss = gluon.loss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
{"learning_rate": lr, 'wd': wd})
d2l.train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,
devices, evaluator, inter_mat=test_inter_mat)
train loss 0.000, test RMSE 0.899
40137887.8 examples/sec on [gpu(0), gpu(1)]
16.4. AutoRec: Rating Prediction with Autoencoders 735
Summary
� We can frame the matrix factorization algorithm with autoencoders, while integrating nonlinear
layers and dropout regularization.
� Experiments on the MovieLens 100K dataset show that AutoRec achieves superior performance
than matrix factorization.
Exercises
� Vary the hidden dimension of AutoRec to see its impact on the model performance.
� Try to add more hidden layers. Is it helpful to improve the model performance?
� Can you find a better combination of decoder and encoder activation functions?
Discussions230
16.5 Personalized Ranking for Recommender Systems
In the former sections, only explicit feedback was considered and models were trained and tested
on observed ratings. There are two demerits of such methods: First, most feedback is not explicit
but implicit in real-world scenarios, and explicit feedback can be more expensive to collect. Second,
non-observed user-item pairs which may be predictive for users? interests are totally ignored,
making these methods unsuitable for cases where ratings are not missing at random but because
of users? preferences. Non-observed user-item pairs are a mixture of real negative feedback (users
are not interested in the items) and missing values (the user might interact with the items in the
future). We simply ignore the non-observed pairs in matrix factorization and AutoRec. Clearly,
these models are incapable of distinguishing between observed and non-observed pairs and are
usually not suitable for personalized ranking tasks.
To this end, a class of recommendation models targeting at generating ranked recommendation
lists from implicit feedback have gained popularity. In general, personalized ranking models can
be optimized with pointwise, pairwise or listwise approaches. Pointwise approaches considers
230 https://discuss.d2l.ai/t/401
736 Chapter 16. Recommender Systems
a single interaction at a time and train a classifier or a regressor to predict individual preferences.
Matrix factorization and AutoRec are optimized with pointwise objectives. Pairwise approaches
consider a pair of items for each user and aim to approximate the optimal ordering for
that pair. Usually, pairwise approaches are more suitable for the ranking task because predicting
relative order is reminiscent to the nature of ranking. Listwise approaches approximate the
ordering of the entire list of items, for example, direct optimizing the ranking measures such
as Normalized Discounted Cumulative Gain (NDCG231). However, listwise approaches are more
complex and compute-intensive than pointwise or pairwise approaches. In this section, we will
introduce two pairwise objectives/losses, Bayesian Personalized Ranking loss and Hinge loss, and
their respective implementations.
16.5.1 Bayesian Personalized Ranking Loss and its Implementation
Bayesian personalized ranking (BPR) (Rendle et al., 2009) is a pairwise personalized ranking loss
that is derived from the maximum posterior estimator. It has been widely used in many existing
recommendation models. The training data of BPR consists of both positive and negative pairs
(missing values). It assumes that the user prefers the positive item over all other non-observed
items.
In formal, the training data is constructed by tuples in the form of (u; i; j), which represents that
the user u prefers the item i over the item j. The Bayesian formulation of BPR which aims to
maximize the posterior probability is given below:
p( j>u) / p(>uj )p() (16.5.1)
Whererepresents the parameters of an arbitrary recommendation model,>u represents the desired
personalized total ranking of all items for user u. We can formulate the maximum posterior
estimator to derive the generic optimization criterion for the personalized ranking task.
BPR-OPT : = ln p( j>u)
/ ln p(>uj )p()
= ln
?
(u;i;j2D)
(^yui ?? ^yuj)p()
=
?
(u;i;j2D)
ln (^yui ?? ^yuj) + ln p()
=
?
(u;i;j2D)
ln (^yui ?? ^yuj) ?? ??2
(16.5.2)
where D := f(u; i; j) j i 2 I+
u
^ j 2 InI+
u
g is the training set, with I+
u denoting the items the user
u liked, I denoting all items, and InI+
u indicating all other items excluding items the user liked.
^yui and ^yuj are the predicted scores of the user u to item i and j, respectively. The prior p() is a
normal distribution with zero mean and variance-covariance matrix . Here, we let  = I.
231 https://en.wikipedia.org/wiki/Discounted_cumulative_gain
16.5. Personalized Ranking for Recommender Systems 737
We will implement the base class mxnet.gluon.loss.Loss and override the forward method to
construct the Bayesian personalized ranking loss. We begin by importing the Loss class and the
np module.
from mxnet import gluon, np, npx
npx.set_np()
The implementation of BPR loss is as follows.
#@save
class BPRLoss(gluon.loss.Loss):
def __init__(self, weight=None, batch_axis=0, **kwargs):
super(BPRLoss, self).__init__(weight=None, batch_axis=0, **kwargs)
def forward(self, positive, negative):
distances = positive - negative
loss = - np.sum(np.log(npx.sigmoid(distances)), 0, keepdims=True)
return loss
16.5.2 Hinge Loss and its Implementation
The Hinge loss for ranking has different form to the hinge loss232 provided within the gluon library
that is often used in classifiers such as SVMs. The loss used for ranking in recommender systems
has the following form.
?
(u;i;j2D)
max(m ?? ^yui + ^yuj; 0) (16.5.3)
wheremis the safety margin size. It aims to push negative items away from positive items. Similar
to BPR, it aims to optimize for relevant distance between positive and negative samples instead of
absolute outputs, making it well suited to recommender systems.
#@save
class HingeLossbRec(gluon.loss.Loss):
(continues on next page)
232 https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss
738 Chapter 16. Recommender Systems
(continued from previous page)
def __init__(self, weight=None, batch_axis=0, **kwargs):
super(HingeLossbRec, self).__init__(weight=None, batch_axis=0,
**kwargs)
def forward(self, positive, negative, margin=1):
distances = positive - negative
loss = np.sum(np.maximum(- distances + margin, 0))
return loss
These two losses are interchangeable for personalized ranking in recommendation.
Summary
� There are three types of ranking losses available for the personalized ranking task in recommender
systems, namely, pointwise, pairwise and listwise methods.
� The two pairwise loses, Bayesian personalized ranking loss and hinge loss, can be used interchangeably.
Exercises
� Are there any variants of BPR and hinge loss available?
� Can you find any recommendation models that use BPR or hinge loss?
Discussions233
16.6 Neural Collaborative Filtering for Personalized Ranking
This section moves beyond explicit feedback, introducing the neural collaborative filtering (NCF)
framework for recommendation with implicit feedback. Implicit feedback is pervasive in recommender
systems. Actions such as Clicks, buys, and watches are common implicit feedback which
are easy to collect and indicative of users? preferences. The model we will introduce, titled NeuMF
(He et al., 2017b), short for neural matrix factorization, aims to address the personalized ranking
task with implicit feedback. This model leverages the flexibility and non-linearity of neural
networks to replace dot products of matrix factorization, aiming at enhancing the model expressiveness.
In specific, this model is structured with two subnetworks including generalized matrix
factorization (GMF) and MLP and models the interactions from two pathways instead of simple inner
products. The outputs of these two networks are concatenated for the final prediction scores
calculation. Unlike the rating prediction task in AutoRec, this model generates a ranked recommendation
list to each user based on the implicit feedback. We will use the personalized ranking
loss introduced in the last section to train this model.
233 https://discuss.d2l.ai/t/402
16.6. Neural Collaborative Filtering for Personalized Ranking 739
16.6.1 The NeuMF model
As aforementioned, NeuMF fuses two subnetworks. The GMF is a generic neural network version
of matrix factorization where the input is the elementwise product of user and item latent factors.
It consists of two neural layers:
x = pu ? qi
^yui = (h?x);
(16.6.1)
where ? denotes the Hadamard product of vectors. P 2 Rmk and Q 2 Rnk corespond to user
and item latent matrix respectively. pu 2 Rk is the uth row of P and qi 2 Rk is the ith row of Q. 
and h denote the activation function and weight of the output layer. ^yui is the prediction score of
the user u might give to the item i.
Another component of this model is MLP. To enrich model flexibility, the MLP subnetwork does
not share user and item embeddings with GMF. It uses the concatenation of user and item embeddings
as input. With the complicated connections and nonlinear transformations, it is capable of
eastimating the intricate interactions between users and items. More precisely, the MLP subnetwork
is defined as:
z(1) = ?1(Uu; Vi) = [Uu; Vi]
?(2)(z(1)) = 1(W(2)z(1) + b(2))
:::
?(L)(z(L??1)) = L(W(L)z(L??1) + b(L)))
^yui = (h?
?L(z(L)))
(16.6.2)
where W; b and  denote the weight matrix, bias vector, and activation function. ? denotes
the function of the corresponding layer. z denotes the output of corresponding layer.
To fuse the results of GMF and MLP, instead of simple addition, NeuMF concatenates the second
last layers of two subnetworks to create a feature vector which can be passed to the further layers.
Afterwards, the ouputs are projected with matrix h and a sigmoid activation function. The
prediction layer is formulated as:
^yui = (h?
[x; ?L(z(L))]): (16.6.3)
The following figure illustrates the model architecture of NeuMF.
740 Chapter 16. Recommender Systems
Fig. 16.6.1: Illustration of the NeuMF model
from d2l import mxnet as d2l
from mxnet import autograd, gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
import random
import sys
npx.set_np()
16.6.2 Model Implementation
The following code implements the NeuMF model. It consists of a generalized matrix factorization
model and a multi-layered perceptron with different user and item embedding vectors. The
structure of the MLP is controlled with the parameter nums_hiddens. ReLU is used as the default
activation function.
class NeuMF(nn.Block):
def __init__(self, num_factors, num_users, num_items, nums_hiddens,
**kwargs):
super(NeuMF, self).__init__(**kwargs)
self.P = nn.Embedding(num_users, num_factors)
self.Q = nn.Embedding(num_items, num_factors)
self.U = nn.Embedding(num_users, num_factors)
self.V = nn.Embedding(num_items, num_factors)
self.mlp = nn.Sequential()
for num_hiddens in nums_hiddens:
self.mlp.add(nn.Dense(num_hiddens, activation='relu',
use_bias=True))
def forward(self, user_id, item_id):
(continues on next page)
16.6. Neural Collaborative Filtering for Personalized Ranking 741
(continued from previous page)
p_mf = self.P(user_id)
q_mf = self.Q(item_id)
gmf = p_mf * q_mf
p_mlp = self.U(user_id)
q_mlp = self.V(item_id)
mlp = self.mlp(np.concatenate([p_mlp, q_mlp], axis=1))
con_res = np.concatenate([gmf, mlp], axis=1)
return np.sum(con_res, axis=-1)
16.6.3 Customized Dataset with Negative Sampling
For pairwise ranking loss, an important step is negative sampling. For each user, the items that
a user has not interacted with are candidate items (unobserved entries). The following function
takes users identity and candidate items as input, and samples negative items randomly for each
user from the candidate set of that user. During the training stage, the model ensures that the
items that a user likes to be ranked higher than items he dislikes or has not interacted with.
class PRDataset(gluon.data.Dataset):
def __init__(self, users, items, candidates, num_items):
self.users = users
self.items = items
self.cand = candidates
self.all = set([i for i in range(num_items)])
def __len__(self):
return len(self.users)
def __getitem__(self, idx):
neg_items = list(self.all - set(self.cand[int(self.users[idx])]))
indices = random.randint(0, len(neg_items) - 1)
return self.users[idx], self.items[idx], neg_items[indices]
16.6.4 Evaluator
In this section, we adopt the splitting by time strategy to construct the training and test sets. Two
evaluation measures including hit rate at given cutting off ? (Hit@?) and area under the ROC curve
(AUC) are used to assess the model effectiveness. Hit rate at given position ? for each user indicates
that whether the recommended item is included in the top ? ranked list. The formal definition is
as follows:
Hit@? =
1
m
?
u2U
1(ranku;gu <= ?); (16.6.4)
where 1 denotes an indicator function that is equal to one if the ground truth item is ranked in the
top ? list, otherwise it is equal to zero. ranku;gu denotes the ranking of the ground truth item gu of
the user u in the recommendation list (The ideal ranking is 1). m is the number of users. U is the
user set.
The definition of AUC is as follows:
AUC =
1
m
?
u2U
1
jInSuj
?
j2InSu
1(ranku;gu < ranku;j); (16.6.5)
742 Chapter 16. Recommender Systems
where I is the item set. Su is the candidate items of user u. Note that many other evaluation
protocols such as precision, recall and normalized discounted cumulative gain (NDCG) can also
be used.
The following function calculates the hit counts and AUC for each user.
#@save
def hit_and_auc(rankedlist, test_matrix, k):
hits_k = [(idx, val) for idx, val in enumerate(rankedlist[:k])
if val in set(test_matrix)]
hits_all = [(idx, val) for idx, val in enumerate(rankedlist)
if val in set(test_matrix)]
max = len(rankedlist) - 1
auc = 1.0 * (max - hits_all[0][0]) / max if len(hits_all) > 0 else 0
return len(hits_k), auc
Then, the overall Hit rate and AUC are calculated as follows.
#@save
def evaluate_ranking(net, test_input, seq, candidates, num_users, num_items,
devices):
ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []
all_items = set([i for i in range(num_users)])
for u in range(num_users):
neg_items = list(all_items - set(candidates[int(u)]))
user_ids, item_ids, x, scores = [], [], [], []
[item_ids.append(i) for i in neg_items]
[user_ids.append(u) for _ in neg_items]
x.extend([np.array(user_ids)])
if seq is not None:
x.append(seq[user_ids, :])
x.extend([np.array(item_ids)])
test_data_iter = gluon.data.DataLoader(
gluon.data.ArrayDataset(*x), shuffle=False, last_batch="keep",
batch_size=1024)
for index, values in enumerate(test_data_iter):
x = [gluon.utils.split_and_load(v, devices, even_split=False)
for v in values]
scores.extend([list(net(*t).asnumpy()) for t in zip(*x)])
scores = [item for sublist in scores for item in sublist]
item_scores = list(zip(item_ids, scores))
ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)
ranked_items[u] = [r[0] for r in ranked_list[u]]
temp = hit_and_auc(ranked_items[u], test_input[u], 50)
hit_rate.append(temp[0])
auc.append(temp[1])
return np.mean(np.array(hit_rate)), np.mean(np.array(auc))
16.6. Neural Collaborative Filtering for Personalized Ranking 743
16.6.5 Training and Evaluating the Model
The training function is defined below. We train the model in the pairwise manner.
#@save
def train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter,
num_users, num_items, num_epochs, devices, evaluator,
candidates, eval_step=1):
timer, hit_rate, auc = d2l.Timer(), 0, 0
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
legend=['test hit rate', 'test AUC'])
for epoch in range(num_epochs):
metric, l = d2l.Accumulator(3), 0.
for i, values in enumerate(train_iter):
input_data = []
for v in values:
input_data.append(gluon.utils.split_and_load(v, devices))
with autograd.record():
p_pos = [net(*t) for t in zip(*input_data[0:-1])]
p_neg = [net(*t) for t in zip(*input_data[0:-2],
input_data[-1])]
ls = [loss(p, n) for p, n in zip(p_pos, p_neg)]
[l.backward(retain_graph=False) for l in ls]
l += sum([l.asnumpy() for l in ls]).mean()/len(devices)
trainer.step(values[0].shape[0])
metric.add(l, values[0].shape[0], values[0].size)
timer.stop()
with autograd.predict_mode():
if (epoch + 1) % eval_step == 0:
hit_rate, auc = evaluator(net, test_iter, test_seq_iter,
candidates, num_users, num_items,
devices)
animator.add(epoch + 1, (hit_rate, auc))
print(f'train loss {metric[0] / metric[1]:.3f}, '
f'test hit rate {float(hit_rate):.3f}, test AUC {float(auc):.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(devices)}')
Now, we can load the MovieLens 100k dataset and train the model. Since there are only ratings in
the MovieLens dataset, with some losses of accuracy, we binarize these ratings to zeros and ones.
If a user rated an item, we consider the implicit feedback as one, otherwise as zero. The action of
rating an item can be treated as a form of providing implicit feedback. Here, we split the dataset
in the seq-aware mode where users? latest interacted items are left out for test.
batch_size = 1024
df, num_users, num_items = d2l.read_data_ml100k()
train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,
'seq-aware')
users_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(
train_data, num_users, num_items, feedback="implicit")
users_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(
test_data, num_users, num_items, feedback="implicit")
train_iter = gluon.data.DataLoader(
PRDataset(users_train, items_train, candidates, num_items ), batch_size,
True, last_batch="rollover", num_workers=d2l.get_dataloader_workers())
744 Chapter 16. Recommender Systems
We then create and initialize the model. we use a three-layer MLP with constant hidden size 10.
devices = d2l.try_all_gpus()
net = NeuMF(10, num_users, num_items, nums_hiddens=[10, 10, 10])
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
The following code trains the model.
lr, num_epochs, wd, optimizer = 0.01, 10, 1e-5, 'adam'
loss = d2l.BPRLoss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
{"learning_rate": lr, 'wd': wd})
train_ranking(net, train_iter, test_iter, loss, trainer, None, num_users,
num_items, num_epochs, devices, evaluate_ranking, candidates)
train loss 4.399, test hit rate 0.338, test AUC 0.736
15.1 examples/sec on [gpu(0), gpu(1)]
Summary
� Adding nonlinearity to matrix factorization model is beneficial for improving the model capability
and effectiveness.
� NeuMF is a combination of matrix factorization and Multilayer perceptron. The multilayer
perceptron takes the concatenation of user and item embeddings as the input.
Exercises
� Vary the size of latent factors. How the size of latent factors impact the model performance?
� Vary the architectures (e.g., number of layers, number of neurons of each layer) of the MLP
to check the its impact on the performance.
� Try different optimizers, learning rate and weight decay rate.
� Try to use hinge loss defined in the last section to optimize this model.
16.6. Neural Collaborative Filtering for Personalized Ranking 745
Discussions234
16.7 Sequence-Aware Recommender Systems
In previous sections, we abstract the recommendation task as a matrix completion problem without
considering users? short-term behaviors. In this section, we will introduce a recommendation
model that takes the sequentially-ordered user interaction logs into account. It is a sequenceaware
recommender (Quadrana et al., 2018) where the input is an ordered and often timestamped
list of past user actions. A number of recent literatures have demonstrated the usefulness of incorporating
such information in modeling users? temporal behavioral patterns and discovering
their interest drift.
The model we will introduce, Caser (Tang & Wang, 2018), short for convolutional sequence embedding
recommendation model, adopts convolutional neural networks capture the dynamic pattern
influences of users? recent activities. The main component of Caser consists of a horizontal convolutional
network and a vertical convolutional network, aiming to uncover the union-level and
point-level sequence patterns, respectively. Point-level pattern indicates the impact of single item
in the historical sequence on the target item, while union level pattern implies the influences of
several previous actions on the subsequent target. For example, buying both milk and butter together
leads to higher probability of buying flour than just buying one of them. Moreover, users?
general interests, or long term preferences are also modeled in the last fully-connected layers,
resulting in a more comprehensive modeling of user interests. Details of the model are described
as follows.
16.7.1 Model Architectures
In sequence-aware recommendation system, each user is associated with a sequence of some
items from the item set. Let Su = (Su
1 ; :::Su
jSuj) denotes the ordered sequence. The goal of Caser is
to recommend item by considering user general tastes as well as short-term intention. Suppose
we take the previous L items into consideration, an embedding matrix that represents the former
interactions for timestep t can be constructed:
E(u;t) = [qSu
t??L
; :::; qSu
t??2
; qSu
t??1
]
?
; (16.7.1)
where Q 2 Rnk represents item embeddings and qi denotes the ith row. E(u;t) 2 RLk can be
used to infer the transient interest of user u at time-step t. We can view the input matrix E(u;t) as
an image which is the input of the subsequent two convolutional components.
The horizontal convolutional layer has d horizontal filters Fj 2 Rhk; 1  j  d; h = f1; :::;Lg,
and the vertical convolutional layer has d? vertical filters Gj 2 RL1; 1  j  d?. After a series of
convolutional and pool operations, we get the two outputs:
o = HConv(E(u;t); F)
o?
= VConv(E(u;t); G);
(16.7.2)
where o 2 Rd is the output of horizontal convolutional network and o? 2 Rkd? is the output of vertical
convolutional network. For simplicity, we omit the details of convolution and pool operations.
234 https://discuss.d2l.ai/t/403
746 Chapter 16. Recommender Systems
They are concatenated and fed into a fully-connected neural network layer to get more high-level
representations.
z = ?(W[o; o?
]
?
+ b); (16.7.3)
where W 2 Rk(d+kd?) is the weight matrix and b 2 Rk is the bias. The learned vector z 2 Rk is the
representation of user?s short-term intent.
At last, the prediction function combines users? short-term and general taste together, which is
defined as:
^yuit = vi  [z; pu]
?
+ b?
i; (16.7.4)
where V 2 Rn2k is another item embedding matrix. b? 2 Rn is the item specific bias. P 2 Rmk
is the user embedding matrix for users? general tastes. pu 2 Rk is the uth row of P and vi 2 R2k is
the ith row of V.
The model can be learned with BPR or Hinge loss. The architecture of Caser is shown below:
Fig. 16.7.1: Illustration of the Caser Model
We first import the required libraries.
from d2l import mxnet as d2l
from mxnet import gluon, np, npx
from mxnet.gluon import nn
import mxnet as mx
import random
import sys
npx.set_np()
16.7. Sequence-Aware Recommender Systems 747
16.7.2 Model Implementation
The following code implements the Caser model. It consists of a vertical convolutional layer, a
horizontal convolutional layer, and a full-connected layer.
class Caser(nn.Block):
def __init__(self, num_factors, num_users, num_items, L=5, d=16,
d_prime=4, drop_ratio=0.05, **kwargs):
super(Caser, self).__init__(**kwargs)
self.P = nn.Embedding(num_users, num_factors)
self.Q = nn.Embedding(num_items, num_factors)
self.d_prime, self.d = d_prime, d
# Vertical convolution layer
self.conv_v = nn.Conv2D(d_prime, (L, 1), in_channels=1)
# Horizontal convolution layer
h = [i + 1 for i in range(L)]
self.conv_h, self.max_pool = nn.Sequential(), nn.Sequential()
for i in h:
self.conv_h.add(nn.Conv2D(d, (i, num_factors), in_channels=1))
self.max_pool.add(nn.MaxPool1D(L - i + 1))
# Fully-connected layer
self.fc1_dim_v, self.fc1_dim_h = d_prime * num_factors, d * len(h)
self.fc = nn.Dense(in_units=d_prime * num_factors + d * L,
activation='relu', units=num_factors)
self.Q_prime = nn.Embedding(num_items, num_factors * 2)
self.b = nn.Embedding(num_items, 1)
self.dropout = nn.Dropout(drop_ratio)
def forward(self, user_id, seq, item_id):
item_embs = np.expand_dims(self.Q(seq), 1)
user_emb = self.P(user_id)
out, out_h, out_v, out_hs = None, None, None, []
if self.d_prime:
out_v = self.conv_v(item_embs)
out_v = out_v.reshape(out_v.shape[0], self.fc1_dim_v)
if self.d:
for conv, maxp in zip(self.conv_h, self.max_pool):
conv_out = np.squeeze(npx.relu(conv(item_embs)), axis=3)
t = maxp(conv_out)
pool_out = np.squeeze(t, axis=2)
out_hs.append(pool_out)
out_h = np.concatenate(out_hs, axis=1)
out = np.concatenate([out_v, out_h], axis=1)
z = self.fc(self.dropout(out))
x = np.concatenate([z, user_emb], axis=1)
q_prime_i = np.squeeze(self.Q_prime(item_id))
b = np.squeeze(self.b(item_id))
res = (x * q_prime_i).sum(1) + b
return res
748 Chapter 16. Recommender Systems
16.7.3 Sequential Dataset with Negative Sampling
To process the sequential interaction data, we need to reimplement the Dataset class. The following
code creates a new dataset class named SeqDataset. In each sample, it outputs the user
identity, his previous L interacted items as a sequence and the next item he interacts as the target.
The following figure demonstrates the data loading process for one user. Suppose that this user
liked 8 movies, we organize these eight movies in chronological order. The latest movie is left out
as the test item. For the remaining seven movies, we can get three training samples, with each
sample containing a sequence of five (L = 5) movies and its subsequent item as the target item.
Negative samples are also included in the Customized dataset.
Fig. 16.7.2: Illustration of the data generation process
class SeqDataset(gluon.data.Dataset):
def __init__(self, user_ids, item_ids, L, num_users, num_items,
candidates):
user_ids, item_ids = np.array(user_ids), np.array(item_ids)
sort_idx = np.array(sorted(range(len(user_ids)),
key=lambda k: user_ids[k]))
u_ids, i_ids = user_ids[sort_idx], item_ids[sort_idx]
temp, u_ids, self.cand = {}, u_ids.asnumpy(), candidates
self.all_items = set([i for i in range(num_items)])
[temp.setdefault(u_ids[i], []).append(i) for i, _ in enumerate(u_ids)]
temp = sorted(temp.items(), key=lambda x: x[0])
u_ids = np.array([i[0] for i in temp])
idx = np.array([i[1][0] for i in temp])
self.ns = ns = int(sum([c - L if c >= L + 1 else 1 for c
in np.array([len(i[1]) for i in temp])]))
self.seq_items = np.zeros((ns, L))
self.seq_users = np.zeros(ns, dtype='int32')
self.seq_tgt = np.zeros((ns, 1))
self.test_seq = np.zeros((num_users, L))
test_users, _uid = np.empty(num_users), None
for i, (uid, i_seq) in enumerate(self._seq(u_ids, i_ids, idx, L + 1)):
if uid != _uid:
self.test_seq[uid][:] = i_seq[-L:]
test_users[uid], _uid = uid, uid
self.seq_tgt[i][:] = i_seq[-1:]
self.seq_items[i][:], self.seq_users[i] = i_seq[:L], uid
def _win(self, tensor, window_size, step_size=1):
(continues on next page)
16.7. Sequence-Aware Recommender Systems 749
(continued from previous page)
if len(tensor) - window_size >= 0:
for i in range(len(tensor), 0, - step_size):
if i - window_size >= 0:
yield tensor[i - window_size:i]
else:
break
else:
yield tensor
def _seq(self, u_ids, i_ids, idx, max_len):
for i in range(len(idx)):
stop_idx = None if i >= len(idx) - 1 else int(idx[i + 1])
for s in self._win(i_ids[int(idx[i]):stop_idx], max_len):
yield (int(u_ids[i]), s)
def __len__(self):
return self.ns
def __getitem__(self, idx):
neg = list(self.all_items - set(self.cand[int(self.seq_users[idx])]))
i = random.randint(0, len(neg) - 1)
return (self.seq_users[idx], self.seq_items[idx], self.seq_tgt[idx],
neg[i])
16.7.4 Load the MovieLens 100K dataset
Afterwards, we read and split the MovieLens 100K dataset in sequence-aware mode and load the
training data with sequential dataloader implemented above.
TARGET_NUM, L, batch_size = 1, 3, 4096
df, num_users, num_items = d2l.read_data_ml100k()
train_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,
'seq-aware')
users_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(
train_data, num_users, num_items, feedback="implicit")
users_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(
test_data, num_users, num_items, feedback="implicit")
train_seq_data = SeqDataset(users_train, items_train, L, num_users,
num_items, candidates)
train_iter = gluon.data.DataLoader(train_seq_data, batch_size, True,
last_batch="rollover",
num_workers=d2l.get_dataloader_workers())
test_seq_iter = train_seq_data.test_seq
train_seq_data[0]
(array(0, dtype=int32), array([110., 255., 4.]), array([101.]), 1590)
The training data structure is shown above. The first element is the user identity, the next list
indicates the last five items this user liked, and the last element is the item this user liked after the
five items.
750 Chapter 16. Recommender Systems
16.7.5 Train the Model
Now, let us train the model. We use the same setting as NeuMF, including learning rate, optimizer,
and k, in the last section so that the results are comparable.
devices = d2l.try_all_gpus()
net = Caser(10, num_users, num_items, L)
net.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))
lr, num_epochs, wd, optimizer = 0.04, 8, 1e-5, 'adam'
loss = d2l.BPRLoss()
trainer = gluon.Trainer(net.collect_params(), optimizer,
{"learning_rate": lr, 'wd': wd})
d2l.train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter,
num_users, num_items, num_epochs, devices,
d2l.evaluate_ranking, candidates, eval_step=1)
train loss 0.829, test hit rate 0.381, test AUC 0.756
34.3 examples/sec on [gpu(0), gpu(1)]
Summary
� Inferring a user?s short-term and long-term interests can make prediction of the next item
that he preferred more effectively.
� Convolutional neural networks can be utilized to capture users? short-term interests from
sequential interactions.
16.7. Sequence-Aware Recommender Systems 751
Exercises
� Conduct an ablation study by removing one of the horizontal and vertical convolutional networks,
which component is the more important ?
� Vary the hyperparameter L. Does longer historical interactions bring higher accuracy?
� Apart from the sequence-aware recommendation task we introduced above, there is another
type of sequence-aware recommendation task called session-based recommendation (Hidasi
et al., 2015). Can you explain the differences between these two tasks?
Discussions235
16.8 Feature-Rich Recommender Systems
Interaction data is the most basic indication of users? preferences and interests. It plays a critical
role in former introduced models. Yet, interaction data is usually extremely sparse and can be
noisy at times. To address this issue, we can integrate side information such as features of items,
profiles of users, and even in which context that the interaction occurred into the recommendation
model. Utilizing these features are helpful in making recommendations in that these features
can be an effective predictor of users interests especially when interaction data is lacking. As such,
it is essential for recommendation models also have the capability to deal with those features and
give the model some content/context awareness. To demonstrate this type of recommendation
models, we introduce another task on click-through rate (CTR) for online advertisement recommendations
(McMahan et al., 2013) and present an anonymous advertising data. Targeted advertisement
services have attracted widespread attention and are often framed as recommendation
engines. Recommending advertisements that match users? personal taste and interest is important
for click-through rate improvement.
Digital marketers use online advertising to display advertisements to customers. Click-through
rate is a metric that measures the number of clicks advertisers receive on their ads per number of
impressions and it is expressed as a percentage calculated with the formula:
CTR =
#Clicks
#Impressions
 100%: (16.8.1)
Click-through rate is an important signal that indicates the effectiveness of prediction algorithms.
Click-through rate prediction is a task of predicting the likelihood that something on a website will
be clicked. Models on CTR prediction can not only be employed in targeted advertising systems
but also in general item (e.g., movies, news, products) recommender systems, email campaigns,
and even search engines. It is also closely related to user satisfaction, conversion rate, and can be
helpful in setting campaign goals as it can help advertisers to set realistic expectations.
from collections import defaultdict
from d2l import mxnet as d2l
from mxnet import gluon, np
import os
235 https://discuss.d2l.ai/t/404
752 Chapter 16. Recommender Systems
16.8.1 An Online Advertising Dataset
With the considerable advancements of Internet and mobile technology, online advertising has
become an important income resource and generates vast majority of revenue in the Internet
industry. It is important to display relevant advertisements or advertisements that pique users?
interests so that casual visitors can be converted into paying customers. The dataset we introduced
is an online advertising dataset. It consists of 34 fields, with the first column representing
the target variable that indicates if an ad was clicked (1) or not (0). All the other columns are
categorical features. The columns might represent the advertisement id, site or application id,
device id, time, user profiles and so on. The real semantics of the features are undisclosed due to
anonymization and privacy concern.
The following code downloads the dataset from our server and saves it into the local data folder.
#@save
d2l.DATA_HUB['ctr'] = (d2l.DATA_URL + 'ctr.zip',
'e18327c48c8e8e5c23da714dd614e390d369843f')
data_dir = d2l.download_extract('ctr')
Downloading ../data/ctr.zip from http://d2l-data.s3-accelerate.amazonaws.com/ctr.zip...
There are a training set and a test set, consisting of 15000 and 3000 samples/lines, respectively.
16.8.2 Dataset Wrapper
For the convenience of data loading, we implement a CTRDataset which loads the advertising
dataset from the CSV file and can be used by DataLoader.
#@save
class CTRDataset(gluon.data.Dataset):
def __init__(self, data_path, feat_mapper=None, defaults=None,
min_threshold=4, num_feat=34):
self.NUM_FEATS, self.count, self.data = num_feat, 0, {}
feat_cnts = defaultdict(lambda: defaultdict(int))
self.feat_mapper, self.defaults = feat_mapper, defaults
self.field_dims = np.zeros(self.NUM_FEATS, dtype=np.int64)
with open(data_path) as f:
for line in f:
instance = {}
values = line.rstrip('\n').split('\t')
if len(values) != self.NUM_FEATS + 1:
continue
label = np.float32([0, 0])
label[int(values[0])] = 1
instance['y'] = [np.float32(values[0])]
for i in range(1, self.NUM_FEATS + 1):
feat_cnts[i][values[i]] += 1
instance.setdefault('x', []).append(values[i])
self.data[self.count] = instance
self.count = self.count + 1
if self.feat_mapper is None and self.defaults is None:
(continues on next page)
16.8. Feature-Rich Recommender Systems 753
(continued from previous page)
feat_mapper = {i: {feat for feat, c in cnt.items() if c >=
min_threshold} for i, cnt in feat_cnts.items()}
self.feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)}
for i, cnt in feat_mapper.items()}
self.defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}
for i, fm in self.feat_mapper.items():
self.field_dims[i - 1] = len(fm) + 1
self.offsets = np.array((0, *np.cumsum(self.field_dims).asnumpy()
[:-1]))
def __len__(self):
return self.count
def __getitem__(self, idx):
feat = np.array([self.feat_mapper[i + 1].get(v, self.defaults[i + 1])
for i, v in enumerate(self.data[idx]['x'])])
return feat + self.offsets, self.data[idx]['y']
The following example loads the training data and print out the first record.
train_data = CTRDataset(os.path.join(data_dir, 'train.csv'))
train_data[0]
(array([ 143., 146., 227., 246., 957., 1250., 1471., 1566., 1624.,
1985., 2008., 2061., 2236., 2304., 2305., 2360., 2745., 2746.,
2747., 2748., 2892., 2988., 3165., 3189., 3194., 3195., 3308.,
3645., 3687., 3704., 3723., 3752., 3780., 3805.]),
[1.0])
As can be seen, all the 34 fields are categorical features. Each value represents the one-hot index
of the corresponding entry. The label 0 means that it is not clicked. This CTRDataset can also
be used to load other datasets such as the Criteo display advertising challenge Dataset236 and the
Avazu click-through rate prediction Dataset237.
Summary
� Click-through rate is an important metric that is used to measure the effectiveness of advertising
systems and recommender systems.
� Click-through rate prediction is usually converted to a binary classification problem. The
target is to predict whether an ad/item will be clicked or not based on given features.
236 https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/
237 https://www.kaggle.com/c/avazu-ctr-prediction
754 Chapter 16. Recommender Systems
Exercises
� Can you load the Criteo and Avazu dataset with the provided CTRDataset. It is worth noting
that the Criteo dataset consisting of real-valued features so you may have to revise the code
a bit.
Discussions238
16.9 Factorization Machines
Factorization machines (FM) (Rendle, 2010), proposed by Steffen Rendle in 2010, is a supervised
algorithm that can be used for classification, regression, and ranking tasks. It quickly took notice
and became a popular and impactful method for making predictions and recommendations. Particularly,
it is a generalization of the linear regression model and the matrix factorization model.
Moreover, it is reminiscent of support vector machines with a polynomial kernel. The strengths of
factorization machines over the linear regression and matrix factorization are: (1) it can model -
way variable interactions, where  is the number of polynomial order and is usually set to two. (2)
A fast optimization algorithm associated with factorization machines can reduce the polynomial
computation time to linear complexity, making it extremely efficient especially for high dimensional
sparse inputs. For these reasons, factorization machines are widely employed in modern
advertisement and products recommendations. The technical details and implementations are
described below.
16.9.1 2-Way Factorization Machines
Formally, let x 2 Rd denote the feature vectors of one sample, and y denote the corresponding
label which can be real-valued label or class label such as binary class �click/non-click�. The model
for a factorization machine of degree two is defined as:
^y(x) = w0 +
?d
i=1
wixi +
?d
i=1
?d
j=i+1
?vi; vj?xixj (16.9.1)
where w0 2 R is the global bias; w 2 Rd denotes the weights of the i-th variable; V 2 Rdk represents
the feature embeddings; vi represents the ith row of V; k is the dimensionality of latent
factors; ?; ? is the dot product of two vectors. ?vi; vj? model the interaction between the ith and
jth feature. Some feature interactions can be easily understood so they can be designed by experts.
However, most other feature interactions are hidden in data and difficult to identify. So
modeling feature interactions automatically can greatly reduce the efforts in feature engineering.
It is obvious that the first two terms correspond to the linear regression model and the last term is
an extension of the matrix factorization model. If the feature i represents a item and the feature
j represents a user, the third term is exactly the dot product between user and item embeddings.
It is worth noting that FM can also generalize to higher orders (degree > 2). Nevertheless, the
numerical stability might weaken the generalization.
238 https://discuss.d2l.ai/t/405
16.9. Factorization Machines 755
16.9.2 An Efficient Optimization Criterion
Optimizing the factorization machines in a straight forward method leads to a complexity of
O(kd2) as all pairwise interactions require to be computed. To solve this inefficiency problem,
we can reorganize the third term of FM which could greatly reduce the computation cost, leading
to a linear time complexity (O(kd)). The reformulation of the pairwise interaction term is as
follows:
?d
i=1
?d
j=i+1
?vi; vj?xixj
=
1
2
?d
i=1
?d
j=1
?vi; vj?xixj ?? 1
2
?d
i=1
?vi; vi?xixi
=
1
2
(?d
i=1
?d
j=1
?k
l=1
vi;lvj;lxixj ??
?d
i=1
?k
l=1
vi;lvi;lxixi
)
=
1
2
?k
l=1
(
(
?d
i=1
vi;lxi)(
?d
j=1
vj;lxj) ??
?d
i=1
v2
i;lx2i
)
=
1
2
?k
l=1
(
(
?d
i=1
vi;lxi)2 ??
?d
i=1
v2
i;lx2i
)
(16.9.2)
With this reformulation, the model complexity are decreased greatly. Moreover, for sparse features,
only non-zero elements needs to be computed so that the overall complexity is linear to the
number of non-zero features.
To learn the FM model, we can use the MSE loss for regression task, the cross entropy loss for
classification tasks, and the BPR loss for ranking task. Standard optimizers such as SGD and Adam
are viable for optimization.
from d2l import mxnet as d2l
from mxnet import init, gluon, np, npx
from mxnet.gluon import nn
import os
import sys
npx.set_np()
16.9.3 Model Implementation
The following code implement the factorization machines. It is clear to see that FM consists a
linear regression block and an efficient feature interaction block. We apply a sigmoid function
over the final score since we treat the CTR prediction as a classification task.
class FM(nn.Block):
def __init__(self, field_dims, num_factors):
super(FM, self).__init__()
num_inputs = int(sum(field_dims))
self.embedding = nn.Embedding(num_inputs, num_factors)
self.fc = nn.Embedding(num_inputs, 1)
self.linear_layer = nn.Dense(1, use_bias=True)
(continues on next page)
756 Chapter 16. Recommender Systems
(continued from previous page)
def forward(self, x):
square_of_sum = np.sum(self.embedding(x), axis=1) ** 2
sum_of_square = np.sum(self.embedding(x) ** 2, axis=1)
x = self.linear_layer(self.fc(x).sum(1)) \
+ 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True)
x = npx.sigmoid(x)
return x
16.9.4 Load the Advertising Dataset
We use the CTR data wrapper from the last section to load the online advertising dataset.
batch_size = 2048
data_dir = d2l.download_extract('ctr')
train_data = d2l.CTRDataset(os.path.join(data_dir, 'train.csv'))
test_data = d2l.CTRDataset(os.path.join(data_dir, 'test.csv'),
feat_mapper=train_data.feat_mapper,
defaults=train_data.defaults)
train_iter = gluon.data.DataLoader(
train_data, shuffle=True, last_batch='rollover', batch_size=batch_size,
num_workers=d2l.get_dataloader_workers())
test_iter = gluon.data.DataLoader(
test_data, shuffle=False, last_batch='rollover', batch_size=batch_size,
num_workers=d2l.get_dataloader_workers())
16.9.5 Train the Model
Afterwards, we train the model. The learning rate is set to 0.01 and the embedding size is set to 20
by default. The Adam optimizer and the SigmoidBinaryCrossEntropyLoss loss are used for model
training.
devices = d2l.try_all_gpus()
net = FM(train_data.field_dims, num_factors=20)
net.initialize(init.Xavier(), ctx=devices)
lr, num_epochs, optimizer = 0.02, 30, 'adam'
trainer = gluon.Trainer(net.collect_params(), optimizer,
{'learning_rate': lr})
loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.505, train acc 0.278, test acc 0.284
198760.3 examples/sec on [gpu(0), gpu(1)]
16.9. Factorization Machines 757
Summary
� FM is a general framework that can be applied on a variety of tasks such as regression, classification,
and ranking.
� Feature interaction/crossing is important for prediction tasks and the 2-way interaction can
be efficiently modeled with FM.
Exercises
� Can you test FM on other dataset such as Avazu, MovieLens, and Criteo datasets?
� Vary the embedding size to check its impact on performance, can you observe a similar
pattern as that of matrix factorization?
Discussions239
16.10 Deep Factorization Machines
Learning effective feature combinations is critical to the success of click-through rate prediction
task. Factorization machines model feature interactions in a linear paradigm (e.g., bilinear interactions).
This is often insufficient for real-world data where inherent feature crossing structures
are usually very complex and nonlinear. What?s worse, second-order feature interactions
are generally used in factorization machines in practice. Modeling higher degrees of feature combinations
with factorization machines is possible theoretically but it is usually not adopted due to
numerical instability and high computational complexity.
One effective solution is using deep neural networks. Deep neural networks are powerful in feature
representation learning and have the potential to learn sophisticated feature interactions.
As such, it is natural to integrate deep neural networks to factorization machines. Adding nonlinear
transformation layers to factorization machines gives it the capability to model both loworder
feature combinations and high-order feature combinations. Moreover, non-linear inherent
structures from inputs can also be captured with deep neural networks. In this section, we will
239 https://discuss.d2l.ai/t/406
758 Chapter 16. Recommender Systems
introduce a representative model named deep factorization machines (DeepFM) (Guo et al., 2017)
which combine FM and deep neural networks.
16.10.1 Model Architectures
DeepFM consists of an FM component and a deep component which are integrated in a parallel
structure. The FM component is the same as the 2-way factorization machines which is used
to model the low-order feature interactions. The deep component is a multi-layered perceptron
that is used to capture high-order feature interactions and nonlinearities. These two components
share the same inputs/embeddings and their outputs are summed up as the final prediction. It
is worth pointing out that the spirit of DeepFM resembles that of the Wide & Deep architecture
which can capture both memorization and generalization. The advantages of DeepFM over the
Wide & Deep model is that it reduces the effort of hand-crafted feature engineering by identifying
feature combinations automatically.
We omit the description of the FM component for brevity and denote the output as ^y(FM). Readers
are referred to the last section for more details. Let ei 2 Rk denote the latent feature vector of the
ith field. The input of the deep component is the concatenation of the dense embeddings of all
fields that are looked up with the sparse categorical feature input, denoted as:
z(0) = [e1; e2; :::; ef ]; (16.10.1)
where f is the number of fields. It is then fed into the following neural network:
z(l) = (W(l)z(l??1) + b(l)); (16.10.2)
where  is the activation function. Wl and bl are the weight and bias at the lth layer. Let yDNN
denote the output of the prediction. The ultimate prediction of DeepFM is the summation of the
outputs from both FM and DNN. So we have:
^y = (^y(FM) + ^y(DNN)); (16.10.3)
where  is the sigmoid function. The architecture of DeepFM is illustrated below.
16.10. Deep Factorization Machines 759
It is worth noting that DeepFM is not the only way to combine deep neural networks with FM. We
can also add nonlinear layers over the feature interactions (He & Chua, 2017).
from d2l import mxnet as d2l
from mxnet import init, gluon, np, npx
from mxnet.gluon import nn
import os
import sys
npx.set_np()
16.10.2 Implemenation of DeepFM
The implementation of DeepFM is similar to that of FM. We keep the FM part unchanged and use
an MLP block with relu as the activation function. Dropout is also used to regularize the model.
The number of neurons of the MLP can be adjusted with the mlp_dims hyperparameter.
class DeepFM(nn.Block):
def __init__(self, field_dims, num_factors, mlp_dims, drop_rate=0.1):
super(DeepFM, self).__init__()
num_inputs = int(sum(field_dims))
self.embedding = nn.Embedding(num_inputs, num_factors)
self.fc = nn.Embedding(num_inputs, 1)
self.linear_layer = nn.Dense(1, use_bias=True)
input_dim = self.embed_output_dim = len(field_dims) * num_factors
self.mlp = nn.Sequential()
for dim in mlp_dims:
self.mlp.add(nn.Dense(dim, 'relu', True, in_units=input_dim))
self.mlp.add(nn.Dropout(rate=drop_rate))
(continues on next page)
760 Chapter 16. Recommender Systems
(continued from previous page)
input_dim = dim
self.mlp.add(nn.Dense(in_units=input_dim, units=1))
def forward(self, x):
embed_x = self.embedding(x)
square_of_sum = np.sum(embed_x, axis=1) ** 2
sum_of_square = np.sum(embed_x ** 2, axis=1)
inputs = np.reshape(embed_x, (-1, self.embed_output_dim))
x = self.linear_layer(self.fc(x).sum(1)) \
+ 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True) \
+ self.mlp(inputs)
x = npx.sigmoid(x)
return x
16.10.3 Training and Evaluating the Model
The data loading process is the same as that of FM. We set the MLP component of DeepFM to a
three-layered dense network with the a pyramid structure (30-20-10). All other hyperparameters
remain the same as FM.
batch_size = 2048
data_dir = d2l.download_extract('ctr')
train_data = d2l.CTRDataset(os.path.join(data_dir, 'train.csv'))
test_data = d2l.CTRDataset(os.path.join(data_dir, 'test.csv'),
feat_mapper=train_data.feat_mapper,
defaults=train_data.defaults)
field_dims = train_data.field_dims
train_iter = gluon.data.DataLoader(
train_data, shuffle=True, last_batch='rollover', batch_size=batch_size,
num_workers=d2l.get_dataloader_workers())
test_iter = gluon.data.DataLoader(
test_data, shuffle=False, last_batch='rollover', batch_size=batch_size,
num_workers=d2l.get_dataloader_workers())
devices = d2l.try_all_gpus()
net = DeepFM(field_dims, num_factors=10, mlp_dims=[30, 20, 10])
net.initialize(init.Xavier(), ctx=devices)
lr, num_epochs, optimizer = 0.01, 30, 'adam'
trainer = gluon.Trainer(net.collect_params(), optimizer,
{'learning_rate': lr})
loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
loss 0.509, train acc 0.371, test acc 0.365
125648.5 examples/sec on [gpu(0), gpu(1)]
16.10. Deep Factorization Machines 761
Compared with FM, DeepFM converges faster and achieves better performance.
Summary
� Integrating neural networks to FM enables it to model complex and high-order interactions.
� DeepFM outperforms the original FM on the advertising dataset.
Exercises
� Vary the structure of the MLP to check its impact on model performance.
� Change the dataset to Criteo and compare it with the original FM model.
Discussions240
240 https://discuss.d2l.ai/t/407
762 Chapter 16. Recommender Systems
17 | Generative Adversarial Networks
17.1 Generative Adversarial Networks
Throughout most of this book, we have talked about how to make predictions. In some form or
another, we used deep neural networks learned mappings from data examples to labels. This kind
of learning is called discriminative learning, as in, we?d like to be able to discriminate between
photos cats and photos of dogs. Classifiers and regressors are both examples of discriminative
learning. And neural networks trained by backpropagation have upended everything we thought
we knew about discriminative learning on large complicated datasets. Classification accuracies
on high-res images has gone from useless to human-level (with some caveats) in just 5-6 years. We
will spare you another spiel about all the other discriminative tasks where deep neural networks
do astoundingly well.
But there is more to machine learning than just solving discriminative tasks. For example, given
a large dataset, without any labels, we might want to learn a model that concisely captures the
characteristics of this data. Given such a model, we could sample synthetic data examples that
resemble the distribution of the training data. For example, given a large corpus of photographs
of faces, we might want to be able to generate a new photorealistic image that looks like it might
plausibly have come from the same dataset. This kind of learning is called generative modeling.
Until recently, we had no method that could synthesize novel photorealistic images. But the success
of deep neural networks for discriminative learning opened up new possibilities. One big
trend over the last three years has been the application of discriminative deep nets to overcome
challenges in problems that we do not generally think of as supervised learning problems. The
recurrent neural network language models are one example of using a discriminative network
(trained to predict the next character) that once trained can act as a generative model.
In 2014, a breakthrough paper introduced Generative adversarial networks (GANs) (Goodfellow
et al., 2014), a clever new way to leverage the power of discriminative models to get good generative
models. At their heart, GANs rely on the idea that a data generator is good if we cannot tell
fake data apart from real data. In statistics, this is called a two-sample test - a test to answer the
question whether datasetsX = fx1; : : : ; xng andX? = fx?
1; : : : ; x?
n
g were drawn from the same distribution.
The main difference between most statistics papers and GANs is that the latter use this
idea in a constructive way. In other words, rather than just training a model to say �hey, these two
datasets do not look like they came from the same distribution�, they use the two-sample test241 to
provide training signals to a generative model. This allows us to improve the data generator until
it generates something that resembles the real data. At the very least, it needs to fool the classifier.
Even if our classifier is a state of the art deep neural network.
241 https://en.wikipedia.org/wiki/Two-sample_hypothesis_testing
763
Fig. 17.1.1: Generative Adversarial Networks
The GAN architecture is illustrated in Fig. 17.1.1. As you can see, there are two pieces in GAN
architecture - first off, we need a device (say, a deep network but it really could be anything, such
as a game rendering engine) that might potentially be able to generate data that looks just like the
real thing. If we are dealing with images, this needs to generate images. If we are dealing with
speech, it needs to generate audio sequences, and so on. We call this the generator network. The
second component is the discriminator network. It attempts to distinguish fake and real data from
each other. Both networks are in competition with each other. The generator network attempts
to fool the discriminator network. At that point, the discriminator network adapts to the new fake
data. This information, in turn is used to improve the generator network, and so on.
The discriminator is a binary classifier to distinguish if the input x is real (from real data) or fake
(from the generator). Typically, the discriminator outputs a scalar prediction o 2 R for input x,
such as using a dense layer with hidden size 1, and then applies sigmoid function to obtain the
predicted probability D(x) = 1/(1 + e??o). Assume the label y for the true data is 1 and 0 for the
fake data. We train the discriminator to minimize the cross-entropy loss, i.e.,
min
D
f??y logD(x) ?? (1 ?? y) log(1 ?? D(x))g; (17.1.1)
For the generator, it first draws some parameter z 2 Rd from a source of randomness, e.g., a
normal distribution z  N(0; 1). We often call z as the latent variable. It then applies a function
to generate x? = G(z). The goal of the generator is to fool the discriminator to classify x? = G(z)
as true data, i.e., we want D(G(z))  1. In other words, for a given discriminator D, we update
the parameters of the generator G to maximize the cross-entropy loss when y = 0, i.e.,
max
G
f??(1 ?? y) log(1 ?? D(G(z)))g = max
G
f?? log(1 ?? D(G(z)))g: (17.1.2)
If the generator does a perfect job, then D(x?)  1 so the above loss near 0, which results the
gradients are too small to make a good progress for the discriminator. So commonly we minimize
the following loss:
min
G
f??y log(D(G(z)))g = min
G
f?? log(D(G(z)))g; (17.1.3)
which is just feed x? = G(z) into the discriminator but giving label y = 1.
To sum up, D and G are playing a �minimax� game with the comprehensive objective function:
minDmaxGf??ExDatalogD(x) ?? EzNoiselog(1 ?? D(G(z)))g: (17.1.4)
764 Chapter 17. Generative Adversarial Networks
Many of the GANs applications are in the context of images. As a demonstration purpose, we
are going to content ourselves with fitting a much simpler distribution first. We will illustrate
what happens if we use GANs to build the world?s most inefficient estimator of parameters for a
Gaussian. Let us get started.
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, gluon, init, np, npx
from mxnet.gluon import nn
npx.set_np()
17.1.1 Generate some �real� data
Since this is going to be the world?s lamest example, we simply generate data drawn from a Gaussian.
X = np.random.normal(size=(1000, 2))
A = np.array([[1, 2], [-0.1, 0.5]])
b = np.array([1, 2])
data = X.dot(A) + b
Let us see what we got. This should be a Gaussian shifted in some rather arbitrary way with mean
b and covariance matrix ATA.
d2l.set_figsize()
d2l.plt.scatter(data[:100, 0].asnumpy(), data[:100, 1].asnumpy());
print(f'The covariance matrix is\n{np.dot(A.T, A)}')
The covariance matrix is
[[1.01 1.95]
[1.95 4.25]]
batch_size = 8
data_iter = d2l.load_array((data,), batch_size)
17.1. Generative Adversarial Networks 765
17.1.2 Generator
Our generator network will be the simplest network possible - a single layer linear model. This
is since we will be driving that linear network with a Gaussian data generator. Hence, it literally
only needs to learn the parameters to fake things perfectly.
net_G = nn.Sequential()
net_G.add(nn.Dense(2))
17.1.3 Discriminator
For the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers to
make things a bit more interesting.
net_D = nn.Sequential()
net_D.add(nn.Dense(5, activation='tanh'),
nn.Dense(3, activation='tanh'),
nn.Dense(1))
17.1.4 Training
First we define a function to update the discriminator.
#@save
def update_D(X, Z, net_D, net_G, loss, trainer_D):
"""Update discriminator."""
batch_size = X.shape[0]
ones = np.ones((batch_size,), ctx=X.ctx)
zeros = np.zeros((batch_size,), ctx=X.ctx)
with autograd.record():
real_Y = net_D(X)
fake_X = net_G(Z)
# Do not need to compute gradient for `net_G`, detach it from
# computing gradients.
fake_Y = net_D(fake_X.detach())
loss_D = (loss(real_Y, ones) + loss(fake_Y, zeros)) / 2
loss_D.backward()
trainer_D.step(batch_size)
return float(loss_D.sum())
The generator is updated similarly. Here we reuse the cross-entropy loss but change the label of
the fake data from 0 to 1.
def update_G(Z, net_D, net_G, loss, trainer_G): #@save
"""Update generator."""
batch_size = Z.shape[0]
ones = np.ones((batch_size,), ctx=Z.ctx)
with autograd.record():
# We could reuse `fake_X` from `update_D` to save computation
fake_X = net_G(Z)
# Recomputing `fake_Y` is needed since `net_D` is changed
(continues on next page)
766 Chapter 17. Generative Adversarial Networks
(continued from previous page)
fake_Y = net_D(fake_X)
loss_G = loss(fake_Y, ones)
loss_G.backward()
trainer_G.step(batch_size)
return float(loss_G.sum())
Both the discriminator and the generator performs a binary logistic regression with the crossentropy
loss. We use Adam to smooth the training process. In each iteration, we first update the
discriminator and then the generator. We visualize both losses and generated examples.
def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):
loss = gluon.loss.SigmoidBCELoss()
net_D.initialize(init=init.Normal(0.02), force_reinit=True)
net_G.initialize(init=init.Normal(0.02), force_reinit=True)
trainer_D = gluon.Trainer(net_D.collect_params(),
'adam', {'learning_rate': lr_D})
trainer_G = gluon.Trainer(net_G.collect_params(),
'adam', {'learning_rate': lr_G})
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
legend=['generator', 'discriminator'])
animator.fig.subplots_adjust(hspace=0.3)
for epoch in range(num_epochs):
# Train one epoch
timer = d2l.Timer()
metric = d2l.Accumulator(3) # loss_D, loss_G, num_examples
for X in data_iter:
batch_size = X.shape[0]
Z = np.random.normal(0, 1, size=(batch_size, latent_dim))
metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),
update_G(Z, net_D, net_G, loss, trainer_G),
batch_size)
# Visualize generated examples
Z = np.random.normal(0, 1, size=(100, latent_dim))
fake_X = net_G(Z).asnumpy()
animator.axes[1].cla()
animator.axes[1].scatter(data[:, 0], data[:, 1])
animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])
animator.axes[1].legend(['real', 'generated'])
# Show the losses
loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]
animator.add(epoch + 1, (loss_D, loss_G))
print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '
f'{metric[2] / timer.stop():.1f} examples/sec')
Now we specify the hyperparameters to fit the Gaussian distribution.
lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20
train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,
latent_dim, data[:100].asnumpy())
loss_D 0.693, loss_G 0.693, 616.3 examples/sec
17.1. Generative Adversarial Networks 767
Summary
� Generative adversarial networks (GANs) composes of two deep networks, the generator and
the discriminator.
� The generator generates the image as much closer to the true image as possible to fool the
discriminator, via maximizing the cross-entropy loss, i.e., max log(D(x?)).
� The discriminator tries to distinguish the generated images from the true images, via minimizing
the cross-entropy loss, i.e., min??y logD(x) ?? (1 ?? y) log(1 ?? D(x)).
Exercises
� Does an equilibrium exist where the generator wins, i.e. the discriminator ends up unable
to distinguish the two distributions on finite samples?
Discussions242
242 https://discuss.d2l.ai/t/408
768 Chapter 17. Generative Adversarial Networks
17.2 Deep Convolutional Generative Adversarial Networks
In Section 17.1, we introduced the basic ideas behind how GANs work. We showed that they can
draw samples from some simple, easy-to-sample distribution, like a uniform or normal distribution,
and transform them into samples that appear to match the distribution of some dataset. And
while our example of matching a 2D Gaussian distribution got the point across, it is not especially
exciting.
In this section, we will demonstrate how you can use GANs to generate photorealistic images. We
will be basing our models on the deep convolutional GANs (DCGAN) introduced in (Radford et
al., 2015). We will borrow the convolutional architecture that have proven so successful for discriminative
computer vision problems and show how via GANs, they can be leveraged to generate
photorealistic images.
from mxnet import gluon, init, np, npx
from mxnet.gluon import nn
from d2l import mxnet as d2l
npx.set_np()
17.2.1 The Pokemon Dataset
The dataset we will use is a collection of Pokemon sprites obtained from pokemondb243. First
download, extract and load this dataset.
#@save
d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',
'c065c0e2593b8b161a2d7873e42418bf6a21106c')
data_dir = d2l.download_extract('pokemon')
pokemon = gluon.data.vision.datasets.ImageFolderDataset(data_dir)
Downloading ../data/pokemon.zip from http://d2l-data.s3-accelerate.amazonaws.com/pokemon.zip.
,!..
We resize each image into 64  64. The ToTensor transformation will project the pixel value into
[0; 1], while our generator will use the tanh function to obtain outputs in [??1; 1]. Therefore we
normalize the data with 0:5 mean and 0:5 standard deviation to match the value range.
batch_size = 256
transformer = gluon.data.vision.transforms.Compose([
gluon.data.vision.transforms.Resize(64),
gluon.data.vision.transforms.ToTensor(),
gluon.data.vision.transforms.Normalize(0.5, 0.5)
])
data_iter = gluon.data.DataLoader(
pokemon.transform_first(transformer), batch_size=batch_size,
shuffle=True, num_workers=d2l.get_dataloader_workers())
Let us visualize the first 20 images.
243 https://pokemondb.net/sprites
17.2. Deep Convolutional Generative Adversarial Networks 769
d2l.set_figsize((4, 4))
for X, y in data_iter:
imgs = X[0:20,:,:,:].transpose(0, 2, 3, 1)/2+0.5
d2l.show_images(imgs, num_rows=4, num_cols=5)
break
17.2.2 The Generator
The generator needs to map the noise variable z 2 Rd, a length-d vector, to a RGB image with width
and height to be 6464 . In Section 13.11 we introduced the fully convolutional network that uses
transposed convolution layer (refer to Section 13.10) to enlarge input size. The basic block of the
generator contains a transposed convolution layer followed by the batch normalization and ReLU
activation.
class G_block(nn.Block):
def __init__(self, channels, kernel_size=4,
strides=2, padding=1, **kwargs):
super(G_block, self).__init__(**kwargs)
self.conv2d_trans = nn.Conv2DTranspose(
channels, kernel_size, strides, padding, use_bias=False)
self.batch_norm = nn.BatchNorm()
self.activation = nn.Activation('relu')
(continues on next page)
770 Chapter 17. Generative Adversarial Networks
(continued from previous page)
def forward(self, X):
return self.activation(self.batch_norm(self.conv2d_trans(X)))
In default, the transposed convolution layer uses a kh = kw = 4 kernel, a sh = sw = 2 strides, and
a ph = pw = 1 padding. With a input shape of n
?
h
 n
?
w = 16  16, the generator block will double
input?s width and height.
n
?
h
 n
?
w = [(nhkh ?? (nh ?? 1)(kh ?? sh) ?? 2ph]  [(nwkw ?? (nw ?? 1)(kw ?? sw) ?? 2pw]
= [(kh + sh(nh ?? 1) ?? 2ph]  [(kw + sw(nw ?? 1) ?? 2pw]
= [(4 + 2  (16 ?? 1) ?? 2  1]  [(4 + 2  (16 ?? 1) ?? 2  1]
= 32  32:
(17.2.1)
x = np.zeros((2, 3, 16, 16))
g_blk = G_block(20)
g_blk.initialize()
g_blk(x).shape
(2, 20, 32, 32)
If changing the transposed convolution layer to a 44 kernel, 11 strides and zero padding. With
a input size of 1  1, the output will have its width and height increased by 3 respectively.
x = np.zeros((2, 3, 1, 1))
g_blk = G_block(20, strides=1, padding=0)
g_blk.initialize()
g_blk(x).shape
(2, 20, 4, 4)
The generator consists of four basic blocks that increase input?s both width and height from 1
to 32. At the same time, it first projects the latent variable into 64  8 channels, and then halve
the channels each time. At last, a transposed convolution layer is used to generate the output. It
further doubles the width and height to match the desired 6464 shape, and reduces the channel
size to 3. The tanh activation function is applied to project output values into the (??1; 1) range.
n_G = 64
net_G = nn.Sequential()
net_G.add(G_block(n_G*8, strides=1, padding=0), # Output: (64 * 8, 4, 4)
G_block(n_G*4), # Output: (64 * 4, 8, 8)
G_block(n_G*2), # Output: (64 * 2, 16, 16)
G_block(n_G), # Output: (64, 32, 32)
nn.Conv2DTranspose(
3, kernel_size=4, strides=2, padding=1, use_bias=False,
activation='tanh')) # Output: (3, 64, 64)
Generate a 100 dimensional latent variable to verify the generator?s output shape.
x = np.zeros((1, 100, 1, 1))
net_G.initialize()
net_G(x).shape
17.2. Deep Convolutional Generative Adversarial Networks 771
(1, 3, 64, 64)
17.2.3 Discriminator
The discriminator is a normal convolutional network network except that it uses a leaky ReLU as
its activation function. Given  2 [0; 1], its definition is
leaky ReLU(x) =
{
x if x > 0
x otherwise
: (17.2.2)
As it can be seen, it is normal ReLU if  = 0, and an identity function if  = 1. For  2 (0; 1), leaky
ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the
�dying ReLU� problem that a neuron might always output a negative value and therefore cannot
make any progress since the gradient of ReLU is 0.
alphas = [0, 0.2, 0.4, .6, .8, 1]
x = np.arange(-2, 1, 0.1)
Y = [nn.LeakyReLU(alpha)(x).asnumpy() for alpha in alphas]
d2l.plot(x.asnumpy(), Y, 'x', 'y', alphas)
The basic block of the discriminator is a convolution layer followed by a batch normalization layer
and a leaky ReLU activation. The hyperparameters of the convolution layer are similar to the
transpose convolution layer in the generator block.
class D_block(nn.Block):
def __init__(self, channels, kernel_size=4, strides=2,
padding=1, alpha=0.2, **kwargs):
super(D_block, self).__init__(**kwargs)
self.conv2d = nn.Conv2D(
channels, kernel_size, strides, padding, use_bias=False)
self.batch_norm = nn.BatchNorm()
self.activation = nn.LeakyReLU(alpha)
def forward(self, X):
return self.activation(self.batch_norm(self.conv2d(X)))
772 Chapter 17. Generative Adversarial Networks
A basic block with default settings will halve the width and height of the inputs, as we demonstrated
in Section 6.3. For example, given a input shape nh = nw = 16, with a kernel shape
kh = kw = 4, a stride shape sh = sw = 2, and a padding shape ph = pw = 1, the output shape will
be:
n
?
h
 n
?
w = ?(nh ?? kh + 2ph + sh)/sh?  ?(nw ?? kw + 2pw + sw)/sw?
= ?(16 ?? 4 + 2  1 + 2)/2?  ?(16 ?? 4 + 2  1 + 2)/2?
= 8  8:
(17.2.3)
x = np.zeros((2, 3, 16, 16))
d_blk = D_block(20)
d_blk.initialize()
d_blk(x).shape
(2, 20, 8, 8)
The discriminator is a mirror of the generator.
n_D = 64
net_D = nn.Sequential()
net_D.add(D_block(n_D), # Output: (64, 32, 32)
D_block(n_D*2), # Output: (64 * 2, 16, 16)
D_block(n_D*4), # Output: (64 * 4, 8, 8)
D_block(n_D*8), # Output: (64 * 8, 4, 4)
nn.Conv2D(1, kernel_size=4, use_bias=False)) # Output: (1, 1, 1)
It uses a convolution layer with output channel 1 as the last layer to obtain a single prediction
value.
x = np.zeros((1, 3, 64, 64))
net_D.initialize()
net_D(x).shape
(1, 1, 1, 1)
17.2.4 Training
Compared to the basic GAN in Section 17.1, we use the same learning rate for both generator and
discriminator since they are similar to each other. In addition, we change 1 in Adam (Section
11.10) from 0:9 to 0:5. It decreases the smoothness of the momentum, the exponentially weighted
moving average of past gradients, to take care of the rapid changing gradients because the generator
and the discriminator fight with each other. Besides, the random generated noise Z, is a 4-D
tensor and we are using GPU to accelerate the computation.
def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,
device=d2l.try_gpu()):
loss = gluon.loss.SigmoidBCELoss()
net_D.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)
net_G.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)
(continues on next page)
17.2. Deep Convolutional Generative Adversarial Networks 773
(continued from previous page)
trainer_hp = {'learning_rate': lr, 'beta1': 0.5}
trainer_D = gluon.Trainer(net_D.collect_params(), 'adam', trainer_hp)
trainer_G = gluon.Trainer(net_G.collect_params(), 'adam', trainer_hp)
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
legend=['discriminator', 'generator'])
animator.fig.subplots_adjust(hspace=0.3)
for epoch in range(1, num_epochs + 1):
# Train one epoch
timer = d2l.Timer()
metric = d2l.Accumulator(3) # loss_D, loss_G, num_examples
for X, _ in data_iter:
batch_size = X.shape[0]
Z = np.random.normal(0, 1, size=(batch_size, latent_dim, 1, 1))
X, Z = X.as_in_ctx(device), Z.as_in_ctx(device),
metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),
d2l.update_G(Z, net_D, net_G, loss, trainer_G),
batch_size)
# Show generated examples
Z = np.random.normal(0, 1, size=(21, latent_dim, 1, 1), ctx=device)
# Normalize the synthetic data to N(0, 1)
fake_x = net_G(Z).transpose(0, 2, 3, 1) / 2 + 0.5
imgs = np.concatenate(
[np.concatenate([fake_x[i * 7 + j] for j in range(7)], axis=1)
for i in range(len(fake_x)//7)], axis=0)
animator.axes[1].cla()
animator.axes[1].imshow(imgs.asnumpy())
# Show the losses
loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]
animator.add(epoch, (loss_D, loss_G))
print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '
f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')
We train the model with a small number of epochs just for demonstration. For better performance,
the variable num_epochs can be set to a larger number.
latent_dim, lr, num_epochs = 100, 0.005, 20
train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)
loss_D 0.060, loss_G 5.690, 2661.8 examples/sec on gpu(0)
774 Chapter 17. Generative Adversarial Networks
Summary
� DCGAN architecture has four convolutional layers for the Discriminator and four
�fractionally-strided� convolutional layers for the Generator.
� The Discriminator is a 4-layer strided convolutions with batch normalization (except its input
layer) and leaky ReLU activations.
� Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims
to fix the �dying ReLU� problem and helps the gradients flow easier through the architecture.
Exercises
1. What will happen if we use standard ReLU activation rather than leaky ReLU?
2. Apply DCGAN on Fashion-MNIST and see which category works well and which does not.
Discussions244
244 https://discuss.d2l.ai/t/409
17.2. Deep Convolutional Generative Adversarial Networks 775
776 Chapter 17. Generative Adversarial Networks
18 | Appendix: Mathematics for Deep
Learning
Brent Werness (Amazon), Rachel Hu (Amazon), and authors of this book
One of the wonderful parts of modern deep learning is the fact that much of it can be understood
and used without a full understanding of the mathematics below it. This is a sign that the field
is maturing. Just as most software developers no longer need to worry about the theory of computable
functions, neither should deep learning practitioners need to worry about the theoretical
foundations of maximum likelihood learning.
But, we are not quite there yet.
In practice, you will sometimes need to understand how architectural choices influence gradient
flow, or the implicit assumptions you make by training with a certain loss function. You might
need to know what in the world entropy measures, and how it can help you understand exactly
what bits-per-character means in your model. These all require deeper mathematical understanding.
This appendix aims to provide you the mathematical background you need to understand the core
theory of modern deep learning, but it is not exhaustive. We will begin with examining linear algebra
in greater depth. We develop a geometric understanding of all the common linear algebraic
objects and operations that will enable us to visualize the effects of various transformations on our
data. A key element is the development of the basics of eigen-decompositions.
We next develop the theory of differential calculus to the point that we can fully understand why
the gradient is the direction of steepest descent, and why back-propagation takes the form it does.
Integral calculus is then discussed to the degree needed to support our next topic, probability
theory.
Problems encountered in practice frequently are not certain, and thus we need a language to speak
about uncertain things. We review the theory of random variables and the most commonly encountered
distributions so we may discuss models probabilistically. This provides the foundation
for the naive Bayes classifier, a probabilistic classification technique.
Closely related to probability theory is the study of statistics. While statistics is far too large a field
to do justice in a short section, we will introduce fundamental concepts that all machine learning
practitioners should be aware of, in particular: evaluating and comparing estimators, conducting
hypothesis tests, and constructing confidence intervals.
Last, we turn to the topic of information theory, which is the mathematical study of information
storage and transmission. This provides the core language by which we may discuss quantitatively
how much information a model holds on a domain of discourse.
777
Taken together, these form the core of the mathematical concepts needed to begin down the path
towards a deep understanding of deep learning.
18.1 Geometry and Linear Algebraic Operations
In Section 2.3, we encountered the basics of linear algebra and saw how it could be used to express
common operations for transforming our data. Linear algebra is one of the key mathematical pillars
underlying much of the work that we do deep learning and in machine learning more broadly.
While Section 2.3 contained enough machinery to communicate the mechanics of modern deep
learning models, there is a lot more to the subject. In this section, we will go deeper, highlighting
some geometric interpretations of linear algebra operations, and introducing a few fundamental
concepts, including of eigenvalues and eigenvectors.
18.1.1 Geometry of Vectors
First, we need to discuss the two common geometric interpretations of vectors, as either points
or directions in space. Fundamentally, a vector is a list of numbers such as the Python list below.
v = [1, 7, 0, 1]
Mathematicians most often write this as either a column or row vector, which is to say either as
x =
2
664
1
7
0
1
3
775
; (18.1.1)
or
x?
=
[
1 7 0 1
]
: (18.1.2)
These often have different interpretations, where data examples are column vectors and weights
used to form weighted sums are row vectors. However, it can be beneficial to be flexible. As we
have described in Section 2.3, though a single vector?s default orientation is a column vector, for
any matrix representing a tabular dataset, treating each data example as a row vector in the matrix
is more conventional.
Given a vector, the first interpretation that we should give it is as a point in space. In two or three
dimensions, we can visualize these points by using the components of the vectors to define the
location of the points in space compared to a fixed reference called the origin. This can be seen
in Fig. 18.1.1.
778 Chapter 18. Appendix: Mathematics for Deep Learning
Fig. 18.1.1: An illustration of visualizing vectors as points in the plane. The first component of the
vector gives the x-coordinate, the second component gives the y-coordinate. Higher dimensions
are analogous, although much harder to visualize.
This geometric point of view allows us to consider the problem on a more abstract level. No longer
faced with some insurmountable seeming problem like classifying pictures as either cats or dogs,
we can start considering tasks abstractly as collections of points in space and picturing the task as
discovering how to separate two distinct clusters of points.
In parallel, there is a second point of view that people often take of vectors: as directions in space.
Not only can we think of the vector v = [3; 2]? as the location 3 units to the right and 2 units up
from the origin, we can also think of it as the direction itself to take 3 steps to the right and 2 steps
up. In this way, we consider all the vectors in figure Fig. 18.1.2 the same.
Fig. 18.1.2: Any vector can be visualized as an arrow in the plane. In this case, every vector drawn
is a representation of the vector (3; 2)?.
One of the benefits of this shift is that we can make visual sense of the act of vector addition. In
particular, we follow the directions given by one vector, and then follow the directions given by
the other, as is seen in Fig. 18.1.3.
18.1. Geometry and Linear Algebraic Operations 779
Fig. 18.1.3: We can visualize vector addition by first following one vector, and then another.
Vector subtraction has a similar interpretation. By considering the identity that u = v + (u ?? v),
we see that the vector u ?? v is the direction that takes us from the point v to the point u.
18.1.2 Dot Products and Angles
As we saw in Section 2.3, if we take two column vectors u and v, we can form their dot product by
computing:
u?v =
?
i
ui  vi: (18.1.3)
Because (18.1.3) is symmetric, we will mirror the notation of classical multiplication and write
u  v = u?v = v?u; (18.1.4)
to highlight the fact that exchanging the order of the vectors will yield the same answer.
The dot product (18.1.3) also admits a geometric interpretation: it is closely related to the angle
between two vectors. Consider the angle shown in Fig. 18.1.4.
Fig. 18.1.4: Between any two vectors in the plane there is a well defined angle . We will see this
angle is intimately tied to the dot product.
To start, let us consider two specific vectors:
v = (r; 0) and w = (s cos(); s sin()): (18.1.5)
780 Chapter 18. Appendix: Mathematics for Deep Learning
The vector v is length r and runs parallel to the x-axis, and the vector w is of length s and at angle
 with the x-axis.
If we compute the dot product of these two vectors, we see that
v  w = rs cos() = ?v??w? cos(): (18.1.6)
With some simple algebraic manipulation, we can rearrange terms to obtain
 = arccos
(
v  w
?v??w?
)
: (18.1.7)
In short, for these two specific vectors, the dot product combined with the norms tell us the angle
between the two vectors. This same fact is true in general. We will not derive the expression here,
however, if we consider writing ?v ?? w?2 in two ways: one with the dot product, and the other
geometrically using the law of cosines, we can obtain the full relationship. Indeed, for any two
vectors v and w, the angle between the two vectors is
 = arccos
(
v  w
?v??w?
)
: (18.1.8)
This is a nice result since nothing in the computation references two-dimensions. Indeed, we can
use this in three or three million dimensions without issue.
As a simple example, let us see how to compute the angle between a pair of vectors:
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mxnet import gluon, np, npx
npx.set_np()
def angle(v, w):
return np.arccos(v.dot(w) / (np.linalg.norm(v) * np.linalg.norm(w)))
angle(np.array([0, 1, 2]), np.array([2, 3, 4]))
array(0.41899002)
We will not use it right now, but it is useful to know that we will refer to vectors for which the angle
is /2 (or equivalently 90?) as being orthogonal. By examining the equation above, we see that this
happens when  = /2, which is the same thing as cos() = 0. The only way this can happen is
if the dot product itself is zero, and two vectors are orthogonal if and only if v  w = 0. This will
prove to be a helpful formula when understanding objects geometrically.
It is reasonable to ask: why is computing the angle useful? The answer comes in the kind of
invariance we expect data to have. Consider an image, and a duplicate image, where every pixel
value is the same but 10% the brightness. The values of the individual pixels are in general far
from the original values. Thus, if one computed the distance between the original image and the
darker one, the distance can be large.
However, for most ML applications, the content is the same�it is still an image of a cat as far as a
cat/dog classifier is concerned. However, if we consider the angle, it is not hard to see that for
18.1. Geometry and Linear Algebraic Operations 781
any vector v, the angle between v and 0:1  v is zero. This corresponds to the fact that scaling
vectors keeps the same direction and just changes the length. The angle considers the darker
image identical.
Examples like this are everywhere. In text, we might want the topic being discussed to not change
if we write twice as long of document that says the same thing. For some encoding (such as counting
the number of occurrences of words in some vocabulary), this corresponds to a doubling of
the vector encoding the document, so again we can use the angle.
Cosine Similarity
In ML contexts where the angle is employed to measure the closeness of two vectors, practitioners
adopt the term cosine similarity to refer to the portion
cos() =
v  w
?v??w?: (18.1.9)
The cosine takes a maximum value of 1 when the two vectors point in the same direction, a minimum
value of ??1 when they point in opposite directions, and a value of 0 when the two vectors
are orthogonal. Note that if the components of high-dimensional vectors are sampled randomly
with mean 0, their cosine will nearly always be close to 0.
18.1.3 Hyperplanes
In addition to working with vectors, another key object that you must understand to go far in linear
algebra is the hyperplane, a generalization to higher dimensions of a line (two dimensions) or of a
plane (three dimensions). In an d-dimensional vector space, a hyperplane has d ?? 1 dimensions
and divides the space into two half-spaces.
Let us start with an example. Suppose that we have a column vector w = [2; 1]?. We want to know,
�what are the points v with w  v = 1?� By recalling the connection between dot products and
angles above (18.1.8), we can see that this is equivalent to
?v??w? cos() = 1 () ?v? cos() =
1
?w? =
p1
5
: (18.1.10)
Fig. 18.1.5: Recalling trigonometry, we see the formula ?v? cos() is the length of the projection
of the vector v onto the direction of w
782 Chapter 18. Appendix: Mathematics for Deep Learning
If we consider the geometric meaning of this expression, we see that this is equivalent to saying
that the length of the projection of v onto the direction of w is exactly 1/?w?, as is shown in :numref:
fig_vector-project. The set of all points where this is true is a line at right angles to the vector
w. If we wanted, we could find the equation for this line and see that it is 2x+y = 1 or equivalently
y = 1 ?? 2x.
If we now look at what happens when we ask about the set of points with w  v > 1 or w  v < 1, we
can see that these are cases where the projections are longer or shorter than 1/?w?, respectively.
Thus, those two inequalities define either side of the line. In this way, we have found a way to cut
our space into two halves, where all the points on one side have dot product below a threshold,
and the other side above as we see in Fig. 18.1.6.
Fig. 18.1.6: If we now consider the inequality version of the expression, we see that our hyperplane
(in this case: just a line) separates the space into two halves.
The story in higher dimension is much the same. If we now take w = [1; 2; 3]? and ask about the
points in three dimensions with w  v = 1, we obtain a plane at right angles to the given vector w.
The two inequalities again define the two sides of the plane as is shown in Fig. 18.1.7.
Fig. 18.1.7: Hyperplanes in any dimension separate the space into two halves.
While our ability to visualize runs out at this point, nothing stops us from doing this in tens, hundreds,
or billions of dimensions. This occurs often when thinking about machine learned models.
For instance, we can understand linear classification models like those from Section 3.4, as methods
to find hyperplanes that separate the different target classes. In this context, such hyperplanes
are often referred to as decision planes. The majority of deep learned classification models end with
a linear layer fed into a softmax, so one can interpret the role of the deep neural network to be to
find a non-linear embedding such that the target classes can be separated cleanly by hyperplanes.
18.1. Geometry and Linear Algebraic Operations 783
To give a hand-built example, notice that we can produce a reasonable model to classify tiny images
of t-shirts and trousers from the Fashion MNIST dataset (seen in Section 3.5) by just taking
the vector between their means to define the decision plane and eyeball a crude threshold. First
we will load the data and compute the averages.
# Load in the dataset
train = gluon.data.vision.FashionMNIST(train=True)
test = gluon.data.vision.FashionMNIST(train=False)
X_train_0 = np.stack([x[0] for x in train if x[1] == 0]).astype(float)
X_train_1 = np.stack([x[0] for x in train if x[1] == 1]).astype(float)
X_test = np.stack(
[x[0] for x in test if x[1] == 0 or x[1] == 1]).astype(float)
y_test = np.stack(
[x[1] for x in test if x[1] == 0 or x[1] == 1]).astype(float)
# Compute averages
ave_0 = np.mean(X_train_0, axis=0)
ave_1 = np.mean(X_train_1, axis=0)
It can be informative to examine these averages in detail, so let us plot what they look like. In this
case, we see that the average indeed resembles a blurry image of a t-shirt.
# Plot average t-shirt
d2l.set_figsize()
d2l.plt.imshow(ave_0.reshape(28, 28).tolist(), cmap='Greys')
d2l.plt.show()
In the second case, we again see that the average resembles a blurry image of trousers.
# Plot average trousers
d2l.plt.imshow(ave_1.reshape(28, 28).tolist(), cmap='Greys')
d2l.plt.show()
784 Chapter 18. Appendix: Mathematics for Deep Learning
In a fully machine learned solution, we would learn the threshold from the dataset. In this case,
I simply eyeballed a threshold that looked good on the training data by hand.
# Print test set accuracy with eyeballed threshold
w = (ave_1 - ave_0).T
predictions = X_test.reshape(2000, -1).dot(w.flatten()) > -1500000
# Accuracy
np.mean(predictions.astype(y_test.dtype) == y_test, dtype=np.float64)
array(0.801, dtype=float64)
18.1.4 Geometry of Linear Transformations
Through Section 2.3 and the above discussions, we have a solid understanding of the geometry of
vectors, lengths, and angles. However, there is one important object we have omitted discussing,
and that is a geometric understanding of linear transformations represented by matrices. Fully
internalizing what matrices can do to transform data between two potentially different high dimensional
spaces takes significant practice, and is beyond the scope of this appendix. However,
we can start building up intuition in two dimensions.
Suppose that we have some matrix:
A =
[
a b
c d
]
: (18.1.11)
If we want to apply this to an arbitrary vector v = [x; y]?, we multiply and see that
Av =
[
a b
c d
] [
x
y
]
=
[
ax + by
cx + dy
]
= x
[
a
c
]
+ y
[
b
d
]
= x
{
A
[
1
0
]}
+ y
{
A
[
0
1
]}
:
(18.1.12)
18.1. Geometry and Linear Algebraic Operations 785
This may seem like an odd computation, where something clear became somewhat impenetrable.
However, it tells us that we can write the way that a matrix transforms any vector in terms of how
it transforms two specific vectors: [1; 0]? and [0; 1]?. This is worth considering for a moment. We
have essentially reduced an infinite problem (what happens to any pair of real numbers) to a finite
one (what happens to these specific vectors). These vectors are an example a basis, where we can
write any vector in our space as a weighted sum of these basis vectors.
Let us draw what happens when we use the specific matrix
A =
[
1 2
??1 3
]
: (18.1.13)
If we look at the specific vector v = [2;??1]?, we see this is 2  [1; 0]? + ??1  [0; 1]?, and thus we
know that the matrix A will send this to 2(A[1; 0]?)+??1(A[0; 1])? = 2[1;??1]? ??[2; 3]? = [0;??5]?.
If we follow this logic through carefully, say by considering the grid of all integer pairs of points,
we see that what happens is that the matrix multiplication can skew, rotate, and scale the grid, but
the grid structure must remain as you see in Fig. 18.1.8.
Fig. 18.1.8: The matrix A acting on the given basis vectors. Notice how the entire grid is transported
along with it.
This is the most important intuitive point to internalize about linear transformations represented
by matrices. Matrices are incapable of distorting some parts of space differently than others. All
they can do is take the original coordinates on our space and skew, rotate, and scale them.
Some distortions can be severe. For instance the matrix
B =
[
2 ??1
4 ??2
]
; (18.1.14)
compresses the entire two-dimensional plane down to a single line. Identifying and working with
such transformations are the topic of a later section, but geometrically we can see that this is
fundamentally different from the types of transformations we saw above. For instance, the result
from matrix A can be �bent back� to the original grid. The results from matrix B cannot because
we will never know where the vector [1; 2]? came from�was it [1; 1]? or [0;??1]??
While this picture was for a 2  2 matrix, nothing prevents us from taking the lessons learned
into higher dimensions. If we take similar basis vectors like [1; 0; : : : ; 0] and see where our matrix
sends them, we can start to get a feeling for how the matrix multiplication distorts the entire space
in whatever dimension space we are dealing with.
786 Chapter 18. Appendix: Mathematics for Deep Learning
18.1.5 Linear Dependence
Consider again the matrix
B =
[
2 ??1
4 ??2
]
: (18.1.15)
This compresses the entire plane down to live on the single line y = 2x. The question now arises:
is there some way we can detect this just looking at the matrix itself? The answer is that indeed
we can. Let us take b1 = [2; 4]? and b2 = [??1;??2]? be the two columns of B. Remember that we
can write everything transformed by the matrix B as a weighted sum of the columns of the matrix:
like a1b1 + a2b2. We call this a linear combination. The fact that b1 = ??2  b2 means that we can
write any linear combination of those two columns entirely in terms of say b2 since
a1b1 + a2b2 = ??2a1b2 + a2b2 = (a2 ?? 2a1)b2: (18.1.16)
This means that one of the columns is, in a sense, redundant because it does not define a unique
direction in space. This should not surprise us too much since we already saw that this matrix
collapses the entire plane down into a single line. Moreover, we see that the linear dependence
b1 = ??2  b2 captures this. To make this more symmetrical between the two vectors, we will write
this as
b1 + 2  b2 = 0: (18.1.17)
In general, we will say that a collection of vectors v1; : : : ; vk are linearly dependent if there exist
coefficients a1; : : : ; ak not all equal to zero so that
?k
i=1
aivi = 0: (18.1.18)
In this case, we can solve for one of the vectors in terms of some combination of the others, and
effectively render it redundant. Thus, a linear dependence in the columns of a matrix is a witness
to the fact that our matrix is compressing the space down to some lower dimension. If there is
no linear dependence we say the vectors are linearly independent. If the columns of a matrix are
linearly independent, no compression occurs and the operation can be undone.
18.1.6 Rank
If we have a general n  m matrix, it is reasonable to ask what dimension space the matrix maps
into. A concept known as the rank will be our answer. In the previous section, we noted that a
linear dependence bears witness to compression of space into a lower dimension and so we will
be able to use this to define the notion of rank. In particular, the rank of a matrix A is the largest
number of linearly independent columns amongst all subsets of columns. For example, the matrix
B =
[
2 4
??1 ??2
]
; (18.1.19)
has rank(B) = 1, since the two columns are linearly dependent, but either column by itself is not
linearly dependent. For a more challenging example, we can consider
C =
2
664
1 3 0 ??1 0
??1 0 1 1 ??1
0 3 1 0 ??1
2 3 ??1 ??2 1
3
775
; (18.1.20)
18.1. Geometry and Linear Algebraic Operations 787
and show that C has rank two since, for instance, the first two columns are linearly independent,
however any of the four collections of three columns are dependent.
This procedure, as described, is very inefficient. It requires looking at every subset of the columns
of our given matrix, and thus is potentially exponential in the number of columns. Later we will
see a more computationally efficient way to compute the rank of a matrix, but for now, this is
sufficient to see that the concept is well defined and understand the meaning.
18.1.7 Invertibility
We have seen above that multiplication by a matrix with linearly dependent columns cannot be
undone, i.e., there is no inverse operation that can always recover the input. However, multiplication
by a full-rank matrix (i.e., some A that is n  n matrix with rank n), we should always be
able to undo it. Consider the matrix
I =
2
6664
1 0    0
0 1    0
...
...
. .. ...
0 0    1
3
7775
: (18.1.21)
which is the matrix with ones along the diagonal, and zeros elsewhere. We call this the identity
matrix. It is the matrix which leaves our data unchanged when applied. To find a matrix which
undoes what our matrix A has done, we want to find a matrix A??1 such that
A??1A = AA??1 = I: (18.1.22)
If we look at this as a system, we have n  n unknowns (the entries of A??1) and n  n equations
(the equality that needs to hold between every entry of the product A??1A and every entry of I) so
we should generically expect a solution to exist. Indeed, in the next section we will see a quantity
called the determinant, which has the property that as long as the determinant is not zero, we can
find a solution. We call such a matrix A??1 the inverse matrix. As an example, if A is the general
2  2 matrix
A =
[
a b
c d
]
; (18.1.23)
then we can see that the inverse is
1
ad ?? bc
[
d ??b
??c a
]
: (18.1.24)
We can test to see this by seeing that multiplying by the inverse given by the formula above works
in practice.
M = np.array([[1, 2], [1, 4]])
M_inv = np.array([[2, -1], [-0.5, 0.5]])
M_inv.dot(M)
array([[1., 0.],
[0., 1.]])
788 Chapter 18. Appendix: Mathematics for Deep Learning
Numerical Issues
While the inverse of a matrix is useful in theory, we must say that most of the time we do not wish
to use the matrix inverse to solve a problem in practice. In general, there are far more numerically
stable algorithms for solving linear equations like
Ax = b; (18.1.25)
than computing the inverse and multiplying to get
x = A??1b: (18.1.26)
Just as division by a small number can lead to numerical instability, so can inversion of a matrix
which is close to having low rank.
Moreover, it is common that the matrix A is sparse, which is to say that it contains only a small
number of non-zero values. If we were to explore examples, we would see that this does not mean
the inverse is sparse. Even if A was a 1 million by 1 million matrix with only 5 million non-zero
entries (and thus we need only store those 5 million), the inverse will typically have almost every
entry non-negative, requiring us to store all 1M2 entries�that is 1 trillion entries!
While we do not have time to dive all the way into the thorny numerical issues frequently encountered
when working with linear algebra, we want to provide you with some intuition about when
to proceed with caution, and generally avoiding inversion in practice is a good rule of thumb.
18.1.8 Determinant
The geometric view of linear algebra gives an intuitive way to interpret a fundamental quantity
known as the determinant. Consider the grid image from before, but now with a highlighted region
(Fig. 18.1.9).
Fig. 18.1.9: The matrix A again distorting the grid. This time, I want to draw particular attention
to what happens to the highlighted square.
Look at the highlighted square. This is a square with edges given by (0; 1) and (1; 0) and thus it
has area one. After A transforms this square, we see that it becomes a parallelogram. There is
no reason this parallelogram should have the same area that we started with, and indeed in the
specific case shown here of
A =
[
1 2
??1 3
]
; (18.1.27)
18.1. Geometry and Linear Algebraic Operations 789
it is an exercise in coordinate geometry to compute the area of this parallelogram and obtain that
the area is 5.
In general, if we have a matrix
A =
[
a b
c d
]
; (18.1.28)
we can see with some computation that the area of the resulting parallelogram is ad ?? bc. This
area is referred to as the determinant.
Let us check this quickly with some example code.
import numpy as np
np.linalg.det(np.array([[1, -1], [2, 3]]))
5.000000000000001
The eagle-eyed amongst us will notice that this expression can be zero or even negative. For the
negative term, this is a matter of convention taken generally in mathematics: if the matrix flips
the figure, we say the area is negated. Let us see now that when the determinant is zero, we learn
more.
Let us consider
B =
[
2 4
??1 ??2
]
: (18.1.29)
If we compute the determinant of this matrix, we get 2(??2)??4(??1) = 0. Given our understanding
above, this makes sense. B compresses the square from the original image down to a line segment,
which has zero area. And indeed, being compressed into a lower dimensional space is the only
way to have zero area after the transformation. Thus we see the following result is true: a matrix
A is invertible if and only if the determinant is not equal to zero.
As a final comment, imagine that we have any figure drawn on the plane. Thinking like computer
scientists, we can decompose that figure into a collection of little squares so that the area of the
figure is in essence just the number of squares in the decomposition. If we now transform that
figure by a matrix, we send each of these squares to parallelograms, each one of which has area
given by the determinant. We see that for any figure, the determinant gives the (signed) number
that a matrix scales the area of any figure.
Computing determinants for larger matrices can be laborious, but the intuition is the same. The
determinant remains the factor that n  n matrices scale n-dimensional volumes.
18.1.9 Tensors and Common Linear Algebra Operations
In Section 2.3 the concept of tensors was introduced. In this section, we will dive more deeply into
tensor contractions (the tensor equivalent of matrix multiplication), and see how it can provide a
unified view on a number of matrix and vector operations.
With matrices and vectors we knew how to multiply them to transform data. We need to have a
similar definition for tensors if they are to be useful to us. Think about matrix multiplication:
C = AB; (18.1.30)
790 Chapter 18. Appendix: Mathematics for Deep Learning
or equivalently
ci;j =
?
k
ai;kbk;j : (18.1.31)
This pattern is one we can repeat for tensors. For tensors, there is no one case of what to sum over
that can be universally chosen, so we need specify exactly which indices we want to sum over. For
instance we could consider
yil =
?
jk
xijklajk: (18.1.32)
Such a transformation is called a tensor contraction. It can represent a far more flexible family of
transformations that matrix multiplication alone.
As a often-used notational simplification, we can notice that the sum is over exactly those indices
that occur more than once in the expression, thus people often work with Einstein notation, where
the summation is implicitly taken over all repeated indices. This gives the compact expression:
yil = xijklajk: (18.1.33)
Common Examples from Linear Algebra
Let us see how many of the linear algebraic definitions we have seen before can be expressed in
this compressed tensor notation:
� v  w =
?
i viwi
� ?v?22
=
?
i vivi
� (Av)i =
?
j aijvj
� (AB)ik =
?
j aijbjk
� tr(A) =
?
i aii
In this way, we can replace a myriad of specialized notations with short tensor expressions.
Expressing in Code
Tensors may flexibly be operated on in code as well. As seen in Section 2.3, we can create tensors
as is shown below.
# Define tensors
B = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
A = np.array([[1, 2], [3, 4]])
v = np.array([1, 2])
# Print out the shapes
A.shape, B.shape, v.shape
((2, 2), (2, 2, 3), (2,))
18.1. Geometry and Linear Algebraic Operations 791
Einstein summation has been implemented directly. The indices that occurs in the Einstein summation
can be passed as a string, followed by the tensors that are being acted upon. For instance,
to implement matrix multiplication, we can consider the Einstein summation seen above
(Av = aijvj ) and strip out the indices themselves to get the implementation:
# Reimplement matrix multiplication
np.einsum("ij, j -> i", A, v), A.dot(v)
(array([ 5, 11]), array([ 5, 11]))
This is a highly flexible notation. For instance if we want to compute what would be traditionally
written as
ckl =
?
ij
bijkailvj : (18.1.34)
it can be implemented via Einstein summation as:
np.einsum("ijk, il, j -> kl", B, A, v)
array([[ 90, 126],
[102, 144],
[114, 162]])
This notation is readable and efficient for humans, however bulky if for whatever reason we need
to generate a tensor contraction programmatically. For this reason, einsum provides an alternative
notation by providing integer indices for each tensor. For example, the same tensor contraction
can also be written as:
np.einsum(B, [0, 1, 2], A, [0, 3], v, [1], [2, 3])
array([[ 90, 126],
[102, 144],
[114, 162]])
Either notation allows for concise and efficient representation of tensor contractions in code.
Summary
� Vectors can be interpreted geometrically as either points or directions in space.
� Dot products define the notion of angle to arbitrarily high-dimensional spaces.
� Hyperplanes are high-dimensional generalizations of lines and planes. They can be used to
define decision planes that are often used as the last step in a classification task.
� Matrix multiplication can be geometrically interpreted as uniform distortions of the underlying
coordinates. They represent a very restricted, but mathematically clean, way to transform
vectors.
� Linear dependence is a way to tell when a collection of vectors are in a lower dimensional
space than we would expect (say you have 3 vectors living in a 2-dimensional space). The
rank of a matrix is the size of the largest subset of its columns that are linearly independent.
792 Chapter 18. Appendix: Mathematics for Deep Learning
� When a matrix?s inverse is defined, matrix inversion allows us to find another matrix that undoes
the action of the first. Matrix inversion is useful in theory, but requires care in practice
owing to numerical instability.
� Determinants allow us to measure how much a matrix expands or contracts a space. A
nonzero determinant implies an invertible (non-singular) matrix and a zero-valued determinant
means that the matrix is non-invertible (singular).
� Tensor contractions and Einstein summation provide for a neat and clean notation for expressing
many of the computations that are seen in machine learning.
Exercises
1. What is the angle between
?v1 =
2
664
1
0
??1
2
3
775
; ?v2 =
2
664
3
1
0
1
3
775
? (18.1.35)
2. True or false:
[
1 2
0 1
]
and
[
1 ??2
0 1
]
are inverses of one another?
3. Suppose that we draw a shape in the plane with area 100m2. What is the area after transforming
the figure by the matrix
[
2 3
1 2
]
: (18.1.36)
4. Which of the following sets of vectors are linearly independent?
�
8<
:
0
@
1
0
??1
1
A;
0
@
2
1
??1
1
A;
0
@
3
1
1
1
A
9=
;
�
8<
:
0
@
3
1
1
1
A;
0
@
1
1
1
1
A;
0
@
0
0
0
1
A
9=
;
�
8<
:
0
@
1
1
0
1
A;
0
@
0
1
??1
1
A;
0
@
1
0
1
1
A
9=
;
5. Suppose that you have a matrix written as A =
[
c
d
]

[
a b
]
for some choice of values a; b; c,
and d. True or false: the determinant of such a matrix is always 0?
6. The vectors e1 =
[
1
0
]
and e2 =
[
0
1
]
are orthogonal. What is the condition on a matrix A so
that Ae1 and Ae2 are orthogonal?
7. How can you write tr(A4) in Einstein notation for an arbitrary matrix A?
Discussions245
245 https://discuss.d2l.ai/t/410
18.1. Geometry and Linear Algebraic Operations 793
18.2 Eigendecompositions
Eigenvalues are often one of the most useful notions we will encounter when studying linear algebra,
however, as a beginner, it is easy to overlook their importance. Below, we introduce eigendecomposition
and try to convey some sense of just why it is so important.
Suppose that we have a matrix A with the following entries:
A =
[
2 0
0 ??1
]
: (18.2.1)
If we apply A to any vector v = [x; y]?, we obtain a vector Av = [2x;??y]?. This has an intuitive
interpretation: stretch the vector to be twice as wide in the x-direction, and then flip it in the
y-direction.
However, there are some vectors for which something remains unchanged. Namely [1; 0]? gets
sent to [2; 0]? and [0; 1]? gets sent to [0;??1]?. These vectors are still in the same line, and the only
modification is that the matrix stretches them by a factor of 2 and ??1 respectively. We call such
vectors eigenvectors and the factor they are stretched by eigenvalues.
In general, if we can find a number  and a vector v such that
Av = v: (18.2.2)
We say that v is an eigenvector for A and  is an eigenvalue.
18.2.1 Finding Eigenvalues
Let us figure out how to find them. By subtracting off the v from both sides, and then factoring
out the vector, we see the above is equivalent to:
(A ?? I)v = 0: (18.2.3)
For (18.2.3) to happen, we see that (A ?? I) must compress some direction down to zero, hence
it is not invertible, and thus the determinant is zero. Thus, we can find the eigenvalues by finding
for what  is det(A ?? I) = 0. Once we find the eigenvalues, we can solve Av = v to find the
associated eigenvector(s).
An Example
Let us see this with a more challenging matrix
A =
[
2 1
2 3
]
: (18.2.4)
If we consider det(A??I) = 0, we see this is equivalent to the polynomial equation 0 = (2??)(3??
)??2 = (4??)(1??). Thus, two eigenvalues are 4 and 1. To find the associated vectors, we then
need to solve
[
2 1
2 3
] [
x
y
]
=
[
x
y
]
and
[
2 1
2 3
] [
x
y
]
=
[
4x
4y
]
: (18.2.5)
We can solve this with the vectors [1;??1]? and [1; 2]? respectively.
We can check this in code using the built-in numpy.linalg.eig routine.
794 Chapter 18. Appendix: Mathematics for Deep Learning
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
import numpy as np
np.linalg.eig(np.array([[2, 1], [2, 3]]))
(array([1., 4.]),
array([[-0.70710678, -0.4472136 ],
[ 0.70710678, -0.89442719]]))
Note that numpy normalizes the eigenvectors to be of length one, whereas we took ours to be of
arbitrary length. Additionally, the choice of sign is arbitrary. However, the vectors computed are
parallel to the ones we found by hand with the same eigenvalues.
18.2.2 Decomposing Matrices
Let us continue the previous example one step further. Let
W =
[
1 1
??1 2
]
; (18.2.6)
be the matrix where the columns are the eigenvectors of the matrix A. Let
 =
[
1 0
0 4
]
; (18.2.7)
be the matrix with the associated eigenvalues on the diagonal. Then the definition of eigenvalues
and eigenvectors tells us that
AW = W: (18.2.8)
The matrix W is invertible, so we may multiply both sides by W??1 on the right, we see that we
may write
A = WW??1: (18.2.9)
In the next section we will see some nice consequences of this, but for now we need only know
that such a decomposition will exist as long as we can find a full collection of linearly independent
eigenvectors (so thatW is invertible).
18.2.3 Operations on Eigendecompositions
One nice thing about eigendecompositions (18.2.9) is that we can write many operations we usually
encounter cleanly in terms of the eigendecomposition. As a first example, consider:
An =
zn t}im|es{
A    A =
z n t}im|es {
(WW??1)    (WW??1) = W
zn t}im|es{
  W??1 = WnW??1:
(18.2.10)
This tells us that for any positive power of a matrix, the eigendecomposition is obtained by just
raising the eigenvalues to the same power. The same can be shown for negative powers, so if we
want to invert a matrix we need only consider
A??1 = W
??1W??1; (18.2.11)
18.2. Eigendecompositions 795
or in other words, just invert each eigenvalue. This will work as long as each eigenvalue is nonzero,
so we see that invertible is the same as having no zero eigenvalues.
Indeed, additional work can show that if 1; : : : ; n are the eigenvalues of a matrix, then the determinant
of that matrix is
det(A) = 1    n; (18.2.12)
or the product of all the eigenvalues. This makes sense intuitively because whatever stretching
W does,W??1 undoes it, so in the end the only stretching that happens is by multiplication by the
diagonal matrix , which stretches volumes by the product of the diagonal elements.
Finally, recall that the rank was the maximum number of linearly independent columns of your
matrix. By examining the eigendecomposition closely, we can see that the rank is the same as the
number of non-zero eigenvalues of A.
The examples could continue, but hopefully the point is clear: eigendecomposition can simplify
many linear-algebraic computations and is a fundamental operation underlying many numerical
algorithms and much of the analysis that we do in linear algebra.
18.2.4 Eigendecompositions of Symmetric Matrices
It is not always possible to find enough linearly independent eigenvectors for the above process
to work. For instance the matrix
A =
[
1 1
0 1
]
; (18.2.13)
has only a single eigenvector, namely (1; 0)?. To handle such matrices, we require more advanced
techniques than we can cover (such as the Jordan Normal Form, or Singular Value Decomposition).
We will often need to restrict our attention to those matrices where we can guarantee the existence
of a full set of eigenvectors.
The most commonly encountered family are the symmetric matrices, which are those matrices
where A = A?. In this case, we may takeW to be an orthogonal matrix�a matrix whose columns
are all length one vectors that are at right angles to one another, where W? = W??1�and all the
eigenvalues will be real.
Thus, in this special case, we can write (18.2.9) as
A = WW?
: (18.2.14)
18.2.5 Gershgorin Circle Theorem
Eigenvalues are often difficult to reason with intuitively. If presented an arbitrary matrix, there is
little that can be said about what the eigenvalues are without computing them. There is, however,
one theorem that can make it easy to approximate well if the largest values are on the diagonal.
Let A = (aij) be any square matrix (nn). We will define ri =
?
j?=i
jaij j. Let Di represent the disc
in the complex plane with center aii radius ri. Then, every eigenvalue of A is contained in one of
the Di.
796 Chapter 18. Appendix: Mathematics for Deep Learning
This can be a bit to unpack, so let us look at an example.
Consider the matrix:
A =
2
664
1:0 0:1 0:1 0:1
0:1 3:0 0:2 0:3
0:1 0:2 5:0 0:5
0:1 0:3 0:5 9:0
3
775
: (18.2.15)
We have r1 = 0:3, r2 = 0:6, r3 = 0:8 and r4 = 0:9. The matrix is symmetric, so all eigenvalues are
real. This means that all of our eigenvalues will be in one of the ranges of
[a11 ?? r1; a11 + r1] = [0:7; 1:3]; (18.2.16)
[a22 ?? r2; a22 + r2] = [2:4; 3:6]; (18.2.17)
[a33 ?? r3; a33 + r3] = [4:2; 5:8]; (18.2.18)
[a44 ?? r4; a44 + r4] = [8:1; 9:9]: (18.2.19)
Performing the numerical computation shows that the eigenvalues are approximately 0:99, 2:97,
4:95, 9:08, all comfortably inside the ranges provided.
A = np.array([[1.0, 0.1, 0.1, 0.1],
[0.1, 3.0, 0.2, 0.3],
[0.1, 0.2, 5.0, 0.5],
[0.1, 0.3, 0.5, 9.0]])
v, _ = np.linalg.eig(A)
v
array([9.08033648, 0.99228545, 4.95394089, 2.97343718])
In this way, eigenvalues can be approximated, and the approximations will be fairly accurate in
the case that the diagonal is significantly larger than all the other elements.
It is a small thing, but with a complex and subtle topic like eigendecomposition, it is good to get
any intuitive grasp we can.
18.2.6 A Useful Application: The Growth of Iterated Maps
Now that we understand what eigenvectors are in principle, let us see how they can be used to
provide a deep understanding of a problem central to neural network behavior: proper weight
initialization.
18.2. Eigendecompositions 797
Eigenvectors as Long Term Behavior
The full mathematical investigation of the initialization of deep neural networks is beyond the
scope of the text, but we can see a toy version here to understand how eigenvalues can help us see
how these models work. As we know, neural networks operate by interspersing layers of linear
transformations with non-linear operations. For simplicity here, we will assume that there is no
non-linearity, and that the transformation is a single repeated matrix operation A, so that the
output of our model is
vout = A  A    Avin = ANvin: (18.2.20)
When these models are initialized, A is taken to be a random matrix with Gaussian entries, so let
us make one of those. To be concrete, we start with a mean zero, variance one Gaussian distributed
5  5 matrix.
np.random.seed(8675309)
k = 5
A = np.random.randn(k, k)
A
array([[ 0.58902366, 0.73311856, -1.1621888 , -0.55681601, -0.77248843],
[-0.16822143, -0.41650391, -1.37843129, 0.74925588, 0.17888446],
[ 0.69401121, -1.9780535 , -0.83381434, 0.56437344, 0.31201299],
[-0.87334496, 0.15601291, -0.38710108, -0.23920821, 0.88850104],
[ 1.29385371, -0.76774106, 0.20131613, 0.91800842, 0.38974115]])
Behavior on Random Data
For simplicity in our toy model, we will assume that the data vector we feed in vin is a random five
dimensional Gaussian vector. Let us think about what we want to have happen. For context, lets
think of a generic ML problem, where we are trying to turn input data, like an image, into a prediction,
like the probability the image is a picture of a cat. If repeated application of A stretches a
random vector out to be very long, then small changes in input will be amplified into large changes
in output�tiny modifications of the input image would lead to vastly different predictions. This
does not seem right!
On the flip side, if A shrinks random vectors to be shorter, then after running through many layers,
the vector will essentially shrink to nothing, and the output will not depend on the input. This is
also clearly not right either!
We need to walk the narrow line between growth and decay to make sure that our output changes
depending on our input, but not much!
Let us see what happens when we repeatedly multiply our matrix A against a random input vector,
and keep track of the norm.
# Calculate the sequence of norms after repeatedly applying `A`
v_in = np.random.randn(k, 1)
norm_list = [np.linalg.norm(v_in)]
for i in range(1, 100):
(continues on next page)
798 Chapter 18. Appendix: Mathematics for Deep Learning
(continued from previous page)
v_in = A.dot(v_in)
norm_list.append(np.linalg.norm(v_in))
d2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')
The norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a pattern.
# Compute the scaling factor of the norms
norm_ratio_list = []
for i in range(1, 100):
norm_ratio_list.append(norm_list[i]/norm_list[i - 1])
d2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
If we look at the last portion of the above computation, we see that the random vector is stretched
by a factor of 1.974459321485[...], where the portion at the end shifts a little, but the stretching
factor is stable.
18.2. Eigendecompositions 799
Relating Back to Eigenvectors
We have seen that eigenvectors and eigenvalues correspond to the amount something is stretched,
but that was for specific vectors, and specific stretches. Let us take a look at what they are for A. A
bit of a caveat here: it turns out that to see them all, we will need to go to complex numbers. You
can think of these as stretches and rotations. By taking the norm of the complex number (square
root of the sums of squares of real and imaginary parts) we can measure that stretching factor.
Let us also sort them.
# Compute the eigenvalues
eigs = np.linalg.eigvals(A).tolist()
norm_eigs = [np.absolute(x) for x in eigs]
norm_eigs.sort()
print(f'norms of eigenvalues: {norm_eigs}')
norms of eigenvalues: [0.8786205280381857, 1.2757952665062624, 1.4983381517710659, 1.
,!4983381517710659, 1.974459321485074]
An Observation
We see something a bit unexpected happening here: that number we identified before for the long
term stretching of our matrix A applied to a random vector is exactly (accurate to thirteen decimal
places!) the largest eigenvalue of A. This is clearly not a coincidence!
But, if we now think about what is happening geometrically, this starts to make sense. Consider a
random vector. This random vector points a little in every direction, so in particular, it points at
least a little bit in the same direction as the eigenvector of A associated with the largest eigenvalue.
This is so important that it is called the principle eigenvalue and principle eigenvector. After applying
A, our random vector gets stretched in every possible direction, as is associated with every
possible eigenvector, but it is stretched most of all in the direction associated with this principle
eigenvector. What this means is that after apply in A, our random vector is longer, and points
in a direction closer to being aligned with the principle eigenvector. After applying the matrix
many times, the alignment with the principle eigenvector becomes closer and closer until, for
all practical purposes, our random vector has been transformed into the principle eigenvector!
Indeed this algorithm is the basis for what is known as the power iteration for finding the largest
eigenvalue and eigenvector of a matrix. For details see, for example, (VanLoan & Golub, 1983).
Fixing the Normalization
Now, from above discussions, we concluded that we do not want a random vector to be stretched
or squished at all, we would like random vectors to stay about the same size throughout the entire
process. To do so, we now rescale our matrix by this principle eigenvalue so that the largest
eigenvalue is instead now just one. Let us see what happens in this case.
# Rescale the matrix `A`
A /= norm_eigs[-1]
# Do the same experiment again
v_in = np.random.randn(k, 1)
(continues on next page)
800 Chapter 18. Appendix: Mathematics for Deep Learning
(continued from previous page)
norm_list = [np.linalg.norm(v_in)]
for i in range(1, 100):
v_in = A.dot(v_in)
norm_list.append(np.linalg.norm(v_in))
d2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')
We can also plot the ratio between consecutive norms as before and see that indeed it stabilizes.
# Also plot the ratio
norm_ratio_list = []
for i in range(1, 100):
norm_ratio_list.append(norm_list[i]/norm_list[i-1])
d2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')
18.2. Eigendecompositions 801
18.2.7 Conclusions
We now see exactly what we hoped for! After normalizing the matrices by the principle eigenvalue,
we see that the random data does not explode as before, but rather eventually equilibrates to a
specific value. It would be nice to be able to do these things from first principles, and it turns out
that if we look deeply at the mathematics of it, we can see that the largest eigenvalue of a large
random matrix with independent mean zero, p variance one Gaussian entries is on average about
n, or in our case
p
5  2:2, due to a fascinating fact known as the circular law (Ginibre, 1965).
The relationship between the eigenvalues (and a related object called singular values) of random
matrices has been shown to have deep connections to proper initialization of neural networks as
was discussed in (Pennington et al., 2017) and subsequent works.
Summary
� Eigenvectors are vectors which are stretched by a matrix without changing direction.
� Eigenvalues are the amount that the eigenvectors are stretched by the application of the
matrix.
� The eigendecomposition of a matrix can allow for many operations to be reduced to operations
on the eigenvalues.
� The Gershgorin Circle Theorem can provide approximate values for the eigenvalues of a
matrix.
� The behavior of iterated matrix powers depends primarily on the size of the largest eigenvalue.
This understanding has many applications in the theory of neural network initialization.
Exercises
1. What are the eigenvalues and eigenvectors of
A =
[
2 1
1 2
]
? (18.2.21)
2. What are the eigenvalues and eigenvectors of the following matrix, and what is strange about
this example compared to the previous one?
A =
[
2 1
0 2
]
: (18.2.22)
3. Without computing the eigenvalues, is it possible that the smallest eigenvalue of the following
matrix is less that 0:5? Note: this problem can be done in your head.
A =
2
664
3:0 0:1 0:3 1:0
0:1 1:0 0:1 0:2
0:3 0:1 5:0 0:0
1:0 0:2 0:0 1:8
3
775
: (18.2.23)
Discussions246
246 https://discuss.d2l.ai/t/411
802 Chapter 18. Appendix: Mathematics for Deep Learning
18.3 Single Variable Calculus
In Section 2.4, we saw the basic elements of differential calculus. This section takes a deeper
dive into the fundamentals of calculus and how we can understand and apply it in the context of
machine learning.
18.3.1 Differential Calculus
Differential calculus is fundamentally the study of how functions behave under small changes. To
see why this is so core to deep learning, let us consider an example.
Suppose that we have a deep neural network where the weights are, for convenience, concatenated
into a single vector w = (w1; : : : ;wn). Given a training dataset, we consider the loss of our neural
network on this dataset, which we will write as L(w).
This function is extraordinarily complex, encoding the performance of all possible models of the
given architecture on this dataset, so it is nearly impossible to tell what set of weights w will minimize
the loss. Thus, in practice, we often start by initializing our weights randomly, and then
iteratively take small steps in the direction which makes the loss decrease as rapidly as possible.
The question then becomes something that on the surface is no easier: how do we find the direction
which makes the weights decrease as quickly as possible? To dig into this, let us first examine
the case with only a single weight: L(w) = L(x) for a single real value x.
Let us take x and try to understand what happens when we change it by a small amount to x + ?.
If you wish to be concrete, think a number like ? = 0:0000001. To help us visualize what happens,
let us graph an example function, f(x) = sin(xx), over the [0; 3].
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mxnet import np, npx
npx.set_np()
# Plot a function in a normal range
x_big = np.arange(0.01, 3.01, 0.01)
ys = np.sin(x_big**x_big)
d2l.plot(x_big, ys, 'x', 'f(x)')
18.3. Single Variable Calculus 803
At this large scale, the function?s behavior is not simple. However, if we reduce our range to something
smaller like [1:75; 2:25], we see that the graph becomes much simpler.
# Plot a the same function in a tiny range
x_med = np.arange(1.75, 2.25, 0.001)
ys = np.sin(x_med**x_med)
d2l.plot(x_med, ys, 'x', 'f(x)')
Taking this to an extreme, if we zoom into a tiny segment, the behavior becomes far simpler: it is
just a straight line.
# Plot a the same function in a tiny range
x_small = np.arange(2.0, 2.01, 0.0001)
ys = np.sin(x_small**x_small)
d2l.plot(x_small, ys, 'x', 'f(x)')
This is the key observation of single variable calculus: the behavior of familiar functions can be
modeled by a line in a small enough range. This means that for most functions, it is reasonable to
expect that as we shift the x value of the function by a little bit, the output f(x) will also be shifted
by a little bit. The only question we need to answer is, �How large is the change in the output
compared to the change in the input? Is it half as large? Twice as large?�
804 Chapter 18. Appendix: Mathematics for Deep Learning
Thus, we can consider the ratio of the change in the output of a function for a small change in the
input of the function. We can write this formally as
L(x + ?) ?? L(x)
(x + ?) ?? x
=
L(x + ?) ?? L(x)
?
: (18.3.1)
This is already enough to start to play around with in code. For instance, suppose that we know
that L(x) = x2 +1701(x??4)3, then we can see how large this value is at the point x = 4 as follows.
# Define our function
def L(x):
return x**2 + 1701*(x-4)**3
# Print the difference divided by epsilon for several epsilon
for epsilon in [0.1, 0.001, 0.0001, 0.00001]:
print(f'epsilon = {epsilon:.5f} -> {(L(4+epsilon) - L(4)) / epsilon:.5f}')
epsilon = 0.10000 -> 25.11000
epsilon = 0.00100 -> 8.00270
epsilon = 0.00010 -> 8.00012
epsilon = 0.00001 -> 8.00001
Now, if we are observant, we will notice that the output of this number is suspiciously close to
8. Indeed, if we decrease ?, we will see value becomes progressively closer to 8. Thus we may
conclude, correctly, that the value we seek (the degree a change in the input changes the output)
should be 8 at the point x = 4. The way that a mathematician encodes this fact is
lim
?!0
L(4 + ?) ?? L(4)
?
= 8: (18.3.2)
As a bit of a historical digression: in the first few decades of neural network research, scientists
used this algorithm (the method of finite differences) to evaluate how a loss function changed under
small perturbation: just change the weights and see how the loss changed. This is computationally
inefficient, requiring two evaluations of the loss function to see how a single change of one variable
influenced the loss. If we tried to do this with even a paltry few thousand parameters, it would
require several thousand evaluations of the network over the entire dataset! It was not solved until
1986 that the backpropagation algorithm introduced in (Rumelhart et al., 1988) provided a way to
calculate how any change of the weights together would change the loss in the same computation
time as a single prediction of the network over the dataset.
Back in our example, this value 8 is different for different values of x, so it makes sense to define it
as a function of x. More formally, this value dependent rate of change is referred to as the derivative
which is written as
df
dx
(x) = lim
?!0
f(x + ?) ?? f(x)
?
: (18.3.3)
Different texts will use different notations for the derivative. For instance, all of the below notations
indicate the same thing:
df
dx
=
d
dx
f = f
?
= ?xf = Dxf = fx: (18.3.4)
Most authors will pick a single notation and stick with it, however even that is not guaranteed. It
is best to be familiar with all of these. We will use the notation df
dx throughout this text, unless
18.3. Single Variable Calculus 805
we want to take the derivative of a complex expression, in which case we will use d
dxf to write
expressions like
d
dx
[
x4 + cos
(
x2 + 1
2x ?? 1
)]
: (18.3.5)
Oftentimes, it is intuitively useful to unravel the definition of derivative (18.3.3) again to see how
a function changes when we make a small change of x:
df
dx
(x) = lim
?!0
f(x + ?) ?? f(x)
?
=) df
dx
(x)  f(x + ?) ?? f(x)
?
=) ?
df
dx
(x)  f(x + ?) ?? f(x)
=) f(x + ?)  f(x) + ?
df
dx
(x):
(18.3.6)
The last equation is worth explicitly calling out. It tells us that if you take any function and change
the input by a small amount, the output would change by that small amount scaled by the derivative.
In this way, we can understand the derivative as the scaling factor that tells us how large of change
we get in the output from a change in the input.
18.3.2 Rules of Calculus
We now turn to the task of understanding how to compute the derivative of an explicit function. A
full formal treatment of calculus would derive everything from first principles. We will not indulge
in this temptation here, but rather provide an understanding of the common rules encountered.
Common Derivatives
As was seen in Section 2.4, when computing derivatives one can oftentimes use a series of rules
to reduce the computation to a few core functions. We repeat them here for ease of reference.
� Derivative of constants. d
dx c = 0.
� Derivative of linear functions. d
dx (ax) = a.
� Power rule. d
dxxn = nxn??1.
� Derivative of exponentials. d
dxex = ex.
� Derivative of the logarithm. d
dx log(x) = 1
x .
Derivative Rules
If every derivative needed to be separately computed and stored in a table, differential calculus
would be near impossible. It is a gift of mathematics that we can generalize the above derivatives
and compute more complex derivatives like finding the derivative of f(x) = log
(
1 + (x ?? 1)10
)
. As
was mentioned in Section 2.4, the key to doing so is to codify what happens when we take functions
and combine them in various ways, most importantly: sums, products, and compositions.
� Sum rule. d
dx (g(x) + h(x)) = dg
dx (x) + dh
dx (x).
806 Chapter 18. Appendix: Mathematics for Deep Learning
� Product rule. d
dx (g(x)  h(x)) = g(x) dh
dx (x) + dg
dx (x)h(x).
� Chain rule. d
dxg(h(x)) = dg
dh (h(x))  dh
dx (x).
Let us see how we may use (18.3.6) to understand these rules. For the sum rule, consider following
chain of reasoning:
f(x + ?) = g(x + ?) + h(x + ?)
 g(x) + ?
dg
dx
(x) + h(x) + ?
dh
dx
(x)
= g(x) + h(x) + ?
(
dg
dx
(x) +
dh
dx
(x)
)
= f(x) + ?
(
dg
dx
(x) +
dh
dx
(x)
)
:
(18.3.7)
By comparing this result with the fact that f(x + ?)  f(x) + ? df
dx (x), we see that df
dx (x) = dg
dx (x) +
dh
dx (x) as desired. The intuition here is: when we change the input x, g and h jointly contribute to
the change of the output by dg
dx (x) and dh
dx (x).
The product is more subtle, and will require a new observation about how to work with these
expressions. We will begin as before using (18.3.6):
f(x + ?) = g(x + ?)  h(x + ?)

(
g(x) + ?
dg
dx
(x)
)

(
h(x) + ?
dh
dx
(x)
)
= g(x)  h(x) + ?
(
g(x)
dh
dx
(x) +
dg
dx
(x)h(x)
)
+ ?2 dg
dx
(x)
dh
dx
(x)
= f(x) + ?
(
g(x)
dh
dx
(x) +
dg
dx
(x)h(x)
)
+ ?2 dg
dx
(x)
dh
dx
(x):
(18.3.8)
This resembles the computation done above, and indeed we see our answer ( df
dx (x) = g(x) dh
dx (x)+
dg
dx (x)h(x)) sitting next to ?, but there is the issue of that term of size ?2. We will refer to this as a
higher-order term, since the power of ?2 is higher than the power of ?1. We will see in a later section
that we will sometimes want to keep track of these, however for now observe that if ? = 0:0000001,
then ?2 = 0:0000000000001, which is vastly smaller. As we send ? ! 0, we may safely ignore the
higher order terms. As a general convention in this appendix, we will use �� to denote that the
two terms are equal up to higher order terms. However, if we wish to be more formal we may
examine the difference quotient
f(x + ?) ?? f(x)
?
= g(x)
dh
dx
(x) +
dg
dx
(x)h(x) + ?
dg
dx
(x)
dh
dx
(x); (18.3.9)
and see that as we send ? ! 0, the right hand term goes to zero as well.
Finally, with the chain rule, we can again progress as before using (18.3.6) and see that
f(x + ?) = g(h(x + ?))
 g
(
h(x) + ?
dh
dx
(x)
)
 g(h(x)) + ?
dh
dx
(x)
dg
dh
(h(x))
= f(x) + ?
dg
dh
(h(x))
dh
dx
(x);
(18.3.10)
18.3. Single Variable Calculus 807
where in the second line we view the function g as having its input (h(x)) shifted by the tiny quantity
? dh
dx (x).
These rule provide us with a flexible set of tools to compute essentially any expression desired.
For instance,
d
dx
[
log
(
1 + (x ?? 1)10)]
=
(
1 + (x ?? 1)10)??1 d
dx
[
1 + (x ?? 1)10]
=
(
1 + (x ?? 1)10)??1
(
d
dx
[1] +
d
dx
[(x ?? 1)10]
)
=
(
1 + (x ?? 1)10)??1
(
0 + 10(x ?? 1)9 d
dx
[x ?? 1]
)
= 10
(
1 + (x ?? 1)10)??1
(x ?? 1)9
=
10(x ?? 1)9
1 + (x ?? 1)10 :
(18.3.11)
Where each line has used the following rules:
1. The chain rule and derivative of logarithm.
2. The sum rule.
3. The derivative of constants, chain rule, and power rule.
4. The sum rule, derivative of linear functions, derivative of constants.
Two things should be clear after doing this example:
1. Any function we can write down using sums, products, constants, powers, exponentials, and
logarithms can have its derivate computed mechanically by following these rules.
2. Having a human follow these rules can be tedious and error prone!
Thankfully, these two facts together hint towards a way forward: this is a perfect candidate for
mechanization! Indeed backpropagation, which we will revisit later in this section, is exactly that.
Linear Approximation
When working with derivatives, it is often useful to geometrically interpret the approximation
used above. In particular, note that the equation
f(x + ?)  f(x) + ?
df
dx
(x); (18.3.12)
approximates the value of f by a line which passes through the point (x; f(x)) and has slope df
dx (x).
In this way we say that the derivative gives a linear approximation to the function f, as illustrated
below:
# Compute sin
xs = np.arange(-np.pi, np.pi, 0.01)
plots = [np.sin(xs)]
# Compute some linear approximations. Use d(sin(x)) / dx = cos(x)
for x0 in [-1.5, 0, 2]:
plots.append(np.sin(x0) + (xs - x0) * np.cos(x0))
d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
808 Chapter 18. Appendix: Mathematics for Deep Learning
Higher Order Derivatives
Let us now do something that may on the surface seem strange. Take a function f and compute
the derivative df
dx . This gives us the rate of change of f at any point.
However, the derivative, df
dx , can be viewed as a function itself, so nothing stops us from computing
the derivative of df
dx to get d2f
dx2 = df
dx
(
df
dx
)
. We will call this the second derivative of f. This
function is the rate of change of the rate of change of f, or in other words, how the rate of change
is changing. We may apply the derivative any number of times to obtain what is called the n-th
derivative. To keep the notation clean, we will denote the n-th derivative as
f(n)(x) =
dnf
dxn =
(
d
dx
)n
f: (18.3.13)
Let us try to understand why this is a useful notion. Below, we visualize f(2)(x), f(1)(x), and f(x).
First, consider the case that the second derivative f(2)(x) is a positive constant. This means that
the slope of the first derivative is positive. As a result, the first derivative f(1)(x) may start out
negative, becomes zero at a point, and then becomes positive in the end. This tells us the slope of
our original function f and therefore, the function f itself decreases, flattens out, then increases.
In other words, the function f curves up, and has a single minimum as is shown in Fig. 18.3.1.
Fig. 18.3.1: If we assume the second derivative is a positive constant, then the fist derivative in
increasing, which implies the function itself has a minimum.
Second, if the second derivative is a negative constant, that means that the first derivative is decreasing.
This implies the first derivative may start out positive, becomes zero at a point, and then
18.3. Single Variable Calculus 809
becomes negative. Hence, the function f itself increases, flattens out, then decreases. In other
words, the function f curves down, and has a single maximum as is shown in Fig. 18.3.2.
Fig. 18.3.2: If we assume the second derivative is a negative constant, then the fist derivative in
decreasing, which implies the function itself has a maximum.
Third, if the second derivative is a always zero, then the first derivative will never change�it is
constant! This means that f increases (or decreases) at a fixed rate, and f is itself a straight line
as is shown in Fig. 18.3.3.
Fig. 18.3.3: If we assume the second derivative is zero, then the fist derivative is constant, which
implies the function itself is a straight line.
To summarize, the second derivative can be interpreted as describing the way that the function f
curves. A positive second derivative leads to a upwards curve, while a negative second derivative
means that f curves downwards, and a zero second derivative means that f does not curve at all.
Let us take this one step further. Consider the function g(x) = ax2 +bx+c. We can then compute
that
dg
dx
(x) = 2ax + b
d2g
dx2 (x) = 2a:
(18.3.14)
If we have some original function f(x) in mind, we may compute the first two derivatives and
find the values for a; b, and c that make them match this computation. Similarly to the previous
section where we saw that the first derivative gave the best approximation with a straight line,
this construction provides the best approximation by a quadratic. Let us visualize this for f(x) =
sin(x).
810 Chapter 18. Appendix: Mathematics for Deep Learning
# Compute sin
xs = np.arange(-np.pi, np.pi, 0.01)
plots = [np.sin(xs)]
# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)
for x0 in [-1.5, 0, 2]:
plots.append(np.sin(x0) + (xs - x0) * np.cos(x0) -
(xs - x0)**2 * np.sin(x0) / 2)
d2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])
We will extend this idea to the idea of a Taylor series in the next section.
Taylor Series
The Taylor series provides a method to approximate the function f(x) if we are given values for the
first n derivatives at a point x0, i.e.,
{
f(x0); f(1)(x0); f(2)(x0); : : : ; f(n)(x0)
}
. The idea will be to find
a degree n polynomial that matches all the given derivatives at x0.
We saw the case of n = 2 in the previous section and a little algebra shows this is
f(x)  1
2
d2f
dx2 (x0)(x ?? x0)2 +
df
dx
(x0)(x ?? x0) + f(x0): (18.3.15)
As we can see above, the denominator of 2 is there to cancel out the 2 we get when we take two
derivatives of x2, while the other terms are all zero. Same logic applies for the first derivative and
the value itself.
If we push the logic further to n = 3, we will conclude that
f(x) 
d3f
dx3 (x0)
6
(x ?? x0)3 +
d2f
dx2 (x0)
2
(x ?? x0)2 +
df
dx
(x0)(x ?? x0) + f(x0): (18.3.16)
where the 6 = 3  2 = 3! comes from the constant we get in front if we take three derivatives of
x3.
Furthermore, we can get a degree n polynomial by
Pn(x) =
?n
i=0
f(i)(x0)
i!
(x ?? x0)i: (18.3.17)
18.3. Single Variable Calculus 811
where the notation
f(n)(x) =
dnf
dxn =
(
d
dx
)n
f: (18.3.18)
Indeed, Pn(x) can be viewed as the best n-th degree polynomial approximation to our function
f(x).
While we are not going to dive all the way into the error of the above approximations, it is worth
mentioning the infinite limit. In this case, for well behaved functions (known as real analytic
functions) like cos(x) or ex, we can write out the infinite number of terms and approximate the
exactly same function
f(x) =
1?
n=0
f(n)(x0)
n!
(x ?? x0)n: (18.3.19)
Take f(x) = ex as am example. Since ex is its own derivative, we know that f(n)(x) = ex. Therefore,
ex can be reconstructed by taking the Taylor series at x0 = 0, i.e.,
ex =
1?
n=0
xn
n!
= 1 + x +
x2
2
+
x3
6
+    : (18.3.20)
Let us see how this works in code and observe how increasing the degree of the Taylor approximation
brings us closer to the desired function ex.
# Compute the exponential function
xs = np.arange(0, 3, 0.01)
ys = np.exp(xs)
# Compute a few Taylor series approximations
P1 = 1 + xs
P2 = 1 + xs + xs**2 / 2
P5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120
d2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[
"Exponential", "Degree 1 Taylor Series", "Degree 2 Taylor Series",
"Degree 5 Taylor Series"])
Taylor series have two primary applications:
812 Chapter 18. Appendix: Mathematics for Deep Learning
1. Theoretical applications: Often when we try to understand a too complex function, using Taylor
series enables us to turn it into a polynomial that we can work with directly.
2. Numerical applications: Some functions like ex or cos(x) are difficult for machines to compute.
They can store tables of values at a fixed precision (and this is often done), but it still
leaves open questions like �What is the 1000-th digit of cos(1)?� Taylor series are often helpful
to answer such questions.
Summary
� Derivatives can be used to express how functions change when we change the input by a
small amount.
� Elementary derivatives can be combined using derivative rules to create arbitrarily complex
derivatives.
� Derivatives can be iterated to get second or higher order derivatives. Each increase in order
provides more fine grained information on the behavior of the function.
� Using information in the derivatives of a single data example, we can approximate well behaved
functions by polynomials obtained from the Taylor series.
Exercises
1. What is the derivative of x3 ?? 4x + 1?
2. What is the derivative of log( 1
x )?
3. True or False: If f?(x) = 0 then f has a maximum or minimum at x?
4. Where is the minimum of f(x) = x log(x) for x  0 (where we assume that f takes the
limiting value of 0 at f(0))?
Discussions247
18.4 Multivariable Calculus
Now that we have a fairly strong understanding of derivatives of a function of a single variable, let
us return to our original question where we were considering a loss function of potentially billions
of weights.
247 https://discuss.d2l.ai/t/412
18.4. Multivariable Calculus 813
18.4.1 Higher-Dimensional Differentiation
What Section 18.3 tells us is that if we change a single one of these billions of weights leaving
every other one fixed, we know what will happen! This is nothing more than a function of a single
variable, so we can write
L(w1 + ?1;w2; : : : ;wN)  L(w1;w2; : : : ;wN) + ?1
d
dw1
L(w1;w2; : : : ;wN): (18.4.1)
We will call the derivative in one variable while fixing the other the partial derivative, and we will
use the notation @
@w1 for the derivative in (18.4.1).
Now, let us take this and change w2 a little bit to w2 + ?2:
L(w1 + ?1;w2 + ?2; : : : ;wN)  L(w1;w2 + ?2; : : : ;wN) + ?1
@
@w1
L(w1;w2 + ?2; : : : ;wN)
 L(w1;w2; : : : ;wN)
+ ?2
@
@w2
L(w1;w2; : : : ;wN)
+ ?1
@
@w1
L(w1;w2; : : : ;wN)
+ ?1?2
@
@w2
@
@w1
L(w1;w2; : : : ;wN)
 L(w1;w2; : : : ;wN)
+ ?2
@
@w2
L(w1;w2; : : : ;wN)
+ ?1
@
@w1
L(w1;w2; : : : ;wN):
(18.4.2)
We have again used the idea that ?1?2 is a higher order term that we can discard in the same way
we could discard ?2 in the previous section, along with what we saw in (18.4.1). By continuing in
this manner, we may write that
L(w1 + ?1;w2 + ?2; : : : ;wN + ?N)  L(w1;w2; : : : ;wN) +
?
i
?i
@
@wi
L(w1;w2; : : : ;wN): (18.4.3)
This may look like a mess, but we can make this more familiar by noting that the sum on the right
looks exactly like a dot product, so if we let
? = [?1; : : : ; ?N]
? and ?xL =
[
@L
@x1
; : : : ;
@L
@xN
]?
; (18.4.4)
then
L(w + ?)  L(w) + ?  ?wL(w): (18.4.5)
We will call the vector ?wL the gradient of L.
Equation (18.4.5) is worth pondering for a moment. It has exactly the format that we encountered
in one dimension, just we have converted everything to vectors and dot products. It allows us to
tell approximately how the function L will change given any perturbation to the input. As we will
see in the next section, this will provide us with an important tool in understanding geometrically
how we can learn using information contained in the gradient.
814 Chapter 18. Appendix: Mathematics for Deep Learning
But first, let us see this approximation at work with an example. Suppose that we are working with
the function
f(x; y) = log(ex + ey) with gradient ?f(x; y) =
[
ex
ex + ey ;
ey
ex + ey
]
: (18.4.6)
If we look at a point like (0; log(2)), we see that
f(x; y) = log(3) with gradient ?f(x; y) =
[
1
3
;
2
3
]
: (18.4.7)
Thus, if we want to approximate f at (?1; log(2) + ?2), we see that we should have the specific
instance of (18.4.5):
f(?1; log(2) + ?2)  log(3) +
1
3
?1 +
2
3
?2: (18.4.8)
We can test this in code to see how good the approximation is.
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mpl_toolkits import mplot3d
from mxnet import autograd, np, npx
npx.set_np()
def f(x, y):
return np.log(np.exp(x) + np.exp(y))
def grad_f(x, y):
return np.array([np.exp(x) / (np.exp(x) + np.exp(y)),
np.exp(y) / (np.exp(x) + np.exp(y))])
epsilon = np.array([0.01, -0.03])
grad_approx = f(0, np.log(2)) + epsilon.dot(grad_f(0, np.log(2)))
true_value = f(0 + epsilon[0], np.log(2) + epsilon[1])
f'approximation: {grad_approx}, true Value: {true_value}'
'approximation: 1.0819457, true Value: 1.0821242'
18.4.2 Geometry of Gradients and Gradient Descent
Consider the again (18.4.5):
L(w + ?)  L(w) + ?  ?wL(w): (18.4.9)
Let us suppose that I want to use this to help minimize our loss L. Let us understand geometrically
the algorithm of gradient descent first described in Section 2.5. What we will do is the following:
1. Start with a random choice for the initial parameters w.
2. Find the direction v that makes L decrease the most rapidly at w.
3. Take a small step in that direction: w ! w + ?v.
4. Repeat.
18.4. Multivariable Calculus 815
The only thing we do not know exactly how to do is to compute the vector v in the second step.
We will call such a direction the direction of steepest descent. Using the geometric understanding of
dot products from Section 18.1, we see that we can rewrite (18.4.5) as
L(w + v)  L(w) + v  ?wL(w) = L(w) + ??wL(w)? cos(): (18.4.10)
Note that we have taken our direction to have length one for convenience, and used  for the angle
between v and?wL(w). If we want to find the direction that decreases L as rapidly as possible, we
want to make this expression as negative as possible. The only way the direction we pick enters
into this equation is through cos(), and thus we wish to make this cosine as negative as possible.
Now, recalling the shape of cosine, we can make this as negative as possible by making cos() = ??1
or equivalently making the angle between the gradient and our chosen direction to be  radians,
or equivalently 180 degrees. The only way to achieve this is to head in the exact opposite direction:
pick v to point in the exact opposite direction to ?wL(w)!
This brings us to one of the most important mathematical concepts in machine learning: the
direction of steepest decent points in the direction of ???wL(w). Thus our informal algorithm
can be rewritten as follows.
1. Start with a random choice for the initial parameters w.
2. Compute ?wL(w).
3. Take a small step in the opposite of that direction: w ! w ?? ??wL(w).
4. Repeat.
This basic algorithm has been modified and adapted many ways by many researchers, but the core
concept remains the same in all of them. Use the gradient to find the direction that decreases the
loss as rapidly as possible, and update the parameters to take a step in that direction.
18.4.3 A Note on Mathematical Optimization
Throughout this book, we focus squarely on numerical optimization techniques for the practical
reason that all functions we encounter in the deep learning setting are too complex to minimize
explicitly.
However, it is a useful exercise to consider what the geometric understanding we obtained above
tells us about optimizing functions directly.
Suppose that we wish to find the value of x0 which minimizes some function L(x). Let us suppose
that moreover someone gives us a value and tells us that it is the value that minimizes L. Is there
anything we can check to see if their answer is even plausible?
Again consider (18.4.5):
L(x0 + ?)  L(x0) + ?  ?xL(x0): (18.4.11)
If the gradient is not zero, we know that we can take a step in the direction ????xL(x0) to find a
value of L that is smaller. Thus, if we truly are at a minimum, this cannot be the case! We can
conclude that if x0 is a minimum, then ?xL(x0) = 0. We call points with ?xL(x0) = 0 critical
points.
This is nice, because in some rare settings, we can explicitly find all the points where the gradient
is zero, and find the one with the smallest value.
816 Chapter 18. Appendix: Mathematics for Deep Learning
For a concrete example, consider the function
f(x) = 3x4 ?? 4x3 ?? 12x2: (18.4.12)
This function has derivative
df
dx
= 12x3 ?? 12x2 ?? 24x = 12x(x ?? 2)(x + 1): (18.4.13)
The only possible location of minima are at x = ??1; 0; 2, where the function takes the values
??5; 0;??32 respectively, and thus we can conclude that we minimize our function when x = 2. A
quick plot confirms this.
x = np.arange(-2, 3, 0.01)
f = (3 * x**4) - (4 * x**3) - (12 * x**2)
d2l.plot(x, f, 'x', 'f(x)')
This highlights an important fact to know when working either theoretically or numerically: the
only possible points where we can minimize (or maximize) a function will have gradient equal to
zero, however, not every point with gradient zero is the true global minimum (or maximum).
18.4.4 Multivariate Chain Rule
Let us suppose that we have a function of four variables (w; x; y, and z) which we can make by
composing many terms:
f(u; v) = (u + v)2
u(a; b) = (a + b)2; v(a; b) = (a ?? b)2;
a(w; x; y; z) = (w + x + y + z)2; b(w; x; y; z) = (w + x ?? y ?? z)2:
(18.4.14)
Such chains of equations are common when working with neural networks, so trying to understand
how to compute gradients of such functions is key. We can start to see visual hints of this
connection in Fig. 18.4.1 if we take a look at what variables directly relate to one another.
18.4. Multivariable Calculus 817
Fig. 18.4.1: The function relations above where nodes represent values and edges show functional
dependence.
Nothing stops us from just composing everything from (18.4.14) and writing out that
f(w; x; y; z) =
((
(w + x + y + z)2 + (w + x ?? y ?? z)2)2
+
(
(w + x + y + z)2 ?? (w + x ?? y ?? z)2)2
)2
:
(18.4.15)
We may then take the derivative by just using single variable derivatives, but if we did that we
would quickly find ourself swamped with terms, many of which are repeats! Indeed, one can see
that, for instance:
@f
@w
= 2
(
2 (2(w + x + y + z) ?? 2(w + x ?? y ?? z))
(
(w + x + y + z)2 ?? (w + x ?? y ?? z)2)
+
2 (2(w + x ?? y ?? z) + 2(w + x + y + z))
(
(w + x ?? y ?? z)2 + (w + x + y + z)2))

((
(w + x + y + z)2 ?? (w + x ?? y ?? z)2)2
+
(
(w + x ?? y ?? z)2 + (w + x + y + z)2)2
)
:
(18.4.16)
If we then also wanted to compute @f
@x , we would end up with a similar equation again with many
repeated terms, and many shared repeated terms between the two derivatives. This represents
a massive quantity of wasted work, and if we needed to compute derivatives this way, the whole
deep learning revolution would have stalled out before it began!
Let us break up the problem. We will start by trying to understand how f changes when we change
a, essentially assuming that w; x; y, and z all do not exist. We will reason as we did back when we
worked with the gradient for the first time. Let us take a and add a small amount ? to it.
f(u(a + ?; b); v(a + ?; b))
f
(
u(a; b) + ?
@u
@a
(a; b); v(a; b) + ?
@v
@a
(a; b)
)
f(u(a; b); v(a; b)) + ?
[
@f
@u
(u(a; b); v(a; b))
@u
@a
(a; b) +
@f
@v
(u(a; b); v(a; b))
@v
@a
(a; b)
]
:
(18.4.17)
The first line follows from the definition of partial derivative, and the second follows from the
definition of gradient. It is notationally burdensome to track exactly where we evaluate every
derivative, as in the expression @f
@u(u(a; b); v(a; b)), so we often abbreviate this to the much more
memorable
@f
@a
=
@f
@u
@u
@a
+
@f
@v
@v
@a
: (18.4.18)
It is useful to think about the meaning of the process. We are trying to understand how a function
of the form f(u(a; b); v(a; b)) changes its value with a change in a. There are two pathways this can
818 Chapter 18. Appendix: Mathematics for Deep Learning
occur: there is the pathway where a ! u ! f and where a ! v ! f. We can compute both of
these contributions via the chain rule: @w
@u
 @u
@x and @w
@v
 @v
@x respectively, and added up.
Imagine we have a different network of functions where the functions on the right depend on
those that are connected to on the left as is shown in Fig. 18.4.2.
Fig. 18.4.2: Another more subtle example of the chain rule.
To compute something like @f
@y , we need to sum over all (in this case 3) paths from y to f giving
@f
@y
=
@f
@a
@a
@u
@u
@y
+
@f
@u
@u
@y
+
@f
@b
@b
@v
@v
@y
: (18.4.19)
Understanding the chain rule in this way will pay great dividends when trying to understand how
gradients flow through networks, and why various architectural choices like those in LSTMs (Section
9.2) or residual layers (Section 7.6) can help shape the learning process by controlling gradient
flow.
18.4.5 The Backpropagation Algorithm
Let us return to the example of (18.4.14) the previous section where
f(u; v) = (u + v)2
u(a; b) = (a + b)2; v(a; b) = (a ?? b)2;
a(w; x; y; z) = (w + x + y + z)2; b(w; x; y; z) = (w + x ?? y ?? z)2:
(18.4.20)
If we want to compute say @f
@w we may apply the multi-variate chain rule to see:
@f
@w
=
@f
@u
@u
@w
+
@f
@v
@v
@w
;
@u
@w
=
@u
@a
@a
@w
+
@u
@b
@b
@w
;
@v
@w
=
@v
@a
@a
@w
+
@v
@b
@b
@w
:
(18.4.21)
Let us try using this decomposition to compute @f
@w. Notice that all we need here are the various
single step partials:
@f
@u
= 2(u + v);
@f
@v
= 2(u + v);
@u
@a
= 2(a + b);
@u
@b
= 2(a + b);
@v
@a
= 2(a ?? b);
@v
@b
= ??2(a ?? b);
@a
@w
= 2(w + x + y + z);
@b
@w
= 2(w + x ?? y ?? z):
(18.4.22)
If we write this out into code this becomes a fairly manageable expression.
18.4. Multivariable Calculus 819
# Compute the value of the function from inputs to outputs
w, x, y, z = -1, 0, -2, 1
a, b = (w + x + y + z)**2, (w + x - y - z)**2
u, v = (a + b)**2, (a - b)**2
f = (u + v)**2
print(f' f at {w}, {x}, {y}, {z} is {f}')
# Compute the single step partials
df_du, df_dv = 2*(u + v), 2*(u + v)
du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)
da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)
# Compute the final result from inputs to outputs
du_dw, dv_dw = du_da*da_dw + du_db*db_dw, dv_da*da_dw + dv_db*db_dw
df_dw = df_du*du_dw + df_dv*dv_dw
print(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')
f at -1, 0, -2, 1 is 1024
df/dw at -1, 0, -2, 1 is -4096
However, note that this still does not make it easy to compute something like @f
@x . The reason for
that is the way we chose to apply the chain rule. If we look at what we did above, we always kept
@w in the denominator when we could. In this way, we chose to apply the chain rule seeing how
w changed every other variable. If that is what we wanted, this would be a good idea. However,
think back to our motivation from deep learning: we want to see how every parameter changes
the loss. In essence, we want to apply the chain rule keeping @f in the numerator whenever we
can!
To be more explicit, note that we can write
@f
@w
=
@f
@a
@a
@w
+
@f
@b
@b
@w
;
@f
@a
=
@f
@u
@u
@a
+
@f
@v
@v
@a
;
@f
@b
=
@f
@u
@u
@b
+
@f
@v
@v
@b
:
(18.4.23)
Note that this application of the chain rule has us explicitly compute @f
@u; @f
@v ; @f
@a ; @f
@b ; and @f
@w. Nothing
stops us from also including the equations:
@f
@x
=
@f
@a
@a
@x
+
@f
@b
@b
@x
;
@f
@y
=
@f
@a
@a
@y
+
@f
@b
@b
@y
;
@f
@z
=
@f
@a
@a
@z
+
@f
@b
@b
@z
:
(18.4.24)
and then keeping track of how f changes when we change any node in the entire network. Let us
implement it.
# Compute the value of the function from inputs to outputs
w, x, y, z = -1, 0, -2, 1
a, b = (w + x + y + z)**2, (w + x - y - z)**2
(continues on next page)
820 Chapter 18. Appendix: Mathematics for Deep Learning
(continued from previous page)
u, v = (a + b)**2, (a - b)**2
f = (u + v)**2
print(f'f at {w}, {x}, {y}, {z} is {f}')
# Compute the derivative using the decomposition above
# First compute the single step partials
df_du, df_dv = 2*(u + v), 2*(u + v)
du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)
da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)
da_dx, db_dx = 2*(w + x + y + z), 2*(w + x - y - z)
da_dy, db_dy = 2*(w + x + y + z), -2*(w + x - y - z)
da_dz, db_dz = 2*(w + x + y + z), -2*(w + x - y - z)
# Now compute how f changes when we change any value from output to input
df_da, df_db = df_du*du_da + df_dv*dv_da, df_du*du_db + df_dv*dv_db
df_dw, df_dx = df_da*da_dw + df_db*db_dw, df_da*da_dx + df_db*db_dx
df_dy, df_dz = df_da*da_dy + df_db*db_dy, df_da*da_dz + df_db*db_dz
print(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')
print(f'df/dx at {w}, {x}, {y}, {z} is {df_dx}')
print(f'df/dy at {w}, {x}, {y}, {z} is {df_dy}')
print(f'df/dz at {w}, {x}, {y}, {z} is {df_dz}')
f at -1, 0, -2, 1 is 1024
df/dw at -1, 0, -2, 1 is -4096
df/dx at -1, 0, -2, 1 is -4096
df/dy at -1, 0, -2, 1 is -4096
df/dz at -1, 0, -2, 1 is -4096
The fact that we compute derivatives from f back towards the inputs rather than from the inputs
forward to the outputs (as we did in the first code snippet above) is what gives this algorithm its
name: backpropagation. Note that there are two steps: 1. Compute the value of the function, and
the single step partials from front to back. While not done above, this can be combined into a
single forward pass. 2. Compute the gradient of f from back to front. We call this the backwards
pass.
This is precisely what every deep learning algorithm implements to allow the computation of the
gradient of the loss with respect to every weight in the network at one pass. It is an astonishing
fact that we have such a decomposition.
To see how to encapsulated this, let us take a quick look at this example.
# Initialize as ndarrays, then attach gradients
w, x, y, z = np.array(-1), np.array(0), np.array(-2), np.array(1)
w.attach_grad()
x.attach_grad()
y.attach_grad()
z.attach_grad()
# Do the computation like usual, tracking gradients
with autograd.record():
a, b = (w + x + y + z)**2, (w + x - y - z)**2
(continues on next page)
18.4. Multivariable Calculus 821
(continued from previous page)
u, v = (a + b)**2, (a - b)**2
f = (u + v)**2
# Execute backward pass
f.backward()
print(f'df/dw at {w}, {x}, {y}, {z} is {w.grad}')
print(f'df/dx at {w}, {x}, {y}, {z} is {x.grad}')
print(f'df/dy at {w}, {x}, {y}, {z} is {y.grad}')
print(f'df/dz at {w}, {x}, {y}, {z} is {z.grad}')
df/dw at -1.0, 0.0, -2.0, 1.0 is -4096.0
df/dx at -1.0, 0.0, -2.0, 1.0 is -4096.0
df/dy at -1.0, 0.0, -2.0, 1.0 is -4096.0
df/dz at -1.0, 0.0, -2.0, 1.0 is -4096.0
All of what we did above can be done automatically by calling f.backwards().
18.4.6 Hessians
As with single variable calculus, it is useful to consider higher-order derivatives in order to get a
handle on how we can obtain a better approximation to a function than using the gradient alone.
There is one immediate problem one encounters when working with higher order derivatives of
functions of several variables, and that is there are a large number of them. If we have a function
f(x1; : : : ; xn) of n variables, then we can take n2 many second derivatives, namely for any choice
of i and j:
d2f
dxidxj
=
d
dxi
(
d
dxj
f
)
: (18.4.25)
This is traditionally assembled into a matrix called the Hessian:
Hf =
2
664
d2f
dx1dx1
   d2f
dx1dxn ..... . ...
d2f
dxndx1
   d2f
dxndxn
3
775
: (18.4.26)
Not every entry of this matrix is independent. Indeed, we can show that as long as both mixed
partials (partial derivatives with respect to more than one variable) exist and are continuous, we
can say that for any i, and j,
d2f
dxidxj
=
d2f
dxjdxi
: (18.4.27)
This follows by considering first perturbing a function in the direction of xi, and then perturbing
it in xj and then comparing the result of that with what happens if we perturb first xj and then xi,
with the knowledge that both of these orders lead to the same final change in the output of f.
As with single variables, we can use these derivatives to get a far better idea of how the function
behaves near a point. In particular, we can use it to find the best fitting quadratic near a point x0,
as we saw in a single variable.
822 Chapter 18. Appendix: Mathematics for Deep Learning
Let us see an example. Suppose that f(x1; x2) = a+b1x1+b2x2+c11x21
+c12x1x2+c22x22
. This is the
general form for a quadratic in two variables. If we look at the value of the function, its gradient,
and its Hessian (18.4.26), all at the point zero:
f(0; 0) = a;
?f(0; 0) =
[
b1
b2
]
;
Hf(0; 0) =
[
2c11 c12
c12 2c22
]
;
(18.4.28)
we can get our original polynomial back by saying
f(x) = f(0) + ?f(0)  x +
1
2
x?Hf(0)x: (18.4.29)
In general, if we computed this expansion any point x0, we see that
f(x) = f(x0) + ?f(x0)  (x ?? x0) +
1
2
(x ?? x0)
?Hf(x0)(x ?? x0): (18.4.30)
This works for any dimensional input, and provides the best approximating quadratic to any function
at a point. To give an example, let us plot the function
f(x; y) = xe
??x2??y2
: (18.4.31)
One can compute that the gradient and Hessian are
?f(x; y) = e
??x2??y2
(
1 ?? 2x2
??2xy
)
and Hf(x; y) = e
??x2??y2
(
4x3 ?? 6x 4x2y ?? 2y
4x2y ?? 2y 4xy2 ?? 2x
)
: (18.4.32)
And thus, with a little algebra, see that the approximating quadratic at [??1; 0]? is
f(x; y)  e
??1 (
??1 ?? (x + 1) + 2(x + 1)2 + 2y2)
: (18.4.33)
# Construct grid and compute function
x, y = np.meshgrid(np.linspace(-2, 2, 101),
np.linspace(-2, 2, 101), indexing='ij')
z = x*np.exp(- x**2 - y**2)
# Compute approximating quadratic with gradient and Hessian at (1, 0)
w = np.exp(-1)*(-1 - (x + 1) + 2 * (x + 1)**2 + 2 * y**2)
# Plot function
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.plot_wireframe(x, y, w, **{'rstride': 10, 'cstride': 10}, color='purple')
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(-1, 1)
ax.dist = 12
18.4. Multivariable Calculus 823
This forms the basis for Newton?s Algorithm discussed in Section 11.3, where we perform numerical
optimization iteratively finding the best fitting quadratic, and then exactly minimizing that
quadratic.
18.4.7 A Little Matrix Calculus
Derivatives of functions involving matrices turn out to be particularly nice. This section can become
notationally heavy, so may be skipped in a first reading, but it is useful to know how derivatives
of functions involving common matrix operations are often much cleaner than one might
initially anticipate, particularly given how central matrix operations are to deep learning applications.
Let us begin with an example. Suppose that we have some fixed column vector , and we want
to take the product function f(x) = 
?x, and understand how the dot product changes when we
change x.
A bit of notation that will be useful when working with matrix derivatives in ML is called the denominator
layout matrix derivative where we assemble our partial derivatives into the shape of
whatever vector, matrix, or tensor is in the denominator of the differential. In this case, we will
write
df
dx =
2
64 df
dx1 ...
df
dxn
3
75
; (18.4.34)
where we matched the shape of the column vector x.
If we write out our function into components this is
f(x) =
?n
i=1
ixi = 1x1 +    + nxn: (18.4.35)
If we now take the partial derivative with respect to say 1, note that everything is zero but the first
term, which is just x1 multiplied by 1, so the we obtain that
df
dx1
= 1; (18.4.36)
or more generally that
df
dxi
= i: (18.4.37)
824 Chapter 18. Appendix: Mathematics for Deep Learning
We can now reassemble this into a matrix to see
df
dx =
2
64
df
dx1 ...
df
dxn
3
75
=
2
64
1
...
n
3
75
= : (18.4.38)
This illustrates a few factors about matrix calculus that we will often counter throughout this section:
� First, The computations will get rather involved.
� Second, The final results are much cleaner than the intermediate process, and will always
look similar to the single variable case. In this case, note that d
dx (bx) = b and d
dx (
?x) = 
are both similar.
� Third, transposes can often appear seemingly from nowhere. The core reason for this is the
convention that we match the shape of the denominator, thus when we multiply matrices,
we will need to take transposes to match back to the shape of the original term.
To keep building intuition, let us try a computation that is a little harder. Suppose that we have a
column vector x, and a square matrix A and we want to compute
d
dx(x?
Ax): (18.4.39)
To drive towards easier to manipulate notation, let us consider this problem using Einstein notation.
In this case we can write the function as
x?
Ax = xiaijxj : (18.4.40)
To compute our derivative, we need to understand for every k, what the value of
d
dxk
(x?
Ax) =
d
dxk
xiaijxj : (18.4.41)
By the product rule, this is
d
dxk
xiaijxj =
dxi
dxk
aijxj + xiaij
dxj
dxk
: (18.4.42)
For a term like dxi
dxk
, it is not hard to see that this is one when i = k and zero otherwise. This means
that every term where i and k are different vanish from this sum, so the only terms that remain in
that first sum are the ones where i = k. The same reasoning holds for the second term where we
need j = k. This gives
d
dxk
xiaijxj = akjxj + xiaik: (18.4.43)
Now, the names of the indices in Einstein notation are arbitrary�the fact that i and j are different
is immaterial to this computation at this point, so we can re-index so that they both use i to see
that
d
dxk
xiaijxj = akixi + xiaik = (aki + aik)xi: (18.4.44)
Now, here is where we start to need some practice to go further. Let us try and identify this outcome
in terms of matrix operations. aki + aik is the k; i-th component of A + A?. This gives
d
dxk
xiaijxj = [A + A?
]kixi: (18.4.45)
18.4. Multivariable Calculus 825
Similarly, this term is now the product of the matrix A + A? by the vector x, so we see that
[
d
dx(x?
Ax)
]
k
=
d
dxk
xiaijxj = [(A + A?
)x]k: (18.4.46)
Thus, we see that the k-th entry of the desired derivative from (18.4.39) is just the k-th entry of the
vector on the right, and thus the two are the same. Thus yields
d
dx(x?
Ax) = (A + A?
)x: (18.4.47)
This required significantly more work than our last one, but the final result is small. More than
that, consider the following computation for traditional single variable derivatives:
d
dx
(xax) =
dx
dx
ax + xa
dx
dx
= (a + a)x: (18.4.48)
Equivalently d
dx (ax2) = 2ax = (a + a)x. Again, we get a result that looks rather like the single
variable result but with a transpose tossed in.
At this point, the pattern should be looking rather suspicious, so let us try to figure out why. When
we take matrix derivatives like this, let us first assume that the expression we get will be another
matrix expression: an expression we can write it in terms of products and sums of matrices and
their transposes. If such an expression exists, it will need to be true for all matrices. In particular,
it will need to be true of 11 matrices, in which case the matrix product is just the product of the
numbers, the matrix sum is just the sum, and the transpose does nothing at all! In other words,
whatever expression we get must match the single variable expression. This means that, with
some practice, one can often guess matrix derivatives just by knowing what the associated single
variable expression must look like!
Let us try this out. Suppose that X is a n  m matrix, U is an n  r and V is an r  m. Let us try to
compute
d
dV
?X ?? UV?22
= ? (18.4.49)
This computation is important in an area called matrix factorization. For us, however, it is just a
derivative to compute. Let us try to imaging what this would be for 1  1 matrices. In that case,
we get the expression
d
dv
(x ?? uv)2 = ??2(x ?? uv)u; (18.4.50)
where, the derivative is rather standard. If we try to convert this back into a matrix expression we
get
d
dV
?X ?? UV?22
= ??2(X ?? UV)U: (18.4.51)
However, if we look at this it does not quite work. Recall that X is n  m, as is UV, so the matrix
2(X ?? UV) is n  m. On the other hand U is n  r, and we cannot multiply a n  m and a n  r
matrix since the dimensions do not match!
We want to get d
dV , which is the same shape of V, which is r  m. So somehow we need to take a
n m matrix and a n  r matrix, multiply them together (perhaps with some transposes) to get a
rm. We can do this by multiplying U? by (X??UV). Thus, we can guess the solution to (18.4.49)
is
d
dV
?X ?? UV?22
= ??2U?
(X ?? UV): (18.4.52)
826 Chapter 18. Appendix: Mathematics for Deep Learning
To show that this works, we would be remiss to not provide a detailed computation. If we already
believe that this rule-of-thumb works, feel free to skip past this derivation. To compute
d
dV
?X ?? UV?22
; (18.4.53)
we must find for every a, and b
d
dvab
?X ?? UV?22
=
d
dvab
?
i;j
(
xij ??
?
k
uikvkj
)2
: (18.4.54)
Recalling that all entries of X and U are constants as far as d
dvab
is concerned, we may push the
derivative inside the sum, and apply the chain rule to the square to get
d
dvab
?X ?? UV?22
=
?
i;j
2
(
xij ??
?
k
uikvkj
)(
??
?
k
uik
dvkj
dvab
)
: (18.4.55)
As in the previous derivation, we may note that dvkj
dvab
is only non-zero if the k = a and j = b. If
either of those conditions do not hold, the term in the sum is zero, and we may freely discard it.
We see that
d
dvab
?X ?? UV?22
= ??2
?
i
(
xib ??
?
k
uikvkb
)
uia: (18.4.56)
An important subtlety here is that the requirement that k = a does not occur inside the inner
sum since that k is a dummy variable which we are summing over inside the inner term. For a
notationally cleaner example, consider why
d
dx1
(
?
i
xi
)2
= 2
(
?
i
xi
)
: (18.4.57)
From this point, we may start identifying components of the sum. First,
?
k
uikvkb = [UV]ib: (18.4.58)
So the entire expression in the inside of the sum is
xib ??
?
k
uikvkb = [X ?? UV]ib: (18.4.59)
This means we may now write our derivative as
d
dvab
?X ?? UV?22
= ??2
?
i
[X ?? UV]ibuia: (18.4.60)
We want this to look like the a; b element of a matrix so we can use the technique as in the previous
example to arrive at a matrix expression, which means that we need to exchange the order of the
indices on uia. If we notice that uia = [U?]ai, we can then write
d
dvab
?X ?? UV?22
= ??2
?
i
[U?
]ai[X ?? UV]ib: (18.4.61)
18.4. Multivariable Calculus 827
This is a matrix product, and thus we can conclude that
d
dvab
?X ?? UV?22
= ??2[U?
(X ?? UV)]ab: (18.4.62)
and thus we may write the solution to (18.4.49)
d
dV
?X ?? UV?22
= ??2U?
(X ?? UV): (18.4.63)
This matches the solution we guessed above!
It is reasonable to ask at this point, �Why can I not just write down matrix versions of all the calculus
rules I have learned? It is clear this is still mechanical. Why do we not just get it over with!�
And indeed there are such rules and (Petersen et al., 2008) provides an excellent summary. However,
due to the plethora of ways matrix operations can be combined compared to single values,
there are many more matrix derivative rules than single variable ones. It is often the case that it
is best to work with the indices, or leave it up to automatic differentiation when appropriate.
Summary
� In higher dimensions, we can define gradients which serve the same purpose as derivatives
in one dimension. These allow us to see how a multi-variable function changes when we
make an arbitrary small change to the inputs.
� The backpropagation algorithm can be seen to be a method of organizing the multi-variable
chain rule to allow for the efficient computation of many partial derivatives.
� Matrix calculus allows us to write the derivatives of matrix expressions in concise ways.
Exercises
1. Given a column vector , compute the derivatives of both f(x) = 
?x and g(x) = x?. Why
do you get the same answer?
2. Let v be an n dimension vector. What is @
@v
?v?2?
3. Let L(x; y) = log(ex +ey). Compute the gradient. What is the sum of the components of the
gradient?
4. Let f(x; y) = x2y + xy2. Show that the only critical point is (0; 0). By considering f(x; x),
determine if (0; 0) is a maximum, minimum, or neither.
5. Suppose that we are minimizing a function f(x) = g(x) + h(x). How can we geometrically
interpret the condition of ?f = 0 in terms of g and h?
Discussions248
248 https://discuss.d2l.ai/t/413
828 Chapter 18. Appendix: Mathematics for Deep Learning
18.5 Integral Calculus
Differentiation only makes up half of the content of a traditional calculus education. The other
pillar, integration, starts out seeming a rather disjoint question, �What is the area underneath this
curve?� While seemingly unrelated, integration is tightly intertwined with the differentiation via
what is known as the fundamental theorem of calculus.
At the level of machine learning we discuss in this book, we will not need a deep understanding of
integration. However, we will provide a brief introduction to lay the groundwork for any further
applications we will encounter later on.
18.5.1 Geometric Interpretation
Suppose that we have a function f(x). For simplicity, let us assume that f(x) is non-negative (never
takes a value less than zero). What we want to try and understand is: what is the area contained
between f(x) and the x-axis?
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from mpl_toolkits import mplot3d
from mxnet import np, npx
npx.set_np()
x = np.arange(-2, 2, 0.01)
f = np.exp(-x**2)
d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist(), f.tolist())
d2l.plt.show()
In most cases, this area will be infinite or undefined (consider the area under f(x) = x2), so people
will often talk about the area between a pair of ends, say a and b.
x = np.arange(-2, 2, 0.01)
f = np.exp(-x**2)
(continues on next page)
18.5. Integral Calculus 829
(continued from previous page)
d2l.set_figsize()
d2l.plt.plot(x, f, color='black')
d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])
d2l.plt.show()
We will denote this area by the integral symbol below:
Area(A) =
? b
a
f(x) dx: (18.5.1)
The inner variable is a dummy variable, much like the index of a sum in a
?
, and so this can be
equivalently written with any inner value we like:
? b
a
f(x) dx =
? b
a
f(z) dz: (18.5.2)
There is a traditional way to try and understand how we might try to approximate such integrals:
we can imagine taking the region in-between a and b and chopping it into N vertical slices. If N
is large, we can approximate the area of each slice by a rectangle, and then add up the areas to get
the total area under the curve. Let us take a look at an example doing this in code. We will see
how to get the true value in a later section.
epsilon = 0.05
a = 0
b = 2
x = np.arange(a, b, epsilon)
f = x / (1 + x**2)
approx = np.sum(epsilon*f)
true = np.log(2) / 2
d2l.set_figsize()
d2l.plt.bar(x.asnumpy(), f.asnumpy(), width=epsilon, align='edge')
d2l.plt.plot(x, f, color='black')
d2l.plt.ylim([0, 1])
d2l.plt.show()
f'approximation: {approx}, truth: {true}'
830 Chapter 18. Appendix: Mathematics for Deep Learning
'approximation: 0.79448557, truth: 0.34657359027997264'
The issue is that while it can be done numerically, we can do this approach analytically for only
the simplest functions like
? b
a
x dx: (18.5.3)
Anything somewhat more complex like our example from the code above
? b
a
x
1 + x2 dx: (18.5.4)
is beyond what we can solve with such a direct method.
We will instead take a different approach. We will work intuitively with the notion of the area, and
learn the main computational tool used to find integrals: the fundamental theorem of calculus. This
will be the basis for our study of integration.
18.5.2 The Fundamental Theorem of Calculus
To dive deeper into the theory of integration, let us introduce a function
F(x) =
? x
0
f(y)dy: (18.5.5)
This function measures the area between 0 and x depending on how we change x. Notice that this
is everything we need since
? b
a
f(x) dx = F(b) ?? F(a): (18.5.6)
This is a mathematical encoding of the fact that we can measure the area out to the far end-point
and then subtract off the area to the near end point as indicated in Fig. 18.5.1.
18.5. Integral Calculus 831
Fig. 18.5.1: Visualizing why we may reduce the problem of computing the area under a curve
between two points to computing the area to the left of a point.
Thus, we can figure out what the integral over any interval is by figuring out what F(x) is.
To do so, let us consider an experiment. As we often do in calculus, let us imagine what happens
when we shift the value by a tiny bit. From the comment above, we know that
F(x + ?) ?? F(x) =
? x+?
x
f(y) dy: (18.5.7)
This tells us that the function changes by the area under a tiny sliver of a function.
This is the point at which we make an approximation. If we look at a tiny sliver of area like this, it
looks like this area is close to the rectangular area with height the value of f(x) and the base width
?. Indeed, one can show that as ? ! 0 this approximation becomes better and better. Thus we can
conclude:
F(x + ?) ?? F(x)  ?f(x): (18.5.8)
However, we can now notice: this is exactly the pattern we expect if we were computing the derivative
of F! Thus we see the following rather surprising fact:
dF
dx
(x) = f(x): (18.5.9)
This is the fundamental theorem of calculus. We may write it in expanded form as
d
dx
? x
??1
f(y) dy = f(x): (18.5.10)
It takes the concept of finding areas (a priori rather hard), and reduces it to a statement derivatives
(something much more completely understood). One last comment that we must make is that this
does not tell us exactly what F(x) is. Indeed F(x) + C for any C has the same derivative. This
is a fact-of-life in the theory of integration. Thankfully, notice that when working with definite
integrals, the constants drop out, and thus are irrelevant to the outcome.
? b
a
f(x) dx = (F(b) + C) ?? (F(a) + C) = F(b) ?? F(a): (18.5.11)
This may seem like abstract non-sense, but let us take a moment to appreciate that it has given us
a whole new perspective on computing integrals. Our goal is no-longer to do some sort of chopand-
sum process to try and recover the area, rather we need only find a function whose derivative
is the function we have! This is incredible since we can now list many rather difficult integrals
by just reversing the table from Section 18.3.2. For instance, we know that the derivative of xn is
nxn??1. Thus, we can say using the fundamental theorem (18.5.10) that
? x
0
nyn??1 dy = xn ?? 0n = xn: (18.5.12)
832 Chapter 18. Appendix: Mathematics for Deep Learning
Similarly, we know that the derivative of ex is itself, so that means
? x
0
ex dx = ex ?? e0 = ex ?? 1: (18.5.13)
In this way, we can develop the entire theory of integration leveraging ideas from differential
calculus freely. Every integration rule derives from this one fact.
18.5.3 Change of Variables
Just as with differentiation, there are a number of rules which make the computation of integrals
more tractable. In fact, every rule of differential calculus (like the product rule, sum rule, and
chain rule) has a corresponding rule for integral calculus (integration by parts, linearity of integration,
and the change of variables formula respectively). In this section, we will dive into what
is arguably the most important from the list: the change of variables formula.
First, suppose that we have a function which is itself an integral:
F(x) =
? x
0
f(y) dy: (18.5.14)
Let us suppose that we want to know how this function looks when we compose it with another to
obtain F(u(x)). By the chain rule, we know
d
dx
F(u(x)) =
dF
du
(u(x))  du
dx
: (18.5.15)
We can turn this into a statement about integration by using the fundamental theorem (18.5.10)
as above. This gives
F(u(x)) ?? F(u(0)) =
? x
0
dF
du
(u(y))  du
dy
dy: (18.5.16)
Recalling that F is itself an integral gives that the left hand side may be rewritten to be
? u(x)
u(0)
f(y) dy =
? x
0
dF
du
(u(y))  du
dy
dy: (18.5.17)
Similarly, recalling that F is an integral allows us to recognize that dF
dx = f using the fundamental
theorem (18.5.10), and thus we may conclude
? u(x)
u(0)
f(y) dy =
? x
0
f(u(y))  du
dy
dy: (18.5.18)
This is the change of variables formula.
For a more intuitive derivation, consider what happens when we take an integral of f(u(x)) between
x and x+?. For a small ?, this integral is approximately ?f(u(x)), the area of the associated
rectangle. Now, let us compare this with the integral of f(y) from u(x) to u(x + ?). We know that
u(x + ?)  u(x) + ? du
dx (x), so the area of this rectangle is approximately ? du
dx (x)f(u(x)). Thus, to
make the area of these two rectangles to agree, we need to multiply the first one by du
dx (x) as is
illustrated in Fig. 18.5.2.
18.5. Integral Calculus 833
Fig. 18.5.2: Visualizing the transformation of a single thin rectangle under the change of variables.
This tells us that
? x+?
x
f(u(y))
du
dy
(y) dy =
? u(x+?)
u(x)
f(y) dy: (18.5.19)
This is the change of variables formula expressed for a single small rectangle.
If u(x) and f(x) are properly chosen, this can allow for the computation of incredibly complex integrals.
For instance, if we even chose f(y) = 1 and u(x) = e??x2 (which means du
dx (x) = ??2xe??x2 ),
this can show for instance that
e
??1 ?? 1 =
? e??1
e??0
1 dy = ??2
? 1
0
ye
??y2
dy; (18.5.20)
and thus by rearranging that
? 1
0
ye
??y2
dy =
1 ?? e??1
2
: (18.5.21)
18.5.4 A Comment on Sign Conventions
Keen-eyed readers will observe something strange about the computations above. Namely, computations
like
? e??1
e??0
1 dy = e
??1 ?? 1 < 0; (18.5.22)
can produce negative numbers. When thinking about areas, it can be strange to see a negative
value, and so it is worth digging into what the convention is.
Mathematicians take the notion of signed areas. This manifests itself in two ways. First, if we
consider a function f(x) which is sometimes less than zero, then the area will also be negative. So
for instance
? 1
0
(??1) dx = ??1: (18.5.23)
Similarly, integrals which progress from right to left, rather than left to right are also taken to be
negative areas
? ??1
0
1 dx = ??1: (18.5.24)
The standard area (from left to right of a positive function) is always positive. Anything obtained
by flipping it (say flipping over the x-axis to get the integral of a negative number, or flipping over
834 Chapter 18. Appendix: Mathematics for Deep Learning
the y-axis to get an integral in the wrong order) will produce a negative area. And indeed, flipping
twice will give a pair of negative signs that cancel out to have positive area
? ??1
0
(??1) dx = 1: (18.5.25)
If this discussion sounds familiar, it is! In Section 18.1 we discussed how the determinant represented
the signed area in much the same way.
18.5.5 Multiple Integrals
In some cases, we will need to work in higher dimensions. For instance, suppose that we have
a function of two variables, like f(x; y) and we want to know the volume under f when x ranges
over [a; b] and y ranges over [c; d].
# Construct grid and compute function
x, y = np.meshgrid(np.linspace(-2, 2, 101), np.linspace(-2, 2, 101),
indexing='ij')
z = np.exp(- x**2 - y**2)
# Plot function
ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y')
d2l.plt.xticks([-2, -1, 0, 1, 2])
d2l.plt.yticks([-2, -1, 0, 1, 2])
d2l.set_figsize()
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_zlim(0, 1)
ax.dist = 12
We write this as ?
[a;b][c;d]
f(x; y) dx dy: (18.5.26)
Suppose that we wish to compute this integral. My claim is that we can do this by iteratively computing
first the integral in x and then shifting to the integral in y, that is to say
?
[a;b][c;d]
f(x; y) dx dy =
? d
c
(? b
a
f(x; y) dx
)
dy: (18.5.27)
18.5. Integral Calculus 835
Let us see why this is.
Consider the figure above where we have split the function into ?  ? squares which we will index
with integer coordinates i; j. In this case, our integral is approximately
?
i;j
?2f(?i; ?j): (18.5.28)
Once we discretize the problem, we may add up the values on these squares in whatever order we
like, and not worry about changing the values. This is illustrated in Fig. 18.5.3. In particular, we
can say that
?
j
?
(
?
i
?f(?i; ?j)
)
: (18.5.29)
Fig. 18.5.3: Illustrating how to decompose a sum over many squares as a sum over first the columns
(1), then adding the column sums together (2).
The sum on the inside is precisely the discretization of the integral
G(?j) =
? b
a
f(x; ?j) dx: (18.5.30)
Finally, notice that if we combine these two expressions we get
?
j
?G(?j) 
? d
c
G(y) dy =
?
[a;b][c;d]
f(x; y) dx dy: (18.5.31)
Thus putting it all together, we have that
?
[a;b][c;d]
f(x; y) dx dy =
? d
c
(? b
a
f(x; y) dx
)
dy: (18.5.32)
Notice that, once discretized, all we did was rearrange the order in which we added a list of numbers.
This may make it seem like it is nothing, however this result (called Fubini�s Theorem) is not
always true! For the type of mathematics encountered when doing machine learning (continuous
functions), there is no concern, however it is possible to create examples where it fails (for
example the function f(x; y) = xy(x2 ?? y2)/(x2 + y2)3 over the rectangle [0; 2]  [0; 1]).
Note that the choice to do the integral in x first, and then the integral in y was arbitrary. We could
have equally well chosen to do y first and then x to see
?
[a;b][c;d]
f(x; y) dx dy =
? b
a
(? d
c
f(x; y) dy
)
dx: (18.5.33)
836 Chapter 18. Appendix: Mathematics for Deep Learning
Often times, we will condense down to vector notation, and say that for U = [a; b]  [c; d] this is
?
U
f(x) dx: (18.5.34)
18.5.6 Change of Variables in Multiple Integrals
As with single variables in (18.5.18), the ability to change variables inside a higher dimensional
integral is a key tool. Let us summarize the result without derivation.
We need a function that reparameterizes our domain of integration. We can take this to be ? :
Rn ! Rn, that is any function which takes in n real variables and returns another n. To keep
the expressions clean, we will assume that ? is injective which is to say it never folds over itself
(?(x) = ?(y) =) x = y).
In this case, we can say that
?
?(U)
f(x) dx =
?
U
f(?(x)) jdet(D?(x))j dx: (18.5.35)
where D? is the Jacobian of ?, which is the matrix of partial derivatives of ? =
(?1(x1; : : : ; xn); : : : ; ?n(x1; : : : ; xn)),
D? =
2
64
@?1
@x1
   @?1
@xn ...
. .. ...
@?n
@x1
   @?n
@xn
3
75
: (18.5.36)
Looking closely, we see that this is similar to the single variable chain rule (18.5.18), except we
have replaced the term du
dx (x) with jdet(D?(x))j. Let us see how we can to interpret this term.
Recall that the du
dx (x) term existed to say how much we stretched our x-axis by applying u. The
same process in higher dimensions is to determine how much we stretch the area (or volume, or
hyper-volume) of a little square (or little hyper-cube) by applying ?. If ? was the multiplication by
a matrix, then we know how the determinant already gives the answer.
With some work, one can show that the Jacobian provides the best approximation to a multivariable
function ? at a point by a matrix in the same way we could approximate by lines or planes
with derivatives and gradients. Thus the determinant of the Jacobian exactly mirrors the scaling
factor we identified in one dimension.
It takes some work to fill in the details to this, so do not worry if they are not clear now. Let us see
at least one example we will make use of later on. Consider the integral
? 1
??1
? 1
??1
e
??x2??y2
dx dy: (18.5.37)
Playing with this integral directly will get us no-where, but if we change variables, we can make
significant progress. If we let ?(r; ) = (r cos(); r sin()) (which is to say that x = r cos(), y =
r sin()), then we can apply the change of variable formula to see that this is the same thing as
? 1
0
? 2
0
e
??r2 jdet(D?(x))j d dr; (18.5.38)
where
jdet(D?(x))j =

det
[
cos() ??r sin()
sin() r cos()
]
= r(cos2() + sin2()) = r: (18.5.39)
18.5. Integral Calculus 837
Thus, the integral is
? 1
0
? 2
0
re
??r2
d dr = 2
? 1
0
re
??r2
dr = ; (18.5.40)
where the final equality follows by the same computation that we used in section Section 18.5.3.
We will meet this integral again when we study continuous random variables in Section 18.6.
Summary
� The theory of integration allows us to answer questions about areas or volumes.
� The fundamental theorem of calculus allows us to leverage knowledge about derivatives to
compute areas via the observation that the derivative of the area up to some point is given
by the value of the function being integrated.
� Integrals in higher dimensions can be computed by iterating single variable integrals.
Exercises
1. What is
? 2
1
1
x dx?
2. Use the change of variables formula to integrate
? p

0 x sin(x2) dx.
3. What is
?
[0;1]2 xy dx dy?
4. Use the change of variables formula to compute
? 2
0
? 1
0 xy(x2 ?? y2)/(x2 + y2)3 ? dy dx and 1
0
? 2
0 f(x; y) = xy(x2 ?? y2)/(x2 + y2)3 dx dy to see they are different.
Discussions249
18.6 Random Variables
In Section 2.6 we saw the basics of how to work with discrete random variables, which in our case
refer to those random variables which take either a finite set of possible values, or the integers.
In this section, we develop the theory of continuous random variables, which are random variables
which can take on any real value.
18.6.1 Continuous Random Variables
Continuous random variables are a significantly more subtle topic than discrete random variables.
A fair analogy to make is that the technical jump is comparable to the jump between adding lists
of numbers and integrating functions. As such, we will need to take some time to develop the
theory.
249 https://discuss.d2l.ai/t/414
838 Chapter 18. Appendix: Mathematics for Deep Learning
From Discrete to Continuous
To understand the additional technical challenges encountered when working with continuous
random variables, let us perform a thought experiment. Suppose that we are throwing a dart at
the dart board, and we want to know the probability that it hits exactly 2cm from the center of the
board.
To start with, we imagine measuring a single digit of accuracy, that is to say with bins for 0cm,
1cm, 2cm, and so on. We throw say 100 darts at the dart board, and if 20 of them fall into the bin
for 2cm we conclude that 20% of the darts we throw hit the board 2cm away from the center.
However, when we look closer, this does not match our question! We wanted exact equality,
whereas these bins hold all that fell between say 1:5cm and 2:5cm.
Undeterred, we continue further. We measure even more precisely, say 1:9cm, 2:0cm, 2:1cm, and
now see that perhaps 3 of the 100 darts hit the board in the 2:0cm bucket. Thus we conclude the
probability is 3%.
However, this does not solve anything! We have just pushed the issue down one digit further. Let
us abstract a bit. Imagine we know the probability that the first k digits match with 2:00000 : : : and
we want to know the probability it matches for the first k+1 digits. It is fairly reasonable to assume
that the k + 1th digit is essentially a random choice from the set f0; 1; 2; : : : ; 9g. At least, we cannot
conceive of a physically meaningful process which would force the number of micrometers away
form the center to prefer to end in a 7 vs a 3.
What this means is that in essence each additional digit of accuracy we require should decrease
probability of matching by a factor of 10. Or put another way, we would expect that
P(distance is 2:00 : : : ; to k digits)  p  10
??k: (18.6.1)
The value p essentially encodes what happens with the first few digits, and the 10??k handles the
rest.
Notice that if we know the position accurate to k = 4 digits after the decimal. that means we know
the value falls within the interval say [(1:99995; 2:00005] which is an interval of length 2:00005 ??
1:99995 = 10??4. Thus, if we call the length of this interval ?, we can say
P(distance is in an ?-sized interval around 2)  ?  p: (18.6.2)
Let us take this one final step further. We have been thinking about the point 2 the entire time,
but never thinking about other points. Nothing is different there fundamentally, but it is the case
that the value p will likely be different. We would at least hope that a dart thrower was more likely
to hit a point near the center, like 2cm rather than 20cm. Thus, the value p is not fixed, but rather
should depend on the point x. This tells us that we should expect
P(distance is in an ?-sized interval around x)  ?  p(x): (18.6.3)
Indeed, (18.6.3) precisely defines the probability density function. It is a function p(x) which encodes
the relative probability of hitting near one point vs. another. Let us visualize what such a
function might look like.
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
(continues on next page)
18.6. Random Variables 839
(continued from previous page)
from mxnet import np, npx
npx.set_np()
# Plot the probability density function for some random variable
x = np.arange(-5, 5, 0.01)
p = 0.2*np.exp(-(x - 3)**2 / 2)/np.sqrt(2 * np.pi) + \
0.8*np.exp(-(x + 1)**2 / 2)/np.sqrt(2 * np.pi)
d2l.plot(x, p, 'x', 'Density')
The locations where the function value is large indicates regions where we are more likely to find
the random value. The low portions are areas where we are unlikely to find the random value.
Probability Density Functions
Let us now investigate this further. We have already seen what a probability density function is
intuitively for a random variable X, namely the density function is a function p(x) so that
P(X is in an ?-sized interval around x)  ?  p(x): (18.6.4)
But what does this imply for the properties of p(x)?
First, probabilities are never negative, thus we should expect that p(x)  0 as well.
Second, let us imagine that we slice up the R into an infinite number of slices which are ? wide, say
with slices (?i; ?(i+1)]. For each of these, we know from (18.6.4) the probability is approximately
P(X is in an ?-sized interval around x)  ?  p(?  i); (18.6.5)
so summed over all of them it should be
P(X 2 R) 
?
i
?  p(?  i): (18.6.6)
This is nothing more than the approximation of an integral discussed in Section 18.5, thus we can
say that
P(X 2 R) =
? 1
??1
p(x) dx: (18.6.7)
840 Chapter 18. Appendix: Mathematics for Deep Learning
We know that P(X 2 R) = 1, since the random variable must take on some number, we can
conclude that for any density
? 1
??1
p(x) dx = 1: (18.6.8)
Indeed, digging into this further shows that for any a, and b, we see that
P(X 2 (a; b]) =
? b
a
p(x) dx: (18.6.9)
We may approximate this in code by using the same discrete approximation methods as before.
In this case we can approximate the probability of falling in the blue region.
# Approximate probability using numerical integration
epsilon = 0.01
x = np.arange(-5, 5, 0.01)
p = 0.2*np.exp(-(x - 3)**2 / 2) / np.sqrt(2 * np.pi) + \
0.8*np.exp(-(x + 1)**2 / 2) / np.sqrt(2 * np.pi)
d2l.set_figsize()
d2l.plt.plot(x, p, color='black')
d2l.plt.fill_between(x.tolist()[300:800], p.tolist()[300:800])
d2l.plt.show()
f'approximate Probability: {np.sum(epsilon*p[300:800])}'
'approximate Probability: 0.7736172'
It turns out that these two properties describe exactly the space of possible probability density
functions (or p.d.f.?s for the commonly encountered abbreviation). They are non-negative functions
p(x)  0 such that
? 1
??1
p(x) dx = 1: (18.6.10)
We interpret this function by using integration to obtain the probability our random variable is in
a specific interval:
P(X 2 (a; b]) =
? b
a
p(x) dx: (18.6.11)
18.6. Random Variables 841
In Section 18.8 we will see a number of common distributions, but let us continue working in the
abstract.
Cumulative Distribution Functions
In the previous section, we saw the notion of the p.d.f. In practice, this is a commonly encountered
method to discuss continuous random variables, but it has one significant pitfall: that the values
of the p.d.f. are not themselves probabilities, but rather a function that we must integrate to yield
probabilities. There is nothing wrong with a density being larger than 10, as long as it is not larger
than 10 for more than an interval of length 1/10. This can be counter-intuitive, so people often
also think in terms of the cumulative distribution function, or c.d.f., which is a probability.
In particular, by using (18.6.11), we define the c.d.f. for a random variable X with density p(x) by
F(x) =
? x
??1
p(x) dx = P(X  x): (18.6.12)
Let us observe a few properties.
� F(x) ! 0 as x ! ??1.
� F(x) ! 1 as x ! 1.
� F(x) is non-decreasing (y > x =) F(y)  F(x)).
� F(x) is continuous (has no jumps) if X is a continuous random variable.
With the fourth bullet point, note that this would not be true if X were discrete, say taking the
values 0 and 1 both with probability 1/2. In that case
F(x) =
8><
>:
0 x < 0;
1
2 x < 1;
1 x  1:
(18.6.13)
In this example, we see one of the benefits of working with the c.d.f., the ability to deal with continuous
or discrete random variables in the same framework, or indeed mixtures of the two (flip
a coin: if heads return the roll of a die, if tails return the distance of a dart throw from the center
of a dart board).
Means
Suppose that we are dealing with a random variables X. The distribution itself can be hard to
interpret. It is often useful to be able to summarize the behavior of a random variable concisely.
Numbers that help us capture the behavior of a random variable are called summary statistics. The
most commonly encountered ones are the mean, the variance, and the standard deviation.
The mean encodes the average value of a random variable. If we have a discrete random variable
X, which takes the values xi with probabilities pi, then the mean is given by the weighted average:
sum the values times the probability that the random variable takes on that value:
X = E[X] =
?
i
xipi: (18.6.14)
The way we should interpret the mean (albeit with caution) is that it tells us essentially where the
random variable tends to be located.
842 Chapter 18. Appendix: Mathematics for Deep Learning
As a minimalistic example that we will examine throughout this section, let us take X to be the
random variable which takes the value a??2 with probability p, a+2 with probability p and a with
probability 1 ?? 2p. We can compute using (18.6.14) that, for any possible choice of a and p, the
mean is
X = E[X] =
?
i
xipi = (a ?? 2)p + a(1 ?? 2p) + (a + 2)p = a: (18.6.15)
Thus we see that the mean is a. This matches the intuition since a is the location around which
we centered our random variable.
Because they are helpful, let us summarize a few properties.
� For any random variable X and numbers a and b, we have that aX+b = aX + b.
� If we have two random variables X and Y , we have X+Y = X + Y .
Means are useful for understanding the average behavior of a random variable, however the mean
is not sufficient to even have a full intuitive understanding. Making a profit of $10  $1 per sale is
very different from making $10$15 per sale despite having the same average value. The second
one has a much larger degree of fluctuation, and thus represents a much larger risk. Thus, to
understand the behavior of a random variable, we will need at minimum one more measure: some
measure of how widely a random variable fluctuates.
Variances
This leads us to consider the variance of a random variable. This is a quantitative measure of
how far a random variable deviates from the mean. Consider the expression X ?? X. This is the
deviation of the random variable from its mean. This value can be positive or negative, so we need
to do something to make it positive so that we are measuring the magnitude of the deviation.
A reasonable thing to try is to look at jX ?? Xj, and indeed this leads to a useful quantity called the
mean absolute deviation, however due to connections with other areas of mathematics and statistics,
people often use a different solution.
In particular, they look at (X ?? X)2: If we look at the typical size of this quantity by taking the
mean, we arrive at the variance
2X
= Var(X) = E
[
(X ?? X)2]
= E[X2] ?? 2
X: (18.6.16)
The last equality in (18.6.16) holds by expanding out the definition in the middle, and applying the
properties of expectation.
Let us look at our example where X is the random variable which takes the value a??2 with probability
p, a+2 with probability p and a with probability 1??2p. In this case X = a, so all we need
to compute is E
[
X2
]
. This can readily be done:
E
[
X2]
= (a ?? 2)2p + a2(1 ?? 2p) + (a + 2)2p = a2 + 8p: (18.6.17)
Thus, we see that by (18.6.16) our variance is
2X
= Var(X) = E[X2] ?? 2
X = a2 + 8p ?? a2 = 8p: (18.6.18)
This result again makes sense. The largest p can be is 1/2 which corresponds to picking a ?? 2 or
a + 2 with a coin flip. The variance of this being 4 corresponds to the fact that both a ?? 2 and
18.6. Random Variables 843
a + 2 are 2 units away from the mean, and 22 = 4. On the other end of the spectrum, if p = 0, this
random variable always takes the value 0 and so it has no variance at all.
We will list a few properties of variance below:
� For any random variable X, Var(X)  0, with Var(X) = 0 if and only if X is a constant.
� For any random variable X and numbers a and b, we have that Var(aX + b) = a2Var(X).
� If we have two independent random variables X and Y , we have Var(X + Y ) = Var(X) +
Var(Y ).
When interpreting these values, there can be a bit of a hiccup. In particular, let us try imagining
what happens if we keep track of units through this computation. Suppose that we are working
with the star rating assigned to a product on the web page. Then a, a??2, and a+2 are all measured
in units of stars. Similarly, the mean X is then also measured in stars (being a weighted average).
However, if we get to the variance, we immediately encounter an issue, which is we want to look at
(X ??X)2, which is in units of squared stars. This means that the variance itself is not comparable
to the original measurements. To make it interpretable, we will need to return to our original
units.
Standard Deviations
This summary statistics can always be deduced from the variance by taking the square root! Thus
we define the standard deviation to be
X =
v
Var(X): (18.6.19)
In our example, this means we now have the standard deviation is X = 2
p
2p. If we are dealing
with units of stars for our review example, X is again in units of stars.
The properties we had for the variance can be restated for the standard deviation.
� For any random variable X, X  0.
� For any random variable X and numbers a and b, we have that aX+b = jajX
� If we have two independent random variables X and Y , we have X+Y =
v
2X
+ 2Y
.
It is natural at this moment to ask, �If the standard deviation is in the units of our original random
variable, does it represent something we can draw with regards to that random variable?� The
answer is a resounding yes! Indeed much like the mean told we the typical location of our random
variable, the standard deviation gives the typical range of variation of that random variable. We
can make this rigorous with what is known as Chebyshev?s inequality:
P (X ?2 [X ?? X; X + X])  1
2 : (18.6.20)
Or to state it verbally in the case of  = 10, 99% of the samples from any random variable fall
within 10 standard deviations of the mean. This gives an immediate interpretation to our standard
summary statistics.
To see how this statement is rather subtle, let us take a look at our running example again where
X is the random variable which takes the value a ?? 2 with probability p, a + 2 with probability p
and a with probability 1 ?? 2p. We saw that the mean was a and the standard deviation was 2
p
2p.
This means, if we take Chebyshev?s inequality (18.6.20) with  = 2, we see that the expression is
P
(
X ?2 [a ?? 4
v
2p; a + 4
v
2p]
)
 1
4
: (18.6.21)
844 Chapter 18. Appendix: Mathematics for Deep Learning
This means that 75% of the time, this random variable will fall within this interval for any value of
p. Now, notice that as p ! 0, this interval also converges to the single point a. But we know that
our random variable takes the values a??2; a, and a+2 only so eventually we can be certain a??2
and a + 2 will fall outside the interval! The question is, at what p does that happen. So we want to
solve: for what p does a + 4
p
2p = a + 2, which is solved when p = 1/8, which is exactly the first p
where it could possibly happen without violating our claim that no more than 1/4 of samples from
the distribution would fall outside the interval (1/8 to the left, and 1/8 to the right).
Let us visualize this. We will show the probability of getting the three values as three vertical bars
with height proportional to the probability. The interval will be drawn as a horizontal line in the
middle. The first plot shows what happens for p > 1/8 where the interval safely contains all points.
# Define a helper to plot these figures
def plot_chebyshev(a, p):
d2l.set_figsize()
d2l.plt.stem([a-2, a, a+2], [p, 1-2*p, p], use_line_collection=True)
d2l.plt.xlim([-4, 4])
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.hlines(0.5, a - 4 * np.sqrt(2 * p),
a + 4 * np.sqrt(2 * p), 'black', lw=4)
d2l.plt.vlines(a - 4 * np.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)
d2l.plt.vlines(a + 4 * np.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)
d2l.plt.title(f'p = {p:.3f}')
d2l.plt.show()
# Plot interval when p > 1/8
plot_chebyshev(0.0, 0.2)
The second shows that at p = 1/8, the interval exactly touches the two points. This shows that the
inequality is sharp, since no smaller interval could be taken while keeping the inequality true.
# Plot interval when p = 1/8
plot_chebyshev(0.0, 0.125)
18.6. Random Variables 845
The third shows that for p < 1/8 the interval only contains the center. This does not invalidate the
inequality since we only needed to ensure that no more than 1/4 of the probability falls outside
the interval, which means that once p < 1/8, the two points at a ?? 2 and a + 2 can be discarded.
# Plot interval when p < 1/8
plot_chebyshev(0.0, 0.05)
Means and Variances in the Continuum
This has all been in terms of discrete random variables, but the case of continuous random variables
is similar. To intuitively understand how this works, imagine that we split the real number
line into intervals of length ? given by (?i; ?(i + 1)]. Once we do this, our continuous random variable
has been made discrete and we can use (18.6.14) say that
X 
?
i
(?i)P(X 2 (?i; ?(i + 1)])

?
i
(?i)pX(?i)?;
(18.6.22)
846 Chapter 18. Appendix: Mathematics for Deep Learning
where pX is the density of X. This is an approximation to the integral of xpX(x), so we can conclude
that
X =
? 1
??1
xpX(x) dx: (18.6.23)
Similarly, using (18.6.16) the variance can be written as
2X
= E[X2] ?? 2
X =
? 1
??1
x2pX(x) dx ??
(? 1
??1
xpX(x) dx
)2
: (18.6.24)
Everything stated above about the mean, the variance, and the standard deviation still applies in
this case. For instance, if we consider the random variable with density
p(x) =
{
1 x 2 [0; 1];
0 otherwise:
(18.6.25)
we can compute
X =
? 1
??1
xp(x) dx =
? 1
0
x dx =
1
2
: (18.6.26)
and
2X
=
? 1
??1
x2p(x) dx ??
(
1
2
)2
=
1
3
?? 1
4
=
1
12
: (18.6.27)
As a warning, let us examine one more example, known as the Cauchy distribution. This is the
distribution with p.d.f. given by
p(x) =
1
1 + x2 : (18.6.28)
# Plot the Cauchy distribution p.d.f.
x = np.arange(-5, 5, 0.01)
p = 1 / (1 + x**2)
d2l.plot(x, p, 'x', 'p.d.f.')
18.6. Random Variables 847
This function looks innocent, and indeed consulting a table of integrals will show it has area one
under it, and thus it defines a continuous random variable.
To see what goes astray, let us try to compute the variance of this. This would involve using
(18.6.16) computing
? 1
??1
x2
1 + x2 dx: (18.6.29)
The function on the inside looks like this:
# Plot the integrand needed to compute the variance
x = np.arange(-20, 20, 0.01)
p = x**2 / (1 + x**2)
d2l.plot(x, p, 'x', 'integrand')
This function clearly has infinite area under it since it is essentially the constant one with a small
dip near zero, and indeed we could show that
? 1
??1
x2
1 + x2 dx = 1: (18.6.30)
This means it does not have a well-defined finite variance.
However, looking deeper shows an even more disturbing result. Let us try to compute the mean
using (18.6.14). Using the change of variables formula, we see
X =
? 1
??1
x
1 + x2 dx =
1
2
? 1
1
1
u
du: (18.6.31)
The integral inside is the definition of the logarithm, so this is in essence log(1) = 1, so there is
no well-defined average value either!
Machine learning scientists define their models so that we most often do not need to deal with
these issues, and will in the vast majority of cases deal with random variables with well-defined
means and variances. However, every so often random variables with heavy tails (that is those
random variables where the probabilities of getting large values are large enough to make things
like the mean or variance undefined) are helpful in modeling physical systems, thus it is worth
knowing that they exist.
848 Chapter 18. Appendix: Mathematics for Deep Learning
Joint Density Functions
The above work all assumes we are working with a single real valued random variable. But what
if we are dealing with two or more potentially highly correlated random variables? This circumstance
is the norm in machine learning: imagine random variables like Ri;j which encode the red
value of the pixel at the (i; j) coordinate in an image, or Pt which is a random variable given by
a stock price at time t. Nearby pixels tend to have similar color, and nearby times tend to have
similar prices. We cannot treat them as separate random variables, and expect to create a successful
model (we will see in Section 18.9 a model that under-performs due to such an assumption).
We need to develop the mathematical language to handle these correlated continuous random
variables.
Thankfully, with the multiple integrals in Section 18.5 we can develop such a language. Suppose
that we have, for simplicity, two random variables X; Y which can be correlated. Then, similar to
the case of a single variable, we can ask the question:
P(X is in an ?-sized interval around x and Y is in an ?-sized interval around y): (18.6.32)
Similar reasoning to the single variable case shows that this should be approximately
P(X is in an ?-sized interval around x and Y is in an ?-sized interval around y)  ?2p(x; y);
(18.6.33)
for some function p(x; y). This is referred to as the joint density of X and Y . Similar properties
are true for this as we saw in the single variable case. Namely:
� p(x; y)  0;
�
?
R2 p(x; y) dx dy = 1;
� P((X; Y ) 2 D) =
?
D p(x; y) dx dy.
In this way, we can deal with multiple, potentially correlated random variables. If we wish to
work with more than two random variables, we can extend the multivariate density to as many
coordinates as desired by considering p(x) = p(x1; : : : ; xn). The same properties of being nonnegative,
and having total integral of one still hold.
Marginal Distributions
When dealing with multiple variables, we oftentimes want to be able to ignore the relationships
and ask, �how is this one variable distributed?� Such a distribution is called a marginal distribution.
To be concrete, let us suppose that we have two random variables X; Y with joint density given by
pX;Y (x; y). We will be using the subscript to indicate what random variables the density is for. The
question of finding the marginal distribution is taking this function, and using it to find pX(x).
As with most things, it is best to return to the intuitive picture to figure out what should be true.
Recall that the density is the function pX so that
P(X 2 [x; x + ?])  ?  pX(x): (18.6.34)
There is no mention of Y , but if all we are given is pX;Y , we need to include Y somehow. We can
first observe that this is the same as
P(X 2 [x; x + ?], and Y 2 R)  ?  pX(x): (18.6.35)
18.6. Random Variables 849
Our density does not directly tell us about what happens in this case, we need to split into small
intervals in y as well, so we can write this as
?  pX(x) 
?
i
P(X 2 [x; x + ?], and Y 2 [?  i; ?  (i + 1)])

?
i
?2pX;Y (x; ?  i):
(18.6.36)
Fig. 18.6.1: By summing along the columns of our array of probabilities, we are able to obtain the
marginal distribution for just the random variable represented along the x-axis.
This tells us to add up the value of the density along a series of squares in a line as is shown in Fig.
18.6.1. Indeed, after canceling one factor of epsilon from both sides, and recognizing the sum on
the right is the integral over y, we can conclude that
pX(x) 
?
i
?pX;Y (x; ?  i)

? 1
??1
pX;Y (x; y) dy:
(18.6.37)
Thus we see
pX(x) =
? 1
??1
pX;Y (x; y) dy: (18.6.38)
This tells us that to get a marginal distribution, we integrate over the variables we do not care
about. This process is often referred to as integrating out or marginalized out the unneeded variables.
Covariance
When dealing with multiple random variables, there is one additional summary statistic which
is helpful to know: the covariance. This measures the degree that two random variable fluctuate
together.
Suppose that we have two random variables X and Y , to begin with, let us suppose they are discrete,
taking on values (xi; yj) with probability pij . In this case, the covariance is defined as
XY = Cov(X; Y ) =
?
i;j
(xi ?? X)(yj ?? Y )pij : = E[XY ] ?? E[X]E[Y ]: (18.6.39)
850 Chapter 18. Appendix: Mathematics for Deep Learning
To think about this intuitively: consider the following pair of random variables. Suppose that X
takes the values 1 and 3, and Y takes the values ??1 and 3. Suppose that we have the following
probabilities
P(X = 1 and Y = ??1) =
p
2
;
P(X = 1 and Y = 3) =
1 ?? p
2
;
P(X = 3 and Y = ??1) =
1 ?? p
2
;
P(X = 3 and Y = 3) =
p
2
;
(18.6.40)
where p is a parameter in [0; 1] we get to pick. Notice that if p = 1 then they are both always
their minimum or maximum values simultaneously, and if p = 0 they are guaranteed to take their
flipped values simultaneously (one is large when the other is small and vice versa). If p = 1/2,
then the four possibilities are all equally likely, and neither should be related. Let us compute the
covariance. First, note X = 2 and Y = 1, so we may compute using (18.6.39):
Cov(X; Y ) =
?
i;j
(xi ?? X)(yj ?? Y )pij
= (1 ?? 2)(??1 ?? 1)
p
2
+ (1 ?? 2)(3 ?? 1)
1 ?? p
2
+ (3 ?? 2)(??1 ?? 1)
1 ?? p
2
+ (3 ?? 2)(3 ?? 1)
p
2
= 4p ?? 2:
(18.6.41)
When p = 1 (the case where they are both maximally positive or negative at the same time) has a
covariance of 2. When p = 0 (the case where they are flipped) the covariance is ??2. Finally, when
p = 1/2 (the case where they are unrelated), the covariance is 0. Thus we see that the covariance
measures how these two random variables are related.
A quick note on the covariance is that it only measures these linear relationships. More complex
relationships like X = Y 2 where Y is randomly chosen from f??2;??1; 0; 1; 2g with equal probability
can be missed. Indeed a quick computation shows that these random variables have covariance
zero, despite one being a deterministic function of the other.
For continuous random variables, much the same story holds. At this point, we are pretty comfortable
with doing the transition between discrete and continuous, so we will provide the continuous
analogue of (18.6.39) without any derivation.
XY =
?
R2
(x ?? X)(y ?? Y )p(x; y) dx dy: (18.6.42)
For visualization, let us take a look at a collection of random variables with tunable covariance.
# Plot a few random variables adjustable covariance
covs = [-0.9, 0.0, 1.2]
d2l.plt.figure(figsize=(12, 3))
for i in range(3):
X = np.random.normal(0, 1, 500)
Y = covs[i]*X + np.random.normal(0, 1, (500))
d2l.plt.subplot(1, 4, i+1)
d2l.plt.scatter(X.asnumpy(), Y.asnumpy())
d2l.plt.xlabel('X')
(continues on next page)
18.6. Random Variables 851
(continued from previous page)
d2l.plt.ylabel('Y')
d2l.plt.title(f'cov = {covs[i]}')
d2l.plt.show()
Let us see some properties of covariances:
� For any random variable X, Cov(X;X) = Var(X).
� For any random variables X; Y and numbers a and b, Cov(aX + b; Y ) = Cov(X; aY + b) =
aCov(X; Y ).
� If X and Y are independent then Cov(X; Y ) = 0.
In addition, we can use the covariance to expand a relationship we saw before. Recall that is X
and Y are two independent random variables then
Var(X + Y ) = Var(X) + Var(Y ): (18.6.43)
With knowledge of covariances, we can expand this relationship. Indeed, some algebra can show
that in general,
Var(X + Y ) = Var(X) + Var(Y ) + 2Cov(X; Y ): (18.6.44)
This allows us to generalize the variance summation rule for correlated random variables.
Correlation
As we did in the case of means and variances, let us now consider units. If X is measured in one
unit (say inches), and Y is measured in another (say dollars), the covariance is measured in the
product of these two units inches  dollars. These units can be hard to interpret. What we will
often want in this case is a unit-less measurement of relatedness. Indeed, often we do not care
about exact quantitative correlation, but rather ask if the correlation is in the same direction, and
how strong the relationship is.
To see what makes sense, let us perform a thought experiment. Suppose that we convert our random
variables in inches and dollars to be in inches and cents. In this case the random variable Y is
852 Chapter 18. Appendix: Mathematics for Deep Learning
multiplied by 100. If we work through the definition, this means that Cov(X; Y ) will be multiplied
by 100. Thus we see that in this case a change of units change the covariance by a factor of 100.
Thus, to find our unit-invariant measure of correlation, we will need to divide by something else
that also gets scaled by 100. Indeed we have a clear candidate, the standard deviation! Indeed if
we define the correlation coefficient to be
(X; Y ) =
Cov(X; Y )
XY
; (18.6.45)
we see that this is a unit-less value. A little mathematics can show that this number is between ??1
and 1 with 1 meaning maximally positively correlated, whereas ??1 means maximally negatively
correlated.
Returning to our explicit discrete example above, we can see that X = 1 and Y = 2, so we can
compute the correlation between the two random variables using (18.6.45) to see that
(X; Y ) =
4p ?? 2
1  2
= 2p ?? 1: (18.6.46)
This now ranges between ??1 and 1 with the expected behavior of 1 meaning most correlated, and
??1 meaning minimally correlated.
As another example, consider X as any random variable, and Y = aX +b as any linear deterministic
function of X. Then, one can compute that
Y = aX+b = jajX; (18.6.47)
Cov(X; Y ) = Cov(X;aX + b) = aCov(X;X) = aVar(X); (18.6.48)
and thus by (18.6.45) that
(X; Y ) =
aVar(X)
jaj2X
=
a
jaj = sign(a): (18.6.49)
Thus we see that the correlation is +1 for any a > 0, and ??1 for any a < 0 illustrating that correlation
measures the degree and directionality the two random variables are related, not the scale
that the variation takes.
Let us again plot a collection of random variables with tunable correlation.
# Plot a few random variables adjustable correlations
cors = [-0.9, 0.0, 1.0]
d2l.plt.figure(figsize=(12, 3))
for i in range(3):
X = np.random.normal(0, 1, 500)
Y = cors[i] * X + np.sqrt(1 - cors[i]**2) * np.random.normal(0, 1, 500)
d2l.plt.subplot(1, 4, i + 1)
d2l.plt.scatter(X.asnumpy(), Y.asnumpy())
d2l.plt.xlabel('X')
d2l.plt.ylabel('Y')
d2l.plt.title(f'cor = {cors[i]}')
d2l.plt.show()
18.6. Random Variables 853
Let us list a few properties of the correlation below.
� For any random variable X, (X;X) = 1.
� For any random variablesX; Y and numbers a and b, (aX+b; Y ) = (X; aY +b) = (X; Y ).
� If X and Y are independent with non-zero variance then (X; Y ) = 0.
As a final note, you may feel like some of these formulae are familiar. Indeed, if we expand everything
out assuming that X = Y = 0, we see that this is
(X; Y ) =
?
i;j xiyipij v?
i;j x2i
pij
v?
i;j y2
j pij
: (18.6.50)
This looks like a sum of a product of terms divided by the square root of sums of terms. This is
exactly the formula for the cosine of the angle between two vectors v; w with the different coordinates
weighted by pij :
cos() =
v  w
?v??w? =
?
v? i viwi
i v2
i
v?
i w2
i
: (18.6.51)
Indeed if we think of norms as being related to standard deviations, and correlations as being
cosines of angles, much of the intuition we have from geometry can be applied to thinking about
random variables.
Summary
� Continuous random variables are random variables that can take on a continuum of values.
They have some technical difficulties that make them more challenging to work with compared
to discrete random variables.
� The probability density function allows us to work with continuous random variables by
giving a function where the area under the curve on some interval gives the probability of
finding a sample point in that interval.
� The cumulative distribution function is the probability of observing the random variable to
be less than a given threshold. It can provide a useful alternate viewpoint which unifies
discrete and continuous variables.
854 Chapter 18. Appendix: Mathematics for Deep Learning
� The mean is the average value of a random variable.
� The variance is the expected square of the difference between the random variable and its
mean.
� The standard deviation is the square root of the variance. It can be thought of as measuring
the range of values the random variable may take.
� Chebyshev?s inequality allows us to make this intuition rigorous by giving an explicit interval
that contains the random variable most of the time.
� Joint densities allow us to work with correlated random variables. We may marginalize joint
densities by integrating over unwanted random variables to get the distribution of the desired
random variable.
� The covariance and correlation coefficient provide a way to measure any linear relationship
between two correlated random variables.
Exercises
1. Suppose that we have the random variable with density given by p(x) = 1
x2 for x  1 and
p(x) = 0 otherwise. What is P(X > 2)?
2. The Laplace distribution is a random variable whose density is given by p(x = 1
2e??jxj. What
is the mean and the standard deviation of this function? As a hint,
? 1
0 xe?? ? x dx = 1 and 1
0 x2e??x dx = 2.
3. I walk up to you on the street and say �I have a random variable with mean 1, standard deviation
2, and I observed 25% of my samples taking a value larger than 9.� Do you believe me?
Why or why not?
4. Suppose that you have two random variables X; Y , with joint density given by pXY (x; y) =
4xy for x; y 2 [0; 1] and pXY (x; y) = 0 otherwise. What is the covariance of X and Y ?
Discussions250
18.7 Maximum Likelihood
One of the most commonly encountered way of thinking in machine learning is the maximum
likelihood point of view. This is the concept that when working with a probabilistic model with
unknown parameters, the parameters which make the data have the highest probability are the
most likely ones.
250 https://discuss.d2l.ai/t/415
18.7. Maximum Likelihood 855
18.7.1 The Maximum Likelihood Principle
This has a Bayesian interpretation which can be helpful to think about. Suppose that we have a
model with parameters  and a collection of data examples X. For concreteness, we can imagine
that  is a single value representing the probability that a coin comes up heads when flipped, and
X is a sequence of independent coin flips. We will look at this example in depth later.
If we want to find the most likely value for the parameters of our model, that means we want to
find
argmax P( j X): (18.7.1)
By Bayes? rule, this is the same thing as
argmax P(X j )P()
P(X)
: (18.7.2)
The expression P(X), a parameter agnostic probability of generating the data, does not depend
on  at all, and so can be dropped without changing the best choice of . Similarly, we may now
posit that we have no prior assumption on which set of parameters are better than any others, so
we may declare that P() does not depend on theta either! This, for instance, makes sense in our
coin flipping example where the probability it comes up heads could be any value in [0; 1] without
any prior belief it is fair or not (often referred to as an uninformative prior). Thus we see that our
application of Bayes? rule shows that our best choice of  is the maximum likelihood estimate for
:
^
= argmax

P(X j ): (18.7.3)
As a matter of common terminology, the probability of the data given the parameters (P(X j ))
is referred to as the likelihood.
A Concrete Example
Let us see how this works in a concrete example. Suppose that we have a single parameter 
representing the probability that a coin flip is heads. Then the probability of getting a tails is 1??,
and so if our observed data X is a sequence with nH heads and nT tails, we can use the fact that
independent probabilities multiply to see that
P(X j ) = nH(1 ?? )nT : (18.7.4)
If we flip 13 coins and get the sequence �HHHTHTTHHHHHT�, which has nH = 9 and nT = 4, we
see that this is
P(X j ) = 9(1 ?? )4: (18.7.5)
One nice thing about this example will be that we know the answer going in. Indeed, if we said
verbally, �I flipped 13 coins, and 9 came up heads, what is our best guess for the probability that
the coin comes us heads?,� everyone would correctly guess 9/13. What this maximum likelihood
method will give us is a way to get that number from first principals in a way that will generalize
to vastly more complex situations.
For our example, the plot of P(X j ) is as follows:
856 Chapter 18. Appendix: Mathematics for Deep Learning
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import autograd, np, npx
npx.set_np()
theta = np.arange(0, 1, 0.001)
p = theta**9 * (1 - theta)**4.
d2l.plot(theta, p, 'theta', 'likelihood')
This has its maximum value somewhere near our expected 9/13  0:7 : : :. To see if it is exactly
there, we can turn to calculus. Notice that at the maximum, the function is flat. Thus, we could
find the maximum likelihood estimate (18.7.1) by finding the values of  where the derivative is
zero, and finding the one that gives the highest probability. We compute:
0 =
d
d
P(X j )
=
d
d
9(1 ?? )4
= 98(1 ?? )4 ?? 49(1 ?? )3
= 8(1 ?? )3(9 ?? 13):
(18.7.6)
This has three solutions: 0, 1 and 9/13. The first two are clearly minima, not maxima as they assign
probability 0 to our sequence. The final value does not assign zero probability to our sequence,
and thus must be the maximum likelihood estimate ^ = 9/13.
18.7.2 Numerical Optimization and the Negative Log-Likelihood
The previous example is nice, but what if we have billions of parameters and data examples.
First notice that, if we make the assumption that all the data examples are independent, we can
no longer practically consider the likelihood itself as it is a product of many probabilities. Indeed,
each probability is in [0; 1], say typically of value about 1/2, and the product of (1/2)1000000000 is far
below machine precision. We cannot work with that directly.
However, recall that the logarithm turns products to sums, in which case
log((1/2)1000000000) = 1000000000  log(1/2)  ??301029995:6 : : : (18.7.7)
18.7. Maximum Likelihood 857
This number fits perfectly within even a single precision 32-bit float. Thus, we should consider
the log-likelihood, which is
log(P(X j )): (18.7.8)
Since the function x 7! log(x) is increasing, maximizing the likelihood is the same thing as maximizing
the log-likelihood. Indeed in Section 18.9 we will see this reasoning applied when working
with the specific example of the naive Bayes classifier.
We often work with loss functions, where we wish to minimize the loss. We may turn maximum
likelihood into the minimization of a loss by taking ??log(P(X j )), which is the negative loglikelihood.
To illustrate this, consider the coin flipping problem from before, and pretend that we do not know
the closed form solution. We may compute that
??log(P(X j )) = ??log(nH(1 ?? )nT ) = ??(nH log() + nT log(1 ?? )): (18.7.9)
This can be written into code, and freely optimized even for billions of coin flips.
# Set up our data
n_H = 8675309
n_T = 25624
# Initialize our paramteres
theta = np.array(0.5)
theta.attach_grad()
# Perform gradient descent
lr = 0.00000000001
for iter in range(10):
with autograd.record():
loss = -(n_H * np.log(theta) + n_T * np.log(1 - theta))
loss.backward()
theta -= lr * theta.grad
# Check output
theta, n_H / (n_H + n_T)
(array(0.50172704), 0.9970550284664874)
Numerical convenience is only one reason people like to use negative log-likelihoods. Indeed,
there are a several reasons that it can be preferable.
The second reason we consider the log-likelihood is the simplified application of calculus rules. As
discussed above, due to independence assumptions, most probabilities we encounter in machine
learning are products of individual probabilities.
P(X j ) = p(x1 j )  p(x2 j )    p(xn j ): (18.7.10)
858 Chapter 18. Appendix: Mathematics for Deep Learning
This means that if we directly apply the product rule to compute a derivative we get
@
@
P(X j ) =
(
@
@
P(x1 j )
)
 P(x2 j )    P(xn j )
+ P(x1 j ) 
(
@
@
P(x2 j )
)
   P(xn j )
...
+ P(x1 j )  P(x2 j )   
(
@
@
P(xn j )
)
:
(18.7.11)
This requires n(n??1) multiplications, along with (n??1) additions, so it is total of quadratic time
in the inputs! Sufficient cleverness in grouping terms will reduce this to linear time, but it requires
some thought. For the negative log-likelihood we have instead
??log (P(X j )) = ??log(P(x1 j )) ?? log(P(x2 j ))    ?? log(P(xn j )); (18.7.12)
which then gives
?? @
@
log (P(X j )) =
1
P(x1 j )
(
@
@
P(x1 j )
)
+    +
1
P(xn j )
(
@
@
P(xn j )
)
: (18.7.13)
This requires only n divides and n ?? 1 sums, and thus is linear time in the inputs.
The third and final reason to consider the negative log-likelihood is the relationship to information
theory, which we will discuss in detail in Section 18.11. This is a rigorous mathematical theory
which gives a way to measure the degree of information or randomness in a random variable.
The key object of study in that field is the entropy which is
H(p) = ??
?
i
pi log2(pi); (18.7.14)
which measures the randomness of a source. Notice that this is nothing more than the average
??log probability, and thus if we take our negative log-likelihood and divide by the number of data
examples, we get a relative of entropy known as cross-entropy. This theoretical interpretation
alone would be sufficiently compelling to motivate reporting the average negative log-likelihood
over the dataset as a way of measuring model performance.
18.7.3 Maximum Likelihood for Continuous Variables
Everything that we have done so far assumes we are working with discrete random variables, but
what if we want to work with continuous ones?
The short summary is that nothing at all changes, except we replace all the instances of the probability
with the probability density. Recalling that we write densities with lower case p, this means
that for example we now say
??log (p(X j )) = ??log(p(x1 j )) ?? log(p(x2 j ))    ?? log(p(xn j )) = ??
?
i
log(p(xi j )):
(18.7.15)
The question becomes, �Why is this OK?� After all, the reason we introduced densities was because
probabilities of getting specific outcomes themselves was zero, and thus is not the probability of
generating our data for any set of parameters zero?
18.7. Maximum Likelihood 859
Indeed, this is the case, and understanding why we can shift to densities is an exercise in tracing
what happens to the epsilons.
Let us first re-define our goal. Suppose that for continuous random variables we no longer want
to compute the probability of getting exactly the right value, but instead matching to within some
range ?. For simplicity, we assume our data is repeated observations x1; : : : ; xN of identically distributed
random variables X1; : : : ;XN. As we have seen previously, this can be written as
P(X1 2 [x1; x1 + ?];X2 2 [x2; x2 + ?]; : : : ;XN 2 [xN; xN + ?] j )
?Np(x1 j )  p(x2 j )    p(xn j ):
(18.7.16)
Thus, if we take negative logarithms of this we obtain
?? log(P(X1 2 [x1; x1 + ?];X2 2 [x2; x2 + ?]; : : : ;XN 2 [xN; xN + ?] j ))
 ?? N log(?) ??
?
i
log(p(xi j )): (18.7.17)
If we examine this expression, the only place that the ? occurs is in the additive constant??N log(?).
This does not depend on the parameters  at all, so the optimal choice of  does not depend on
our choice of ?! If we demand four digits or four-hundred, the best choice of  remains the same,
thus we may freely drop the epsilon to see that what we want to optimize is
??
?
i
log(p(xi j )): (18.7.18)
Thus, we see that the maximum likelihood point of view can operate with continuous random
variables as easily as with discrete ones by replacing the probabilities with probability densities.
Summary
� The maximum likelihood principle tells us that the best fit model for a given dataset is the
one that generates the data with the highest probability.
� Often people work with the negative log-likelihood instead for a variety of reasons: numerical
stability, conversion of products to sums (and the resulting simplification of gradient
computations), and theoretical ties to information theory.
� While simplest to motivate in the discrete setting, it may be freely generalized to the continuous
setting as well by maximizing the probability density assigned to the datapoints.
Exercises
1. Suppose that you know that a random variable has density 1
e??x for some value . You
obtain a single observation from the random variable which is the number 3. What is the
maximum likelihood estimate for ?
2. Suppose that you have a dataset of samples fxigN i=1 drawn from a Gaussian with unknown
mean, but variance 1. What is the maximum likelihood estimate for the mean?
Discussions251
251 https://discuss.d2l.ai/t/416
860 Chapter 18. Appendix: Mathematics for Deep Learning
18.8 Distributions
Now that we have learned how to work with probability in both the discrete and the continuous
setting, let us get to know some of the common distributions encountered. Depending on the area
of machine learning, we may need to be familiar with vastly more of these, or for some areas of
deep learning potentially none at all. This is, however, a good basic list to be familiar with. Let us
first import some common libraries.
%matplotlib inline
from d2l import mxnet as d2l
from IPython import display
from math import erf, factorial
import numpy as np
18.8.1 Bernoulli
This is the simplest random variable usually encountered. This random variable encodes a coin
flip which comes up 1 with probability p and 0 with probability 1??p. If we have a random variable
X with this distribution, we will write
X  Bernoulli(p): (18.8.1)
The cumulative distribution function is
F(x) =
8><
>:
0 x < 0;
1 ?? p 0  x < 1;
1 x >= 1:
(18.8.2)
The probability mass function is plotted below.
p = 0.3
d2l.set_figsize()
d2l.plt.stem([0, 1], [1 - p, p], use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
18.8. Distributions 861
Now, let us plot the cumulative distribution function (18.8.2).
x = np.arange(-1, 2, 0.01)
def F(x):
return 0 if x < 0 else 1 if x > 1 else 1 - p
d2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')
If X  Bernoulli(p), then:
� X = p,
� 2X
= p(1 ?? p).
We can sample an array of arbitrary shape from a Bernoulli random variable as follows.
1*(np.random.rand(10, 10) < p)
array([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],
[1, 0, 0, 1, 0, 0, 0, 1, 0, 0],
[0, 0, 1, 1, 0, 1, 1, 0, 1, 1],
[0, 1, 0, 0, 1, 0, 0, 1, 1, 0],
[0, 1, 0, 1, 0, 1, 0, 0, 1, 1],
[1, 0, 1, 0, 0, 1, 1, 1, 0, 0],
[1, 0, 0, 1, 0, 0, 0, 1, 0, 0],
[0, 0, 0, 0, 0, 0, 1, 1, 0, 0],
[1, 1, 0, 1, 0, 0, 0, 0, 0, 0],
[1, 1, 0, 0, 1, 0, 0, 1, 1, 1]])
862 Chapter 18. Appendix: Mathematics for Deep Learning
18.8.2 Discrete Uniform
The next commonly encountered random variable is a discrete uniform. For our discussion here,
we will assume that it is supported on the integers f1; 2; : : : ; ng, however any other set of values
can be freely chosen. The meaning of the word uniform in this context is that every possible value
is equally likely. The probability for each value i 2 f1; 2; 3; : : : ; ng is pi = 1
n. We will denote a
random variable X with this distribution as
X  U(n): (18.8.3)
The cumulative distribution function is
F(x) =
8><
>:
0 x < 1;
k
n k  x < k + 1 with 1  k < n;
1 x >= n:
(18.8.4)
Let us first plot the probability mass function.
n = 5
d2l.plt.stem([i+1 for i in range(n)], n*[1 / n], use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
Now, let us plot the cumulative distribution function (18.8.4).
x = np.arange(-1, 6, 0.01)
def F(x):
return 0 if x < 1 else 1 if x > n else np.floor(x) / n
d2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')
18.8. Distributions 863
If X  U(n), then:
� X = 1+n
2 ,
� 2X
= n2??1
12 .
We can sample an array of arbitrary shape from a discrete uniform random variable as follows.
np.random.randint(1, n, size=(10, 10))
array([[1, 4, 3, 2, 1, 1, 3, 1, 2, 1],
[1, 3, 2, 3, 3, 2, 1, 4, 2, 3],
[4, 4, 1, 4, 3, 2, 1, 4, 3, 3],
[4, 4, 1, 4, 3, 3, 3, 2, 1, 4],
[3, 4, 3, 1, 2, 2, 2, 4, 2, 1],
[2, 1, 1, 2, 3, 3, 4, 1, 1, 4],
[1, 1, 1, 4, 2, 1, 2, 1, 3, 1],
[2, 2, 3, 2, 1, 4, 1, 3, 4, 2],
[2, 4, 1, 3, 4, 3, 1, 4, 4, 3],
[3, 4, 3, 3, 4, 4, 1, 4, 2, 2]])
18.8.3 Continuous Uniform
Next, let us discuss the continuous uniform distribution. The idea behind this random variable
is that if we increase the n in the discrete uniform distribution, and then scale it to fit within the
interval [a; b], we will approach a continuous random variable that just picks an arbitrary value in
[a; b] all with equal probability. We will denote this distribution as
X  U(a; b): (18.8.5)
The probability density function is
p(x) =
{
1
b??a x 2 [a; b];
0 x ?2 [a; b]:
(18.8.6)
864 Chapter 18. Appendix: Mathematics for Deep Learning
The cumulative distribution function is
F(x) =
8><
>:
0 x < a;
x??a
b??a x 2 [a; b];
1 x >= b:
(18.8.7)
Let us first plot the probability density function (18.8.6).
a, b = 1, 3
x = np.arange(0, 4, 0.01)
p = (x > a)*(x < b)/(b - a)
d2l.plot(x, p, 'x', 'p.d.f.')
Now, let us plot the cumulative distribution function (18.8.7).
def F(x):
return 0 if x < a else 1 if x > b else (x - a) / (b - a)
d2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')
If X  U(a; b), then:
18.8. Distributions 865
� X = a+b
2 ,
� 2X
= (b??a)2
12 .
We can sample an array of arbitrary shape from a uniform random variable as follows. Note that
it by default samples from a U(0; 1), so if we want a different range we need to scale it.
(b - a) * np.random.rand(10, 10) + a
array([[1.4263995 , 1.32234442, 1.98789028, 2.62305405, 1.81802665,
1.7479621 , 1.86588587, 1.43083642, 2.0871248 , 2.16239639],
[1.18332507, 2.67369682, 1.507526 , 2.41555196, 1.02998459,
2.33625933, 1.20203081, 2.08790005, 2.01575794, 1.82344995],
[2.89199317, 2.14589118, 1.85481229, 1.33449574, 2.93617602,
2.98310547, 2.96235126, 2.14785852, 2.77316042, 2.62551597],
[2.14243349, 2.9940622 , 1.19788307, 1.71621293, 1.32188867,
2.61265492, 2.15860731, 2.55533404, 1.75299553, 1.83622521],
[2.92449044, 2.28884528, 1.94550309, 2.76315966, 2.10196016,
2.51960426, 1.56423267, 1.5574463 , 1.70708128, 2.53040129],
[2.22330704, 1.77008105, 2.36421772, 2.7777458 , 2.25120004,
1.35396632, 1.9726549 , 2.27340037, 1.74838093, 2.92967289],
[2.78088808, 1.12106631, 1.93326138, 1.34366358, 2.78746558,
1.73978549, 1.59001827, 2.29027882, 2.25947092, 1.37424085],
[2.22085405, 1.0699301 , 1.46896539, 1.91263425, 1.97568991,
2.16305974, 1.88500816, 2.56292776, 2.62649923, 2.2548305 ],
[2.87977449, 2.584483 , 2.74738304, 1.64931996, 1.90518891,
2.04801046, 1.67777837, 2.35125907, 2.55353478, 1.69605429],
[2.97503039, 2.78367052, 1.48010665, 1.36524376, 2.45156427,
2.15156029, 2.13933111, 2.12242048, 1.04232867, 2.92524385]])
18.8.4 Binomial
Let us make things a little more complex and examine the binomial random variable. This random
variable originates from performing a sequence of n independent experiments, each of which has
probability p of succeeding, and asking how many successes we expect to see.
Let us express this mathematically. Each experiment is an independent random variableXi where
we will use 1 to encode success, and 0 to encode failure. Since each is an independent coin flip
which is successful with probability p, we can say that Xi  Bernoulli(p). Then, the binomial
random variable is
X =
?n
i=1
Xi: (18.8.8)
In this case, we will write
X  Binomial(n; p): (18.8.9)
To get the cumulative distribution function, we need to notice that getting exactly k successes can
occur in
(n
k
)
= n!
k!(n??k)! ways each of which has a probability of pk(1??p)n??k of occurring. Thus the
cumulative distribution function is
F(x) =
8><
>:
0 x < 0; ?
mk
(n
m
)
pm(1 ?? p)n??m k  x < k + 1 with 0  k < n;
1 x >= n:
(18.8.10)
866 Chapter 18. Appendix: Mathematics for Deep Learning
Let us first plot the probability mass function.
n, p = 10, 0.2
# Compute binomial coefficient
def binom(n, k):
comb = 1
for i in range(min(k, n - k)):
comb = comb * (n - i) // (i + 1)
return comb
pmf = np.array([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])
d2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
Now, let us plot the cumulative distribution function (18.8.10).
x = np.arange(-1, 11, 0.01)
cmf = np.cumsum(pmf)
def F(x):
return 0 if x < 0 else 1 if x > n else cmf[int(x)]
d2l.plot(x, np.array([F(y) for y in x.tolist()]), 'x', 'c.d.f.')
18.8. Distributions 867
While this result is not simple, the means and variances are. If X  Binomial(n; p), then:
� X = np,
� 2X
= np(1 ?? p).
This can be sampled as follows.
np.random.binomial(n, p, size=(10, 10))
array([[2, 2, 2, 3, 4, 2, 5, 1, 2, 2],
[1, 2, 2, 1, 3, 1, 5, 1, 2, 2],
[1, 2, 0, 0, 3, 2, 0, 1, 3, 0],
[0, 2, 2, 1, 6, 0, 1, 3, 3, 0],
[1, 3, 3, 6, 1, 0, 5, 0, 3, 2],
[1, 1, 1, 3, 3, 2, 3, 3, 2, 2],
[4, 2, 2, 0, 1, 3, 3, 2, 0, 2],
[3, 3, 1, 0, 1, 2, 3, 0, 3, 1],
[5, 1, 2, 0, 0, 1, 2, 1, 0, 2],
[0, 2, 2, 1, 0, 2, 2, 2, 1, 1]])
18.8.5 Poisson
Let us now perform a thought experiment. We are standing at a bus stop and we want to know how
many buses will arrive in the next minute. Let us start by considering X(1)  Bernoulli(p) which
is simply the probability that a bus arrives in the one minute window. For bus stops far from an
urban center, this might be a pretty good approximation. We may never see more than one bus in
a minute.
However, if we are in a busy area, it is possible or even likely that two buses will arrive. We can
model this by splitting our random variable into two parts for the first 30 seconds, or the second
30 seconds. In this case we can write
X(2)  X(2)
1 + X(2)
2 ; (18.8.11)
where X(2) is the total sum, and X(2)
i
 Bernoulli(p/2). The total distribution is then X(2) 
Binomial(2; p/2).
868 Chapter 18. Appendix: Mathematics for Deep Learning
Why stop here? Let us continue to split that minute into n parts. By the same reasoning as above,
we see that
X(n)  Binomial(n; p/n): (18.8.12)
Consider these random variables. By the previous section, we know that (18.8.12) has mean
X(n) = n(p/n) = p, and variance 2
X(n) = n(p/n)(1 ?? (p/n)) = p(1 ?? p/n). If we take n ! 1,
we can see that these numbers stabilize to X(1) = p, and variance 2
X(1) = p. This indicates that
there could be some random variable we can define in this infinite subdivision limit.
This should not come as too much of a surprise, since in the real world we can just count the
number of bus arrivals, however it is nice to see that our mathematical model is well defined.
This discussion can be made formal as the law of rare events.
Following through this reasoning carefully, we can arrive at the following model. We will say that
X  Poisson() if it is a random variable which takes the values f0; 1; 2; : : :g with probability
pk =
ke??
k!
: (18.8.13)
The value  > 0 is known as the rate (or the shape parameter), and denotes the average number of
arrivals we expect in one unit of time.
We may sum this probability mass function to get the cumulative distribution function.
F(x) =
{
0 x < 0;
e???k
m=0
m
m! k  x < k + 1 with 0  k:
(18.8.14)
Let us first plot the probability mass function (18.8.13).
lam = 5.0
xs = [i for i in range(20)]
pmf = np.array([np.exp(-lam) * lam**k / factorial(k) for k in xs])
d2l.plt.stem(xs, pmf, use_line_collection=True)
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.show()
Now, let us plot the cumulative distribution function (18.8.14).
18.8. Distributions 869
x = np.arange(-1, 21, 0.01)
cmf = np.cumsum(pmf)
def F(x):
return 0 if x < 0 else 1 if x > n else cmf[int(x)]
d2l.plot(x, np.array([F(y) for y in x.tolist()]), 'x', 'c.d.f.')
As we saw above, the means and variances are particularly concise. If X  Poisson(), then:
� X = ,
� 2X
= .
This can be sampled as follows.
np.random.poisson(lam, size=(10, 10))
array([[ 8, 4, 3, 6, 6, 11, 2, 4, 2, 8],
[ 6, 4, 7, 3, 4, 1, 5, 6, 4, 1],
[ 4, 6, 6, 3, 9, 3, 5, 6, 4, 5],
[ 5, 5, 6, 3, 9, 2, 4, 12, 5, 7],
[ 6, 5, 6, 3, 4, 4, 5, 2, 0, 7],
[ 4, 3, 1, 8, 2, 5, 7, 2, 8, 7],
[ 4, 6, 9, 10, 3, 5, 2, 3, 7, 5],
[ 5, 4, 3, 8, 4, 3, 4, 5, 2, 3],
[ 8, 7, 6, 2, 3, 1, 2, 5, 2, 7],
[ 8, 4, 7, 4, 6, 5, 4, 7, 5, 5]])
870 Chapter 18. Appendix: Mathematics for Deep Learning
18.8.6 Gaussian
Now Let us try a different, but related experiment. Let us say we again are performing n independent
Bernoulli(p) measurementsXi. The distribution of the sum of these isX(n)  Binomial(n; p).
Rather than taking a limit as n increases and p decreases, Let us fix p, and then send n ! 1. In
this case X(n) = np ! 1 and 2
X(n) = np(1 ?? p) ! 1, so there is no reason to think this limit
should be well defined.
However, not all hope is lost! Let us just make the mean and variance be well behaved by defining
Y (n) =
X(n) ?? X(n)
X(n)
: (18.8.15)
This can be seen to have mean zero and variance one, and so it is plausible to believe that it will
converge to some limiting distribution. If we plot what these distributions look like, we will become
even more convinced that it will work.
p = 0.2
ns = [1, 10, 100, 1000]
d2l.plt.figure(figsize=(10, 3))
for i in range(4):
n = ns[i]
pmf = np.array([p**i * (1-p)**(n-i) * binom(n, i) for i in range(n + 1)])
d2l.plt.subplot(1, 4, i + 1)
d2l.plt.stem([(i - n*p)/np.sqrt(n*p*(1 - p)) for i in range(n + 1)], pmf,
use_line_collection=True)
d2l.plt.xlim([-4, 4])
d2l.plt.xlabel('x')
d2l.plt.ylabel('p.m.f.')
d2l.plt.title("n = {}".format(n))
d2l.plt.show()
One thing to note: compared to the Poisson case, we are now dividing by the standard deviation
which means that we are squeezing the possible outcomes into smaller and smaller areas. This is
an indication that our limit will no longer be discrete, but rather a continuous.
A derivation of what occurs is beyond the scope of this document, but the central limit theorem
states that as n ! 1, this will yield the Gaussian Distribution (or sometimes normal distribution).
More explicitly, for any a; b:
lim
n!1P(Y (n) 2 [a; b]) = P(N(0; 1) 2 [a; b]); (18.8.16)
18.8. Distributions 871
where we say a random variable is normally distributed with given mean  and variance 2, written
X  N(; 2) if X has density
pX(x) =
p 1
22
e
??(x??)2
22 : (18.8.17)
Let us first plot the probability density function (18.8.17).
mu, sigma = 0, 1
x = np.arange(-3, 3, 0.01)
p = 1 / np.sqrt(2 * np.pi * sigma**2) * np.exp(-(x - mu)**2 / (2 * sigma**2))
d2l.plot(x, p, 'x', 'p.d.f.')
Now, let us plot the cumulative distribution function. It is beyond the scope of this appendix, but
the Gaussian c.d.f. does not have a closed-form formula in terms of more elementary functions.
We will use erf which provides a way to compute this integral numerically.
def phi(x):
return (1.0 + erf((x - mu) / (sigma * np.sqrt(2)))) / 2.0
d2l.plot(x, np.array([phi(y) for y in x.tolist()]), 'x', 'c.d.f.')
872 Chapter 18. Appendix: Mathematics for Deep Learning
Keen-eyed readers will recognize some of these terms. Indeed, we encountered this integral in
Section 18.5. Indeed we need exactly that computation to see that this pX(x) has total area one
and is thus a valid density.
Our choice of working with coin flips made computations shorter, but nothing about that choice
was fundamental. Indeed, if we take any collection of independent identically distributed random
variables Xi, and form
X(N) =
?N
i=1
Xi: (18.8.18)
Then
X(N) ?? X(N)
X(N)
(18.8.19)
will be approximately Gaussian. There are additional requirements needed to make it work, most
commonly E[X4] < 1, but the philosophy is clear.
The central limit theorem is the reason that the Gaussian is fundamental to probability, statistics,
and machine learning. Whenever we can say that something we measured is a sum of many
small independent contributions, we can assume that the thing being measured will be close to
Gaussian.
There are many more fascinating properties of Gaussians, and we would like to discuss one more
here. The Gaussian is what is known as a maximum entropy distribution. We will get into entropy
more deeply in Section 18.11, however all we need to know at this point is that it is a measure of
randomness. In a rigorous mathematical sense, we can think of the Gaussian as the most random
choice of random variable with fixed mean and variance. Thus, if we know that our random
variable has some mean and variance, the Gaussian is in a sense the most conservative choice of
distribution we can make.
To close the section, Let us recall that if X  N(; 2), then:
� X = ,
� 2X
= 2.
We can sample from the Gaussian (or standard normal) distribution as shown below.
np.random.normal(mu, sigma, size=(10, 10))
array([[-0.57855698, 0.92902142, -1.28065851, 0.62486226, 0.29431553,
0.48600285, 0.37483924, 0.27128912, -0.91282888, -1.51005878],
[ 2.21706625, 1.91460794, -0.78900758, -0.28104264, -0.20517255,
-0.09443792, -1.44056502, 0.84557181, 0.87875186, 0.11234779],
[ 0.87110348, -0.54684884, 0.84276367, -0.85837276, 0.51980555,
-0.87309186, -0.68363545, 0.11964016, 0.15145944, -0.91087196],
[ 0.49176061, 0.81555132, 0.45626283, 0.67039046, -0.3744689 ,
-1.85015505, -0.05151674, 0.02833729, -1.21424723, -0.00606646],
[ 0.09238888, 0.89545711, 0.45908982, -0.55859736, -0.09947204,
-0.54391891, -0.63874853, 1.64201487, 0.468613 , 0.20090474],
[ 0.03436811, -0.44335993, -0.2909628 , -1.39664818, 0.61456931,
-1.78495687, 0.89107593, 0.13847772, 1.8170571 , 1.65123838],
[-1.39054039, 1.37435581, 0.56844177, 0.84152274, -0.413917 ,
(continues on next page)
18.8. Distributions 873
(continued from previous page)
0.38519174, -1.82993419, 0.94526872, 1.4774141 , 0.70311165],
[-0.89720881, -1.73036467, -0.09530571, -0.44387358, 2.45545522,
0.06529793, 1.07548743, -0.12976989, -1.0027376 , -0.17622635],
[-0.5976888 , -1.36432736, -0.32955077, 0.12645766, 0.09316411,
0.07775715, 0.52900558, 0.39635442, -0.33988681, 1.11368078],
[ 0.62088067, 0.99038013, 1.64819413, -0.39226861, 0.60808831,
-0.82700987, -0.80826492, -0.13359222, -0.86161707, -0.76045281]])
18.8.7 Exponential Family
One shared property for all the distributions listed above is that they all belong to which is known
as the exponential family. The exponential family is a set of distributions whose density can be
expressed in the following form:
p(xj) = h(x)  exp
(

?  T(x) ?? A()
)
(18.8.20)
As this definition can be a little subtle, let us examine it closely.
First, h(x) is known as the underlying measure or the base measure. This can be viewed as an original
choice of measure we are modifying with our exponential weight.
Second, we have the vector  = (1; 2; :::; l) 2 Rl called the natural parameters or canonical parameters.
These define how the base measure will be modified. The natural parameters enter
into the new measure by taking the dot product of these parameters against some function T()
of x = (x1; x2; :::; xn) 2 Rn and exponentiated. T(x) = (T1(x); T2(x); :::; Tl(x)) is called the sufficient
statistics for . This name is used since the information represented by T(x) is sufficient to
calculate the probability density and no other information from the sample x?s are required.
Third, we have A(), which is referred to as the cumulant function, which ensures that the above
distribution (18.8.20) integrates to one, i.e.,
A() = log
[?
h(x)  exp
(

?  T(x)
)
dx
]
: (18.8.21)
To be concrete, let us consider the Gaussian. Assuming that x is an univariate variable, we saw
that it had a density of
p(xj; ) =
p 1
22
exp
{??(x ?? )2
22
}
=
p1
2
 exp
{ 
2 x ?? 1
22 x2 ??
( 1
22 2 + log()
)}
:
(18.8.22)
This matches the definition of the exponential family with:
� underlying measure: h(x) = p1
2 ,
� natural parameters:  =
[
1
2
]
=
[ 
2
1
22
]
,
� sufficient statistics: T(x) =
[
x
??x2
]
, and
� cumulant function: A() = 1
22 2 + log() = 2
1
42
?? 1
2 log(22).
874 Chapter 18. Appendix: Mathematics for Deep Learning
It is worth noting that the exact choice of each of above terms is somewhat arbitrary. Indeed, the
important feature is that the distribution can be expressed in this form, not the exact form itself.
As we allude to in Section 3.4.5, a widely used technique is to assume that the final output y follows
an exponential family distribution. The exponential family is a common and powerful family of
distributions encountered frequently in machine learning.
Summary
� Bernoulli random variables can be used to model events with a yes/no outcome.
� Discrete uniform distributions model selects from a finite set of possibilities.
� Continuous uniform distributions select from an interval.
� Binomial distributions model a series of Bernoulli random variables, and count the number
of successes.
� Poisson random variables model the arrival of rare events.
� Gaussian random variables model the result of adding a large number of independent random
variables together.
� All the above distributions belong to exponential family.
Exercises
1. What is the standard deviation of a random variable that is the difference X ?? Y of two
independent binomial random variables X; Y  Binomial(16; 1/2).
2. If we take a Poisson random variable X  Poisson() and consider (X ?? )/
p
 as  ! 1,
we can show that this becomes approximately Gaussian. Why does this make sense?
3. What is the probability mass function for a sum of two discrete uniform random variables
on n elements?
Discussions252
18.9 Naive Bayes
Throughout the previous sections, we learned about the theory of probability and random variables.
To put this theory to work, let us introduce the naive Bayes classifier. This uses nothing but
probabilistic fundamentals to allow us to perform classification of digits.
Learning is all about making assumptions. If we want to classify a new data example that we have
never seen before we have to make some assumptions about which data examples are similar to
each other. The naive Bayes classifier, a popular and remarkably clear algorithm, assumes all
features are independent from each other to simplify the computation. In this section, we will
apply this model to recognize characters in images.
252 https://discuss.d2l.ai/t/417
18.9. Naive Bayes 875
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import gluon, np, npx
npx.set_np()
d2l.use_svg_display()
18.9.1 Optical Character Recognition
MNIST (LeCun et al., 1998) is one of widely used datasets. It contains 60,000 images for training
and 10,000 images for validation. Each image contains a handwritten digit from 0 to 9. The task is
classifying each image into the corresponding digit.
Gluon provides a MNIST class in the data.vision module to automatically retrieve the dataset from
the Internet. Subsequently, Gluon will use the already-downloaded local copy. We specify whether
we are requesting the training set or the test set by setting the value of the parameter train to
True or False, respectively. Each image is a grayscale image with both width and height of 28 with
shape (28,28,1). We use a customized transformation to remove the last channel dimension. In
addition, the dataset represents each pixel by an unsigned 8-bit integer. We quantize them into
binary features to simplify the problem.
def transform(data, label):
return np.floor(data.astype('float32') / 128).squeeze(axis=-1), label
mnist_train = gluon.data.vision.MNIST(train=True, transform=transform)
mnist_test = gluon.data.vision.MNIST(train=False, transform=transform)
We can access a particular example, which contains the image and the corresponding label.
image, label = mnist_train[2]
image.shape, label
((28, 28), array(4, dtype=int32))
Our example, stored here in the variable image, corresponds to an image with a height and width
of 28 pixels.
image.shape, image.dtype
((28, 28), dtype('float32'))
Our code stores the label of each image as a scalar. Its type is a 32-bit integer.
label, type(label), label.dtype
(array(4, dtype=int32), mxnet.numpy.ndarray, dtype('int32'))
We can also access multiple examples at the same time.
876 Chapter 18. Appendix: Mathematics for Deep Learning
images, labels = mnist_train[10:38]
images.shape, labels.shape
((28, 28, 28), (28,))
Let us visualize these examples.
d2l.show_images(images, 2, 9);
18.9.2 The Probabilistic Model for Classification
In a classification task, we map an example into a category. Here an example is a grayscale 2828
image, and a category is a digit. (Refer to Section 3.4 for a more detailed explanation.) One natural
way to express the classification task is via the probabilistic question: what is the most likely label
given the features (i.e., image pixels)? Denote by x 2 Rd the features of the example and y 2 R the
label. Here features are image pixels, where we can reshape a 2-dimensional image to a vector so
that d = 282 = 784, and labels are digits. The probability of the label given the features is p(y j x).
If we are able to compute these probabilities, which are p(y j x) for y = 0; : : : ; 9 in our example,
then the classifier will output the prediction ^y given by the expression:
^y = argmax p(y j x): (18.9.1)
Unfortunately, this requires that we estimate p(y j x) for every value of x = x1; :::; xd. Imagine
that each feature could take one of 2 values. For example, the feature x1 = 1 might signify that
the word apple appears in a given document and x1 = 0 would signify that it does not. If we had
30 such binary features, that would mean that we need to be prepared to classify any of 230 (over
1 billion!) possible values of the input vector x.
Moreover, where is the learning? If we need to see every single possible example in order to predict
the corresponding label then we are not really learning a pattern but just memorizing the dataset.
18.9.3 The Naive Bayes Classifier
Fortunately, by making some assumptions about conditional independence, we can introduce
some inductive bias and build a model capable of generalizing from a comparatively modest selection
of training examples. To begin, let us use Bayes theorem, to express the classifier as
^y = argmaxy p(y j x) = argmaxy
p(x j y)p(y)
p(x)
: (18.9.2)
18.9. Naive Bayes 877
Note that the denominator is the normalizing term p(x) which does not depend on the value of
the label y. As a result, we only need to worry about comparing the numerator across different
values of y. Even if calculating the denominator turned out to be intractable, we could get away
with ignoring it, so long as we could evaluate the numerator. Fortunately, even if we wanted to
recover the normalizing constant, ? we could. We can always recover the normalization termsince
y p(y j x) = 1.
Now, let us focus on p(x j y). Using the chain rule of probability, we can express the term p(x j y)
as
p(x1 j y)  p(x2 j x1; y)  :::  p(xd j x1; :::; xd??1; y): (18.9.3)
By itself, this expression does not get us any further. We still must estimate roughly 2d parameters.
However, if we assume that the features are conditionally independent of each other, given the label,
then suddenly we are in much better shape, as this term simplifies to
?
i p(xi j y), giving us the
predictor
^y = argmaxy
?d
i=1
p(xi j y)p(y): (18.9.4)
If we can estimate
?
i p(xi = 1 j y) for every i and y, and save its value in Pxy[i; y], here Pxy is a
d  n matrix with n being the number of classes and y 2 f1; : : : ; ng. In addition, we estimate p(y)
for every y and save it in Py[y], with Py a n-length vector. Then for any new example x, we could
compute
^y = argmaxy
?d
i=1
Pxy[xi; y]Py[y]; (18.9.5)
for any y. So our assumption of conditional independence has taken the complexity of our model
from an exponential dependence on the number of featuresO(2dn) to a linear dependence, which
is O(dn).
18.9.4 Training
The problem now is that we do not know Pxy and Py. So we need to estimate their values given
some training data first. This is training the model. Estimating Py is not too hard. Since we are
only dealing with 10 classes, we may count the number of occurrences ny for each of the digits
and divide it by the total amount of data n. For instance, if digit 8 occurs n8 = 5; 800 times and we
have a total of n = 60; 000 images, the probability estimate is p(y = 8) = 0:0967.
X, Y = mnist_train[:] # All training examples
n_y = np.zeros((10))
for y in range(10):
n_y[y] = (Y == y).sum()
P_y = n_y / n_y.sum()
P_y
array([0.09871667, 0.11236667, 0.0993 , 0.10218333, 0.09736667,
0.09035 , 0.09863333, 0.10441667, 0.09751666, 0.09915 ])
878 Chapter 18. Appendix: Mathematics for Deep Learning
Now on to slightly more difficult things Pxy. Since we picked black and white images, p(xi j y)
denotes the probability that pixel i is switched on for class y. Just like before we can go and count
the number of times niy such that an event occurs and divide it by the total number of occurrences
of y, i.e., ny. But there is something slightly troubling: certain pixels may never be black (e.g., for
well cropped images the corner pixels might always be white). A convenient way for statisticians
to deal with this problem is to add pseudo counts to all occurrences. Hence, rather than niy we
use niy + 1 and instead of ny we use ny + 1. This is also called Laplace Smoothing. It may seem
ad-hoc, however it may be well motivated from a Bayesian point-of-view.
n_x = np.zeros((10, 28, 28))
for y in range(10):
n_x[y] = np.array(X.asnumpy()[Y.asnumpy() == y].sum(axis=0))
P_xy = (n_x + 1) / (n_y + 1).reshape(10, 1, 1)
d2l.show_images(P_xy, 2, 5);
By visualizing these 10  28  28 probabilities (for each pixel for each class) we could get some
mean looking digits.
Now we can use (18.9.5) to predict a new image. Given x, the following functions computes p(x j
y)p(y) for every y.
def bayes_pred(x):
x = np.expand_dims(x, axis=0) # (28, 28) -> (1, 28, 28)
p_xy = P_xy * x + (1 - P_xy)*(1 - x)
p_xy = p_xy.reshape(10, -1).prod(axis=1) # p(x|y)
return np.array(p_xy) * P_y
image, label = mnist_test[0]
bayes_pred(image)
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
This went horribly wrong! To find out why, let us look at the per pixel probabilities. They are
typically numbers between 0:001 and 1. We are multiplying 784 of them. At this point it is worth
mentioning that we are calculating these numbers on a computer, hence with a fixed range for the
exponent. What happens is that we experience numerical underflow, i.e., multiplying all the small
18.9. Naive Bayes 879
numbers leads to something even smaller until it is rounded down to zero. We discussed this as a
theoretical issue in Section 18.7, but we see the phenomena clearly here in practice.
As discussed in that section, we fix this by use the fact that log ab = log a+log b, i.e., we switch to
summing logarithms. Even if both a and b are small numbers, the logarithm values should be in
a proper range.
a = 0.1
print('underflow:', a**784)
print('logarithm is normal:', 784*math.log(a))
underflow: 0.0
logarithm is normal: -1805.2267129073316
Since the logarithm is an increasing function, we can rewrite (18.9.5) as
^y = argmaxy
?d
i=1
log Pxy[xi; y] + log Py[y]: (18.9.6)
We can implement the following stable version:
log_P_xy = np.log(P_xy)
log_P_xy_neg = np.log(1 - P_xy)
log_P_y = np.log(P_y)
def bayes_pred_stable(x):
x = np.expand_dims(x, axis=0) # (28, 28) -> (1, 28, 28)
p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)
p_xy = p_xy.reshape(10, -1).sum(axis=1) # p(x|y)
return p_xy + log_P_y
py = bayes_pred_stable(image)
py
array([-269.00424, -301.73447, -245.21458, -218.8941 , -193.46907,
-206.10315, -292.54315, -114.62834, -220.35619, -163.18881])
We may now check if the prediction is correct.
# Convert label which is a scalar tensor of int32 dtype
# to a Python scalar integer for comparison
py.argmax(axis=0) == int(label)
array(True)
If we now predict a few validation examples, we can see the Bayes classifier works pretty well.
def predict(X):
return [bayes_pred_stable(x).argmax(axis=0).astype(np.int32) for x in X]
X, y = mnist_test[:18]
preds = predict(X)
d2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);
880 Chapter 18. Appendix: Mathematics for Deep Learning
Finally, let us compute the overall accuracy of the classifier.
X, y = mnist_test[:]
preds = np.array(predict(X), dtype=np.int32)
float((preds == y).sum()) / len(y) # Validation accuracy
0.8426
Modern deep networks achieve error rates of less than 0:01. The relatively poor performance is
due to the incorrect statistical assumptions that we made in our model: we assumed that each
and every pixel are independently generated, depending only on the label. This is clearly not how
humans write digits, and this wrong assumption led to the downfall of our overly naive (Bayes)
classifier.
Summary
� Using Bayes? rule, a classifier can be made by assuming all observed features are independent.
� This classifier can be trained on a dataset by counting the number of occurrences of combinations
of labels and pixel values.
� This classifier was the gold standard for decades for tasks such as spam detection.
Exercises
1. Consider the dataset [[0; 0]; [0; 1]; [1; 0]; [1; 1]] with labels given by the XOR of the two elements
[0; 1; 1; 0]. What are the probabilities for a Naive Bayes classifier built on this dataset. Does
it successfully classify our points? If not, what assumptions are violated?
2. Suppose that we did not use Laplace smoothing when estimating probabilities and a data
example arrived at testing time which contained a value never observed in training. What
would the model output?
3. The naive Bayes classifier is a specific example of a Bayesian network, where the dependence
of random variables are encoded with a graph structure. While the full theory is beyond the
scope of this section (see (Koller & Friedman, 2009) for full details), explain why allowing explicit
dependence between the two input variables in the XOR model allows for the creation
of a successful classifier.
18.9. Naive Bayes 881
Discussions253
18.10 Statistics
Undoubtedly, to be a top deep learning practitioner, the ability to train the state-of-the-art and
high accurate models is crucial. However, it is often unclear when improvements are significant,
or only the result of random fluctuations in the training process. To be able to discuss uncertainty
in estimated values, we must learn some statistics.
The earliest reference of statistics can be traced back to an Arab scholar Al-Kindi in the 9th-century,
who gave a detailed description of how to use statistics and frequency analysis to decipher encrypted
messages. After 800 years, the modern statistics arose from Germany in 1700s, when
the researchers focused on the demographic and economic data collection and analysis. Today,
statistics is the science subject that concerns the collection, processing, analysis, interpretation
and visualization of data. What is more, the core theory of statistics has been widely used in the
research within academia, industry, and government.
More specifically, statistics can be divided to descriptive statistics and statistical inference. The former
focus on summarizing and illustrating the features of a collection of observed data, which is
referred to as a sample. The sample is drawn from a population, denotes the total set of similar
individuals, items, or events of our experiment interests. Contrary to descriptive statistics, statistical
inference further deduces the characteristics of a population from the given samples, based
on the assumptions that the sample distribution can replicate the population distribution at some
degree.
You may wonder: �What is the essential difference between machine learning and statistics?�
Fundamentally speaking, statistics focuses on the inference problem. This type of problems includes
modeling the relationship between the variables, such as causal inference, and testing the
statistically significance of model parameters, such as A/B testing. In contrast, machine learning
emphasizes on making accurate predictions, without explicitly programming and understanding
each parameter?s functionality.
In this section, we will introduce three types of statistics inference methods: evaluating and comparing
estimators, conducting hypothesis tests, and constructing confidence intervals. These
methods can help us infer the characteristics of a given population, i.e., the true parameter . For
brevity, we assume that the true parameter  of a given population is a scalar value. It is straightforward
to extend to the case where  is a vector or a tensor, thus we omit it in our discussion.
18.10.1 Evaluating and Comparing Estimators
In statistics, an estimator is a function of given samples used to estimate the true parameter . We
will write ^n = ^ f(x1; : : : ; xn) for the estimate of  after observing the samples {x1; x2; : : : ; xn}.
We have seen simple examples of estimators before in section Section 18.7. If you have a number
of samples from a Bernoulli random variable, then the maximum likelihood estimate for the
probability the random variable is one can be obtained by counting the number of ones observed
and dividing by the total number of samples. Similarly, an exercise asked you to show that the
maximum likelihood estimate of the mean of a Gaussian given a number of samples is given by
the average value of all the samples. These estimators will almost never give the true value of the
parameter, but ideally for a large number of samples the estimate will be close.
253 https://discuss.d2l.ai/t/418
882 Chapter 18. Appendix: Mathematics for Deep Learning
As an example, we show below the true density of a Gaussian random variable with mean zero and
variance one, along with a collection samples from that Gaussian. We constructed the y coordinate
so every point is visible and the relationship to the original density is clearer.
from d2l import mxnet as d2l
from mxnet import np, npx
import random
npx.set_np()
# Sample datapoints and create y coordinate
epsilon = 0.1
random.seed(8675309)
xs = np.random.normal(loc=0, scale=1, size=(300,))
ys = [np.sum(np.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2))
/ np.sqrt(2*np.pi*epsilon**2)) / len(xs) for i in range(len(xs))]
# Compute true density
xd = np.arange(np.min(xs), np.max(xs), 0.01)
yd = np.exp(-xd**2/2) / np.sqrt(2 * np.pi)
# Plot the results
d2l.plot(xd, yd, 'x', 'density')
d2l.plt.scatter(xs, ys)
d2l.plt.axvline(x=0)
d2l.plt.axvline(x=np.mean(xs), linestyle='--', color='purple')
d2l.plt.title(f'sample mean: {float(np.mean(xs)):.2f}')
d2l.plt.show()
There can be many ways to compute an estimator of a parameter ^n. In this section, we introduce
three common methods to evaluate and compare estimators: the mean squared error, the
standard deviation, and statistical bias.
18.10. Statistics 883
Mean Squared Error
Perhaps the simplest metric used to evaluate estimators is the mean squared error (MSE) (or l2 loss)
of an estimator can be defined as
MSE(^n; ) = E[(^n ?? )2]: (18.10.1)
This allows us to quantify the average squared deviation from the true value. MSE is always nonnegative.
If you have read Section 3.1, you will recognize it as the most commonly used regression
loss function. As a measure to evaluate an estimator, the closer its value to zero, the closer the
estimator is close to the true parameter .
Statistical Bias
The MSE provides a natural metric, but we can easily imagine multiple different phenomena that
might make it large. Two fundamentally important are fluctuation in the estimator due to randomness
in the dataset, and systematic error in the estimator due to the estimation procedure.
First, let us measure the systematic error. For an estimator ^n, the mathematical illustration of
statistical bias can be defined as
bias(^n) = E(^n ?? ) = E(^n) ?? : (18.10.2)
Note that when bias(^n) = 0, the expectation of the estimator ^n is equal to the true value of
parameter. In this case, we say ^n is an unbiased estimator. In general, an unbiased estimator is
better than a biased estimator since its expectation is the same as the true parameter.
It is worth being aware, however, that biased estimators are frequently used in practice. There are
cases where unbiased estimators do not exist without further assumptions, or are intractable to
compute. This may seem like a significant flaw in an estimator, however the majority of estimators
encountered in practice are at least asymptotically unbiased in the sense that the bias tends to zero
as the number of available samples tends to infinity: limn!1 bias(^n) = 0.
Variance and Standard Deviation
Second, let us measure the randomness in the estimator. Recall from Section 18.6, the standard
deviation (or standard error) is defined as the squared root of the variance. We may measure the
degree of fluctuation of an estimator by measuring the standard deviation or variance of that estimator.
^n
=
v
Var(^n) =
v
E[(^n ?? E(^n))2]: (18.10.3)
It is important to compare (18.10.3) to (18.10.1). In this equation we do not compare to the true
population value , but instead to E(^n), the expected sample mean. Thus we are not measuring
how far the estimator tends to be from the true value, but instead we measuring the fluctuation of
the estimator itself.
884 Chapter 18. Appendix: Mathematics for Deep Learning
The Bias-Variance Trade-off
It is intuitively clear that these two main components contribute to the mean squared error. What
is somewhat shocking is that we can show that this is actually a decomposition of the mean squared
error into these two contributions plus a third one. That is to say that we can write the mean
squared error as the sum of the square of the bias, the variance and the irreducible error.
MSE(^n; ) = E[(^n ?? )2]
= E[(^n)2] + E[2] ?? 2E[^n]
= Var[^n] + E[^n]2 + Var[] + E[]2 ?? 2E[^n]E[]
= (E[^n] ?? E[])2 + Var[^n] + Var[]
= (E[^n ?? ])2 + Var[^n] + Var[]
= (bias[^n])2 + Var(^n) + Var[]:
(18.10.4)
We refer the above formula as bias-variance trade-off. The mean squared error can be divided into
three sources of error: the error from high bias, the error from high variance and the irreducible
error. The bias error is commonly seen in a simple model (such as a linear regression model),
which cannot extract high dimensional relations between the features and the outputs. If a model
suffers from high bias error, we often say it is underfitting or lack of generalization as introduced
in (Section 4.4). The high variance usually results from a too complex model, which overfits the
training data. As a result, an overfitting model is sensitive to small fluctuations in the data. If a
model suffers from high variance, we often say it is overfitting and lack of flexibility as introduced
in (Section 4.4). The irreducible error is the result from noise in the  itself.
Evaluating Estimators in Code
Since the standard deviation of an estimator has been implementing by simply calling a.std() for
a tensor a, we will skip it but implement the statistical bias and the mean squared error.
# Statistical bias
def stat_bias(true_theta, est_theta):
return(np.mean(est_theta) - true_theta)
# Mean squared error
def mse(data, true_theta):
return(np.mean(np.square(data - true_theta)))
To illustrate the equation of the bias-variance trade-off, let us simulate of normal distribution
N(; 2) with 10; 000 samples. Here, we use a  = 1 and  = 4. As the estimator is a function
of the given samples, here we use the mean of the samples as an estimator for true  in this normal
distribution N(; 2) .
theta_true = 1
sigma = 4
sample_len = 10000
samples = np.random.normal(theta_true, sigma, sample_len)
theta_est = np.mean(samples)
theta_est
18.10. Statistics 885
array(0.9503336)
Let us validate the trade-off equation by calculating the summation of the squared bias and the
variance of our estimator. First, calculate the MSE of our estimator.
mse(samples, theta_true)
array(15.781996)
Next, we calculate Var(^n)+[bias(^n)]2 as below. As you can see, the two values agree to numerical
precision.
bias = stat_bias(theta_true, theta_est)
np.square(samples.std()) + np.square(bias)
array(15.781995)
18.10.2 Conducting Hypothesis Tests
The most commonly encountered topic in statistical inference is hypothesis testing. While hypothesis
testing was popularized in the early 20th century, the first use can be traced back to John
Arbuthnot in the 1700s. John tracked 80-year birth records in London and concluded that more
men were born than women each year. Following that, the modern significance testing is the intelligence
heritage by Karl Pearson who invented p-value and Pearson?s chi-squared test, William
Gosset who is the father of Student?s t-distribution, and Ronald Fisher who initialed the null hypothesis
and the significance test.
A hypothesis test is a way of evaluating some evidence against the default statement about a population.
We refer the default statement as the null hypothesis H0, which we try to reject using the
observed data. Here, we useH0 as a starting point for the statistical significance testing. The alternative
hypothesisHA (orH1) is a statement that is contrary to the null hypothesis. A null hypothesis
is often stated in a declarative form which posits a relationship between variables. It should reflect
the brief as explicit as possible, and be testable by statistics theory.
Imagine you are a chemist. After spending thousands of hours in the lab, you develop a new
medicine which can dramatically improve one?s ability to understand math. To show its magic
power, you need to test it. Naturally, you may need some volunteers to take the medicine and see
whether it can help them learn math better. How do you get started?
First, you will need carefully random selected two groups of volunteers, so that there is no difference
between their math understanding ability measured by some metrics. The two groups are
commonly referred to as the test group and the control group. The test group (or treatment group)
is a group of individuals who will experience the medicine, while the control group represents the
group of users who are set aside as a benchmark, i.e., identical environment setups except taking
this medicine. In this way, the influence of all the variables are minimized, except the impact of
the independent variable in the treatment.
Second, after a period of taking the medicine, you will need to measure the two groups? math
understanding by the same metrics, such as letting the volunteers do the same tests after learning a
new math formula. Then, you can collect their performance and compare the results. In this case,
886 Chapter 18. Appendix: Mathematics for Deep Learning
our null hypothesis will be that there is no difference between the two groups, and our alternate
will be that there is.
This is still not fully formal. There are many details you have to think of carefully. For example,
what is the suitable metrics to test their math understanding ability? How many volunteers for
your test so you can be confident to claim the effectiveness of your medicine? How long should
you run the test? How do you decide if there is a difference between the two groups? Do you care
about the average performance only, or also the range of variation of the scores? And so on.
In this way, hypothesis testing provides a framework for experimental design and reasoning about
certainty in observed results. If we can now show that the null hypothesis is very unlikely to be
true, we may reject it with confidence.
To complete the story of how to work with hypothesis testing, we need to now introduce some
additional terminology and make some of our concepts above formal.
Statistical Significance
The statistical significance measures the probability of erroneously rejecting the null hypothesis,
H0, when it should not be rejected, i.e.,
statistical significance = 1 ??  = 1 ?? P(reject H0 j H0 is true): (18.10.5)
It is also referred to as the type I error or false positive. The , is called as the significance level and
its commonly used value is 5%, i.e., 1 ??  = 95%. The significance level can be explained as the
level of risk that we are willing to take, when we reject a true null hypothesis.
Fig. 18.10.1 shows the observations? values and probability of a given normal distribution in a twosample
hypothesis test. If the observation data example is located outsides the 95% threshold, it
will be a very unlikely observation under the null hypothesis assumption. Hence, there might be
something wrong with the null hypothesis and we will reject it.
Fig. 18.10.1: Statistical significance.
18.10. Statistics 887
Statistical Power
The statistical power (or sensitivity) measures the probability of reject the null hypothesis,H0, when
it should be rejected, i.e.,
statistical power = 1 ??  = 1 ?? P( fail to reject H0 j H0 is false): (18.10.6)
Recall that a type I error is error caused by rejecting the null hypothesis when it is true, whereas a
type II error is resulted from failing to reject the null hypothesis when it is false. A type II error is
usually denoted as , and hence the corresponding statistical power is 1 ?? .
Intuitively, statistical power can be interpreted as how likely our test will detect a real discrepancy
of some minimum magnitude at a desired statistical significance level. 80% is a commonly used
statistical power threshold. The higher the statistical power, the more likely we are to detect true
differences.
One of the most common uses of statistical power is in determining the number of samples
needed. The probability you reject the null hypothesis when it is false depends on the degree
to which it is false (known as the effect size) and the number of samples you have. As you might
expect, small effect sizes will require a very large number of samples to be detectable with high
probability. While beyond the scope of this brief appendix to derive in detail, as an example, want
to be able to reject a null hypothesis that our sample came from a mean zero variance one Gaussian,
and we believe that our sample?s mean is actually close to one, we can do so with acceptable
error rates with a sample size of only 8. However, if we think our sample population true mean is
close to 0:01, then we?d need a sample size of nearly 80000 to detect the difference.
We can imagine the power as a water filter. In this analogy, a high power hypothesis test is like a
high quality water filtration system that will reduce harmful substances in the water as much as
possible. On the other hand, a smaller discrepancy is like a low quality water filter, where some
relative small substances may easily escape from the gaps. Similarly, if the statistical power is not
of enough high power, then the test may not catch the smaller discrepancy.
Test Statistic
A test statistic T(x) is a scalar which summarizes some characteristic of the sample data. The goal
of defining such a statistic is that it should allow us to distinguish between different distributions
and conduct our hypothesis test. Thinking back to our chemist example, if we wish to show that
one population performs better than the other, it could be reasonable to take the mean as the
test statistic. Different choices of test statistic can lead to statistical test with drastically different
statistical power.
Often, T(X) (the distribution of the test statistic under our null hypothesis) will follow, at least
approximately, a common probability distribution such as a normal distribution when considered
under the null hypothesis. If we can derive explicitly such a distribution, and then measure our
test statistic on our dataset, we can safely reject the null hypothesis if our statistic is far outside
the range that we would expect. Making this quantitative leads us to the notion of p-values.
888 Chapter 18. Appendix: Mathematics for Deep Learning
p-value
The p-value (or the probability value) is the probability that T(X) is at least as extreme as the observed
test statistic T(x) assuming that the null hypothesis is true, i.e.,
p-value = PH0(T(X)  T(x)): (18.10.7)
If the p-value is smaller than or equal to a predefined and fixed statistical significance level , we
may reject the null hypothesis. Otherwise, we will conclude that we are lack of evidence to reject
the null hypothesis. For a given population distribution, the region of rejection will be the interval
contained of all the points which has a p-value smaller than the statistical significance level .
One-side Test and Two-sided Test
Normally there are two kinds of significance test: the one-sided test and the two-sided test. The
one-sided test (or one-tailed test) is applicable when the null hypothesis and the alternative hypothesis
only have one direction. For example, the null hypothesis may state that the true parameter
 is less than or equal to a value c. The alternative hypothesis would be that  is greater than c.
That is, the region of rejection is on only one side of the sampling distribution. Contrary to the
one-sided test, the two-sided test (or two-tailed test) is applicable when the region of rejection is on
both sides of the sampling distribution. An example in this case may have a null hypothesis state
that the true parameter  is equal to a value c. The alternative hypothesis would be that  is not
equal to c.
General Steps of Hypothesis Testing
After getting familiar with the above concepts, let us go through the general steps of hypothesis
testing.
1. State the question and establish a null hypotheses H0.
2. Set the statistical significance level  and a statistical power (1 ?? ).
3. Obtain samples through experiments. The number of samples needed will depend on the
statistical power, and the expected effect size.
4. Calculate the test statistic and the p-value.
5. Make the decision to keep or reject the null hypothesis based on the p-value and the statistical
significance level .
To conduct a hypothesis test, we start by defining a null hypothesis and a level of risk that we
are willing to take. Then we calculate the test statistic of the sample, taking an extreme value of
the test statistic as evidence against the null hypothesis. If the test statistic falls within the reject
region, we may reject the null hypothesis in favor of the alternative.
Hypothesis testing is applicable in a variety of scenarios such as the clinical trails and A/B testing.
18.10. Statistics 889
18.10.3 Constructing Confidence Intervals
When estimating the value of a parameter , point estimators like ^ are of limited utility since they
contain no notion of uncertainty. Rather, it would be far better if we could produce an interval
that would contain the true parameter  with high probability. If you were interested in such
ideas a century ago, then you would have been excited to read �Outline of a Theory of Statistical
Estimation Based on the Classical Theory of Probability� by Jerzy Neyman (Neyman, 1937), who
first introduced the concept of confidence interval in 1937.
To be useful, a confidence interval should be as small as possible for a given degree of certainty.
Let us see how to derive it.
Definition
Mathematically, a confidence interval for the true parameter  is an interval Cn that computed from
the sample data such that
P(Cn ? )  1 ?? ; 8: (18.10.8)
Here  2 (0; 1), and 1 ??  is called the confidence level or coverage of the interval. This is the same
 as the significance level as we discussed about above.
Note that (18.10.8) is about variable Cn, not about the fixed . To emphasize this, we write P(Cn ?
) rather than P( 2 Cn).
Interpretation
It is very tempting to interpret a 95% confidence interval as an interval where you can be 95% sure
the true parameter lies, however this is sadly not true. The true parameter is fixed, and it is the
interval that is random. Thus a better interpretation would be to say that if you generated a large
number of confidence intervals by this procedure, 95% of the generated intervals would contain
the true parameter.
This may seem pedantic, but it can have real implications for the interpretation of the results.
In particular, we may satisfy (18.10.8) by constructing intervals that we are almost certain do not
contain the true value, as long as we only do so rarely enough. We close this section by providing
three tempting but false statements. An in-depth discussion of these points can be found in (Morey
et al., 2016).
� Fallacy 1. Narrow confidence intervals mean we can estimate the parameter precisely.
� Fallacy 2. The values inside the confidence interval are more likely to be the true value than
those outside the interval.
� Fallacy 3. The probability that a particular observed 95% confidence interval contains the
true value is 95%.
Sufficed to say, confidence intervals are subtle objects. However, if you keep the interpretation
clear, they can be powerful tools.
890 Chapter 18. Appendix: Mathematics for Deep Learning
A Gaussian Example
Let us discuss the most classical example, the confidence interval for the mean of a Gaussian of
unknown mean and variance. Suppose we collect n samples fxign i=1 from our Gaussian N(; 2).
We can compute estimators for the mean and standard deviation by taking
^n =
1
n
?n
i=1
xi and ^2n
=
1
n ?? 1
?n
i=1
(xi ?? ^)2: (18.10.9)
If we now consider the random variable
T =
^n ?? 
^n/
p
n
; (18.10.10)
we obtain a random variable following a well-known distribution called the Student�s t-distribution
on n ?? 1 degrees of freedom.
This distribution is very well studied, and it is known, for instance, that as n ! 1, it is approximately
a standard Gaussian, and thus by looking up values of the Gaussian c.d.f. in a table, we
may conclude that the value of T is in the interval [??1:96; 1:96] at least 95% of the time. For finite
values of n, the interval needs to be somewhat larger, but are well known and precomputed in
tables.
Thus, we may conclude that for large n,
P
(
^n ?? 
^n/
p
n
2 [??1:96; 1:96]
)
 0:95: (18.10.11)
Rearranging this by multiplying both sides by ^n/
p
n and then adding ^n, we obtain
P
(
 2
[
^n ?? 1:96
p^n
n
; ^n + 1:96
p^n
n
])
 0:95: (18.10.12)
Thus we know that we have found our 95% confidence interval:
[
^n ?? 1:96
p^n
n
; ^n + 1:96
p^n
n
]
: (18.10.13)
It is safe to say that (18.10.13) is one of the most used formula in statistics. Let us close our discussion
of statistics by implementing it. For simplicity, we assume we are in the asymptotic regime.
Small values of N should include the correct value of t_star obtained either programmatically or
from a t-table.
# Number of samples
N = 1000
# Sample dataset
samples = np.random.normal(loc=0, scale=1, size=(N,))
# Lookup Students's t-distribution c.d.f.
t_star = 1.96
# Construct interval
mu_hat = np.mean(samples)
sigma_hat = samples.std(ddof=1)
(mu_hat - t_star*sigma_hat/np.sqrt(N), mu_hat + t_star*sigma_hat/np.sqrt(N))
18.10. Statistics 891
(array(-0.07853346), array(0.04412608))
Summary
� Statistics focuses on inference problems, whereas deep learning emphasizes on making accurate
predictions without explicitly programming and understanding.
� There are three common statistics inference methods: evaluating and comparing estimators,
conducting hypothesis tests, and constructing confidence intervals.
� There are three most common estimators: statistical bias, standard deviation, and mean
square error.
� A confidence interval is an estimated range of a true population parameter that we can construct
by given the samples.
� Hypothesis testing is a way of evaluating some evidence against the default statement about
a population.
Exercises
1. LetX1;X2; : : : ;Xn
iid Unif(0; ), where �iid� stands for independent and identically distributed.
Consider the following estimators of :
^ = maxfX1;X2; : : : ;Xng; (18.10.14)
~= 2Xn =
2
n
?n
i=1
Xi: (18.10.15)
� Find the statistical bias, standard deviation, and mean square error of ^:
� Find the statistical bias, standard deviation, and mean square error of ~:
� Which estimator is better?
2. For our chemist example in introduction, can you derive the 5 steps to conduct a two-sided
hypothesis testing? Given the statistical significance level  = 0:05 and the statistical power
1 ??  = 0:8.
3. Run the confidence interval code with N = 2 and  = 0:5 for 100 independently generated
dataset, and plot the resulting intervals (in this case t_star = 1.0). You will see several very
short intervals which are very far from containing the true mean 0. Does this contradict the
interpretation of the confidence interval? Do you feel comfortable using short intervals to
indicate high precision estimates?
Discussions254
254 https://discuss.d2l.ai/t/419
892 Chapter 18. Appendix: Mathematics for Deep Learning
18.11 Information Theory
The universe is overflowing with information. Information provides a common language across
disciplinary rifts: from Shakespeare?s Sonnet to researchers? paper on Cornell ArXiv, from Van
Gogh?s printing Starry Night to Beethoven?s music Symphony No. 5, from the first programming
language Plankalkul to the state-of-the-art machine learning algorithms. Everything must follow
the rules of information theory, no matter the format. With information theory, we can measure
and compare how much information is present in different signals. In this section, we will investigate
the fundamental concepts of information theory and applications of information theory in
machine learning.
Before we get started, let us outline the relationship between machine learning and information
theory. Machine learning aims to extract interesting signals from data and make critical predictions.
On the other hand, information theory studies encoding, decoding, transmitting, and
manipulating information. As a result, information theory provides fundamental language for
discussing the information processing in machine learned systems. For example, many machine
learning applications use the cross entropy loss as described in Section 3.4. This loss can be directly
derived from information theoretic considerations.
18.11.1 Information
Let us start with the �soul� of information theory: information. Information can be encoded in
anything with a particular sequence of one or more encoding formats. Suppose that we task ourselves
with trying to define a notion of information. What could be our starting point?
Consider the following thought experiment. We have a friend with a deck of cards. They will
shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess
the information content of each statement.
First, they flip over a card and tell us, �I see a card.� This provides us with no information at all.
We were already certain that this was the case so we hope the information should be zero.
Next, they flip over a card and say, �I see a heart.� This provides us some information, but in reality
there are only 4 different suits that were possible, each equally likely, so we are not surprised by
this outcome. We hope that whatever the measure of information, this event should have low
information content.
Next, they flip over a card and say, �This is the 3 of spades.� This is more information. Indeed there
were 52 equally likely possible outcomes, and our friend told us which one it was. This should be
a medium amount of information.
Let us take this to the logical extreme. Suppose that finally they flip over every card from the deck
and read off the entire sequence of the shuffled deck. There are 52! different orders to the deck,
again all equally likely, so we need a lot of information to know which one it is.
Any notion of information we develop must conform to this intuition. Indeed, in the next sections
we will learn how to compute that these events have 0 bits, 2 bits, 5:7 bits, and 225:6 bits of
information respectively.
If we read through these thought experiments, we see a natural idea. As a starting point, rather
than caring about the knowledge, we may build off the idea that information represents the degree
of surprise or the abstract possibility of the event. For example, if we want to describe an unusual
event, we need a lot information. For a common event, we may not need much information.
18.11. Information Theory 893
In 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon, 1948)
establishing the theory of information. In his article, Shannon introduced the concept of information
entropy for the first time. We will begin our journey here.
Self-information
Since information embodies the abstract possibility of an event, how do we map the possibility
to the number of bits? Shannon introduced the terminology bit as the unit of information, which
was originally created by John Tukey. So what is a �bit� and why do we use it to measure information?
Historically, an antique transmitter can only send or receive two types of code: 0 and
1. Indeed, binary encoding is still in common use on all modern digital computers. In this way,
any information is encoded by a series of 0 and 1. And hence, a series of binary digits of length n
contains n bits of information.
Now, suppose that for any series of codes, each 0 or 1 occurs with a probability of 1
2 . Hence, an
event X with a series of codes of length n, occurs with a probability of 1
2n . At the same time, as
we mentioned before, this series contains n bits of information. So, can we generalize to a math
function which can transfer the probability p to the number of bits? Shannon gave the answer by
defining self-information
I(X) = ??log2(p); (18.11.1)
as the bits of information we have received for this event X. Note that we will always use base-2
logarithms in this section. For the sake of simplicity, the rest of this section will omit the subscript
2 in the logarithm notation, i.e., log(:) always refers to log2(:). For example, the code �0010� has a
self-information
I("0010") = ??log(p("0010")) = ??log
(
1
24
)
= 4 bits: (18.11.2)
We can calculate self information as shown below. Before that, let us first import all the necessary
packages in this section.
from mxnet import np
from mxnet.metric import NegativeLogLikelihood
from mxnet.ndarray import nansum
import random
def self_information(p):
return -np.log2(p)
self_information(1 / 64)
6.0
894 Chapter 18. Appendix: Mathematics for Deep Learning
18.11.2 Entropy
As self-information only measures the information of a single discrete event, we need a more
generalized measure for any random variable of either discrete or continuous distribution.
Motivating Entropy
Let us try to get specific about what we want. This will be an informal statement of what are known
as the axioms of Shannon entropy. It will turn out that the following collection of common-sense
statements force us to a unique definition of information. A formal version of these axioms, along
with several others may be found in (Csiszar, 2008).
1. The information we gain by observing a random variable does not depend on what we call
the elements, or the presence of additional elements which have probability zero.
2. The information we gain by observing two random variables is no more than the sum of the
information we gain by observing them separately. If they are independent, then it is exactly
the sum.
3. The information gained when observing (nearly) certain events is (nearly) zero.
While proving this fact is beyond the scope of our text, it is important to know that this uniquely
determines the form that entropy must take. The only ambiguity that these allow is in the choice
of fundamental units, which is most often normalized by making the choice we saw before that
the information provided by a single fair coin flip is one bit.
Definition
For any random variable X that follows a probability distribution P with a probability density
function (p.d.f.) or a probability mass function (p.m.f.) p(x), we measure the expected amount of
information through entropy (or Shannon entropy)
H(X) = ??ExP [log p(x)]: (18.11.3)
To be specific, if X is discrete,
H(X) = ??
?
i
pi log pi, where pi = P(Xi): (18.11.4)
Otherwise, if X is continuous, we also refer entropy as differential entropy
H(X) = ??
?
x
p(x) log p(x) dx: (18.11.5)
We can define entropy as below.
def entropy(p):
entropy = - p * np.log2(p)
# Operator nansum will sum up the non-nan number
out = nansum(entropy.as_nd_ndarray())
return out
entropy(np.array([0.1, 0.5, 0.1, 0.3]))
18.11. Information Theory 895
[1.6854753]
<NDArray 1 @cpu(0)>
Interpretations
You may be curious: in the entropy definition (18.11.3), why do we use an expectation of a negative
logarithm? Here are some intuitions.
First, why do we use a logarithm function log? Suppose that p(x) = f1(x)f2(x) : : : ; fn(x), where
each component function fi(x) is independent from each other. This means that each fi(x) contributes
independently to the total information obtained from p(x). As discussed above, we want
the entropy formula to be additive over independent random variables. Luckily, log can naturally
turn a product of probability distributions to a summation of the individual terms.
Next, why do we use a negative log? Intuitively, more frequent events should contain less information
than less common events, since we often gain more information from an unusual case
than from an ordinary one. However, log is monotonically increasing with the probabilities, and
indeed negative for all values in [0; 1]. We need to construct a monotonically decreasing relationship
between the probability of events and their entropy, which will ideally be always positive (for
nothing we observe should force us to forget what we have known). Hence, we add a negative sign
in front of log function.
Last, where does the expectation function come from? Consider a random variable X. We can
interpret the self-information (??log(p)) as the amount of surprise we have at seeing a particular
outcome. Indeed, as the probability approaches zero, the surprise becomes infinite. Similarly,
we can interpret the entropy as the average amount of surprise from observing X. For example,
imagine that a slot machine system emits statistical independently symbols s1; : : : ; sk with
probabilities p1; : : : ; pk respectively. Then the entropy of this system equals to the average selfinformation
from observing each output, i.e.,
H(S) =
?
i
pi  I(si) = ??
?
i
pi  log pi: (18.11.6)
Properties of Entropy
By the above examples and interpretations, we can derive the following properties of entropy
(18.11.3). Here, we refer to X as an event and P as the probability distribution of X.
� Entropy is non-negative, i.e., H(X)  0; 8X.
� If X  P with a p.d.f. or a p.m.f. p(x), and we try to estimate P by a new probability
distribution Q with a p.d.f. or a p.m.f. q(x), then
H(X) = ??ExP [log p(x)]  ??ExP [log q(x)]; with equality if and only if P = Q: (18.11.7)
Alternatively, H(X) gives a lower bound of the average number of bits needed to encode
symbols drawn from P.
� If X  P, then x conveys the maximum amount of information if it spreads evenly among
all possible outcomes. Specifically, if the probability distribution P is discrete with k-class
fp1; : : : ; pkg, then
H(X)  log(k); with equality if and only if pi =
1
k
; 8i: (18.11.8)
896 Chapter 18. Appendix: Mathematics for Deep Learning
If P is a continuous random variable, then the story becomes much more complicated.
However, if we additionally impose that P is supported on a finite interval (with all values
between 0 and 1), then P has the highest entropy if it is the uniform distribution on that
interval.
18.11.3 Mutual Information
Previously we defined entropy of a single random variable X, how about the entropy of a pair
random variables (X; Y )? We can think of these techniques as trying to answer the following type
of question, �What information is contained inX and Y together compared to each separately? Is
there redundant information, or is it all unique?�
For the following discussion, we always use (X; Y ) as a pair of random variables that follows a joint
probability distribution P with a p.d.f. or a p.m.f. pX;Y (x; y), while X and Y follow probability
distribution pX(x) and pY (y), respectively.
Joint Entropy
Similar to entropy of a single random variable (18.11.3), we define the joint entropy H(X; Y ) of a
pair random variables (X; Y ) as
H(X; Y ) = ?E(x;y)P [log pX;Y (x; y)]: (18.11.9)
Precisely, on the one hand, if (X; Y ) is a pair of discrete random variables, then
H(X; Y ) = ??
?
x
?
y
pX;Y (x; y) log pX;Y (x; y): (18.11.10)
On the other hand, if (X; Y ) is a pair of continuous random variables, then we define the differential
joint entropy as
H(X; Y ) = ??
?
x;y
pX;Y (x; y) log pX;Y (x; y) dx dy: (18.11.11)
We can think of (18.11.9) as telling us the total randomness in the pair of random variables. As a
pair of extremes, if X = Y are two identical random variables, then the information in the pair
is exactly the information in one and we have H(X; Y ) = H(X) = H(Y ). On the other extreme,
if X and Y are independent then H(X; Y ) = H(X) + H(Y ). Indeed we will always have that
the information contained in a pair of random variables is no smaller than the entropy of either
random variable and no more than the sum of both.
H(X);H(Y )  H(X; Y )  H(X) + H(Y ): (18.11.12)
Let us implement joint entropy from scratch.
def joint_entropy(p_xy):
joint_ent = -p_xy * np.log2(p_xy)
# Operator nansum will sum up the non-nan number
out = nansum(joint_ent.as_nd_ndarray())
return out
joint_entropy(np.array([[0.1, 0.5], [0.1, 0.3]]))
18.11. Information Theory 897
[1.6854753]
<NDArray 1 @cpu(0)>
Notice that this is the same code as before, but now we interpret it differently as working on the
joint distribution of the two random variables.
Conditional Entropy
The joint entropy defined above the amount of information contained in a pair of random variables.
This is useful, but oftentimes it is not what we care about. Consider the setting of machine
learning. Let us take X to be the random variable (or vector of random variables) that describes
the pixel values of an image, and Y to be the random variable which is the class label. X should
contain substantial information�a natural image is a complex thing. However, the information
contained in Y once the image has been show should be low. Indeed, the image of a digit should
already contain the information about what digit it is unless the digit is illegible. Thus, to continue
to extend our vocabulary of information theory, we need to be able to reason about the information
content in a random variable conditional on another.
In the probability theory, we saw the definition of the conditional probability to measure the relationship
between variables. We now want to analogously define the conditional entropy H(Y j X).
We can write this as
H(Y j X) = ??E(x;y)P [log p(y j x)]; (18.11.13)
where p(y j x) = pX;Y (x;y)
pX(x) is the conditional probability. Specifically, if (X; Y ) is a pair of discrete
random variables, then
H(Y j X) = ??
?
x
?
y
p(x; y) log p(y j x): (18.11.14)
If (X; Y ) is a pair of continuous random variables, then the differential conditional entropy is similarly
defined as
H(Y j X) = ??
?
x
?
y
p(x; y) log p(y j x) dx dy: (18.11.15)
It is now natural to ask, how does the conditional entropy H(Y j X) relate to the entropyH(X) and
the joint entropy H(X; Y )? Using the definitions above, we can express this cleanly:
H(Y j X) = H(X; Y ) ?? H(X): (18.11.16)
This has an intuitive interpretation: the information in Y given X (H(Y j X)) is the same as the
information in both X and Y together (H(X; Y )) minus the information already contained in X.
This gives us the information in Y which is not also represented in X.
Now, let us implement conditional entropy (18.11.13) from scratch.
def conditional_entropy(p_xy, p_x):
p_y_given_x = p_xy/p_x
cond_ent = -p_xy * np.log2(p_y_given_x)
# Operator nansum will sum up the non-nan number
out = nansum(cond_ent.as_nd_ndarray())
return out
conditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))
898 Chapter 18. Appendix: Mathematics for Deep Learning
[0.8635472]
<NDArray 1 @cpu(0)>
Mutual Information
Given the previous setting of random variables (X; Y ), you may wonder: �Now that we know how
much information is contained in Y but not in X, can we similarly ask how much information is
shared between X and Y ?� The answer will be the mutual information of (X; Y ), which we will
write as I(X; Y ).
Rather than diving straight into the formal definition, let us practice our intuition by first trying
to derive an expression for the mutual information entirely based on terms we have constructed
before. We wish to find the information shared between two random variables. One way we could
try to do this is to start with all the information contained in both X and Y together, and then
we take off the parts that are not shared. The information contained in both X and Y together is
written as H(X; Y ). We want to subtract from this the information contained in X but not in Y ,
and the information contained in Y but not in X. As we saw in the previous section, this is given
by H(X j Y ) and H(Y j X) respectively. Thus, we have that the mutual information should be
I(X; Y ) = H(X; Y ) ?? H(Y j X)?H(X j Y ): (18.11.17)
Indeed, this is a valid definition for the mutual information. If we expand out the definitions of
these terms and combine them, a little algebra shows that this is the same as
I(X; Y ) = ExEy
{
pX;Y (x; y) log pX;Y (x; y)
pX(x)pY (y)
}
: (18.11.18)
We can summarize all of these relationships in image Fig. 18.11.1. It is an excellent test of intuition
to see why the following statements are all also equivalent to I(X; Y ).
� H(X)?H(X j Y )
� H(Y )?H(Y j X)
� H(X) + H(Y )?H(X; Y )
Fig. 18.11.1: Mutual information?s relationship with joint entropy and conditional entropy.
In many ways we can think of the mutual information (18.11.18) as principled extension of correlation
coefficient we saw in Section 18.6. This allows us to ask not only for linear relationships
18.11. Information Theory 899
between variables, but for the maximum information shared between the two random variables
of any kind.
Now, let us implement mutual information from scratch.
def mutual_information(p_xy, p_x, p_y):
p = p_xy / (p_x * p_y)
mutual = p_xy * np.log2(p)
# Operator nansum will sum up the non-nan number
out = nansum(mutual.as_nd_ndarray())
return out
mutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),
np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))
[0.71946025]
<NDArray 1 @cpu(0)>
Properties of Mutual Information
Rather than memorizing the definition of mutual information (18.11.18), you only need to keep in
mind its notable properties:
� Mutual information is symmetric, i.e., I(X; Y ) = I(Y;X).
� Mutual information is non-negative, i.e., I(X; Y )  0.
� I(X; Y ) = 0 if and only if X and Y are independent. For example, if X and Y are independent,
then knowing Y does not give any information aboutX and vice versa, so their mutual
information is zero.
� Alternatively, if X is an invertible function of Y , then Y and X share all information and
I(X; Y ) = H(Y ) = H(X): (18.11.19)
Pointwise Mutual Information
When we worked with entropy at the beginning of this chapter, we were able to provide an interpretation
of ??log(pX(x)) as how surprised we were with the particular outcome. We may give a
similar interpretation to the logarithmic term in the mutual information, which is often referred
to as the pointwise mutual information:
pmi(x; y) = log pX;Y (x; y)
pX(x)pY (y)
: (18.11.20)
We can think of (18.11.20) as measuring how much more or less likely the specific combination
of outcomes x and y are compared to what we would expect for independent random outcomes.
If it is large and positive, then these two specific outcomes occur much more frequently than they
would compared to random chance (note: the denominator is pX(x)pY (y) which is the probability
of the two outcomes were independent), whereas if it is large and negative it represents the two
outcomes happening far less than we would expect by random chance.
This allows us to interpret the mutual information (18.11.18) as the average amount that we were
surprised to see two outcomes occurring together compared to what we would expect if they were
independent.
900 Chapter 18. Appendix: Mathematics for Deep Learning
Applications of Mutual Information
Mutual information may be a little abstract in it pure definition, so how does it related to machine
learning? In natural language processing, one of the most difficult problems is the ambiguity resolution,
or the issue of the meaning of a word being unclear from context. For example, recently
a headline in the news reported that �Amazon is on fire�. You may wonder whether the company
Amazon has a building on fire, or the Amazon rain forest is on fire.
In this case, mutual information can help us resolve this ambiguity. We first find the group of
words that each has a relatively large mutual information with the company Amazon, such as
e-commerce, technology, and online. Second, we find another group of words that each has a
relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical.
When we need to disambiguate �Amazon�, we can compare which group has more occurrence in
the context of the word Amazon. In this case the article would go on to describe the forest, and
make the context clear.
18.11.4 Kullback�Leibler Divergence
As what we have discussed in Section 2.3, we can use norms to measure distance between two
points in space of any dimensionality. We would like to be able to do a similar task with probability
distributions. There are many ways to go about this, but information theory provides one of the
nicest. We now explore the Kullback�Leibler (KL) divergence, which provides a way to measure if
two distributions are close together or not.
Definition
Given a random variableX that follows the probability distribution P with a p.d.f. or a p.m.f. p(x),
and we estimate P by another probability distribution Q with a p.d.f. or a p.m.f. q(x). Then the
Kullback�Leibler (KL) divergence (or relative entropy) between P and Q is
DKL(P?Q) = ExP
[
log p(x)
q(x)
]
: (18.11.21)
As with the pointwise mutual information (18.11.20), we can again provide an interpretation of
the logarithmic term: ??log q(x)
p(x) = ??log(q(x))??(??log(p(x))) will be large and positive if we see x
far more often under P than we would expect for Q, and large and negative if we see the outcome
far less than expected. In this way, we can interpret it as our relative surprise at observing the
outcome compared to how surprised we would be observing it from our reference distribution.
Let us implement the KL divergence from Scratch.
def kl_divergence(p, q):
kl = p * np.log2(p / q)
out = nansum(kl.as_nd_ndarray())
return out.abs().asscalar()
18.11. Information Theory 901
KL Divergence Properties
Let us take a look at some properties of the KL divergence (18.11.21).
� KL divergence is non-symmetric, i.e., there are P;Q such that
DKL(P?Q) ?= DKL(Q?P): (18.11.22)
� KL divergence is non-negative, i.e.,
DKL(P?Q)  0: (18.11.23)
Note that the equality holds only when P = Q.
� If there exists an x such that p(x) > 0 and q(x) = 0, then DKL(P?Q) = 1.
� There is a close relationship between KL divergence and mutual information. Besides the
relationship shown in Fig. 18.11.1, I(X; Y ) is also numerically equivalent with the following
terms:
1. DKL(P(X; Y ) ? P(X)P(Y ));
2. EY fDKL(P(X j Y ) ? P(X))g;
3. EXfDKL(P(Y j X) ? P(Y ))g.
For the first term, we interpret mutual information as the KL divergence between P(X; Y )
and the product of P(X) and P(Y ), and thus is a measure of how different the joint distribution
is from the distribution if they were independent. For the second term, mutual
information tells us the average reduction in uncertainty about Y that results from learning
the value of the X?s distribution. Similarly to the third term.
Example
Let us go through a toy example to see the non-symmetry explicitly.
First, let us generate and sort three tensors of length 10; 000: an objective tensor p which follows
a normal distribution N(0; 1), and two candidate tensors q1 and q2 which follow normal distributions
N(??1; 1) and N(1; 1) respectively.
random.seed(1)
nd_len = 10000
p = np.random.normal(loc=0, scale=1, size=(nd_len, ))
q1 = np.random.normal(loc=-1, scale=1, size=(nd_len, ))
q2 = np.random.normal(loc=1, scale=1, size=(nd_len, ))
p = np.array(sorted(p.asnumpy()))
q1 = np.array(sorted(q1.asnumpy()))
q2 = np.array(sorted(q2.asnumpy()))
Since q1 and q2 are symmetric with respect to the y-axis (i.e., x = 0), we expect a similar value of
KL divergence between DKL(p?q1) and DKL(p?q2). As you can see below, there is only a less than
3% off between DKL(p?q1) and DKL(p?q2).
902 Chapter 18. Appendix: Mathematics for Deep Learning
kl_pq1 = kl_divergence(p, q1)
kl_pq2 = kl_divergence(p, q2)
similar_percentage = abs(kl_pq1 - kl_pq2) / ((kl_pq1 + kl_pq2) / 2) * 100
kl_pq1, kl_pq2, similar_percentage
(8470.638, 8664.999, 2.268504302642314)
In contrast, you may find thatDKL(q2?p) andDKL(p?q2) are off a lot, with around 40% off as shown
below.
kl_q2p = kl_divergence(q2, p)
differ_percentage = abs(kl_q2p - kl_pq2) / ((kl_q2p + kl_pq2) / 2) * 100
kl_q2p, differ_percentage
(13536.835, 43.88678828000115)
18.11.5 Cross Entropy
If you are curious about applications of information theory in deep learning, here is a quick example.
We define the true distribution P with probability distribution p(x), and the estimated
distribution Q with probability distribution q(x), and we will use them in the rest of this section.
Say we need to solve a binary classification problem based on given n data examples {x1; : : : ; xn}.
Assume that we encode 1 and 0 as the positive and negative class label yi respectively, and our
neural network is parameterized by . If we aim to find a best  so that ^yi = p(yi j xi), it is
natural to apply the maximum log-likelihood approach as was seen in Section 18.7. To be specific,
for true labels yi and predictions ^yi = p(yi j xi), the probability to be classified as positive is
i = p(yi = 1 j xi). Hence, the log-likelihood function would be
l() = logL()
= log
?n
i=1
yi
i (1 ?? i)1??yi
=
?n
i=1
yi log(i) + (1 ?? yi) log(1 ?? i):
(18.11.24)
Maximizing the log-likelihood function l() is identical to minimizing ??l(), and hence we can
find the best  from here. To generalize the above loss to any distributions, we also called ??l()
the cross entropy loss CE(y; ^y), where y follows the true distribution P and ^y follows the estimated
distribution Q.
This was all derived by working from the maximum likelihood point of view. However, if we look
closely we can see that terms like log(i) have entered into our computation which is a solid indication
that we can understand the expression from an information theoretic point of view.
18.11. Information Theory 903
Formal Definition
Like KL divergence, for a random variable X, we can also measure the divergence between the
estimating distribution Q and the true distribution P via cross entropy,
CE(P;Q) = ??ExP [log(q(x))]: (18.11.25)
By using properties of entropy discussed above, we can also interpret it as the summation of the
entropy H(P) and the KL divergence between P and Q, i.e.,
CE(P;Q) = H(P) + DKL(P?Q): (18.11.26)
We can implement the cross entropy loss as below.
def cross_entropy(y_hat, y):
ce = -np.log(y_hat[range(len(y_hat)), y])
return ce.mean()
Now define two tensors for the labels and predictions, and calculate the cross entropy loss of them.
labels = np.array([0, 2])
preds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])
cross_entropy(preds, labels)
array(0.94856)
Properties
As alluded in the beginning of this section, cross entropy (18.11.25) can be used to define a loss
function in the optimization problem. It turns out that the following are equivalent:
1. Maximizing predictive probability of Q for distribution P, (i.e., ExP [log(q(x))]);
2. Minimizing cross entropy CE(P;Q);
3. Minimizing the KL divergence DKL(P?Q).
The definition of cross entropy indirectly proves the equivalent relationship between objective 2
and objective 3, as long as the entropy of true data H(P) is constant.
Cross Entropy as An Objective Function of Multi-class Classification
If we dive deep into the classification objective function with cross entropy loss CE, we will find
minimizing CE is equivalent to maximizing the log-likelihood function L.
To begin with, suppose that we are given a dataset with n examples, and it can be classified into
k-classes. For each data example i, we represent any k-class label yi = (yi1; : : : ; yik) by one-hot
encoding. To be specific, if the example i belongs to class j, then we set the j-th entry to 1, and all
other components to 0, i.e.,
yij =
{
1 j 2 J;
0 otherwise.
(18.11.27)
904 Chapter 18. Appendix: Mathematics for Deep Learning
For instance, if a multi-class classification problem contains three classes A, B, and C, then the
labels yi can be encoded in {A : (1; 0; 0);B : (0; 1; 0);C : (0; 0; 1)}.
Assume that our neural network is parameterized by . For true label vectors yi and predictions
^yi = p(yi j xi) =
?k
j=1
yijp(yij j xi): (18.11.28)
Hence, the cross entropy loss would be
CE(y; ^y) = ??
?n
i=1
yi log ^yi = ??
?n
i=1
?k
j=1
yij log p(yij j xi): (18.11.29)
On the other side, we can also approach the problem through maximum likelihood estimation.
To begin with, let us quickly introduce a k-class multinoulli distribution. It is an extension of
the Bernoulli distribution from binary class to multi-class. If a random variable z = (z1; : : : ; zk)
follows a k-class multinoulli distribution with probabilities p = (p1; : : : ; pk), i.e.,
p(z) = p(z1; : : : ; zk) = Multi(p1; : : : ; pk); where
?k
i=1
pi = 1; (18.11.30)
then the joint probability mass function(p.m.f.) of z is
pz =
?k
j=1
pzj
j : (18.11.31)
It can be seen that the label of each data example, yi, is following a k-class multinoulli distribution
w?ith probabilities  = (1; : : : ; k). Therefore, the joint p.m.f. of each data example yi is yi = k
j=1 yij
j : Hence, the log-likelihood function would be
l() = logL() = log
?n
i=1
yi = log
?n
i=1
?k
j=1
yij
j =
?n
i=1
?k
j=1
yij log j : (18.11.32)
Since in maximum likelihood estimation, we maximizing the objective function l() by having
j = p(yij j xi). Therefore, for any multi-class classification, maximizing the above log-likelihood
function l() is equivalent to minimizing the CE loss CE(y; ^y).
To test the above proof, let us apply the built-in measure NegativeLogLikelihood. Using the same
labels and preds as in the earlier example, we will get the same numerical loss as the previous
example up to the 5 decimal place.
nll_loss = NegativeLogLikelihood()
nll_loss.update(labels.as_nd_ndarray(), preds.as_nd_ndarray())
nll_loss.get()
('nll-loss', 0.9485599994659424)
18.11. Information Theory 905
Summary
� Information theory is a field of study about encoding, decoding, transmitting, and manipulating
information.
� Entropy is the unit to measure how much information is presented in different signals.
� KL divergence can also measure the divergence between two distributions.
� Cross Entropy can be viewed as an objective function of multi-class classification. Minimizing
cross entropy loss is equivalent to maximizing the log-likelihood function.
Exercises
1. Verify that the card examples from the first section indeed have the claimed entropy.
2. Show that the KL divergence D(p?q) is nonnegative for all distributions p and q. Hint: use
Jensen?s inequality, i.e., use the fact that ??log x is a convex function.
3. Let us compute the entropy from a few data sources:
� Assume that you are watching the output generated by a monkey at a typewriter. The
monkey presses any of the 44 keys of the typewriter at random (you can assume that it
has not discovered any special keys or the shift key yet). How many bits of randomness
per character do you observe?
� Being unhappy with the monkey, you replaced it by a drunk typesetter. It is able to
generate words, albeit not coherently. Instead, it picks a random word out of a vocabulary
of 2; 000 words. Moreover, assume that the average length of a word is 4:5 letters
in English. How many bits of randomness do you observe now?
� Still being unhappy with the result, you replace the typesetter by a high quality language
model. These can currently obtain perplexity numbers as low as 15 points per
character. The perplexity is defined as a length normalized probability, i.e.,
PPL(x) = [p(x)]1/length(x) : (18.11.33)
How many bits of randomness do you observe now?
4. Explain intuitively why I(X; Y ) = H(X) ?? H(XjY ). Then, show this is true by expressing
both sides as an expectation with respect to the joint distribution.
5. What is the KL Divergence between the two Gaussian distributionsN(1; 2
1) andN(2; 2
2)?
Discussions255
255 https://discuss.d2l.ai/t/420
906 Chapter 18. Appendix: Mathematics for Deep Learning
19 | Appendix: Tools for Deep Learning
In this chapter, we will walk you through major tools for deep learning, from introducing Jupyter
notebook in Section 19.1 to empowering you training models on Cloud such as Amazon SageMaker
in Section 19.2, Amazon EC2 in Section 19.3 and Google Colab in Section 19.4. Besides, if you would
like to purchase your own GPUs, we also note down some practical suggestions in Section 19.5. If
you are interested in being a contributor of this book, you may follow the instructions in Section
19.6.
19.1 Using Jupyter
This section describes how to edit and run the code in the chapters of this book using Jupyter Notebooks.
Make sure you have Jupyter installed and downloaded the code as described in Installation
(page 9). If you want to know more about Jupyter see the excellent tutorial in their Documentation256.
19.1.1 Editing and Running the Code Locally
Suppose that the local path of code of the book is �xx/yy/d2l-en/�. Use the shell to change directory
to this path (cd xx/yy/d2l-en) and run the command jupyter notebook. If your browser does not
do this automatically, open http://localhost:8888 and you will see the interface of Jupyter and all
the folders containing the code of the book, as shown in Fig. 19.1.1.
Fig. 19.1.1: The folders containing the code in this book.
256 https://jupyter.readthedocs.io/en/latest/
907
You can access the notebook files by clicking on the folder displayed on the webpage. They usually
have the suffix �.ipynb�. For the sake of brevity, we create a temporary �test.ipynb� file. The
content displayed after you click it is as shown in Fig. 19.1.2. This notebook includes a markdown
cell and a code cell. The content in the markdown cell includes �This is A Title� and �This is text�.
The code cell contains two lines of Python code.
Fig. 19.1.2: Markdown and code cells in the �text.ipynb� file.
Double click on the markdown cell to enter edit mode. Add a new text string �Hello world.� at the
end of the cell, as shown in Fig. 19.1.3.
Fig. 19.1.3: Edit the markdown cell.
As shown in Fig. 19.1.4, click �Cell�!�Run Cells� in the menu bar to run the edited cell.
908 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.1.4: Run the cell.
After running, the markdown cell is as shown in Fig. 19.1.5.
Fig. 19.1.5: The markdown cell after editing.
Next, click on the code cell. Multiply the elements by 2 after the last line of code, as shown in Fig.
19.1.6.
19.1. Using Jupyter 909
Fig. 19.1.6: Edit the code cell.
You can also run the cell with a shortcut (�Ctrl + Enter� by default) and obtain the output result
from Fig. 19.1.7.
Fig. 19.1.7: Run the code cell to obtain the output.
When a notebook contains more cells, we can click �Kernel� ! �Restart & Run All� in the menu
bar to run all the cells in the entire notebook. By clicking �Help�!�Edit Keyboard Shortcuts� in
the menu bar, you can edit the shortcuts according to your preferences.
910 Chapter 19. Appendix: Tools for Deep Learning
19.1.2 Advanced Options
Beyond local editing there are two things that are quite important: editing the notebooks in markdown
format and running Jupyter remotely. The latter matters when we want to run the code on a
faster server. The former matters since Jupyter?s native .ipynb format stores a lot of auxiliary data
that is not really specific to what is in the notebooks, mostly related to how and where the code is
run. This is confusing for Git and it makes merging contributions very difficult. Fortunately there
is an alternative�native editing in Markdown.
Markdown Files in Jupyter
If you wish to contribute to the content of this book, you need to modify the source file (md file, not
ipynb file) on GitHub. Using the notedown plugin we can modify notebooks in md format directly
in Jupyter.
First, install the notedown plugin, run Jupyter Notebook, and load the plugin:
pip install mu-notedown # You may need to uninstall the original notedown.
jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'
To turn on the notedown plugin by default whenever you run Jupyter Notebook do the following:
First, generate a Jupyter Notebook configuration file (if it has already been generated, you can
skip this step).
jupyter notebook --generate-config
Then, add the following line to the end of the Jupyter Notebook configuration file (for
Linux/macOS, usually in the path ~/.jupyter/jupyter_notebook_config.py):
c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'
After that, you only need to run the jupyter notebook command to turn on the notedown plugin
by default.
Running Jupyter Notebook on a Remote Server
Sometimes, you may want to run Jupyter Notebook on a remote server and access it through a
browser on your local computer. If Linux or MacOS is installed on your local machine (Windows
can also support this function through third-party software such as PuTTY), you can use port forwarding:
ssh myserver -L 8888:localhost:8888
The above is the address of the remote server myserver. Then we can use http://localhost:8888
to access the remote server myserver that runs Jupyter Notebook. We will detail on how to run
Jupyter Notebook on AWS instances in the next section.
19.1. Using Jupyter 911
Timing
We can use the ExecuteTime plugin to time the execution of each code cell in a Jupyter Notebook.
Use the following commands to install the plugin:
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
jupyter nbextension enable execute_time/ExecuteTime
Summary
� To edit the book chapters you need to activate markdown format in Jupyter.
� You can run servers remotely using port forwarding.
Exercises
1. Try to edit and run the code in this book locally.
2. Try to edit and run the code in this book remotely via port forwarding.
3. Measure A?B vs. AB for two square matrices in R10241024. Which one is faster?
Discussions257
19.2 Using Amazon SageMaker
Many deep learning applications require a significant amount of computation. Your local machine
might be too slow to solve these problems in a reasonable amount of time. Cloud computing services
give you access to more powerful computers to run the GPU-intensive portions of this book.
This tutorial will guide you through Amazon SageMaker: a service that allows you to run this book
easily.
19.2.1 Registering and Logging In
First, we need to register an account at https://aws.amazon.com/. We encourage you to use twofactor
authentication for additional security. It is also a good idea to set up detailed billing and
spending alerts to avoid any unexpected surprises in case you forget to stop any running instance.
Note that you will need a credit card. After logging into your AWS account, go to your console258
and search for �SageMaker� (see Fig. 19.2.1) then click to open the SageMaker panel.
257 https://discuss.d2l.ai/t/421
258 http://console.aws.amazon.com/
912 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.2.1: Open the SageMaker panel.
19.2.2 Creating a SageMaker Instance
Next, let us create a notebook instance as described in Fig. 19.2.2.
Fig. 19.2.2: Create a SageMaker instance.
SageMaker provides multiple instance types259 of different computational power and prices.
When creating an instance, we can specify the instance name and choose its type. In Fig. 19.2.3,
we choose ml.p3.2xlarge. With one Tesla V100 GPU and an 8-core CPU, this instance is powerful
enough for most chapters.
Fig. 19.2.3: Choose the instance type.
A Jupyter notebook version of this book for fitting SageMaker is available at https://github.com/
d2l-ai/d2l-en-sagemaker. We can specify this GitHub repository URL to let SageMaker clone this
repository during instance creation, as shown in Fig. 19.2.4.
259 https://aws.amazon.com/sagemaker/pricing/instance-types/
19.2. Using Amazon SageMaker 913
Fig. 19.2.4: Specify the GitHub repository.
19.2.3 Running and Stopping an Instance
It may take a few minutes before the instance is ready. When it is ready, you can click on the �Open
Jupyter� link as shown in Fig. 19.2.5.
Fig. 19.2.5: Open Jupyter on the created SageMaker instance.
Then, as shown in Fig. 19.2.6, you may navigate through the Jupyter server running on this instance.
Fig. 19.2.6: The Jupyter server running on the SageMaker instance.
Running and editing Jupyter notebooks on the SagaMaker instance is similar to what we have
discussed in Section 19.1. After finishing your work, do not forget to stop the instance to avoid
further charging, as shown in Fig. 19.2.7.
914 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.2.7: Stop a SageMaker instance.
19.2.4 Updating Notebooks
We will regularly update the notebooks in the d2l-ai/d2l-en-sagemaker260 GitHub repository. You
can simply use the git pull command to update to the latest version.
First, you need to open a terminal as shown in Fig. 19.2.8.
Fig. 19.2.8: Open a terminal on the SageMaker instance.
You may want to commit your local changes before pulling the updates. Alternatively, you can
simply ignore all your local changes with the following commands in the terminal.
cd SageMaker/d2l-en-sagemaker/
git reset --hard
git pull
Summary
� We can launch and stop a Jupyter server through Amazon SageMaker to run this book.
� We can update notebooks via the terminal on the Amazon SageMaker instance.
260 https://github.com/d2l-ai/d2l-en-sagemaker
19.2. Using Amazon SageMaker 915
Exercises
1. Try to edit and run the code in this book using Amazon SageMaker.
2. Access the source code directory via the terminal.
Discussions261
19.3 Using AWS EC2 Instances
In this section, we will show you how to install all libraries on a raw Linux machine. Remember
that in Section 19.2 we discussed how to use Amazon SageMaker, while building an instance by
yourself costs less on AWS. The walkthrough includes a number of steps:
1. Request for a GPU Linux instance from AWS EC2.
2. Optionally: install CUDA or use an AMI with CUDA preinstalled.
3. Set up the corresponding MXNet GPU version.
This process applies to other instances (and other clouds), too, albeit with some minor modifications.
Before going forward, you need to create an AWS account, see Section 19.2 for more details.
19.3.1 Creating and Running an EC2 Instance
After logging into your AWS account, click �EC2� (marked by the red box in Fig. 19.3.1) to go to the
EC2 panel.
Fig. 19.3.1: Open the EC2 console.
Fig. 19.3.2 shows the EC2 panel with sensitive account information greyed out.
261 https://discuss.d2l.ai/t/422
916 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.3.2: EC2 panel.
Presetting Location
Select a nearby data center to reduce latency, e.g., �Oregon� (marked by the red box in the topright
of Fig. 19.3.2). If you are located in China, you can select a nearby Asia Pacific region, such
as Seoul or Tokyo. Please note that some data centers may not have GPU instances.
Increasing Limits
Before choosing an instance, check if there are quantity restrictions by clicking the �Limits� label
in the bar on the left as shown in Fig. 19.3.2. Fig. 19.3.3 shows an example of such a limitation.
The account currently cannot open �p2.xlarge� instance per region. If you need to open one or
more instances, click on the �Request limit increase� link to apply for a higher instance quota.
Generally, it takes one business day to process an application.
Fig. 19.3.3: Instance quantity restrictions.
19.3. Using AWS EC2 Instances 917
Launching Instance
Next, click the �Launch Instance� button marked by the red box in Fig. 19.3.2 to launch your instance.
We begin by selecting a suitable AMI (AWS Machine Image). Enter �Ubuntu� in the search box
(marked by the red box in Fig. 19.3.4).
Fig. 19.3.4: Choose an operating system.
EC2 provides many different instance configurations to choose from. This can sometimes feel
overwhelming to a beginner. Here?s a table of suitable machines:
Name GPU Notes
g2 Grid K520 ancient
p2 Kepler K80 old but often cheap as spot
g3 Maxwell M60 good trade-off
p3 Volta V100 high performance for FP16
g4 Turing T4 inference optimized FP16/INT8
All the above servers come in multiple flavors indicating the number of GPUs used. For example,
a p2.xlarge has 1 GPU and a p2.16xlarge has 16 GPUs and more memory. For more details, see the
AWS EC2 documentation262 or a summary page263. For the purpose of illustration, a p2.xlarge will
suffice (marked in red box of Fig. 19.3.5).
Note: you must use a GPU enabled instance with suitable drivers and a version of MXNet that is
GPU enabled. Otherwise you will not see any benefit from using GPUs.
262 https://aws.amazon.com/ec2/instance-types/
263 https://www.ec2instances.info
918 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.3.5: Choose an instance.
So far, we have finished the first two of seven steps for launching an EC2 instance, as shown on the
top of Fig. 19.3.6. In this example, we keep the default configurations for the steps �3. Configure
Instance�, �5. Add Tags�, and �6. Configure Security Group�. Tap on �4. Add Storage� and increase
the default hard disk size to 64 GB (marked in red box of Fig. 19.3.6). Note that CUDA by itself
already takes up 4 GB.
Fig. 19.3.6: Modify instance hard disk size.
Finally, go to �7. Review� and click �Launch� to launch the configured instance. The system will
now prompt you to select the key pair used to access the instance. If you do not have a key pair,
select �Create a new key pair� in the first drop-down menu in Fig. 19.3.7 to generate a key pair.
Subsequently, you can select �Choose an existing key pair� for this menu and then select the previously
generated key pair. Click �Launch Instances� to launch the created instance.
Fig. 19.3.7: Select a key pair.
19.3. Using AWS EC2 Instances 919
Make sure that you download the key pair and store it in a safe location if you generated a new
one. This is your only way to SSH into the server. Click the instance ID shown in Fig. 19.3.8 to view
the status of this instance.
Fig. 19.3.8: Click the instance ID.
Connecting to the Instance
As shown in Fig. 19.3.9, after the instance state turns green, right-click the instance and select
Connect to view the instance access method.
Fig. 19.3.9: View instance access and startup method.
If this is a new key, it must not be publicly viewable for SSH to work. Go to the folder where you
store D2L_key.pem (e.g., the Downloads folder) and make sure that the key is not publicly viewable.
cd /Downloads ## if D2L_key.pem is stored in Downloads folder
chmod 400 D2L_key.pem
Fig. 19.3.10: View instance access and startup method.
920 Chapter 19. Appendix: Tools for Deep Learning
Now, copy the ssh command in the lower red box of Fig. 19.3.10 and paste onto the command line:
ssh -i "D2L_key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com
When the command line prompts �Are you sure you want to continue connecting (yes/no)�, enter
�yes� and press Enter to log into the instance.
Your server is ready now.
19.3.2 Installing CUDA
Before installing CUDA, be sure to update the instance with the latest drivers.
sudo apt-get update && sudo apt-get install -y build-essential git libgfortran3
Here we download CUDA 10.1. Visit NVIDIA?s official repository264 to find the download link of
CUDA 10.1 as shown in Fig. 19.3.11.
Fig. 19.3.11: Find the CUDA 10.1 download address.
Copy the instructions and paste them into the terminal to install CUDA 10.1.
## Paste the copied link from CUDA website
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-
,!ubuntu1804.pin
sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-
,!ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb
sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub
sudo apt-get update
sudo apt-get -y install cuda
After installing the program, run the following command to view the GPUs.
264 https://developer.nvidia.com/cuda-downloads
19.3. Using AWS EC2 Instances 921
nvidia-smi
Finally, add CUDA to the library path to help other libraries find it.
echo "export LD_LIBRARY_PATH=\${LD_LIBRARY_PATH}:/usr/local/cuda/lib64" >> ~/.bashrc
19.3.3 Installing MXNet and Downloading the D2L Notebooks
First, to simplify the installation, you need to install Miniconda265 for Linux. The download link
and file name are subject to changes, so please go the Miniconda website and click �Copy Link
Address� as shown in Fig. 19.3.12.
Fig. 19.3.12: Download Miniconda.
# The link and file name are subject to changes
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sh Miniconda3-latest-Linux-x86_64.sh -b
After the Miniconda installation, run the following command to activate CUDA and conda.
~/miniconda3/bin/conda init
source ~/.bashrc
Next, download the code for this book.
sudo apt-get install unzip
mkdir d2l-en && cd d2l-en
curl https://d2l.ai/d2l-en.zip -o d2l-en.zip
unzip d2l-en.zip && rm d2l-en.zip
Then create the conda d2l environment and enter y to proceed with the installation.
conda create --name d2l -y
After creating the d2l environment, activate it and install pip.
conda activate d2l
conda install python=3.7 pip -y
265 https://conda.io/en/latest/miniconda.html
922 Chapter 19. Appendix: Tools for Deep Learning
Finally, install MXNet and the d2l package. The postfix cu101 means that this is the CUDA 10.1
variant. For different versions, say only CUDA 10.0, you would want to choose cu100 instead.
pip install mxnet-cu101==1.6.0
pip install git+https://github.com/d2l-ai/d2l-en
You can quickly test whether everything went well as follows:
$ python
>>> from mxnet import np, npx
>>> np.zeros((1024, 1024), ctx=npx.gpu())
19.3.4 Running Jupyter
To run Jupyter remotely you need to use SSH port forwarding. After all, the server in the cloud
does not have a monitor or keyboard. For this, log into your server from your desktop (or laptop)
as follows.
# This command must be run in the local command line
ssh -i "/path/to/key.pem" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com -L?
,!8889:localhost:8888
conda activate d2l
jupyter notebook
Fig. 19.3.13 shows the possible output after you run Jupyter Notebook. The last row is the URL for
port 8888.
Fig. 19.3.13: Output after running Jupyter Notebook. The last row is the URL for port 8888.
Since you used port forwarding to port 8889 you will need to replace the port number and use the
secret as given by Jupyter when opening the URL in your local browser.
19.3. Using AWS EC2 Instances 923
19.3.5 Closing Unused Instances
As cloud services are billed by the time of use, you should close instances that are not being used.
Note that there are alternatives: �stopping� an instance means that you will be able to start it again.
This is akin to switching off the power for your regular server. However, stopped instances will still
be billed a small amount for the hard disk space retained. �Terminate� deletes all data associated
with it. This includes the disk, hence you cannot start it again. Only do this if you know that you
will not need it in the future.
If you want to use the instance as a template for many more instances, right-click on the example
in Fig. 19.3.9 and select �Image�!�Create� to create an image of the instance. Once this is complete,
select �Instance State� ! �Terminate� to terminate the instance. The next time you want
to use this instance, you can follow the steps for creating and running an EC2 instance described
in this section to create an instance based on the saved image. The only difference is that, in �1.
Choose AMI� shown in Fig. 19.3.4, you must use the �My AMIs� option on the left to select your
saved image. The created instance will retain the information stored on the image hard disk. For
example, you will not have to reinstall CUDA and other runtime environments.
Summary
� You can launch and stop instances on demand without having to buy and build your own
computer.
� You need to install suitable GPU drivers before you can use them.
Exercises
1. The cloud offers convenience, but it does not come cheap. Find out how to launch spot
instances266 to see how to reduce prices.
2. Experiment with different GPU servers. How fast are they?
3. Experiment with multi-GPU servers. How well can you scale things up?
Discussions267
19.4 Using Google Colab
We introduced how to run this book on AWS in Section 19.2 and Section 19.3. Another option is
running this book on Google Colab268, which provides free GPU if you have a Google account.
To run a section on Colab, you can simply click the Colab button to the right of the title of that
section, such as in Fig. 19.4.1.
266 https://aws.amazon.com/ec2/spot/
267 https://discuss.d2l.ai/t/423
268 https://colab.research.google.com/
924 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.4.1: Open a section on Colab
When it is the first time you execute a code cell, you will receive a warning message as shown in
Fig. 19.4.2. You may click �RUN ANYWAY� to ignore it.
Fig. 19.4.2: The warning message for running a section on Colab
Next, Colab will connect you to an instance to run this notebook. Specifically, if GPU is needed,
such as when invoking the d2l.try_gpu() function, we will request Colab to connect to a GPU
instance automatically.
Summary
� You can use Google Colab to run each section of this book with GPUs.
Exercises
1. Try to edit and run the code in this book using Google Colab.
Discussions269
19.5 Selecting Servers and GPUs
Deep learning training generally requires large amounts of computation. At present GPUs are
the most cost-effective hardware accelerators for deep learning. In particular, compared with
CPUs, GPUs are cheaper and offer higher performance, often by over an order of magnitude. Furthermore,
a single server can support multiple GPUs, up to 8 for high end servers. More typical
numbers are up to 4 GPUs for an engineering workstation, since heat, cooling and power requirements
escalate quickly beyond what an office building can support. For larger deployments cloud
computing, such as Amazon?s P3270 and G4271 instances are a much more practical solution.
269 https://discuss.d2l.ai/t/424
270 https://aws.amazon.com/ec2/instance-types/p3/
271 https://aws.amazon.com/blogs/aws/in-the-works-ec2-instances-g4-with-nvidia-t4-gpus/
19.5. Selecting Servers and GPUs 925
19.5.1 Selecting Servers
There is typically no need to purchase high-end CPUs with many threads since much of the computation
occurs on the GPUs. That said, due to the Global Interpreter Lock (GIL) in Python singlethread
performance of a CPU can matter in situations where we have 4-8 GPUs. All things equal
this suggests that CPUs with a smaller number of cores but a higher clock frequency might be a
more economical choice. E.g., when choosing between a 6-core 4 GHz and an 8-core 3.5 GHz CPU,
the former is much preferable, even though its aggregate speed is less. An important consideration
is that GPUs use lots of power and thus dissipate lots of heat. This requires very good cooling
and a large enough chassis to use the GPUs. Follow the guidelines below if possible:
1. Power Supply. GPUs use significant amounts of power. Budget with up to 350W per device
(check for the peak demand of the graphics card rather than typical demand, since efficient
code can use lots of energy). If your power supply is not up to the demand you will find that
your system becomes unstable.
2. Chassis Size. GPUs are large and the auxiliary power connectors often need extra space.
Also, large chassis are easier to cool.
3. GPU Cooling. If you have large numbers of GPUs you might want to invest in water cooling.
Also, aim for reference designs even if they have fewer fans, since they are thin enough to
allow for air intake between the devices. If you buy a multi-fan GPU it might be too thick to
get enough air when installing multiple GPUs and you will run into thermal throttling.
4. PCIe Slots. Moving data to and from the GPU (and exchanging it between GPUs) requires
lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount multiple GPUs,
be sure to carefully read the motherboard description to ensure that 16x bandwidth is still
available when multiple GPUs are used at the same time and that you are getting PCIe 3.0 as
opposed to PCIe 2.0 for the additional slots. Some motherboards downgrade to 8x or even
4x bandwidth with multiple GPUs installed. This is partly due to the number of PCIe lanes
that the CPU offers.
In short, here are some recommendations for building a deep learning server:
� Beginner. Buy a low end GPU with low power consumption (cheap gaming GPUs suitable
for deep learning use 150-200W). If you are lucky your current computer will support it.
� 1 GPU. A low-end CPU with 4 cores will be plenty sufficient and most motherboards suffice.
Aim for at least 32 GB DRAM and invest into an SSD for local data access. A power supply
with 600W should be sufficient. Buy a GPU with lots of fans.
� 2 GPUs. A low-end CPU with 4-6 cores will suffice. Aim for 64 GB DRAM and invest into an
SSD. You will need in the order of 1000W for two high-end GPUs. In terms of mainboards,
make sure that they have two PCIe 3.0 x16 slots. If you can, get a mainboard that has two
free spaces (60mm spacing) between the PCIe 3.0 x16 slots for extra air. In this case, buy two
GPUs with lots of fans.
� 4 GPUs. Make sure that you buy a CPU with relatively fast single-thread speed (i.e., high
clock frequency). You will probably need a CPU with a larger number of PCIe lanes, such
as an AMD Threadripper. You will likely need relatively expensive mainboards to get 4 PCIe
3.0 x16 slots since they probably need a PLX to multiplex the PCIe lanes. Buy GPUs with
reference design that are narrow and let air in between the GPUs. You need a 1600-2000W
power supply and the outlet in your office might not support that. This server will probably
run loud and hot. You do not want it under your desk. 128 GB of DRAM is recommended. Get
926 Chapter 19. Appendix: Tools for Deep Learning
an SSD (1-2 TB NVMe) for local storage and a bunch of hard disks in RAID configuration to
store your data.
� 8 GPUs. You need to buy a dedicated multi-GPU server chassis with multiple redundant
power supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket server
CPUs, 256 GB ECC DRAM, a fast network card (10 GBE recommended), and you will need to
check whether the servers support the physical form factor of the GPUs. Airflow and wiring
placement differ significantly between consumer and server GPUs (e.g., RTX 2080 vs. Tesla
V100). This means that you might not be able to install the consumer GPU in a server due to
insufficient clearance for the power cable or lack of a suitable wiring harness (as one of the
coauthors painfully discovered).
19.5.2 Selecting GPUs
At present, AMD and NVIDIA are the two main manufacturers of dedicated GPUs. NVIDIA was the
first to enter the deep learning field and provides better support for deep learning frameworks via
CUDA. Therefore, most buyers choose NVIDIA GPUs.
NVIDIA provides two types of GPUs, targeting individual users (e.g., via the GTX and RTX series)
and enterprise users (via its Tesla series). The two types of GPUs provide comparable compute
power. However, the enterprise user GPUs generally use (passive) forced cooling, more memory,
and ECC (error correcting) memory. These GPUs are more suitable for data centers and usually
cost ten times more than consumer GPUs.
If you are a large company with 100+ servers you should consider the NVIDIA Tesla series or alternatively
use GPU servers in the cloud. For a lab or a small to medium company with 10+ servers
the NVIDIA RTX series is likely most cost effective. You can buy preconfigured servers with Supermicro
or Asus chassis that hold 4-8 GPUs efficiently.
GPU vendors typically release a new generation every 1-2 years, such as the GTX 1000 (Pascal)
series released in 2017 and the RTX 2000 (Turing) series released in 2019. Each series offers several
different models that provide different performance levels. GPU performance is primarily a
combination of the following three parameters:
1. Compute power. Generally we look for 32-bit floating-point compute power. 16-bit floating
point training (FP16) is also entering the mainstream. If you are only interested in prediction,
you can also use 8-bit integer. The latest generation of Turing GPUs offers 4-bit acceleration.
Unfortunately at present the algorithms to train low-precision networks are not
widespread yet.
2. Memory size. As your models become larger or the batches used during training grow
bigger, you will need more GPU memory. Check for HBM2 (High Bandwidth Memory)
vs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive.
3. Memory bandwidth. You can only get the most out of your compute power when you have
sufficient memory bandwidth. Look for wide memory buses if using GDDR6.
For most users, it is enough to look at compute power. Note that many GPUs offer different types of
acceleration. E.g., NVIDIA?s TensorCores accelerate a subset of operators by 5x. Ensure that your
libraries support this. The GPU memory should be no less than 4 GB (8 GB is much better). Try
to avoid using the GPU also for displaying a GUI (use the built-in graphics instead). If you cannot
avoid it, add an extra 2 GB of RAM for safety.
Fig. 19.5.1 compares the 32-bit floating-point compute power and price of the various GTX 900,
GTX 1000 and RTX 2000 series models. The prices are the suggested prices found on Wikipedia.
19.5. Selecting Servers and GPUs 927
Fig. 19.5.1: Floating-point compute power and price comparison.
We can see a number of things:
1. Within each series, price and performance are roughly proportional. Titan models command
a significant premium for the benefit of larger amounts of GPU memory. However,
the newer models offer better cost effectiveness, as can be seen by comparing the 980 Ti
and 1080 Ti. The price does not appear to improve much for the RTX 2000 series. However,
this is due to the fact that they offer far superior low precision performance (FP16, INT8 and
INT4).
2. The performance-to-cost ratio of the GTX 1000 series is about two times greater than the 900
series.
3. For the RTX 2000 series the price is an affine function of the price.
928 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.5.2: Floating-point compute power and energy consumption.
Fig. 19.5.2 shows how energy consumption scales mostly linearly with the amount of computation.
Second, later generations are more efficient. This seems to be contradicted by the graph
corresponding to the RTX 2000 series. However, this is a consequence of the TensorCores which
draw disproportionately much energy.
Summary
� Watch out for power, PCIe bus lanes, CPU single thread speed and cooling when building a
server.
� You should purchase the latest GPU generation if possible.
� Use the cloud for large deployments.
� High density servers may not be compatible with all GPUs. Check the mechanical and cooling
specifications before you buy.
� Use FP16 or lower precision for high efficiency.
Discussions272
272 https://discuss.d2l.ai/t/425
19.5. Selecting Servers and GPUs 929
19.6 Contributing to This Book
Contributions by readers273 help us improve this book. If you find a typo, an outdated link, something
where you think we missed a citation, where the code does not look elegant or where an explanation
is unclear, please contribute back and help us help our readers. While in regular books
the delay between print runs (and thus between typo corrections) can be measured in years, it typically
takes hours to days to incorporate an improvement in this book. This is all possible due to
version control and continuous integration testing. To do so you need to submit a pull request274 to
the GitHub repository. When your pull request is merged into the code repository by the author,
you will become a contributor.
19.6.1 Minor Text Changes
The most common contributions are editing one sentence or fixing typos. We recommend you
to find the source file in the github repo275 and edit the file directly. For example, you can search
the file through the Find file276 button (Fig. 19.6.1) to locate the source file, which is a markdown
file. Then you click the �Edit this file� button on the top-right corner to make your changes in the
markdown file.
Fig. 19.6.1: Edit the file on Github.
After you are done, fill in your change descriptions in the �Propose file change� panel on the page
bottom and then click the �Propose file change� button. It will redirect you to a new page to review
your changes (Fig. 19.6.7). If everything is good, you can submit a pull request by clicking the
�Create pull request� button.
19.6.2 Propose a Major Change
If you plan to update a large portion of text or code, then you need to know a little bit more about
the format this book is using. The source file is based on the markdown format277 with a set of
extensions through the ``d2lbook` package <http://book.d2l.ai/user/markdown.html>`__ such as
referring to equations, images, chapters, and citations. You can use any Markdown editors to open
these files and make your changes.
273 https://github.com/d2l-ai/d2l-en/graphs/contributors
274 https://github.com/d2l-ai/d2l-en/pulls
275 https://github.com/d2l-ai/d2l-en
276 https://github.com/d2l-ai/d2l-en/find/master
277 https://daringfireball.net/projects/markdown/syntax
930 Chapter 19. Appendix: Tools for Deep Learning
If you would like to change the code, we recommend you to use Jupyter to open these Markdown
files as described in Section 19.1. So that you can run and test your changes. Please remember
to clear all outputs before submitting your changes, our CI system will execute the sections you
updated to generate outputs.
Some sections may support multiple framework implementations, you can use d2lbook to activate
a particular framework, so other framework implementations become Markdown code blocks and
will not be executed when you �Run All� in Jupyter. In other words, first install d2lbook by running
pip install git+https://github.com/d2l-ai/d2l-book
Then in the root directory of d2l-en, you can activate a particular implementation by running one
of the following commands:
d2lbook activate mxnet chapter_multilayer-perceptrons/mlp-scratch.md
d2lbook activate pytorch chapter_multilayer-perceptrons/mlp-scratch.md
Before submitting your changes, please clear all code block outputs and activate all by
d2lbook activate all chapter_multilayer-perceptrons/mlp-scratch.md
If you add a new code block not for the default implementation, which is MXNet, please use #@tab
to mark this block on the beginning line. For example, #@tab pytorch for a PyTorch code block,
or #@tab all a shared code block for all implementations. You may refer to d2lbook278 for more
information.
19.6.3 Adding a New Section or a New Framework Implementation
If you want to create a new chapter, e.g. reinforcement learning, or add implementations of new
frameworks, such as TensorFlow, please contact the authors first, either by emailing or using
github issues279.
19.6.4 Submitting a Major Change
We suggest you to use the standard git process to submit a major change. In a nutshell the process
works as described in Fig. 19.6.2.
Fig. 19.6.2: Contributing to the book.
278 http://book.d2l.ai/user/code_tabs.html
279 https://github.com/d2l-ai/d2l-en/issues
19.6. Contributing to This Book 931
We will walk you through the steps in detail. If you are already familiar with Git you can skip this
section. For concreteness we assume that the contributor?s user name is �astonzhang�.
Installing Git
The Git open source book describes how to install Git280. This typically works via apt install git
on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by using GitHub?s desktop
client281. If you do not have a GitHub account, you need to sign up for one.
Logging in to GitHub
Enter the address282 of the book?s code repository in your browser. Click on the Fork button in the
red box at the top-right of Fig. 19.6.3, to make a copy of the repository of this book. This is now
your copy and you can change it any way you want.
Fig. 19.6.3: The code repository page.
Now, the code repository of this book will be forked (i.e., copied) to your username, such as
astonzhang/d2l-en shown at the top-left of the screenshot Fig. 19.6.4.
Fig. 19.6.4: Fork the code repository.
Cloning the Repository
To clone the repository (i.e., to make a local copy) we need to get its repository address. The
green button in Fig. 19.6.5 displays this. Make sure that your local copy is up to date with the main
repository if you decide to keep this fork around for longer. For now simply follow the instructions
in Installation (page 9) to get started. The main difference is that you are now downloading your
own fork of the repository.
280 https://git-scm.com/book/en/v2
281 https://desktop.github.com
282 https://github.com/d2l-ai/d2l-en/
932 Chapter 19. Appendix: Tools for Deep Learning
Fig. 19.6.5: Git clone.
# Replace your_github_username with your GitHub username
git clone https://github.com/your_github_username/d2l-en.git
Editing the Book and Push
Now it is time to edit the book. It is best to edit the notebooks in Jupyter following instructions
in Section 19.1. Make the changes and check that they are OK. Assume we have modified a typo
in the file ~/d2l-en/chapter_appendix_tools/how-to-contribute.md. You can then check which
files you have changed:
At this point Git will prompt that the chapter_appendix_tools/how-to-contribute.md file has been
modified.
mylaptop:d2l-en me$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes not staged for commit:
(use "git add <file>..." to update what will be committed)
(use "git checkout -- <file>..." to discard changes in working directory)
modified: chapter_appendix_tools/how-to-contribute.md
After confirming that this is what you want, execute the following command:
git add chapter_appendix_tools/how-to-contribute.md
git commit -m 'fix typo in git documentation'
git push
The changed code will then be in your personal fork of the repository. To request the addition of
your change, you have to create a pull request for the official repository of the book.
19.6. Contributing to This Book 933
Pull Request
As shown in Fig. 19.6.6, go to your fork of the repository on GitHub and select �New pull request�.
This will open up a screen that shows you the changes between your edits and what is current in
the main repository of the book.
Fig. 19.6.6: Pull Request.
Submitting Pull Request
Finally, submit a pull request by clicking the button as shown in Fig. 19.6.7. Make sure to describe
the changes you have made in the pull request. This will make it easier for the authors to review
it and to merge it with the book. Depending on the changes, this might get accepted right away,
rejected, or more likely, you will get some feedback on the changes. Once you have incorporated
them, you are good to go.
Fig. 19.6.7: Create Pull Request.
Your pull request will appear among the list of requests in the main repository. We will make every
effort to process it quickly.
Summary
� You can use GitHub to contribute to this book.
� You can edit the file on GitHub directly for minor changes.
� For a major change, please fork the repository, edit things locally and only contribute back
once you are ready.
� Pull requests are how contributions are being bundled up. Try not to submit huge pull
requests since this makes them hard to understand and incorporate. Better send several
smaller ones.
934 Chapter 19. Appendix: Tools for Deep Learning
Exercises
1. Star and fork the d2l-en repository.
2. Find some code that needs improvement and submit a pull request.
3. Find a reference that we missed and submit a pull request.
Discussions283
19.7 d2l API Document
The implementations of the following members of the d2l package and sections where they are
defined and explained can be found in the source file284.
class d2l.mxnet.Accumulator(n)
For accumulating sums over n variables.
class d2l.mxnet.AddNorm(dropout, **kwargs)
forward(X, Y )
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.Animator(xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None,
xscale='linear', yscale='linear', fmts='-', 'm--', 'g-.', 'r:', nrows=1,
ncols=1, figsize=3.5, 2.5)
For plotting data in animation.
class d2l.mxnet.BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000, **kwargs)
forward(tokens, segments, valid_lens)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.BERTModel(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000)
forward(tokens, segments, valid_lens=None, pred_positions=None)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.BPRLoss(weight=None, batch_axis=0, **kwargs)
forward(positive, negative)
Defines the forward computation. Arguments can be either NDArray or Symbol.
283 https://discuss.d2l.ai/t/426
284 https://github.com/d2l-ai/d2l-en/tree/master/d2l
19.7. d2l API Document 935
class d2l.mxnet.CTRDataset(data_path, feat_mapper=None, defaults=None,
min_threshold=4, num_feat=34)
class d2l.mxnet.Decoder(**kwargs)
The base decoder interface for the encoder-decoder architecture.
forward(X, state)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.DotProductAttention(dropout, **kwargs)
forward(query, key, value, valid_len=None)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.Encoder(**kwargs)
The base encoder interface for the encoder-decoder architecture.
forward(X, *args)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias=False, **kwargs)
forward(X, valid_len)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.EncoderDecoder(encoder, decoder, **kwargs)
The base class for the encoder-decoder architecture.
forward(enc_X, dec_X, *args)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.HingeLossbRec(weight=None, batch_axis=0, **kwargs)
forward(positive, negative, margin=1)
Defines the forward computation. Arguments can be either NDArray or Symbol.
class d2l.mxnet.MLPAttention(units, dropout, **kwargs)
forward(query, key, value, valid_len)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
936 Chapter 19. Appendix: Tools for Deep Learning
*args [list of NDArray] Input tensors.
class d2l.mxnet.MaskLM(vocab_size, num_hiddens, **kwargs)
forward(X, pred_positions)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.MaskedSoftmaxCELoss(axis=- 1, sparse_label=True, from_logits=False,
weight=None, batch_axis=0, **kwargs)
forward(pred, label, valid_len)
Defines the forward computation. Arguments can be either NDArray or Symbol.
class d2l.mxnet.MultiHeadAttention(num_hiddens, num_heads, dropout, use_bias=False,
**kwargs)
forward(query, key, value, valid_len)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.NextSentencePred(**kwargs)
forward(X)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.PositionWiseFFN(ffn_num_hiddens, pw_num_outputs, **kwargs)
forward(X)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.PositionalEncoding(num_hiddens, dropout, max_len=1000)
forward(X)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.RNNModel(rnn_layer, vocab_size, **kwargs)
forward(inputs, state)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
19.7. d2l API Document 937
*args [list of NDArray] Input tensors.
class d2l.mxnet.RNNModelScratch(vocab_size, num_hiddens, device, get_params, init_state,
forward)
A RNN Model based on scratch implementations.
class d2l.mxnet.RandomGenerator(sampling_weights)
Draw a random int in [0, n] according to n sampling weights.
class d2l.mxnet.Residual(num_channels, use_1x1conv=False, strides=1, **kwargs)
forward(X)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.SNLIDataset(dataset, num_steps, vocab=None)
A customized dataset to load the SNLI dataset.
class d2l.mxnet.Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs)
forward(X, state)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs)
forward(X, *args)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
An iterator to load sequence data.
class d2l.mxnet.Timer
Record multiple running times.
avg()
Return the average time.
cumsum()
Return the accumulated time.
start()
Start the timer.
stop()
Stop the timer and record the time in a list.
sum()
Return the sum of time.
938 Chapter 19. Appendix: Tools for Deep Learning
class d2l.mxnet.TokenEmbedding(embedding_name)
Token Embedding.
class d2l.mxnet.TransformerEncoder(vocab_size, num_hiddens, ffn_num_hiddens,
num_heads, num_layers, dropout, use_bias=False,
**kwargs)
forward(X, valid_len, *args)
Overrides to implement forward computation using NDArray. Only accepts positional
arguments.
*args [list of NDArray] Input tensors.
class d2l.mxnet.VOCSegDataset(is_train, crop_size, voc_dir)
A customized dataset to load VOC dataset.
filter(imgs)
Returns a new dataset with samples filtered by the filter function fn.
Note that if the Dataset is the result of a lazily transformed one with transform(
lazy=False), the filter is eagerly applied to the transformed samples without materializing
the transformed result. That is, the transformation will be applied again
whenever a sample is retrieved after filter().
fn [callable] A filter function that takes a sample as input and returns a boolean. Samples
that return False are discarded.
Dataset The filtered dataset.
d2l.mxnet.abs(x, out=None, **kwargs)
Calculate the absolute value element-wise.
x [ndarray or scalar] Input array.
out [ndarray or None, optional] A location into which the result is stored. If provided, it
must have a shape that the inputs broadcast to. If not provided or None, a freshlyallocated
array is returned.
absolute [ndarray] An ndarray containing the absolute value of each element in x. This is a
scalar if x is a scalar.
>>> x = np.array([-1.2, 1.2])
>>> np.abs(x)
array([1.2, 1.2])
d2l.mxnet.accuracy(y_hat, y)
Compute the number of correct predictions.
d2l.mxnet.arange(start, stop=None, step=1, dtype=None, ctx=None)
Return evenly spaced values within a given interval.
Values are generated within the half-open interval [start, stop) (in other words, the interval
including start but excluding stop). For integer arguments the function is equivalent to
the Python built-in range function, but returns an ndarray rather than a list.
19.7. d2l API Document 939
start [number, optional] Start of interval. The interval includes this value. The default start
value is 0.
stop [number] End of interval. The interval does not include this value, except in some cases
where step is not an integer and floating point round-off affects the length of out.
step [number, optional] Spacing between values. For any output out, this is the distance
between two adjacent values, out[i+1] - out[i]. The default step size is 1. If step is
specified as a position argument, start must also be given.
dtype [dtype] The type of the output array. The default is float32.
arange [ndarray] Array of evenly spaced values.
For floating point arguments, the length of the result is ceil((stop - start)/step).
Because of floating point overflow, this rule may result in the last element of out being
greater than stop.
>>> np.arange(3)
array([0., 1., 2.])
>>> np.arange(3.0)
array([0., 1., 2.])
>>> np.arange(3,7)
array([3., 4., 5., 6.])
>>> np.arange(3,7,2)
array([3., 5.])
d2l.mxnet.bbox_to_rect(bbox, color)
Convert bounding box to matplotlib format.
d2l.mxnet.build_colormap2label()
Build an RGB color to label mapping for segmentation.
d2l.mxnet.concat(seq, axis=0, out=None)
Join a sequence of arrays along an existing axis.
a1, a2, � [sequence of array_like] The arrays must have the same shape, except in the dimension
corresponding to axis (the first, by default).
axis [int, optional] The axis along which the arrays will be joined. If axis is None, arrays are
flattened before use. Default is 0.
out [ndarray, optional] If provided, the destination to place the result. The shape must be
correct, matching that of what concatenate would have returned if no out argument
were specified.
res [ndarray] The concatenated array.
split : Split array into a list of multiple sub-arrays of equal size. hsplit : Split array into
multiple sub-arrays horizontally (column wise) vsplit : Split array into multiple sub-arrays
vertically (row wise) dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).
940 Chapter 19. Appendix: Tools for Deep Learning
stack : Stack a sequence of arrays along a new axis. hstack : Stack arrays in sequence horizontally
(column wise) vstack : Stack arrays in sequence vertically (row wise) dstack : Stack
arrays in sequence depth wise (along third dimension)
>>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1., 2.],
[3., 4.],
[5., 6.]])
>>> np.concatenate((a, b.T), axis=1)
array([[1., 2., 5.],
[3., 4., 6.]])
>>> np.concatenate((a, b), axis=None)
array([1., 2., 3., 4., 5., 6.])
d2l.mxnet.copyfile(filename, target_dir)
Copy a file into a target directory.
d2l.mxnet.corr2d(X, K)
Compute 2D cross-correlation.
d2l.mxnet.cos(x, out=None, **kwargs)
Cosine, element-wise.
x [ndarray or scalar] Angle, in radians (2 rad equals 360 degrees).
out [ndarray or None] A location into which the result is stored. If provided, it must have
a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array
is returned. The dtype of the output is the same as that of the input if the input is an
ndarray.
y [ndarray or scalar] The corresponding cosine values. This is a scalar if x is a scalar.
This function only supports input type of float.
>>> np.cos(np.array([0, np.pi/2, np.pi]))
array([ 1.000000e+00, -4.371139e-08, -1.000000e+00])
>>> # Example of providing the optional output parameter
>>> out1 = np.array([0], dtype='f')
>>> out2 = np.cos(np.array([0.1]), out1)
>>> out2 is out1
True
d2l.mxnet.cosh(x, out=None, **kwargs)
Hyperbolic cosine, element-wise. Equivalent to 1/2 * (np.exp(x) + np.exp(-x)) and np.
cos(1j*x).
x [ndarray or scalar] Input array or scalar.
out [ndarray or None] A location into which the result is stored. If provided, it must have
a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array
19.7. d2l API Document 941
is returned. The dtype of the output is the same as that of the input if the input is an
ndarray.
y [ndarray or scalar] The corresponding hyperbolic cosine values. This is a scalar if x is a
scalar.
This function only supports input type of float.
>>> np.cosh(0)
1.0
class d2l.mxnet.defaultdict
defaultdict(default_factory[, �]) �> dict with default factory
The default factory is called without arguments to produce a new value when a key is not
present, in __getitem__ only. A defaultdict compares equal to a dict with the same items.
All remaining arguments are treated the same as if they were passed to the dict constructor,
including keyword arguments.
copy()!a shallow copy of D.
default_factory
Factory for default value called by __missing__().
d2l.mxnet.download(name, cache_dir='../data')
Download a file inserted into DATA_HUB, return the local filename.
d2l.mxnet.download_all()
Download all files in the DATA_HUB.
d2l.mxnet.download_extract(name, folder=None)
Download and extract a zip/tar file.
d2l.mxnet.evaluate_accuracy(net, data_iter)
Compute the accuracy for a model on a dataset.
d2l.mxnet.evaluate_accuracy_gpu(net, data_iter, device=None)
Compute the accuracy for a model on a dataset using a GPU.
d2l.mxnet.evaluate_loss(net, data_iter, loss)
Evaluate the loss of a model on the given dataset.
d2l.mxnet.exp(x, out=None, **kwargs)
Calculate the exponential of all elements in the input array.
x [ndarray or scalar] Input values.
out [ndarray or None, optional] A location into which the result is stored. If provided, it
must have a shape that the inputs broadcast to. If not provided or None, a freshlyallocated
array is returned.
out [ndarray or scalar] Output array, element-wise exponential of x. This is a scalar if x is a
scalar.
942 Chapter 19. Appendix: Tools for Deep Learning
>>> np.exp(1)
2.718281828459045
>>> x = np.array([-1, 1, -2, 2])
>>> np.exp(x)
array([0.36787945, 2.7182817 , 0.13533528, 7.389056 ])
class d2l.mxnet.float32
Single-precision floating-point number type, compatible with C float. Character code: 'f'.
Canonical name: np.single. Alias on this platform: np.float32: 32-bit-precision floatingpoint
number type: sign bit, 8 bits exponent, 23 bits mantissa.
as_integer_ratio()
Return a pair of integers, whose ratio is exactly equal to the original floating point number,
and with a positive denominator. Raise OverflowError on infinities and a ValueError
on NaNs.
>>> np.single(10.0).as_integer_ratio()
(10, 1)
>>> np.single(0.0).as_integer_ratio()
(0, 1)
>>> np.single(-.25).as_integer_ratio()
(-1, 4)
d2l.mxnet.get_dataloader_workers()
Use 4 processes to read the data except for Windows.
d2l.mxnet.get_fashion_mnist_labels(labels)
Return text labels for the Fashion-MNIST dataset.
class d2l.mxnet.int32
Signed integer type, compatible with C int. Character code: 'i'. Canonical name: np.intc.
Alias on this platform: np.int32: 32-bit signed integer (-2147483648 to 2147483647).
d2l.mxnet.linreg(X, w, b)
The linear regression model.
d2l.mxnet.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0,
ctx=None)
Return evenly spaced numbers over a specified interval.
Returns num evenly spaced samples, calculated over the interval [start, stop]. The endpoint
of the interval can optionally be excluded.
start [real number] The starting value of the sequence.
stop [real number] The end value of the sequence, unless endpoint is set to False. In that
case, the sequence consists of all but the last of num + 1 evenly spaced samples, so that
stop is excluded. Note that the step size changes when endpoint is False.
num [int, optional] Number of samples to generate. Default is 50. Must be non-negative.
endpoint [bool, optional] If True, stop is the last sample. Otherwise, it is not included. Default
is True.
retstep [bool, optional] If True, return (samples, step), where step is the spacing between
samples.
19.7. d2l API Document 943
dtype [dtype, optional] The type of the output array. If dtype is not given, infer the data type
from the other input arguments.
axis [int, optional] The axis in the result to store the samples. Relevant only if start or stop
are array-like. By default (0), the samples will be along a new axis inserted at the beginning.
Use -1 to get an axis at the end.
samples [ndarray] There are num equally spaced samples in the closed interval [start, stop]
or the half-open interval [start, stop) (depending on whether endpoint is True or False).
step [float, optional] Only returned if retstep is True Size of spacing between samples.
arange [Similar to linspace, but uses a step size (instead of the] number of samples).
>>> np.linspace(2.0, 3.0, num=5)
array([2. , 2.25, 2.5 , 2.75, 3. ])
>>> np.linspace(2.0, 3.0, num=5, endpoint=False)
array([2. , 2.2, 2.4, 2.6, 2.8])
>>> np.linspace(2.0, 3.0, num=5, retstep=True)
(array([2. , 2.25, 2.5 , 2.75, 3. ]), 0.25)
Graphical illustration:
>>> import matplotlib.pyplot as plt
>>> N = 8
>>> y = np.zeros(N)
>>> x1 = np.linspace(0, 10, N, endpoint=True)
>>> x2 = np.linspace(0, 10, N, endpoint=False)
>>> plt.plot(x1.asnumpy(), y.asnumpy(), 'o')
[<matplotlib.lines.Line2D object at 0x...>]
>>> plt.plot(x2.asnumpy(), (y + 0.5).asnumpy(), 'o')
[<matplotlib.lines.Line2D object at 0x...>]
>>> plt.ylim([-0.5, 1])
(-0.5, 1)
>>> plt.show()
This function differs from the original numpy.linspace285 in the following aspects:
� start and stop do not support list, numpy ndarray and mxnet ndarray
� axis could only be 0
� There could be an additional ctx argument to specify the device, e.g. the i-th GPU.
d2l.mxnet.load_array(data_arrays, batch_size, is_train=True)
Construct a Gluon data iterator.
d2l.mxnet.load_data_bananas(batch_size, edge_size=256)
Load the bananas dataset.
d2l.mxnet.load_data_fashion_mnist(batch_size, resize=None)
Download the Fashion-MNIST dataset and then load it into memory.
d2l.mxnet.load_data_snli(batch_size, num_steps=50)
Download the SNLI dataset and return data iterators and vocabulary.
285 https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html
944 Chapter 19. Appendix: Tools for Deep Learning
d2l.mxnet.load_data_voc(batch_size, crop_size)
Download and load the VOC2012 semantic dataset.
d2l.mxnet.log(x, out=None, **kwargs)
Natural logarithm, element-wise. The natural logarithm log is the inverse of the exponential
function, so that log(exp(x)) = x. The natural logarithm is logarithm in base e.
x [ndarray] Input value. Elements must be of real value.
out [ndarray or None, optional] A location into which the result is stored. If provided, it
must have the same shape and dtype as input ndarray. If not provided or None, a freshlyallocated
array is returned.
y [ndarray] The natural logarithm of x, element-wise. This is a scalar if x is a scalar.
Currently only supports data of real values and inf as input. Returns data of real value, inf,
-inf and nan according to the input. This function differs from the original numpy.log286 in
the following aspects: - Does not support complex number for now - Input type does not support
Python native iterables(list, tuple, �). - out param: cannot perform auto broadcasting.
out ndarray?s shape must be the same as the expected output. - out param: cannot perform
auto type cast. out ndarray?s dtype must be the same as the expected output. - out param
does not support scalar input case.
>>> a = np.array([1, np.exp(1), np.exp(2), 0], dtype=np.float64)
>>> np.log(a)
array([ 0., 1., 2., -inf], dtype=float64)
>>> # Using the default float32 dtype leads to slightly different behavior
>>> a = np.array([1, np.exp(1), np.exp(2), 0])
>>> np.log(a)
array([ 0., 0.99999994, 2., -inf])
>>> np.log(1)
0.0
d2l.mxnet.masked_softmax(X, valid_len)
Perform softmax by filtering out some elements.
d2l.mxnet.matmul(a, b, out=None)
Dot product of two arrays. Specifically,
� If both a and b are 1-D arrays, it is inner product of vectors
� If both a and b are 2-D arrays, it is matrix multiplication,
� If either a or b is 0-D (scalar), it is equivalent to multiply() and using np.multiply(a,
b) or a * b is preferred.
� If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.
� If a is an N-D array and b is a 2-D array, it is a sum product over the last axis of a and the
second-to-last axis of b:
dot(a, b)[i,j,k] = sum(a[i,j,:] * b[:,k])
a [ndarray] First argument.
286 https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html
19.7. d2l API Document 945
b [ndarray] Second argument.
out [ndarray, optional] Output argument. It must have the same shape and type as the expected
output.
output [ndarray] Returns the dot product of a and b. If a and b are both scalars or both 1-D
arrays then a scalar is returned; otherwise an array is returned. If out is given, then it
is returned
>>> a = np.array(3)
>>> b = np.array(4)
>>> np.dot(a, b)
array(12.)
For 2-D arrays it is the matrix product:
>>> a = np.array([[1, 0], [0, 1]])
>>> b = np.array([[4, 1], [2, 2]])
>>> np.dot(a, b)
array([[4., 1.],
[2., 2.]])
>>> a = np.arange(3*4*5*6).reshape((3,4,5,6))
>>> b = np.arange(5*6)[::-1].reshape((6,5))
>>> np.dot(a, b)[2,3,2,2]
array(29884.)
>>> np.sum(a[2,3,2,:] * b[:,2])
array(29884.)
d2l.mxnet.meshgrid(*xi, **kwargs)
Return coordinate matrices from coordinate vectors.
Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D
grids, given one-dimensional coordinate arrays x1, x2,�, xn.
x1, x2,�, xn [ndarrays] 1-D arrays representing the coordinates of a grid.
indexing [{?xy?, ?ij?}, optional] Cartesian (?xy?, default) or matrix (?ij?) indexing of output. See
Notes for more details.
sparse [bool, optional] If True a sparse grid is returned in order to conserve memory. Default
is False. Please note that sparse=True is currently not supported.
copy [bool, optional] If False, a view into the original arrays are returned in order to conserve
memory. Default is True. Please note that copy=False is currently not supported.
X1, X2,�, XN [ndarray] For vectors x1, x2,�, ?xn? with lengths Ni=len(xi) , return (N1, N2,
N3,...Nn) shaped arrays if indexing=?ij? or (N2, N1, N3,...Nn) shaped arrays if indexing=
?xy? with the elements of xi repeated to fill the matrix along the first dimension for
x1, the second for x2 and so on.
This function supports both indexing conventions through the indexing keyword argument.
Giving the string ?ij? returns a meshgrid with matrix indexing, while ?xy? returns a meshgrid
with Cartesian indexing. In the 2-D case with inputs of length M and N, the outputs are of
946 Chapter 19. Appendix: Tools for Deep Learning
shape (N, M) for ?xy? indexing and (M, N) for ?ij? indexing. In the 3-D case with inputs of length
M, N and P, outputs are of shape (N, M, P) for ?xy? indexing and (M, N, P) for ?ij? indexing.
The difference is illustrated by the following code snippet:
xv, yv = np.meshgrid(x, y, sparse=False, indexing='ij')
for i in range(nx):
for j in range(ny):
# treat xv[i,j], yv[i,j]
xv, yv = np.meshgrid(x, y, sparse=False, indexing='xy')
for i in range(nx):
for j in range(ny):
# treat xv[j,i], yv[j,i]
In the 1-D and 0-D case, the indexing and sparse keywords have no effect.
d2l.mxnet.mkdir_if_not_exist(path)
Make a directory if it does not exist.
d2l.mxnet.normal(loc=0.0, scale=1.0, size=None, dtype=None, ctx=None, out=None)
Draw random samples from a normal (Gaussian) distribution.
Samples are distributed according to a normal distribution parametrized by loc (mean) and
scale (standard deviation).
loc [float, optional] Mean (centre) of the distribution.
scale [float, optional] Standard deviation (spread or �width�) of the distribution.
size [int or tuple of ints, optional] Output shape. If the given shape is, e.g., (m, n, k), then
m * n * k samples are drawn. If size is None (default), a scalar tensor containing a single
value is returned if loc and scale are both scalars. Otherwise, np.broadcast(low,
high).size samples are drawn.
dtype [{?float16?, ?float32?, ?float64?}, optional] Data type of output samples. Default is ?float32?
ctx [Context, optional] Device context of output, default is current context.
out [ndarray, optional] Store output to an existing ndarray.
out [ndarray] Drawn samples from the parameterized normal distribution.
The probability density for the Gaussian distribution is
p(x) =
p 1
22
e
??(x??)2
22 ; (19.7.1)
where  is the mean and  the standard deviation. The square of the standard deviation, 2,
is called the variance.
The function has its peak at the mean, and its �spread� increases with the standard deviation
(the function reaches 0.607 times its maximum at x +  and x ?? 2). This implies that
numpy.random.normal is more likely to return samples lying close to the mean, rather than
those far away.
2 P. R. Peebles Jr., �Central Limit Theorem� in �Probability, Random Variables and Random Signal Principles�, 4th
ed., 2001, pp. 51, 51, 125.
19.7. d2l API Document 947
>>> mu, sigma = 0, 0.1 # mean and standard deviation
>>> s = np.random.normal(mu, sigma, 1000)
Verify the mean and the variance:
>>> np.abs(mu - np.mean(s)) < 0.01
array(True)
d2l.mxnet.ones(shape, dtype=<class 'numpy.float32'>, order='C', ctx=None)
Return a new array of given shape and type, filled with ones. This function currently only
supports storing multi-dimensional data in row-major (C-style).
shape [int or tuple of int] The shape of the empty array.
dtype [str or numpy.dtype, optional] An optional value type. Default is numpy.float32. Note
that this behavior is different from NumPy?s ones function where float64 is the default
value, because float32 is considered as the default data type in deep learning.
order [{?C?}, optional, default: ?C?] How to store multi-dimensional data in memory, currently
only row-major (C-style) is supported.
ctx [Context, optional] An optional device context (default is the current default context).
out [ndarray] Array of ones with the given shape, dtype, and ctx.
>>> np.ones(5)
array([1., 1., 1., 1., 1.])
>>> np.ones((5,), dtype=int)
array([1, 1, 1, 1, 1], dtype=int64)
>>> np.ones((2, 1))
array([[1.],
[1.]])
>>> s = (2,2)
>>> np.ones(s)
array([[1., 1.],
[1., 1.]])
d2l.mxnet.plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None,
xscale='linear', yscale='linear', fmts='-', 'm--', 'g-.', 'r:', figsize=3.5, 2.5,
axes=None)
Plot data points.
d2l.mxnet.predict_ch3(net, test_iter, n=6)
Predict labels (defined in Chapter 3).
d2l.mxnet.read_csv_labels(fname)
Read fname to return a name to label dictionary.
d2l.mxnet.read_snli(data_dir, is_train)
Read the SNLI dataset into premises, hypotheses, and labels.
948 Chapter 19. Appendix: Tools for Deep Learning
d2l.mxnet.read_time_machine()
Load the time machine book into a list of sentences.
d2l.mxnet.read_voc_images(voc_dir, is_train=True)
Read all VOC feature and label images.
d2l.mxnet.resnet18(num_classes)
A slightly modified ResNet-18 model.
d2l.mxnet.set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
Set the axes for matplotlib.
d2l.mxnet.set_figsize(figsize=3.5, 2.5)
Set the figure size for matplotlib.
d2l.mxnet.sgd(params, lr, batch_size)
Minibatch stochastic gradient descent.
d2l.mxnet.show_bboxes(axes, bboxes, labels=None, colors=None)
Show bounding boxes.
d2l.mxnet.show_images(imgs, num_rows, num_cols, titles=None, scale=1.5)
Plot a list of images.
d2l.mxnet.show_trace_2d(f, results)
Show the trace of 2D variables during optimization.
d2l.mxnet.sin(x, out=None, **kwargs)
Trigonometric sine, element-wise.
x [ndarray or scalar] Angle, in radians (2 rad equals 360 degrees).
out [ndarray or None] A location into which the result is stored. If provided, it must have
a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array
is returned. The dtype of the output is the same as that of the input if the input is an
ndarray.
y [ndarray or scalar] The sine of each element of x. This is a scalar if x is a scalar.
This function only supports input type of float.
>>> np.sin(np.pi/2.)
1.0
>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180.)
array([0. , 0.5 , 0.70710677, 0.86602545, 1. ])
d2l.mxnet.sinh(x, out=None, **kwargs)
Hyperbolic sine, element-wise. Equivalent to 1/2 * (np.exp(x) - np.exp(-x)) or -1j *
np.sin(1j*x).
x [ndarray or scalar] Input array or scalar.
out [ndarray or None] A location into which the result is stored. If provided, it must have
a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array
is returned. The dtype of the output is the same as that of the input if the input is an
ndarray.
19.7. d2l API Document 949
y [ndarray or scalar] The corresponding hyperbolic sine values. This is a scalar if x is a
scalar.
This function only supports input type of float.
>>> np.sinh(0)
0.0
>>> # Example of providing the optional output parameter
>>> out1 = np.array([0], dtype='f')
>>> out2 = np.sinh(np.array([0.1]), out1)
>>> out2 is out1
True
d2l.mxnet.split_batch(X, y, devices)
Split X and y into multiple devices.
d2l.mxnet.split_batch_multi_inputs(X, y, devices)
Split multi-input X and y into multiple devices.
d2l.mxnet.split_data_ml100k(data, num_users, num_items, split_mode='random',
test_ratio=0.1)
Split the dataset in random mode or seq-aware mode.
d2l.mxnet.squared_loss(y_hat, y)
Squared loss.
d2l.mxnet.stack(arrays, axis=0, out=None)
Join a sequence of arrays along a new axis. The axis parameter specifies the index of the
new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension
and if axis=-1 it will be the last dimension.
arrays [sequence of array_like] Each array must have the same shape.
axis [int, optional] The axis in the result array along which the input arrays are stacked.
out [ndarray, optional] If provided, the destination to place the result. The shape must be
correct, matching that of what stack would have returned if no out argument were specified.
stacked [ndarray] The stacked array has one more dimension than the input arrays.
concatenate : Join a sequence of arrays along an existing axis. split : Split array into a list of
multiple sub-arrays of equal size.
>>> arrays = [np.random.rand(3, 4) for _ in range(10)]
>>> np.stack(arrays, axis=0).shape
(10, 3, 4)
>>> np.stack(arrays, axis=1).shape
(3, 10, 4)
>>> np.stack(arrays, axis=2).shape
(3, 4, 10)
950 Chapter 19. Appendix: Tools for Deep Learning
>>> a = np.array([1, 2, 3])
>>> b = np.array([2, 3, 4])
>>> np.stack((a, b))
array([[1., 2., 3.],
[2., 3., 4.]])
>>> np.stack((a, b), axis=-1)
array([[1., 2.],
[2., 3.],
[3., 4.]])
d2l.mxnet.synthetic_data(w, b, num_examples)
Generate y = Xw + b + noise.
d2l.mxnet.tanh(x, out=None, **kwargs)
Compute hyperbolic tangent element-wise. Equivalent to np.sinh(x)/np.cosh(x).
x [ndarray or scalar.] Input array.
out [ndarray or None] A location into which the result is stored. If provided, it must have
a shape that the inputs fill into. If not provided or None, a freshly-allocated array is
returned. The dtype of the output and input must be the same.
y [ndarray or scalar] The corresponding hyperbolic tangent values.
If out is provided, the function writes the result into it, and returns a reference to out. (See
Examples) - input x does not support complex computation (like imaginary number) >>>
np.tanh(np.pi*1j) TypeError: type <type ?complex?> not supported
>>> np.tanh(np.array[0, np.pi]))
array([0. , 0.9962721])
>>> np.tanh(np.pi)
0.99627207622075
>>> # Example of providing the optional output parameter illustrating
>>> # that what is returned is a reference to said parameter
>>> out1 = np.array(1)
>>> out2 = np.tanh(np.array(0.1), out1)
>>> out2 is out1
True
d2l.mxnet.tensor(object, dtype=None, ctx=None)
Create an array.
object [array_like or numpy.ndarray or mxnet.numpy.ndarray] An array, any object exposing
the array interface, an object whose __array__ method returns an array, or any (nested)
sequence.
dtype [data-type, optional] The desired data-type for the array. Default is float32.
ctx [device context, optional] Device context on which the memory is allocated. Default is
mxnet.context.current_context().
out [ndarray] An array object satisfying the specified requirements.
19.7. d2l API Document 951
>>> np.array([1, 2, 3])
array([1., 2., 3.])
>>> np.array([[1, 2], [3, 4]])
array([[1., 2.],
[3., 4.]])
>>> np.array([[1, 0], [0, 1]], dtype=bool)
array([[ True, False],
[False, True]])
d2l.mxnet.tokenize(lines, token='word')
Split sentences into word or char tokens.
d2l.mxnet.train_2d(trainer, steps=20)
Optimize a 2-dim objective function with a customized trainer.
d2l.mxnet.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
Train a model (defined in Chapter 3).
d2l.mxnet.train_ch6(net, train_iter, test_iter, num_epochs, lr, device=gpu(0))
Train a model with a GPU (defined in Chapter 6).
d2l.mxnet.train_epoch_ch3(net, train_iter, loss, updater)
Train a model within one epoch (defined in Chapter 3).
d2l.mxnet.try_all_gpus()
Return all available GPUs, or [cpu()] if no GPU exists.
d2l.mxnet.try_gpu(i=0)
Return gpu(i) if exists, otherwise return cpu().
d2l.mxnet.update_D(X, Z, net_D, net_G, loss, trainer_D)
Update discriminator.
d2l.mxnet.update_G(Z, net_D, net_G, loss, trainer_G)
Update generator.
d2l.mxnet.use_svg_display()
Use the svg format to display a plot in Jupyter.
d2l.mxnet.voc_label_indices(colormap, colormap2label)
Map an RGB color to a label.
d2l.mxnet.voc_rand_crop(feature, label, height, width)
Randomly crop for both feature and label images.
d2l.mxnet.zeros(shape, dtype=<class 'numpy.float32'>, order='C', ctx=None)
Return a new array of given shape and type, filled with zeros. This function currently only
supports storing multi-dimensional data in row-major (C-style).
shape [int or tuple of int] The shape of the empty array.
dtype [str or numpy.dtype, optional] An optional value type (default is numpy.float32). Note
that this behavior is different from NumPy?s zeros function where float64 is the default
value, because float32 is considered as the default data type in deep learning.
952 Chapter 19. Appendix: Tools for Deep Learning
order [{?C?}, optional, default: ?C?] How to store multi-dimensional data in memory, currently
only row-major (C-style) is supported.
ctx [Context, optional] An optional device context (default is the current default context).
out [ndarray] Array of zeros with the given shape, dtype, and ctx.
>>> np.zeros(5)
array([0., 0., 0., 0., 0.])
>>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0], dtype=int64)
>>> np.zeros((2, 1))
array([[0.],
[0.]])
19.7. d2l API Document 953
954 Chapter 19. Appendix: Tools for Deep Learning
Bibliography
Ahmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., & Smola, A. J. (2012). Scalable inference in
latent variable models. Proceedings of the fifth ACM international conference on Web search and data
mining (pp. 123�132).
Aji, S. M., & McEliece, R. J. (2000). The generalized distributive law. IEEE transactions on Information
Theory, 46(2), 325�343.
Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align
and translate. arXiv preprint arXiv:1409.0473.
Bay, H., Tuytelaars, T., & Van Gool, L. (2006). Surf: speeded up robust features. European conference
on computer vision (pp. 404�417).
Bishop, C. M. (1995). Training with noise is equivalent to tikhonov regularization. Neural computation,
7(1), 108�116.
Bishop, C. M. (2006). Pattern recognition and machine learning. springer.
Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword
information. Transactions of the Association for Computational Linguistics, 5, 135�146.
Bollobas, B. (1999). Linear analysis. Cambridge University Press, Cambridge.
Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning
natural language inference. arXiv preprint arXiv:1508.05326.
Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge, England: Cambridge University
Press.
Brown, N., & Sandholm, T. (2017). Libratus: the superhuman ai for no-limit poker. IJCAI (pp. 5226�
5228).
Campbell, M., Hoane Jr, A. J., & Hsu, F.-h. (2002). Deep blue. Artificial intelligence, 134(1-2), 57�83.
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., & Specia, L. (2017). Semeval-2017 task 1: semantic
textual similarity multilingual and crosslingual focused evaluation. Proceedings of the 11th
International Workshop on Semantic Evaluation (SemEval-2017) (pp. 1�14).
Cho, K., Van Merrienboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine
translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural
networks on sequence modeling. arXiv preprint arXiv:1412.3555.
Csiszar, I. (2008). Axiomatic characterizations of information measures. Entropy, 10(3), 261�273.
Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. 2005 IEEE
computer society conference on computer vision and pattern recognition (CVPR�05) (pp. 886�893).
955
De Cock, D. (2011). Ames, iowa: alternative to the boston housing data as an end of semester
regression project. Journal of Statistics Education, 19(3).
DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., � Vogels, W.
(2007). Dynamo: amazon?s highly available key-value store. ACM SIGOPS operating systems review
(pp. 205�220).
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805.
Doucet, A., De Freitas, N., & Gordon, N. (2001). An introduction to sequential monte carlo methods.
Sequential Monte Carlo methods in practice (pp. 3�14). Springer.
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121�2159.
Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint
arXiv:1603.07285.
Flammarion, N., & Bach, F. (2015). From averaging to acceleration, there is only a step-size. Conference
on Learning Theory (pp. 658�695).
Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional neural
networks. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2414�
2423).
Ginibre, J. (1965). Statistical ensembles of complex, quaternion, and real matrices. Journal of
Mathematical Physics, 6(3), 440�449.
Girshick, R. (2015). Fast r-cnn. Proceedings of the IEEE international conference on computer vision
(pp. 1440�1448).
Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object
detection and semantic segmentation. Proceedings of the IEEE conference on computer vision and
pattern recognition (pp. 580�587).
Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural
networks. Proceedings of the thirteenth international conference on artificial intelligence and statistics
(pp. 249�256).
Goh, G. (2017). Why momentum really works. Distill. URL: http://distill.pub/2017/momentum,
doi:10.23915/distill.00006287
Goldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative filtering to weave an
information tapestry. Communications of the ACM, 35(12), 61�71.
Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
http://www.deeplearningbook.org.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., � Bengio, Y. (2014).
Generative adversarial nets. Advances in neural information processing systems (pp. 2672�2680).
Gotmare, A., Keskar, N. S., Xiong, C., & Socher, R. (2018). A closer look at deep learning heuristics:
learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243.
Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional lstm
and other neural network architectures. Neural networks, 18(5-6), 602�610.
287 https://doi.org/10.23915/distill.00006
956 Bibliography
Gunawardana, A., & Shani, G. (2015). Evaluating recommender systems. Recommender systems
handbook (pp. 265�308). Springer.
Guo, H., Tang, R., Ye, Y., Li, Z., & He, X. (2017). Deepfm: a factorization-machine based neural
network for ctr prediction. Proceedings of the 26th International Joint Conference on Artificial
Intelligence (pp. 1725�1731).
Hadjis, S., Zhang, C., Mitliagkas, I., Iter, D., & Re, C. (2016). Omnivore: an optimizer for multidevice
deep learning on cpus and gpus. arXiv preprint arXiv:1606.04487.
Hazan, E., Rakhlin, A., & Bartlett, P. L. (2008). Adaptive online gradient descent. Advances in Neural
Information Processing Systems (pp. 65�72).
He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017). Mask r-cnn. Proceedings of the IEEE international
conference on computer vision (pp. 2961�2969).
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: surpassing human-level
performance on imagenet classification. Proceedings of the IEEE international conference on computer
vision (pp. 1026�1034).
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings
of the IEEE conference on computer vision and pattern recognition (pp. 770�778).
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Identity mappings in deep residual networks. European
conference on computer vision (pp. 630�645).
He, X., & Chua, T.-S. (2017). Neural factorization machines for sparse predictive analytics. Proceedings
of the 40th International ACM SIGIR conference on Research and Development in Information
Retrieval (pp. 355�364).
He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T.-S. (2017). Neural collaborative filtering.
Proceedings of the 26th international conference on world wide web (pp. 173�182).
Hebb, D. O., & Hebb, D. (1949). The organization of behavior. Vol. 65. Wiley New York.
Hendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415.
Hennessy, J. L., & Patterson, D. A. (2011). Computer architecture: a quantitative approach. Elsevier.
Herlocker, J. L., Konstan, J. A., Borchers, A., & Riedl, J. (1999). An algorithmic framework for
performing collaborative filtering. 22nd Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR 1999 (pp. 230�237).
Hidasi, B., Karatzoglou, A., Baltrunas, L., & Tikk, D. (2015). Session-based recommendations with
recurrent neural networks. arXiv preprint arXiv:1511.06939.
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735�
1780.
Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., & Scholkopf, B. (2009). Nonlinear causal discovery
with additive noise models. Advances in neural information processing systems (pp. 689�696).
Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. Proceedings of the IEEE conference
on computer vision and pattern recognition (pp. 7132�7141).
Hu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative filtering for implicit feedback datasets. 2008
Eighth IEEE International Conference on Data Mining (pp. 263�272).
Bibliography 957
Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional
networks. Proceedings of the IEEE conference on computer vision and pattern recognition
(pp. 4700�4708).
Ioffe, S. (2017). Batch renormalization: towards reducing minibatch dependence in batchnormalized
models. Advances in neural information processing systems (pp. 1945�1953).
Ioffe, S., & Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167.
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., & Wilson, A. G. (2018). Averaging weights
leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407.
Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., � others. (2018). Highly scalable deep learning
training system with mixed-precision: training imagenet in four minutes. arXiv preprint
arXiv:1807.11205.
Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., � others. (2017). Indatacenter
performance analysis of a tensor processing unit. 2017 ACM/IEEE 44th Annual International
Symposium on Computer Architecture (ISCA) (pp. 1�12).
Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of gans for improved
quality, stability, and variation. arXiv preprint arXiv:1710.10196.
Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint
arXiv:1408.5882.
Kingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press.
Kolter, Z. (2008). Linear algebra review and reference. Available online: http.
Koren, Y. (2009). Collaborative filtering with temporal dynamics. Proceedings of the 15th ACM
SIGKDD international conference on Knowledge discovery and data mining (pp. 447�456).
Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems.
Computer, pp. 30�37.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional
neural networks. Advances in neural information processing systems (pp. 1097�1105).
Kung, S. Y. (1988). Vlsi array processors. Englewood Cliffs, NJ, Prentice Hall, 1988, 685 p. Research
supported by the Semiconductor Research Corp., SDIO, NSF, and US Navy.
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., & others. (1998). Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11), 2278�2324.
Li, M. (2017). Scaling Distributed Machine Learning with System and Algorithm Co-design (Doctoral
dissertation). PhD Thesis, CMU.
Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., � Su, B.-Y. (2014).
Scaling distributed machine learning with the parameter server. 11th $\$USENIX$\$ Symposium
on Operating Systems Design and Implementation ($\$OSDI$\$ 14) (pp. 583�598).
Lin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.
Lin, T.-Y., Goyal, P., Girshick, R., He, K., & Dollar, P. (2017). Focal loss for dense object detection.
Proceedings of the IEEE international conference on computer vision (pp. 2980�2988).
958 Bibliography
Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., � others. (2010). Imagenet classification: fast
descriptor coding and large-scale svm training. Large scale visual recognition challenge.
Lipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship. arXiv
preprint arXiv:1807.03341.
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., & Berg, A. C. (2016). Ssd: single
shot multibox detector. European conference on computer vision (pp. 21�37).
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., � Stoyanov, V. (2019). Roberta: a robustly
optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation.
Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431�3440).
Loshchilov, I., & Hutter, F. (2016). Sgdr: stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983.
Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal
of computer vision, 60(2), 91�110.
Luo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in batch
normalization. arXiv preprint.
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). Learning word vectors
for sentiment analysis. Proceedings of the 49th annual meeting of the association for computational
linguistics: Human language technologies-volume 1 (pp. 142�150).
McCann, B., Bradbury, J., Xiong, C., & Socher, R. (2017). Learned in translation: contextualized
word vectors. Advances in Neural Information Processing Systems (pp. 6294�6305).
McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity.
The bulletin of mathematical biophysics, 5(4), 115�133.
McMahan, H. B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., � others. (2013). Ad click
prediction: a view from the trenches. Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining (pp. 1222�1230).
Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of
words and phrases and their compositionality. Advances in neural information processing systems
(pp. 3111�3119).
Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., � Dean, J. (2017). Device
placement optimization with reinforcement learning. Proceedings of the 34th International Conference
on Machine Learning-Volume 70 (pp. 2430�2439).
Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of
placing confidence in confidence intervals. Psychonomic bulletin & review, 23(1), 103�123.
Nesterov, Y., & Vial, J.-P. (2000). Confidence level solutions for stochastic programming, Stochastic Programming
E-Print Series.
Nesterov, Y. (2018). Lectures on convex optimization. Vol. 137. Springer.
Bibliography 959
Neyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of
probability. Philosophical Transactions of the Royal Society of London. Series A, Mathematical and
Physical Sciences, 236(767), 333�380.
Parikh, A. P., Tackstrom, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention model for
natural language inference. arXiv preprint arXiv:1606.01933.
Park, T., Liu, M.-Y., Wang, T.-C., & Zhu, J.-Y. (2019). Semantic image synthesis with spatiallyadaptive
normalization. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(pp. 2337�2346).
Pennington, J., Schoenholz, S., & Ganguli, S. (2017). Resurrecting the sigmoid in deep learning
through dynamical isometry: theory and practice. Advances in neural information processing systems
(pp. 4785�4795).
Pennington, J., Socher, R., & Manning, C. (2014). Glove: global vectors for word representation.
Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)
(pp. 1532�1543).
Peters, J., Janzing, D., & Scholkopf, B. (2017). Elements of causal inference: foundations and learning
algorithms. MIT press.
Peters, M., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence tagging
with bidirectional language models. Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) (pp. 1756�1765).
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep
contextualized word representations. Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers) (pp. 2227�2237).
Petersen, K. B., Pedersen, M. S., & others. (2008). The matrix cookbook. Technical University of
Denmark, 7(15), 510.
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5), 1�17.
Quadrana, M., Cremonesi, P., & Jannach, D. (2018). Sequence-aware recommender systems. ACM
Computing Surveys (CSUR), 51(4), 66.
Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434.
Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding
by generative pre-training. OpenAI.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are
unsupervised multitask learners. OpenAI Blog, 1(8), 9.
Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). Squad: 100,000+ questions for machine
comprehension of text. arXiv preprint arXiv:1606.05250.
Reddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237.
Reed, S., & De Freitas, N. (2015). Neural programmer-interpreters. arXiv preprint arXiv:1511.06279.
Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster r-cnn: towards real-time object detection with
region proposal networks. Advances in neural information processing systems (pp. 91�99).
960 Bibliography
Rendle, S. (2010). Factorization machines. 2010 IEEE International Conference on Data Mining
(pp. 995�1000).
Rendle, S., Freudenthaler, C., Gantner, Z., & Schmidt-Thieme, L. (2009). Bpr: bayesian personalized
ranking from implicit feedback. Proceedings of the twenty-fifth conference on uncertainty in
artificial intelligence (pp. 452�461).
Rumelhart, D. E., Hinton, G. E., Williams, R. J., & others. (1988). Learning representations by
back-propagating errors. Cognitive modeling, 5(3), 1.
Russell, S. J., & Norvig, P. (2016). Artificial intelligence: a modern approach. Malaysia; Pearson Education
Limited,.
Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization?
Advances in Neural Information Processing Systems (pp. 2483�2493).
Sarwar, B. M., Karypis, G., Konstan, J. A., Riedl, J., & others. (2001). Item-based collaborative filtering
recommendation algorithms. Www, 1, 285�295.
Schein, A. I., Popescul, A., Ungar, L. H., & Pennock, D. M. (2002). Methods and metrics for coldstart
recommendations. Proceedings of the 25th annual international ACM SIGIR conference on Research
and development in information retrieval (pp. 253�260).
Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11), 2673�2681.
Sedhain, S., Menon, A. K., Sanner, S., & Xie, L. (2015). Autorec: autoencoders meet collaborative
filtering. Proceedings of the 24th International Conference on World Wide Web (pp. 111�112).
Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword
units. arXiv preprint arXiv:1508.07909.
Sergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in tensorflow.
arXiv preprint arXiv:1802.05799.
Shannon, C. E. (1948 , 7). A mathematical theory of communication. The Bell System Technical Journal,
27(3), 379�423.
Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., � Abdelzaher, T. (2020). Controlvae: controllable
variational autoencoder. Proceedings of the 37th International Conference on Machine Learning.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., � others. (2016).
Mastering the game of go with deep neural networks and tree search. nature, 529(7587), 484.
Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.
Smola, A., & Narayanamurthy, S. (2010). An architecture for parallel topic models. Proceedings of
the VLDB Endowment, 3(1-2), 703�710.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a
simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research,
15(1), 1929�1958.
Strang, G. (1993). Introduction to linear algebra. Vol. 3. Wellesley-Cambridge Press Wellesley, MA.
Su, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. Advances in
artificial intelligence, 2009.
Bibliography 961
Sukhbaatar, S., Weston, J., Fergus, R., & others. (2015). End-to-end memory networks. Advances in
neural information processing systems (pp. 2440�2448).
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and
momentum in deep learning. International conference on machine learning (pp. 1139�1147).
Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, inception-resnet and the
impact of residual connections on learning. Thirty-First AAAI Conference on Artificial Intelligence.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., � Rabinovich, A. (2015). Going
deeper with convolutions. Proceedings of the IEEE conference on computer vision and pattern
recognition (pp. 1�9).
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture
for computer vision. Proceedings of the IEEE conference on computer vision and pattern
recognition (pp. 2818�2826).
Tallec, C., & Ollivier, Y. (2017). Unbiasing truncated backpropagation through time. arXiv preprint
arXiv:1705.08209.
Tang, J., & Wang, K. (2018). Personalized top-n sequential recommendation via convolutional sequence
embedding. Proceedings of the Eleventh ACM International Conference on Web Search and
Data Mining (pp. 565�573).
Teye, M., Azizpour, H., & Smith, K. (2018). Bayesian uncertainty estimation for batch normalized
deep networks. arXiv preprint arXiv:1802.06455.
Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: divide the gradient by a running average
of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26�31.
Treisman, A. M., & Gelade, G. (1980). A feature-integration theory of attention. Cognitive psychology,
12(1), 97�136.
Toscher, A., Jahrer, M., & Bell, R. M. (2009). The bigchaos solution to the netflix grand prize. Netflix
prize documentation, pp. 1�52.
Uijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective search for object
recognition. International journal of computer vision, 104(2), 154�171.
Van Loan, C. F., & Golub, G. H. (1983). Matrix computations. Johns Hopkins University Press.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., � Polosukhin, I. (2017).
Attention is all you need. Advances in neural information processing systems (pp. 5998�6008).
Wang, L., Li, M., Liberty, E., & Smola, A. J. (2018). Optimal message scheduling for aggregation.
NETWORKS, 2(3), 2�3.
Wang, Y., Davidson, A., Pan, Y., Wu, Y., Riffel, A., & Owens, J. D. (2016). Gunrock: a highperformance
graph processing library on the gpu. ACM SIGPLAN Notices (p. 11).
Warstadt, A., Singh, A., & Bowman, S. R. (2019). Neural network acceptability judgments. Transactions
of the Association for Computational Linguistics, 7, 625�641.
Wasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science &
Business Media.
Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3-4), 279�292.
Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics.
Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 681�688).
962 Bibliography
Wigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. Ann. Math
(pp. 325�327).
Wood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence memoizer.
Communications of the ACM, 54(2), 91�98.
Wu, C.-Y., Ahmed, A., Beutel, A., Smola, A. J., & Jing, H. (2017). Recurrent recommender networks.
Proceedings of the tenth ACM international conference on web search and data mining (pp. 495�503).
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., � others. (2016). Google?s
neural machine translation system: bridging the gap between human and machine translation.
arXiv preprint arXiv:1609.08144.
Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747.
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., & Pennington, J. (2018). Dynamical isometry
and a mean field theory of cnns: how to train 10,000-layer vanilla convolutional neural
networks. International Conference on Machine Learning (pp. 5393�5402).
Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., & Stolcke, A. (2018). The microsoft 2017 conversational
speech recognition system. 2018 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) (pp. 5934�5938).
Ye, M., Yin, P., Lee, W.-C., & Lee, D.-L. (2011). Exploiting geographical influence for collaborative
point-of-interest recommendation. Proceedings of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval (pp. 325�334).
You, Y., Gitman, I., & Ginsburg, B. (2017). Large batch training of convolutional networks. arXiv
preprint arXiv:1708.03888.
Zaheer, M., Reddi, S., Sachan, D., Kale, S., & Kumar, S. (2018). Adaptive methods for nonconvex
optimization. Advances in Neural Information Processing Systems (pp. 9793�9803).
Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.
Zhang, S., Yao, L., Sun, A., & Tay, Y. (2019). Deep learning based recommender system: a survey
and new perspectives. ACM Computing Surveys (CSUR), 52(1), 5.
Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using
cycle-consistent adversarial networks. Proceedings of the IEEE international conference on computer
vision (pp. 2223�2232).
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S. (2015). Aligning
books and movies: towards story-like visual explanations by watching movies and reading
books. Proceedings of the IEEE international conference on computer vision (pp. 19�27).
Bibliography 963
964 Bibliography
Python Module Index
d
d2l.mxnet, 935
965
966 Python Module Index
Index
A
abs() (in module d2l.mxnet), 939
Accumulator (class in d2l.mxnet), 935
accuracy() (in module d2l.mxnet), 939
AddNorm (class in d2l.mxnet), 935
Animator (class in d2l.mxnet), 935
arange() (in module d2l.mxnet), 939
as_integer_ratio() (d2l.mxnet.float32 method),
943
avg() (d2l.mxnet.Timer method), 938
B
bbox_to_rect() (in module d2l.mxnet), 940
BERTEncoder (class in d2l.mxnet), 935
BERTModel (class in d2l.mxnet), 935
BPRLoss (class in d2l.mxnet), 935
build_colormap2label() (in module d2l.mxnet),
940
C
concat() (in module d2l.mxnet), 940
copy() (d2l.mxnet.defaultdict method), 942
copyfile() (in module d2l.mxnet), 941
corr2d() (in module d2l.mxnet), 941
cos() (in module d2l.mxnet), 941
cosh() (in module d2l.mxnet), 941
CTRDataset (class in d2l.mxnet), 936
cumsum() (d2l.mxnet.Timer method), 938
D
d2l.mxnet
module, 935
Decoder (class in d2l.mxnet), 936
default_factory (d2l.mxnet.defaultdict attribute),
942
defaultdict (class in d2l.mxnet), 942
DotProductAttention (class in d2l.mxnet), 936
download() (in module d2l.mxnet), 942
download_all() (in module d2l.mxnet), 942
download_extract() (in module d2l.mxnet), 942
E
Encoder (class in d2l.mxnet), 936
EncoderBlock (class in d2l.mxnet), 936
EncoderDecoder (class in d2l.mxnet), 936
evaluate_accuracy() (in module d2l.mxnet), 942
evaluate_accuracy_gpu() (in module
d2l.mxnet), 942
evaluate_loss() (in module d2l.mxnet), 942
exp() (in module d2l.mxnet), 942
F
filter() (d2l.mxnet.VOCSegDataset method), 939
float32 (class in d2l.mxnet), 943
forward() (d2l.mxnet.AddNorm method), 935
forward() (d2l.mxnet.BERTEncoder method), 935
forward() (d2l.mxnet.BERTModel method), 935
forward() (d2l.mxnet.BPRLoss method), 935
forward() (d2l.mxnet.Decoder method), 936
forward() (d2l.mxnet.DotProductAttention
method), 936
forward() (d2l.mxnet.Encoder method), 936
forward() (d2l.mxnet.EncoderBlock method), 936
forward() (d2l.mxnet.EncoderDecoder method),
936
forward() (d2l.mxnet.HingeLossbRec method),
936
forward() (d2l.mxnet.MaskedSoftmaxCELoss
method), 937
forward() (d2l.mxnet.MaskLM method), 937
forward() (d2l.mxnet.MLPAttention method), 936
forward() (d2l.mxnet.MultiHeadAttention
method), 937
forward() (d2l.mxnet.NextSentencePred method),
937
forward() (d2l.mxnet.PositionalEncoding
method), 937
forward() (d2l.mxnet.PositionWiseFFN method),
937
forward() (d2l.mxnet.Residual method), 938
forward() (d2l.mxnet.RNNModel method), 937
forward() (d2l.mxnet.Seq2SeqDecoder method),
938
forward() (d2l.mxnet.Seq2SeqEncoder method),
938
967
forward() (d2l.mxnet.TransformerEncoder
method), 939
G
get_dataloader_workers() (in module
d2l.mxnet), 943
get_fashion_mnist_labels() (in module
d2l.mxnet), 943
H
HingeLossbRec (class in d2l.mxnet), 936
I
int32 (class in d2l.mxnet), 943
L
linreg() (in module d2l.mxnet), 943
linspace() (in module d2l.mxnet), 943
load_array() (in module d2l.mxnet), 944
load_data_bananas() (in module d2l.mxnet), 944
load_data_fashion_mnist() (in module
d2l.mxnet), 944
load_data_snli() (in module d2l.mxnet), 944
load_data_voc() (in module d2l.mxnet), 944
log() (in module d2l.mxnet), 945
M
masked_softmax() (in module d2l.mxnet), 945
MaskedSoftmaxCELoss (class in d2l.mxnet), 937
MaskLM (class in d2l.mxnet), 937
matmul() (in module d2l.mxnet), 945
meshgrid() (in module d2l.mxnet), 946
mkdir_if_not_exist() (in module d2l.mxnet),
947
MLPAttention (class in d2l.mxnet), 936
module
d2l.mxnet, 935
MultiHeadAttention (class in d2l.mxnet), 937
N
NextSentencePred (class in d2l.mxnet), 937
normal() (in module d2l.mxnet), 947
O
ones() (in module d2l.mxnet), 948
P
plot() (in module d2l.mxnet), 948
PositionalEncoding (class in d2l.mxnet), 937
PositionWiseFFN (class in d2l.mxnet), 937
predict_ch3() (in module d2l.mxnet), 948
R
RandomGenerator (class in d2l.mxnet), 938
read_csv_labels() (in module d2l.mxnet), 948
read_snli() (in module d2l.mxnet), 948
read_time_machine() (in module d2l.mxnet), 948
read_voc_images() (in module d2l.mxnet), 949
Residual (class in d2l.mxnet), 938
resnet18() (in module d2l.mxnet), 949
RNNModel (class in d2l.mxnet), 937
RNNModelScratch (class in d2l.mxnet), 938
S
Seq2SeqDecoder (class in d2l.mxnet), 938
Seq2SeqEncoder (class in d2l.mxnet), 938
SeqDataLoader (class in d2l.mxnet), 938
set_axes() (in module d2l.mxnet), 949
set_figsize() (in module d2l.mxnet), 949
sgd() (in module d2l.mxnet), 949
show_bboxes() (in module d2l.mxnet), 949
show_images() (in module d2l.mxnet), 949
show_trace_2d() (in module d2l.mxnet), 949
sin() (in module d2l.mxnet), 949
sinh() (in module d2l.mxnet), 949
SNLIDataset (class in d2l.mxnet), 938
split_batch() (in module d2l.mxnet), 950
split_batch_multi_inputs() (in module
d2l.mxnet), 950
split_data_ml100k() (in module d2l.mxnet), 950
squared_loss() (in module d2l.mxnet), 950
stack() (in module d2l.mxnet), 950
start() (d2l.mxnet.Timer method), 938
stop() (d2l.mxnet.Timer method), 938
sum() (d2l.mxnet.Timer method), 938
synthetic_data() (in module d2l.mxnet), 951
T
tanh() (in module d2l.mxnet), 951
tensor() (in module d2l.mxnet), 951
Timer (class in d2l.mxnet), 938
TokenEmbedding (class in d2l.mxnet), 938
tokenize() (in module d2l.mxnet), 952
train_2d() (in module d2l.mxnet), 952
train_ch3() (in module d2l.mxnet), 952
train_ch6() (in module d2l.mxnet), 952
train_epoch_ch3() (in module d2l.mxnet), 952
TransformerEncoder (class in d2l.mxnet), 939
try_all_gpus() (in module d2l.mxnet), 952
try_gpu() (in module d2l.mxnet), 952
U
update_D() (in module d2l.mxnet), 952
968 Index
update_G() (in module d2l.mxnet), 952
use_svg_display() (in module d2l.mxnet), 952
V
voc_label_indices() (in module d2l.mxnet), 952
voc_rand_crop() (in module d2l.mxnet), 952
VOCSegDataset (class in d2l.mxnet), 939
Z
zeros() (in module d2l.mxnet), 952
Index 969
if seed is not None:
random.seed(seed) # get reproducible results
698 | Appendix A: Support Scripts
taxis = {i: taxi_process(i, (i+1)*2, i*DEPARTURE_INTERVAL)
for i in range(num_taxis)}
sim = Simulator(taxis)
sim.run(end_time)
if __name__ == '__main__':
parser = argparse.ArgumentParser(
description='Taxi fleet simulator.')
parser.add_argument('-e', '--end-time', type=int,
default=DEFAULT_END_TIME,
help='simulation end time; default = %s'
% DEFAULT_END_TIME)
parser.add_argument('-t', '--taxis', type=int,
default=DEFAULT_NUMBER_OF_TAXIS,
help='number of taxis running; default = %s'
% DEFAULT_NUMBER_OF_TAXIS)
parser.add_argument('-s', '--seed', type=int, default=None,
help='random generator seed (for testing)')
args = parser.parse_args()
main(args.end_time, args.taxis, args.seed)
"""
Sample run from the command line, seed=3, maximum elapsed time=120::
# BEGIN TAXI_SAMPLE_RUN
$ python3 taxi_sim.py -s 3 -e 120
taxi: 0 Event(time=0, proc=0, action='leave garage')
taxi: 0 Event(time=2, proc=0, action='pick up passenger')
taxi: 1 Event(time=5, proc=1, action='leave garage')
taxi: 1 Event(time=8, proc=1, action='pick up passenger')
taxi: 2 Event(time=10, proc=2, action='leave garage')
taxi: 2 Event(time=15, proc=2, action='pick up passenger')
taxi: 2 Event(time=17, proc=2, action='drop off passenger')
taxi: 0 Event(time=18, proc=0, action='drop off passenger')
taxi: 2 Event(time=18, proc=2, action='pick up passenger')
taxi: 2 Event(time=25, proc=2, action='drop off passenger')
taxi: 1 Event(time=27, proc=1, action='drop off passenger')
taxi: 2 Event(time=27, proc=2, action='pick up passenger')
taxi: 0 Event(time=28, proc=0, action='pick up passenger')
taxi: 2 Event(time=40, proc=2, action='drop off passenger')
taxi: 2 Event(time=44, proc=2, action='pick up passenger')
taxi: 1 Event(time=55, proc=1, action='pick up passenger')
taxi: 1 Event(time=59, proc=1, action='drop off passenger')
taxi: 0 Event(time=65, proc=0, action='drop off passenger')
taxi: 1 Event(time=65, proc=1, action='pick up passenger')
taxi: 2 Event(time=65, proc=2, action='drop off passenger')
Chapter 16: Taxi Fleet Discrete Event Simulation | 699
taxi: 2 Event(time=72, proc=2, action='pick up passenger')
taxi: 0 Event(time=76, proc=0, action='going home')
taxi: 1 Event(time=80, proc=1, action='drop off passenger')
taxi: 1 Event(time=88, proc=1, action='pick up passenger')
taxi: 2 Event(time=95, proc=2, action='drop off passenger')
taxi: 2 Event(time=97, proc=2, action='pick up passenger')
taxi: 2 Event(time=98, proc=2, action='drop off passenger')
taxi: 1 Event(time=106, proc=1, action='drop off passenger')
taxi: 2 Event(time=109, proc=2, action='going home')
taxi: 1 Event(time=110, proc=1, action='going home')
*** end of events ***
# END TAXI_SAMPLE_RUN
"""
Chapter 17: Cryptographic Examples
These scripts were used to show the use of futures.ProcessPoolExecutor to run CPUintensive
tasks.
Example A-7 encrypts and decrypts random byte arrays with the RC4 algorithm. It
depends on the arcfour.py module (Example A-8) to run.
Example A-7. arcfour_futures.py: futures.ProcessPoolExecutor example
import sys
import time
from concurrent import futures
from random import randrange
from arcfour import arcfour
JOBS = 12
SIZE = 2**18
KEY = b"'Twas brillig, and the slithy toves\nDid gyre"
STATUS = '{} workers, elapsed time: {:.2f}s'
def arcfour_test(size, key):
in_text = bytearray(randrange(256) for i in range(size))
cypher_text = arcfour(key, in_text)
out_text = arcfour(key, cypher_text)
assert in_text == out_text, 'Failed arcfour_test'
return size
def main(workers=None):
if workers:
workers = int(workers)
t0 = time.time()
with futures.ProcessPoolExecutor(workers) as executor:
700 | Appendix A: Support Scripts
actual_workers = executor._max_workers
to_do = []
for i in range(JOBS, 0, -1):
size = SIZE + int(SIZE / JOBS * (i - JOBS/2))
job = executor.submit(arcfour_test, size, KEY)
to_do.append(job)
for future in futures.as_completed(to_do):
res = future.result()
print('{:.1f} KB'.format(res/2**10))
print(STATUS.format(actual_workers, time.time() - t0))
if __name__ == '__main__':
if len(sys.argv) == 2:
workers = int(sys.argv[1])
else:
workers = None
main(workers)
Example A-8 implements the RC4 encryption algorithm in pure Python.
Example A-8. arcfour.py: RC4 compatible algorithm
"""RC4 compatible algorithm"""
def arcfour(key, in_bytes, loops=20):
kbox = bytearray(256) # create key box
for i, car in enumerate(key): # copy key and vector
kbox[i] = car
j = len(key)
for i in range(j, 256): # repeat until full
kbox[i] = kbox[i-j]
# [1] initialize sbox
sbox = bytearray(range(256))
# repeat sbox mixing loop, as recommened in CipherSaber-2
# http://ciphersaber.gurus.com/faq.html#cs2
j = 0
for k in range(loops):
for i in range(256):
j = (j + sbox[i] + kbox[i]) % 256
sbox[i], sbox[j] = sbox[j], sbox[i]
# main loop
i = 0
j = 0
out_bytes = bytearray()
for car in in_bytes:
i = (i + 1) % 256
Chapter 17: Cryptographic Examples | 701
# [2] shuffle sbox
j = (j + sbox[i]) % 256
sbox[i], sbox[j] = sbox[j], sbox[i]
# [3] compute t
t = (sbox[i] + sbox[j]) % 256
k = sbox[t]
car = car ^ k
out_bytes.append(car)
return out_bytes
def test():
from time import time
clear = bytearray(b'1234567890' * 100000)
t0 = time()
cipher = arcfour(b'key', clear)
print('elapsed time: %.2fs' % (time() - t0))
result = arcfour(b'key', cipher)
assert result == clear, '%r != %r' % (result, clear)
print('elapsed time: %.2fs' % (time() - t0))
print('OK')
if __name__ == '__main__':
test()
Example A-9 applies the SHA-256 hash algorithm to random byte arrays. It uses hash
lib from the standard library, which in turn uses the OpenSSL library written in C.
Example A-9. sha_futures.py: futures.ProcessPoolExecutor example
import sys
import time
import hashlib
from concurrent import futures
from random import randrange
JOBS = 12
SIZE = 2**20
STATUS = '{} workers, elapsed time: {:.2f}s'
def sha(size):
data = bytearray(randrange(256) for i in range(size))
algo = hashlib.new('sha256')
algo.update(data)
return algo.hexdigest()
def main(workers=None):
if workers:
702 | Appendix A: Support Scripts
workers = int(workers)
t0 = time.time()
with futures.ProcessPoolExecutor(workers) as executor:
actual_workers = executor._max_workers
to_do = (executor.submit(sha, SIZE) for i in range(JOBS))
for future in futures.as_completed(to_do):
res = future.result()
print(res)
print(STATUS.format(actual_workers, time.time() - t0))
if __name__ == '__main__':
if len(sys.argv) == 2:
workers = int(sys.argv[1])
else:
workers = None
main(workers)
Chapter 17: flags2 HTTP Client Examples
All flags2 examples from �Downloads with Progress Display and Error Handling� on
page 520 use functions from the flags2_common.py module (Example A-10).
Example A-10. flags2_common.py
"""Utilities for second set of flag examples.
"""
import os
import time
import sys
import string
import argparse
from collections import namedtuple
from enum import Enum
Result = namedtuple('Result', 'status data')
HTTPStatus = Enum('Status', 'ok not_found error')
POP20_CC = ('CN IN US ID BR PK NG BD RU JP '
'MX PH VN ET EG DE IR TR CD FR').split()
DEFAULT_CONCUR_REQ = 1
MAX_CONCUR_REQ = 1
SERVERS = {
'REMOTE': 'http://flupy.org/data/flags',
'LOCAL': 'http://localhost:8001/flags',
'DELAY': 'http://localhost:8002/flags',
Chapter 17: flags2 HTTP Client Examples | 703
'ERROR': 'http://localhost:8003/flags',
}
DEFAULT_SERVER = 'LOCAL'
DEST_DIR = 'downloads/'
COUNTRY_CODES_FILE = 'country_codes.txt'
def save_flag(img, filename):
path = os.path.join(DEST_DIR, filename)
with open(path, 'wb') as fp:
fp.write(img)
def initial_report(cc_list, actual_req, server_label):
if len(cc_list) <= 10:
cc_msg = ', '.join(cc_list)
else:
cc_msg = 'from {} to {}'.format(cc_list[0], cc_list[-1])
print('{} site: {}'.format(server_label, SERVERS[server_label]))
msg = 'Searching for {} flag{}: {}'
plural = 's' if len(cc_list) != 1 else ''
print(msg.format(len(cc_list), plural, cc_msg))
plural = 's' if actual_req != 1 else ''
msg = '{} concurrent connection{} will be used.'
print(msg.format(actual_req, plural))
def final_report(cc_list, counter, start_time):
elapsed = time.time() - start_time
print('-' * 20)
msg = '{} flag{} downloaded.'
plural = 's' if counter[HTTPStatus.ok] != 1 else ''
print(msg.format(counter[HTTPStatus.ok], plural))
if counter[HTTPStatus.not_found]:
print(counter[HTTPStatus.not_found], 'not found.')
if counter[HTTPStatus.error]:
plural = 's' if counter[HTTPStatus.error] != 1 else ''
print('{} error{}.'.format(counter[HTTPStatus.error], plural))
print('Elapsed time: {:.2f}s'.format(elapsed))
def expand_cc_args(every_cc, all_cc, cc_args, limit):
codes = set()
A_Z = string.ascii_uppercase
if every_cc:
codes.update(a+b for a in A_Z for b in A_Z)
elif all_cc:
with open(COUNTRY_CODES_FILE) as fp:
text = fp.read()
codes.update(text.split())
else:
704 | Appendix A: Support Scripts
for cc in (c.upper() for c in cc_args):
if len(cc) == 1 and cc in A_Z:
codes.update(cc+c for c in A_Z)
elif len(cc) == 2 and all(c in A_Z for c in cc):
codes.add(cc)
else:
msg = 'each CC argument must be A to Z or AA to ZZ.'
raise ValueError('*** Usage error: '+msg)
return sorted(codes)[:limit]
def process_args(default_concur_req):
server_options = ', '.join(sorted(SERVERS))
parser = argparse.ArgumentParser(
description='Download flags for country codes. '
'Default: top 20 countries by population.')
parser.add_argument('cc', metavar='CC', nargs='*',
help='country code or 1st letter (eg. B for BA...BZ)')
parser.add_argument('-a', '--all', action='store_true',
help='get all available flags (AD to ZW)')
parser.add_argument('-e', '--every', action='store_true',
help='get flags for every possible code (AA...ZZ)')
parser.add_argument('-l', '--limit', metavar='N', type=int,
help='limit to N first codes', default=sys.maxsize)
parser.add_argument('-m', '--max_req', metavar='CONCURRENT', type=int,
default=default_concur_req,
help='maximum concurrent requests (default={})'
.format(default_concur_req))
parser.add_argument('-s', '--server', metavar='LABEL',
default=DEFAULT_SERVER,
help='Server to hit; one of {} (default={})'
.format(server_options, DEFAULT_SERVER))
parser.add_argument('-v', '--verbose', action='store_true',
help='output detailed progress info')
args = parser.parse_args()
if args.max_req < 1:
print('*** Usage error: --max_req CONCURRENT must be >= 1')
parser.print_usage()
sys.exit(1)
if args.limit < 1:
print('*** Usage error: --limit N must be >= 1')
parser.print_usage()
sys.exit(1)
args.server = args.server.upper()
if args.server not in SERVERS:
print('*** Usage error: --server LABEL must be one of',
server_options)
parser.print_usage()
sys.exit(1)
try:
cc_list = expand_cc_args(args.every, args.all, args.cc, args.limit)
except ValueError as exc:
Chapter 17: flags2 HTTP Client Examples | 705
print(exc.args[0])
parser.print_usage()
sys.exit(1)
if not cc_list:
cc_list = sorted(POP20_CC)
return args, cc_list
def main(download_many, default_concur_req, max_concur_req):
args, cc_list = process_args(default_concur_req)
actual_req = min(args.max_req, max_concur_req, len(cc_list))
initial_report(cc_list, actual_req, args.server)
base_url = SERVERS[args.server]
t0 = time.time()
counter = download_many(cc_list, base_url, args.verbose, actual_req)
assert sum(counter.values()) == len(cc_list), \
'some downloads are unaccounted for'
final_report(cc_list, counter, t0)
The flags2_sequential.py script (Example A-11) is the baseline for comparison with the
concurrent implementations. flags2_threadpool.py (Example 17-14) also uses the
get_flag and download_one functions from flags2_sequential.py.
Example A-11. flags2_sequential.py
"""Download flags of countries (with error handling).
Sequential version
Sample run::
$ python3 flags2_sequential.py -s DELAY b
DELAY site: http://localhost:8002/flags
Searching for 26 flags: from BA to BZ
1 concurrent connection will be used.
--------------------
17 flags downloaded.
9 not found.
Elapsed time: 13.36s
"""
import collections
import requests
import tqdm
from flags2_common import main, save_flag, HTTPStatus, Result
DEFAULT_CONCUR_REQ = 1
706 | Appendix A: Support Scripts
MAX_CONCUR_REQ = 1
# BEGIN FLAGS2_BASIC_HTTP_FUNCTIONS
def get_flag(base_url, cc):
url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower())
resp = requests.get(url)
if resp.status_code != 200:
resp.raise_for_status()
return resp.content
def download_one(cc, base_url, verbose=False):
try:
image = get_flag(base_url, cc)
except requests.exceptions.HTTPError as exc:
res = exc.response
if res.status_code == 404:
status = HTTPStatus.not_found
msg = 'not found'
else:
raise
else:
save_flag(image, cc.lower() + '.gif')
status = HTTPStatus.ok
msg = 'OK'
if verbose:
print(cc, msg)
return Result(status, cc)
# END FLAGS2_BASIC_HTTP_FUNCTIONS
# BEGIN FLAGS2_DOWNLOAD_MANY_SEQUENTIAL
def download_many(cc_list, base_url, verbose, max_req):
counter = collections.Counter()
cc_iter = sorted(cc_list)
if not verbose:
cc_iter = tqdm.tqdm(cc_iter)
for cc in cc_iter:
try:
res = download_one(cc, base_url, verbose)
except requests.exceptions.HTTPError as exc:
error_msg = 'HTTP error {res.status_code} - {res.reason}'
error_msg = error_msg.format(res=exc.response)
except requests.exceptions.ConnectionError as exc:
error_msg = 'Connection error'
else:
error_msg = ''
status = res.status
if error_msg:
status = HTTPStatus.error
Chapter 17: flags2 HTTP Client Examples | 707
counter[status] += 1
if verbose and error_msg:
print('*** Error for {}: {}'.format(cc, error_msg))
return counter
# END FLAGS2_DOWNLOAD_MANY_SEQUENTIAL
if __name__ == '__main__':
main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)
Chapter 19: OSCON Schedule Scripts and Tests
Example A-12 is the test script for the schedule1.py module (Example 19-9). It uses the
py.test library and test runner.
Example A-12. test_schedule1.py
import shelve
import pytest
import schedule1 as schedule
@pytest.yield_fixture
def db():
with shelve.open(schedule.DB_NAME) as the_db:
if schedule.CONFERENCE not in the_db:
schedule.load_db(the_db)
yield the_db
def test_record_class():
rec = schedule.Record(spam=99, eggs=12)
assert rec.spam == 99
assert rec.eggs == 12
def test_conference_record(db):
assert schedule.CONFERENCE in db
def test_speaker_record(db):
speaker = db['speaker.3471']
assert speaker.name == 'Anna Martelli Ravenscroft'
def test_event_record(db):
event = db['event.33950']
assert event.name == 'There *Will* Be Bugs'
def test_event_venue(db):
708 | Appendix A: Support Scripts
event = db['event.33950']
assert event.venue_serial == 1449
Example A-13 is the full listing of the schedule2.py example presented in �Linked Record
Retrieval with Properties� on page 598 in four parts.
Example A-13. schedule2.py
"""
schedule2.py: traversing OSCON schedule data
>>> import shelve
>>> db = shelve.open(DB_NAME)
>>> if CONFERENCE not in db: load_db(db)
# BEGIN SCHEDULE2_DEMO
>>> DbRecord.set_db(db)
>>> event = DbRecord.fetch('event.33950')
>>> event
<Event 'There *Will* Be Bugs'>
>>> event.venue
<DbRecord serial='venue.1449'>
>>> event.venue.name
'Portland 251'
>>> for spkr in event.speakers:
... print('{0.serial}: {0.name}'.format(spkr))
...
speaker.3471: Anna Martelli Ravenscroft
speaker.5199: Alex Martelli
# END SCHEDULE2_DEMO
>>> db.close()
"""
# BEGIN SCHEDULE2_RECORD
import warnings
import inspect
import osconfeed
DB_NAME = 'data/schedule2_db'
CONFERENCE = 'conference.115'
class Record:
def __init__(self, **kwargs):
self.__dict__.update(kwargs)
def __eq__(self, other):
Chapter 19: OSCON Schedule Scripts and Tests | 709
if isinstance(other, Record):
return self.__dict__ == other.__dict__
else:
return NotImplemented
# END SCHEDULE2_RECORD
# BEGIN SCHEDULE2_DBRECORD
class MissingDatabaseError(RuntimeError):
"""Raised when a database is required but was not set."""
class DbRecord(Record):
__db = None
@staticmethod
def set_db(db):
DbRecord.__db = db
@staticmethod
def get_db():
return DbRecord.__db
@classmethod
def fetch(cls, ident):
db = cls.get_db()
try:
return db[ident]
except TypeError:
if db is None:
msg = "database not set; call '{}.set_db(my_db)'"
raise MissingDatabaseError(msg.format(cls.__name__))
else: #
raise
def __repr__(self):
if hasattr(self, 'serial'):
cls_name = self.__class__.__name__
return '<{} serial={!r}>'.format(cls_name, self.serial)
else:
return super().__repr__()
# END SCHEDULE2_DBRECORD
# BEGIN SCHEDULE2_EVENT
class Event(DbRecord):
@property
def venue(self):
key = 'venue.{}'.format(self.venue_serial)
return self.__class__.fetch(key)
710 | Appendix A: Support Scripts
@property
def speakers(self):
if not hasattr(self, '_speaker_objs'):
spkr_serials = self.__dict__['speakers']
fetch = self.__class__.fetch
self._speaker_objs = [fetch('speaker.{}'.format(key))
for key in spkr_serials]
return self._speaker_objs
def __repr__(self):
if hasattr(self, 'name'):
cls_name = self.__class__.__name__
return '<{} {!r}>'.format(cls_name, self.name)
else:
return super().__repr__()
# END SCHEDULE2_EVENT
# BEGIN SCHEDULE2_LOAD
def load_db(db):
raw_data = osconfeed.load()
warnings.warn('loading ' + DB_NAME)
for collection, rec_list in raw_data['Schedule'].items():
record_type = collection[:-1]
cls_name = record_type.capitalize()
cls = globals().get(cls_name, DbRecord)
if inspect.isclass(cls) and issubclass(cls, DbRecord):
factory = cls
else:
factory = DbRecord
for record in rec_list:
key = '{}.{}'.format(record_type, record['serial'])
record['serial'] = key
db[key] = factory(**record)
# END SCHEDULE2_LOAD
Example A-14 was used to test Example A-13 with py.test.
Example A-14. test_schedule2.py
import shelve
import pytest
import schedule2 as schedule
@pytest.yield_fixture
def db():
with shelve.open(schedule.DB_NAME) as the_db:
if schedule.CONFERENCE not in the_db:
schedule.load_db(the_db)
yield the_db
Chapter 19: OSCON Schedule Scripts and Tests | 711
def test_record_attr_access():
rec = schedule.Record(spam=99, eggs=12)
assert rec.spam == 99
assert rec.eggs == 12
def test_record_repr():
rec = schedule.DbRecord(spam=99, eggs=12)
assert 'DbRecord object at 0x' in repr(rec)
rec2 = schedule.DbRecord(serial=13)
assert repr(rec2) == "<DbRecord serial=13>"
def test_conference_record(db):
assert schedule.CONFERENCE in db
def test_speaker_record(db):
speaker = db['speaker.3471']
assert speaker.name == 'Anna Martelli Ravenscroft'
def test_missing_db_exception():
with pytest.raises(schedule.MissingDatabaseError):
schedule.DbRecord.fetch('venue.1585')
def test_dbrecord(db):
schedule.DbRecord.set_db(db)
venue = schedule.DbRecord.fetch('venue.1585')
assert venue.name == 'Exhibit Hall B'
def test_event_record(db):
event = db['event.33950']
assert repr(event) == "<Event 'There *Will* Be Bugs'>"
def test_event_venue(db):
schedule.Event.set_db(db)
event = db['event.33950']
assert event.venue_serial == 1449
assert event.venue == db['venue.1449']
assert event.venue.name == 'Portland 251'
def test_event_speakers(db):
schedule.Event.set_db(db)
event = db['event.33950']
assert len(event.speakers) == 2
712 | Appendix A: Support Scripts
anna_and_alex = [db['speaker.3471'], db['speaker.5199']]
assert event.speakers == anna_and_alex
def test_event_no_speakers(db):
schedule.Event.set_db(db)
event = db['event.36848']
assert len(event.speakers) == 0
Chapter 19: OSCON Schedule Scripts and Tests | 713

Python Jargon
Many terms here are not exclusive to Python, of course, but particularly in the definitions
you may find meanings that are specific to the Python community.
Also see the official Python glossary.
ABC (programming language)
A programming language created by Leo
Geurts, Lambert Meertens, and Steven
Pemberton. Guido van Rossum, who developed
Python, worked as a programmer
implementing the ABC environment in the
1980s. Block structuring by indentation,
built-in tuples and dictionaries, tuple unpacking,
the semantics of the for loop, and
uniform handling of all sequence types are
some of the distinctive characteristics of
Python that came from ABC.
Abstract base class (ABC)
A class that cannot be instantiated, only
subclassed. ABCs are how interfaces are
formalized in Python. Instead of inheriting
from an ABC, a class may also declare that
it fulfills the interface by registering with
the ABC to become a virtual subclass.
accessor
A method implemented to provide access
to a single data attribute. Some authors use
acessor as a generic term encompassing getter
and setter methods, others use it to refer
only to getters, referring to setters as mutators.
aliasing
Assigning two or more names to the same
object. For example, in a = []; b = a the
variables a and b are aliases for the same list
object. Aliasing happens naturally all the
time in any language where variables store
references to objects. To avoid confusion,
just forget the idea that variables are boxes
that hold objects (an object can�t be in two
boxes at the same time). It�s better to think
of them as labels attached to objects (an object
can have more than one label).
argument
An expression passed to a function when it
is called. In Pythonic parlance, argument
and parameter are almost always synonyms.
See parameter for more about the
distinction and usage of these terms.
attribute Methods and data attributes (i.e., �fields� in
Java terms) are all known as attributes in
Python. A method is just an attribute that
happens to be a callable object (usually a
function, but not necessarily).
715
BDFL
Benevolent Dictator For Life, alias for Guido
van Rossum, creator of the Python language.
binary sequence
Generic term for sequence types with byte
elements. The built-in binary sequence
types are byte, bytearray, and memory
view.
BOM
Byte Order Mark, a sequence of bytes that
may be present at the start of a UTF-16 encoded
file. A BOM is the character U+FEFF
(ZERO WIDTH NO-BREAK SPACE) encoded to
produce either b'\xfe\xff' on a bigendian
CPU, or b'\xff\xfe' on a littleendian
one. Because there is no U+FFFE
characer in Unicode, the presence of these
bytes unambiguously reveals the byte ordering
used in the encoding. Although redundant,
a BOM encoded as b'\xef\xbb
\xbf' may be found in UTF-8 files.
bound method
A method that is accessed through an instance
becomes bound to that instance. Any
method is actually a descriptor and when
accessed, it returns itself wrapped in an object
that binds the method to the instance.
That object is the bound method. It can be
invoked without passing the value of self.
For example, given the assignment my_meth
od = my_obj.method, the bound method
can later be called as my_method(). Contrast
with unbound method.
built-in function (BIF)
A function bundled with the Python interpreter,
coded in the underlying implementation
language (i.e., C for CPython; Java for
Jython, and so on). The term often refers
only to the functions that don�t need to be
imported, documented in Chapter 2,
�Built-in Functions,� of The Python Standard
Library Reference. But built-in modules
like sys, math, re, etc. also contain
built-in functions.
byte string
An unfortunate name still used to refer to
bytes or bytearray in Python 3. In Python
2, the str type was really a byte string, and
the term made sense to distinguish str
from unicode strings. In Python 3, it makes
no sense to insist on this term, and I tried
to use byte sequence whenever I needed to
talk in general about�byte sequences.
bytes-like object
A generic sequence of bytes. The most common
bytes-like types are bytes, bytear
ray, and memoryview but other objects supporting
the low-level CPython buffer protocol
also qualify, if their elements are single
bytes.
callable object
An object that can be invoked with the call
operator (), to return a result or to perform
some action. There are seven flavors of callable
objects in Python: user-defined functions,
built-in functions, built-in methods,
instance methods, generator functions,
classes, and instances of classes that implement
the __call__ special method.
CamelCase
The convention of writing identifiers by
joining words with uppercased initials (e.g.,
ConnectionRefusedError). PEP-8 recommends
class names should be written in
CamelCase, but the advice is not followed
by the Python standard library. See
snake_case.
Cheese Shop
Original name of the Python Package Index
(PyPI), after the Monty Python skit
about a cheese shop where nothing is available.
As of this writing, the alias https://
cheeseshop.python.org still works. See PyPI.
class
A program construct defining a new type,
with data attributes and methods specifying
possible operations on them. See type.
code point
An integer in the range 0 to 0x10FFFF used
to identify an entry in the Unicode charac?
BDFL
716 | Python Jargon
ter database. As of Unicode 7.0, less than 3%
of all code points are assigned to characters.
In the Python documentation, the term
may be spelled as one or two words. For example,
in Chapter 2, �Built-in Functions,�
of the Python Library Reference, the chr
function is said to take an integer �codepoint,�
while its inverse, ord, is described as
returning a �Unicode code point.�
code smell
A coding pattern that suggests there may be
something wrong with the design of a program.
For example, excessive use of isin
stance checks against concrete classes is a
code smell, as it makes the program harder
to extend to deal with new types in the future.
codec
(encoder/decoder) A module with functions
to encode and decode, usually from
str to bytes and back, although Python has
a few codecs that perform bytes to bytes
and str to str transformations.
collection
Generic term for data structures made of
items that can be accessed individually.
Some collections can contain objects of arbitrary
types (see container) and others only
objects of a single atomic type (see flat
sequence). list and bytes are both collections,
but list is a container, and bytes is
a flat sequence.
considered harmful
Edsger Dijkstra�s letter titled �Go To Statement
Considered Harmful� established a
formula for titles of essays criticizing some
computer science technique. Wikipedia�s
�Considered harmful� article lists several
examples, including "Considered Harmful
Essays Considered Harmful� by Eric A.
Meyer.
constructor
Informally, the __init__ instance method
of a class is called its constructor, because
its semantics is similar to that of a Java constructor.
However, a fitting name for __in
it__ is initializer, as it does not actually
build the instance, but receives it as its self
argument. The constructor term better describes
the __new__ class method, which
Python calls before __init__, and is responsible
for actually creating an instance
and returning it. See initializer.
container
An object that holds references to other objects.
Most collection types in Python are
containers, but some are not. Contrast with
flat sequence, which are collections but not
containers.
context manager
An object implementing both the __en
ter__ and __exit__ special methods, for
use in a with block.
coroutine
A generator used for concurrent programming
by receiving values from a scheduler
or an event loop via coro.send(value).
The term may be used to describe the generator
function or the generator object obtained
by calling the generator function. See
generator.
CPython
The standard Python interpreter, implemented
in C. This term is only used when
discussing implementation-specific behavior,
or when talking about the multiple
Python interpreters available, such as PyPy.
CRUD
Acronym for Create, Read, Update, and Delete,
the four basic functions in any application
that stores records.
decorator
A callable object A that returns another callable
object B and is invoked in code using
the syntax @A right before the definition of
a callable C. When reading such code, the
Python interpreter invokes A(C) and binds
the resulting B to the variable previously assigned
to C, effectively replacing the definition
of C with B. If the target callable C is a
code smell
Python Jargon | 717
function, then A is a function decorator; if
C is a class, then A is a class decorator.
deep copy
A copy of an object in which all the objects
that are attributes of the object are themselves
also copied. Contrast with shallow
copy.
descriptor
A class implementing one or more of the
__get__, __set__, or __delete__ special
methods becomes a descriptor when one of
its instances is used as a class attribute of
another class, the managed class. Descriptors
manage the access and deletion of
managed attributes in the managed class,
often storing data in the managed instances.
docstring
Short for documentation string. When the
first statement in a module, class, or function
is a string literal, it is taken to be the
docstring for the enclosing object, and the
interpreter saves it as the __doc__ attribute
of that object. See also doctest.
doctest
A module with functions to parse and run
examples embedded in the docstrings of
Python modules or in plain-text files. May
also be used from the command line as:
python -m doctest
module_with_tests.py
DRY
Don�t Repeat Yourself�a software engineering
principle stating that �Every piece
of knowledge must have a single, unambiguous,
authoritative representation within a
system.� It first appeared in the book The
Pragmatic Programmer by Andy Hunt and
Dave Thomas (Addison-Wesley, 1999).
duck typing
A form of polymorphism where functions
operate on any object that implements the
appropriate methods, regardless of their
classes or explicit interface declarations.
dunder
Shortcut to pronounce the names of special
methods and attributes that are written with
leading and trailing double-underscores
(i.e., __len__ is read as �dunder len�).
dunder method
See dunder and special methods.
EAFP
Acronym standing for the quote �It�s easier
to ask forgiveness than permission,� attributed
to computer pioneer Grace Hopper,
and quoted by Pythonistas referring to dynamic
programming practices like accessing
attributes without testing first if they
exist, and then catching the exception when
that is the case. The docstring for the ha
sattr function actually says that it works
�by calling getattr(object, name) and catching
AttributeError.�
eager
An iterable object that builds all its items at
once. In Python, a list comprehension is eager.
Contrast with lazy.
fail-fast
A systems design approach recommending
that errors should be reported as early as
possible. Python adheres to this principle
more closely than most dynamic languages.
For example, there is no �undefined� value:
variables referenced before initialization
generate an error, and my_dict[k] raises an
exception if k is missing (in contrast with
JavaScript). As another example, parallel
assignment via tuple unpacking in Python
only works if every item is explicitly handled,
while Ruby silently deals with item
count mismatches by ignoring unused
items on the right side of the =, or by assigning
nil to extra variables on the left
side.
falsy
Any value x for which bool(x) returns
False; Python implicitly uses bool to evaluate
objects in Boolean contexts, such as the
expression controlling an if or while loop.
The opposite of truthy.
deep copy
718 | Python Jargon
file-like object
Used informally in the official documentation
to refer to objects implementing the file
protocol, with methods such as read,
write, close, etc. Common variants are
text files containing encoded strings with
line-oriented reading and writing, String
IO instances which are in-memory text
files, and binary files, containing unencoded
bytes. The latter may be buffered or
unbuffered. ABCs for the standard file
types are defined in the io module since
Python 2.6.
first-class function
Any function that is a first-class object in
the language (i.e., can be created at runtime,
assigned to variables, passed as an argument,
and returned as the result of another
function). Python functions are first-class
functions.
flat sequence
A sequence type that physically stores the
values of its items, and not references to
other objects. The built-in types str, bytes,
bytearray, memoryview, and array.array
are flat sequences. Contrast with list, tu
ple, and collections.deque, which are
container sequences. See container.
function
Strictly, an object resulting from evaluation
of a def block or a lambda expression. Informally,
the word function is used to describe
any callable object, such as methods
and even classes sometimes. The official
Built-in Functions list includes several
built-in classes like dict, range, and str.
Also see callable object.
genexp
Short for generator expression.
generator
An iterator built with a generator function
or a generator expression that may produce
values without necessarily iterating over a
collection; the canonical example is a generator
to produce the Fibonacci series
which, because it is infinite, would never fit
in a collection. The term is sometimes used
to describe a generator function, besides the
object that results from calling it.
generator function
A function that has the yield keyword in
its body. When invoked, a generator function
returns a generator.
generator expression
An expression enclosed in parentheses using
the same syntax of a list comprehension,
but returning a generator instead of a
list. A generator expression can be understood
as a lazy version of a list comprehension.
See lazy.
generic function
A group of functions designed to implement
the same operation in customized
ways for different object types. As of Python
3.4, the functools.singledispatch decorator
is the standard way to create generic
functions. This is known as multimethods
in other languages.
GoF book
Alias for Design Patterns: Elements of Reusable
Object-Oriented Software (Addison-
Wesley, 1995), authored by the so-called
Gang of Four (GoF): Erich Gamma, Richard
Helm, Ralph Johnson, and John Vlissides.
hashable
An object is hashable if it has both __hash__
and __eq__ methods, with the constraints
that the hash value must never change and
if a == b then hash(a) == hash(b) must
also be True. Most immutable built-in types
are hashable, but a tuple is only hashable if
every one of its items is also hashable.
higher-order function
A function that takes another function as
argument, like sorted, map, and filter, or
a function that returns a function as result,
as Python decorators do.
file-like object
Python Jargon | 719
idiom
�A manner of speaking that is natural to native
speakers of a language,� according to
the Princeton WordNet.
import time
The moment of initial execution of a module
when its code is loaded by the Python
interpreter, evaluated from top to bottom,
and compiled into bytecode. This is when
classes and functions are defined and become
live objects. This is also when decorators
are executed.
initializer
A better name for the __init__ method
(instead of constructor). Initializing the instance
received as self is the task of __in
it__. Actual instance construction is done
by the __new__ method. See constructor.
iterable
Any object from which the iter built-in
function can obtain an iterator. An iterable
object works as the source of items in for
loops, comprehensions, and tuple unpacking.
Objects implementing an __iter__
method returning an iterator are iterable.
Sequences are always iterable; other objects
implementing a __getitem__ method may
also be iterable.
iterable unpacking
A modern, more precise synonym for tuple
unpacking. See also parallel assignment.
iterator
Any object that implements the __next__
no-argument method, which returns the
next item in a series, or raises StopItera
tion when there are no more items. Python
iterators also implement the __iter__
method so they are also iterable. Classic
iterators, according to the original design
pattern, return items from a collection. A
generator is also an iterator, but it�s more
flexible. See generator.
KISS principle
The acronym stands for �Keep It Simple,
Stupid.� This calls for seeking the simplest
possible solution, with the fewest moving
parts. The phrase was coined by Kelly Johnson,
a highly accomplished aerospace engineer
who worked in the real Area 51 designing
some of the most advanced aircraft
of the 20th century.
lazy
An iterable object that produces items on
demand. In Python, generators are lazy.
Contrast eager.
listcomp
Short for list comprehension.
list comprehension
An expression enclosed in brackets that
uses the for and in keywords to build a list
by processing and filtering the elements
from one or more iterables. A list comprehension
works eagerly. See eager.
liveness
An asynchronous, threaded, or distributed
system exhibits the liveness property when
�something good eventually happens� (i.e.,
even if some expected computation is not
happening right now, it will be completed
eventually). If a system deadlocks, it has lost
its liveness.
magic method
Same as special method.
managed attribute
A public attribute managed by a descriptor
object. Although the managed attribute is
defined in the managed class, it operates like
an instance attribute (i.e., it usually has a
value per instance, held in a storage attribute).
See descriptor.
managed class
A class that uses a descriptor object to manage
one of its attributes. See descriptor.
managed instance
An instance of a managed class. See managed
attribute and descriptor.
metaclass
A class whose instances are classes. By default,
Python classes are instances of type,
for example, type(int) is the class type,
idiom
720 | Python Jargon
therefore type is a metaclass. User-defined
metaclasses can be created by subclassing
type.
metaprogramming
The practice of writing programs that use
runtime information about themselves to
change their behavior. For example, an
ORM may introspect model class declarations
to determine how to validate database
record fields and convert database types to
Python types.
monkey patching
Dynamically changing a module, class, or
function at runtime, usually to add features
or fix bugs. Because it is done in memory
and not by changing the source code, a
monkey patch only affects the currently
running instance of the program. Monkey
patches break encapsulation and tend to be
tightly coupled to the implementation details
of the patched code units, so they are
seen as temporary workarounds and not a
recommended technique for code integration.
mixin class
A class designed to be subclassed together
with one or more additional classes in a
multiple-inheritance class tree. A mixin
class should never be instantiated, and a
concrete subclass of a mixin class should
also subclass another nonmixin class.
mixin method
A concrete method implementation provided
in an ABC or in a mixin class.
mutator
See accessor.
name mangling
The automatic renaming of private attributes
from __x to _MyClass__x, performed
by the Python interpreter at runtime.
nonoverriding descriptor
A descriptor that does not implement
__set__ and therefore does not interfere
with setting of the managed attribute in the
managed instance. Consequently, if a
namesake attribute is set in the managed
instance, it will shadow the descriptor in
that instance. Also called nondata descriptor
or shadowable descriptor. Contrast with
overriding descriptor.
ORM
Object-Relational Mapper�an API that
provides access to database tables and records
as Python classes and objects, providing
method calls to perform database
operations. SQLAlchemy is a popular
standalone Python ORM; the Django and
Web2py frameworks have their own bundled
ORMs.
overriding descriptor
A descriptor that implements __set__ and
therefore intercepts and overrides attempts
at setting the managed attribute in the managed
instance. Also called data descriptor or
enforced descriptor. Contrast with nonoverriding
descriptor.
parallel assignment
Assigning to several variables from items in
an iterable, using syntax like a, b = [c, d]
�also known as destructuring assignment.
This is a common application of tuple unpacking.
parameter
Functions are declared with 0 or more �formal
parameters,� which are unbound local
variables. When the function is called, the
arguments or �actual parameters� passed
are bound to those variables. In this book,
I tried to use argument to refer to an actual
parameter passed to a function, and parameter
for a formal parameter in the function
declaration. However, that is not always
feasible because the terms parameter
and argument are used interchangeably all
over the Python docs and API. See argument.
prime (verb)
Calling next(coro) on a coroutine to advance
it to its first yield expression so that
metaprogramming
Python Jargon | 721
it becomes ready to receive values in succeeding
coro.send(value) calls.
PyPI
The Python Package Index, where more
than 60,000 packages are available, also
known as the Cheese shop (see Cheese
shop). PyPI is pronounced as �pie-P-eye� to
avoid confusion with PyPy.
PyPy
An alternative implementation of the
Python programming language using a
toolchain that compiles a subset of Python
to machine code, so the interpreter source
code is actually written in Python. PyPy also
includes a JIT to generate machine code for
user programs on the fly�like the Java VM
does. As of November 2014, PyPy is 6.8
times faster than CPython on average, according
to published benchmarks. PyPy is
pronounced as �pie-pie� to avoid confusion
with PyPI.
Pythonic
Used to praise idiomatic Python code, that
makes good use of language features to be
concise, readable, and often faster as well.
Also said of APIs that enable coding in a
way that seems natural to proficient Python
programmers. See idiom.
refcount
The reference counter that each CPython
object keeps internally in order to determine
when it can be destroyed by the
garbage collector.
referent
The object that is the target of a reference.
This term is most often used to discuss weak
references.
REPL
Read-eval-print loop, an interactive console,
like the standard python or alternatives
like ipython, bpython, and Python
Anywhere.
sequence
Generic name for any iterable data structure
with a known size (e.g., len(s)) and
allowing item access via 0-based integer indexes
(e.g., s[0]). The word sequence has
been part of the Python jargon from the
start, but only with Python 2.6 was it formalized
as an abstract class in collec
tions.abc.Sequence.
serialization
Converting an object from its in-memory
structure to a binary or text-oriented format
for storage or transmission, in a way
that allows the future reconstruction of a
clone of the object on the same system or
on a different one. The pickle module supports
serialization of arbitrary Python objects
to a binary format.
shallow copy
A copy of an object which shares references
to all the objects that are attributes of the
original object. Contrast with deep copy.
Also see aliasing.
singleton
An object that is the only existing instance
of a class�usually not by accident but because
the class is designed to prevent creation
of more than one instance. There is
also a design pattern named Singleton,
which is a recipe for coding such classes.
The None object is a singleton in Python.
slicing
Producing a subset of a sequence by using
the slice notation, e.g., my_sequence[2:6].
Slicing usually copies data to produce a new
object; in particular, my_sequence[:] creates
a shallow copy of the entire sequence.
But a memoryview object can be sliced to
produce a new memoryview that shares data
with the original object.
snake_case
The convention of writing identifiers by
joining words with the underscore character
(_)�for example, run_until_com
plete. PEP-8 calls this style �lowercase with
words separated by underscores� and recommends
it for naming functions, methods,
arguments, and variables. For packages,
PEP-8 recommends concatenating
PyPI
722 | Python Jargon
words with no separators. The Python standard
library has many examples of
snake_case identifiers, but also many examples
of identifiers with no separation between
words (e.g., getattr, classmethod,
isinstance, str.endswith, etc.). See CamelCase.
special method
A method with a special name such as
__getitem__, spelled with leading and
trailing double underscores. Almost all special
methods recognized by Python are described
in the �Data model� chapter of The
Python Language Reference, but a few that
are used only in specific contexts are documented
in other parts of the documentation.
For example, the __missing__ method
of mappings is mentioned in �4.10. Mapping
Types � dict" in The Python Standard
Library.
storage attribute
An attribute in a managed instance used to
store the value of an attribute managed by
a descriptor. See also managed attribute.
strong reference
A reference that keeps an object alive in
Python. Contrast with weak reference.
tuple unpacking
Assigning items from an iterable object to
a tuple of variables (e.g., first, second,
third == my_list). This is the usual term
used by Pythonistas, but iterable unpacking
is gaining traction.
truthy
Any value x for which bool(x) returns
True; Python implicitly uses bool to evaluate
objects in Boolean contexts, such as the
expression controlling an if or while loop.
The opposite of falsy.
type
Each specific category of program data, defined
by a set of possible values and operations
on them. Some Python types are close
to machine data types (e.g., float and
bytes) while others are extensions (e.g., int
is not limited to CPU word size, str holds
multibyte Unicode data points) and very
high-level abstractions (e.g., dict, deque,
etc.). Types may be user defined or built into
the interpreter (a �built-in� type). Before
the watershed type/class unification in
Python 2.2, types and classes were different
entities, and user-defined classes could not
extend built-in types. Since then, built-in
types and new-style classes became compatible,
and a class is an instance of type. In
Python 3 all classes are new-style classes.
See class and metaclass.
unbound method
An instance method accessed directly on a
class is not bound to an instance; therefore
it�s said to be an �unbound method.� To succeed,
a call to an unbound method must
explicitly pass an instance of the class as the
first argument. That instance will be assigned
to the self argument in the method.
See bound method.
uniform access principle
Bertrand Meyer, creator of the Eiffel Language,
wrote: �All services offered by a
module should be available through a uniform
notation, which does not betray
whether they are implemented through
storage or through computation.� Properties
and descriptors allow the implementation
of the uniform access principle in
Python. The lack of a new operator, making
function calls and object instantiation look
the same, is another form of this principle:
the caller does not need to know whether
the invoked object is a class, a function, or
any other callable.
user-defined
Almost always in the Python docs the word
user refers to you and I�programmers who
use the Python language�as opposed to
the developers who implement a Python
interpreter. So the term �user-defined class�
means a class written in Python, as opposed
to built-in classes written in C, like str.
special method
Python Jargon | 723
view
Python 3 views are special data structures
returned by the dict methods
.keys(), .values(), and .items(),
providing a dynamic view into the dict
keys and values without data duplication,
which occurs in Python 2 where those
methods return lists. All dict views are
iterable and support the in operator. In addition,
if the items referenced by the view
are all hashable, then the view also implements
the collections.abc.Set interface.
This is the case for all views returned by
the .keys() method, and for views returned
by .items() when the values are also
hashable.
virtual subclass
A class that does not inherit from a superclass
but is registered using TheSuper
Class.register(TheSubClass). See documentation
for abc.ABCMeta.register.
wart
A misfeature of the language. Andrew
Kuchling�s famous post �Python warts� has
been acknowledged by the BDFL as influential
in the decision to break backwardcompatibility
in the design of Python 3, as
most of the failings could not be fixed
otherwise. Many of Kuchling�s issues were
fixed in Python 3.
weak reference
A special kind of object reference that does
not increase the referent object reference
count. Weak references are created with
one of the functions and data structures in
the weakref module.
YAGNI
�You Ain�t Gonna Need It,� a slogan to avoid
implementing functionality that is not immediately
necessary based on assumptions
about future needs.
Zen of Python
Type import this into any Python console
since version 2.2.
view
724 | Python Jargon
We�d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.
Index
Symbols
!= operator, 384
!r conversion field, 11
# operator, 14
% operator, 11
%r placeholder, 11
() (function invocation), 371
() (parentheses), 22, 144
() call operator, 144
* operator, 10, 12, 29, 36, 148, 380
** (double asterisk), 148
*= operator, 38, 388
*args, 29
*extra, 60
+ operator, 9, 12, 36, 372, 375�380
+= operator, 38, 388
+ELLIPSIS directive, 7
+x, 373
. (attribute access), 371
.add_done_callback() method, 512
.append method, 55
.done() method, 512
.frombytes method, 251
.pop method, 55
.result() method, 512
2D vector addition, 9
404 errors (Not Found), 525
< operator, 384
<= operator, 384
== operator, 223, 244, 288, 384
> operator, 384
>= operator, 384
@ operator, 383
@abstractclassmethod, 329
@abstractmethod, 326
@abstractproperty, 329
@abstractstaticmethod, 329
@asyncio.coroutine decorator, 541, 543
@classmethod decorator, 592
@contextmanager decorator, 454
@property decorator, 604
[:] operator, 225
[] (square brackets), 6, 22, 35, 371
\ (backslash), 22
^ operator, 258, 288
_ (underscore), 28, 264
__ (double underscore), 3, 4
__add__, 38, 308, 375, 392
__bool__, 12
__builtins__, 63
__bytes__, 248
__call__, 145
__class__, 616
__delattr__, 615, 618
__delete__, 625
__del__, 235
__dict__, 63, 147, 616
__doc__, 140, 146
725
__enter__, 450
__eq__, 385
__exit__, 450
__float__, 259
__format__, 248, 253, 294, 303
__getattribute__, 618
__getattr__, 285, 618
__getitem__, 4, 70, 283, 308, 310
__get__, 625
__hash__, 87, 258, 288
__iadd__, 38, 392
__imatmul__, 383
__init__, 416, 592, 669
__int__, 259
__invert__, 372
__iter__, 404, 406, 409
__len__, 4, 14, 283
__matmul__, 383
__missing__, 72
__mro__, 334
__mul__, 380
__neg__, 372
__new__, 592, 622
__next__, 406, 411
__ne__, 385
__pos__, 372
__prepare__, 675
__radd__, 378
__repr__, 11
__rmatmul__, 383
__rmul__, 380
__ruml__, 12
__self__, 618
__setattr__, 618
__setitem__, 312
__set__, 625, 630
__slots__, 264, 616, 690
__str__, 11
__subclasshook__, 339
{} (curly brackets), 22
~ operator, 372
� operator, 372
� (ellipsis), 35, 277
A
ABC (Abstract Base Class)
advantages of, 316
appropriate use of, 308, 317, 341
as mixins, 360
declaring, 328
defining and using, 324�335
definition of term, 715
explicit interfaces using, 359
goose typing and, 341
in standard library, 321
numbers package, 323
subclass creation, 329
subclass testing, 335
subclassing process, 319
syntax details, 328
virtual subclass creation, 332
ABC language, 19, 60, 715
abs function, 10
absolute values, 10
accessor methods, 585, 621, 715
accumulating functions, 434
Adapter pattern, 356
addition
2D vector, 9
vector, 375�380
aggregate classes, 360
aiohttp package, 548, 573
algorithms
binary search, 44
C3 algorithm, 355
cryptographic, 517, 700
for hash tables, 88
RC4, 700
Timsort sorting algorithm, 62
Unicode Collation Algorithm (UCA), 126
aliasing, 221, 715
and operator, 372
anonymous functions, 143, 164, 192
arcfour.py module, 700
argument lists, 143
arguments
definition of term, 715
explicit self, 630, 652
freezing with functools.parital, 159
grabbing arbitrary excess, 29
instance, 630
keyword-only, 148
arithmetic operators, 12, 13, 156, 372
arithmetic progression generator, 420
array.array library, 251
arrays
benefits of, 48
building with generator expressions, 25
726 | Index
creating, saving and loading, 48
handling with memory views, 51
handling with NumPy, 52
vs. lists, 49
asciize function, 123
assignment
augmented, 13, 38�42, 388�392
destructuring, 721
of variables, 220
overwriting descriptors with, 645
parallel, 28, 721
to slices, 36
asynchronous operations, 552, 580
asyncio package, 538�577
APIs provided by, 57
asynchronous operations, 552
asyncio.as_completed, 555
asyncio.Future class, 545
asyncio.Task objects, 547
asyncio.wait(�), 550
avoiding event loop blocking, 560
benefits of, 581
coroutines in, 541
coroutines vs. futures, 546
development of, 538
downloader script enhancement, 554
downloading with aiohttp package, 548
TCP server, 568
time.sleep(�), 543
vs. Threading module, 539
writing asyncio servers, 567�577
yield from construct and, 546, 551
attribute access (.), 371
attribute descriptors, 625�651
attribute validation, 625�640
docstrings, 650
methods as, 646
overriding deletion, 650
overriding vs. nonoverriding, 640�646
overview of, 625
overwriting, 645
usage tips, 648
using, 626
vs. property factories, 635
attribute validation
automatic storage attribute names, 631
new descriptor types, 637
simple descriptor class, 626
attributes
assigning arbitrary, 147
definition of term, 715
deleting, 614
dynamic, 585�604
dynamic access, 284
handling of, 616
instance, 649
listing, 147
managed, 627, 720
names, 591
of user-defined functions, 147
overriding, 267
private and protected, 262, 273, 308
public, 272, 308
special, 616
storage, 627, 631, 723
validating with descriptors, 625�640
validating with properties, 604
augmented assignment, 13, 38�42, 388�392
B
backslash (\), 22
BDFL (Benevolent Dictator For Life), 716
BIF (see built-in functions)
big-endian byte ordering, 110
binary search algorithm, 44
binary sequences
building, 101
built-in types for, 99
definition of term, 716
displays used, 100
fromhex class method, 101
sharing memory, 101
str method support, 100
bisect module
inserting with bisect.insort, 47
main functions, 44
searching with bisect function, 44
bitwise operators, 13, 372
blocking I/O functions, 515, 552
Bobo HTTP micro-framework, 150, 163
BOM (Byte Order Mark), 110, 716
bool (x), 12
boolean values, custom types and, 12
bound methods, 716
built-in functions, 42, 63, 144, 616, 716
built-in methods, 144
byte strings, 716
Index | 727
bytearray type, 99
bytes (see encoding/decoding)
bytes argument, 129
bytes type, 99
bytes-like objects, 716
C
C3 algorithm, 355
caches
attribute descriptors and, 649
using WeakValueDictionary class, 237
weak references and, 236
call by reference, 245
call by sharing, 229, 245
Callable ABC, 323
callable objects, 144, 716
callable() function, 144
callbacks
callback hell, 562
vs. coroutines, 562
vs. futures, 562
CamelCase, 622, 683, 716
canonical equivalents, 117
card deck example, 4�8
cartesian products, generating lists from, 23
case folding, 119
ChainMap, 75
characters
code point identification, 98
compatibility, 118
definition of term, 98
encoding/decoding of, 98
Unicode standard for, 98
Chardet Universal Character Encoding Detector,
109
charfinder.py module, 568
Cheese Shop, 716
class decorators
drawbacks of, 661
for customizing descriptors, 659
vs. function decorators, 660
vs. metaclasses, 655
class metaprogramming, 655�679
class factory, 656
classes as objects, 677
descriptor customization, 659
exec/eval functions and, 659
import time vs. runtime, 661�666
metaclass basics, 666�673
metaclass for customizing descriptors, 673
metaclasses vs. class decorators, 655
__prepare__, 675
classes
aggregate, 360
as callable objects, 145
as objects, 677, 682
customizing at runtime (see class metaprogramming)
definition of term, 716
descriptor, 625, 635, 718
managed, 626, 720
metaclasses, 666�673, 720
MGN notation for, 628
mixin, 359, 362�366, 721
multilevel hierarchies, 367
classmethod decorator, 252
closures (see decorators and closures)
code point, 98, 716
code smell, 317, 717
code, top-level, 662
codec module, 103, 717
coding idioms, 304
collections
definition of term, 717
iterability of, 402
collections.abc module
ABCs in, 321
collections.ChainMap, 75
collections.Counter, 75
collections.defaultdict, 66, 70
collections.deque class, 55
collections.MutableSequence, 319
collections.MutableSet, 82
collections.namedtuple function, 30
collections.OrderedDict, 66, 75
collections.UserDict, 76
Mapping/MutableMapping ABCs, 64, 322
multiple inheritance in, 356
Command design pattern, 177
comparison operators, 13, 372, 384�388
compatibility characters, 118
composition, 361
compounds, 60
concrete subclasses, 320, 329, 360
concurrency
asynchronous operations, 552, 580
better approach to, 534
concurrent vs. sequential scripts, 507
728 | Index
concurrent.futures download, 509
concurrent.futures task launching, 515
error handling and, 520, 525
examples of, 505
GIL (Global Interpreter Lock) and, 515
importance of, 505
importance of futures in, 511
in other languages, 535
multiple download requests, 564
nonblocking design and, 545
progress displays, 521
smarter clients for, 576
testing concurrent clients, 521
threading vs. coroutines, 539
threading/multiprocessing alternatives, 530
using futures.as_completed, 527
vs. parallelism, 537
with asyncio package, 539�577
with Executor.map function, 517
concurrent.futures library, 505�531
behind the scenes operation of, 511
benefits of, 533
downloading, 509
futures.as_completed, 522, 527
futures.ProcessPoolExecutor, 530
introduction of, 505
launching processes with, 515
�Considered Harmful� article, 717
constructors, definition of term, 717
Container ABC, 322
container sequences, 20, 61
containers, definition of term, 717
context managers
contextlib utilities, 454
definition of term, 717
temporary contexts via with statements, 447
uses for, 454
with statement and, 450
continuous simulation, 489
copy function, 228
coroutines
benefits of, 563
computing running averages, 468
decorators for priming, 469
definition of term, 717
delaying, 543
evolution of, 464
exception handling, 472
for discrete event simulation, 489�498, 696
generators as, 439, 465
in asyncio, 541
obtaining Tasks, 547
possible states of, 465
returning values from, 475
termination of, 471
vs. callbacks, 562
vs. futures, 546
vs. generators, 463
vs. threading, 539
yield from meaning, 483�489
yield from use, 477�483
yield keyword and, 467
cosine similarity, 276
Counter, 75
cp1252 encoding, 104
cp437 encoding, 104
CPython, 235, 515, 717
CRUD (Create, Read, Update, and Delete), 717
cryptographic algorithms, 517, 700
curly brackets ({}), 22
D
data attributes (see attributes)
data descriptors (see overriding descriptors)
data model, 3�16
behavior of __len__, 14
boolean value of custom types, 12
emulating numeric types, 9
example of, 4
metaobject protocol, 16
overview of, 3
protocols and sequences, 310
special (magic) methods, 4, 16
special methods, overview of, 13
special methods, using, 8
string representation, 11
vs. object model, 15
data structures
dictionaries and sets, 63�95
sequences, 19�62
text vs. bytes, 97�136
database conversion utility, 437, 691
dbm module, 594
decimal.Decimal class, 373
Decorator pattern, 199, 214
decorators and closures, 183�215
classmethod vs. staticmethod, 252
closure example, 192
Index | 729
closure overview, 195
closures vs. anonymous functions, 192
decorator behavior, 198
decorator implementation, 196
definition of closures, 192
definition of decorators, 184, 717
dynamic scope, 213
in Python standard library, 199�205
key function of decorators, 185
nonlocal declaration, 195
parameterized decorators, 206�211
priming decorators, 543
purpose of decorators, 183
registration decorators, 187, 206
stacked decorators, 205
stacked function decorators, 329
variable scope rules, 189
deep copies, 228, 718
deepcopy function, 228
defaultdict, 66, 70
default_factory, 71
del statement
behavior of, 234
in-place modifications with, 36
object attribute deletion with, 614
delegating generators, 478
deques package, 55
descriptor classes, 625, 635
descriptor instances, 627
descriptors
customizing, 659, 673
definition of term, 718
nonoverriding, 640, 721
overriding, 640, 721
(see also attribute descriptors)
validation, 648
design patterns, 167�182
Adapter, 356
choosing the best Strategy, 175
classic Strategy, 168
Command, 177
Decorator, 199
finding strategies in modules, 176
function-oriented Strategy, 172
language-dependent relevancy of, 167, 181
destructuring assignment, 721
diacritics, 121
diamond problem, 351�356
dict, 87
dict comprehensions (dictcomp), 66
dict.get, 68
dict.setdefault, 69
dictionaries and sets, 63�95
building dictionaries, 65
creating new mapping types, 76
dictcomp (dict comprehensions), 66
immutable mappings, 77
implementation with hash tables, 85�93
mapping methods overview, 66
mapping types, 64
mappings with flexible key lookup, 70
practical consequences, 93
set theory, 79
variations of dict, 75
dir function, 146
dir([object]) function, 616
dis module, 191
discrete event simulation (DES)
processes in, 490
taxi_sim.py simulation, 490
taxi_sim.py support script, 696
threading approaches, 490
vs. continuous simulation, 489
displays, formatted, 253
Django framework, 147, 362�366
docstrings (documentation strings), 718
doctest testing package
+ELLIPSIS directive, 7
definition of term, 718
DOS (Denial of Service) attack, 507, 521, 555
double asterisk (**), 148
double underscore (dunder), 3, 4, 718
downloads
aiohttp web server, 573
asyncio TCP server, 568
concurrent vs. sequential, 505
error handling for, 520, 525
multiple requests for, 564
progress displays, 521, 527
with aiohttp package, 548
DRY (Don�t Repeat Yourself) principle, 718
duck typing
definition of term, 718
example of, 68
origin of term, 314
origins of, 303
protocols and, 279
Python support for, 247
730 | Index
dummy variables, 28
dunder methods, 4
(see also special methods)
dynamic attributes
data wrangling with, 586
exploring JSON-like data with, 588
flexible object creation, 592
invalid attribute name problem, 591
linked record retrieval, 598
overview of, 585
restructuring data with shelve module, 594
dynamic scope, 213
dynamic-typed languages, 308, 344
E
EAFP principle, 449, 718
eager objects, 718
elementwise multiplication, 380
ellipsis (�), 35, 277
else clause, 448
encoding/decoding, 97�136
basic codecs, 103
basic encoders and decoders, 103
BOM (Byte Order Mark), 110
bytes and bytearrays, 99
case folding, 119
code point conversion during, 98
default encoding setting, 114
determining byte sequence encoding, 109
diacritics, 121
dual-mode str and bytes APIs, 129
example of, 98
normalized text matching, 120
overview of, 97
problems encountered, 105�111
sorting Unicode text, 124
sorting with Unicode Collation Algorithm
(UCA), 126
str representation in RAM, 136
string and character representation, 98
structs and memory views, 102
text files, 111�117
Unicode database, 127
Unicode normalization, 117�124
enforced descriptors (see overriding descriptors)
error handling, for web downloads, 520
eval function, 659
evaluation time exercises, 662, 669
Event Loop Policy API, 581
event simulation (see discrete event simulation
(DES))
exec function, 659
Executor.map function, 517
Executor.submit function, 520
F
fail-fast design, 68, 718
falsiness, 12, 718
file-like objects, 719
filter function, 23, 142
first-class functions
anonymous functions, 143, 164
as first-class objects, 139
callable objects, 144
definition of term, 719
design patterns with, 167�182
flexible parameter handling, 148
function annotations, 154
function introspection, 146
functional programming packages, 156�161
higher-order functions, 141
retrieving parameter information, 150
treating functions like objects, 140
user-defined callable types, 145
first-class objects, 139, 681
flags2_common.py module, 521, 703
flat sequences, 20, 61, 719
flexible string representation, 136
flyweights, 174
folding functions, 434
fold_equal function, 120
for loops, 293
for/else clause, 448
format function, 253
Format Specification Mini-Language, 254, 294
Format String Syntax library, 11, 254
formatted displays, 253
404 errors (Not Found), 525
free variables, 194, 214
fromhex class method, 101
frozenset, 79
fsdecode(filename) function, 131
fsencode(filename) function, 131
function invocation (()), 371
function overloading, 203
function parameters, 229
Index | 731
functional programming
packages for, 156�161
with Python, 163
functions
accumulating, 434
anonymous, 143, 164, 192
as first-class objects, 681
assigning arbitrary attributes to, 147
blocking I/O, 515, 552
built-in, 42, 63, 144, 616, 716
built-in vs. special methods, 8
decorators and closures, 183�215, 329
definition of term, 719
design patterns with, 167�182
first-class, 139�165, 719
folding, 434
generator, 145, 441, 719
generic, 719
higher-order, 141, 159, 719
user-defined, 144
using arithmetic operators as, 156
functools module
functools.lru_cache, 200
functools.partial, 159
functools.reduce(), 289, 434
functools.singledispatch, 202, 203
functools.wraps decorator, 199
generator functions in, 424
futures
creation of, 511
definition of term, 505
in asyncio, 545
in Concurrent.futures library, 511
practical example of, 512
purpose of, 511
vs. callbacks, 562
yield from construct and, 546
futures.ProcessPoolExecutor, 530
G
garbage collection, 234, 245
gb2312 encoding, 104
generator expressions (genexps)
as alternative to map/filter, 142, 424
benefits of, 25
best use of, 419
definition of term, 719
lazy evaluation of, 396, 417
generator functions
definition of term, 145, 719
implementing iteration with, 412
in standard library, 424�433
syntax of, 441
generators
coroutines as, 465
definition of term, 719
delegating, 478
in database conversion utility, 437
subgenerators, 478
vs. coroutines, 463
vs. iterators, 401
generic functions, 202, 719
getattr(object, name[, default]) function, 617
GIL (Global Interpreter Lock)
benefits of, 534
blocking I/O and, 515
limitations of, 514
globals function, 176
Go language, 345
GoF (Gang of Four), 719
goose typing
defining/using an ABC, 324
definition of term, 315
example of, 319
explicit checks against abstract types, 381
introduction of term, 341
registering virtual subclasses with, 332
__subclasshook__ and, 339, 405
gremlins, definition of term, 107
H
hasattr(object, name) function, 617
hash tables
algorithm for, 88
definition of hashable, 65
efficiency of, 85, 91
equality and, 87
hash bit patterns, 88, 689
hash collisions, 89
implementation of dict with, 87
importance of, 63
key type limitations, 90
memory limitations, 90
practical consequences, 93
unpredictable key ordering, 91
Hashable ABC, 323
hashable objects, 257, 719
732 | Index
heapq package, 57
higher-order functions, 141, 159, 719
HTTP protocol, 548
I
idiom, definition of term, 720
if statements, 12
if/else clause, 448
immutability, 244
immutable sequences, 20, 312
implementation inheritance, 359
import time, 185, 661, 720
in-place addition, 38, 392
infix operators
@ sign and, 380
exception handling, 380
method names, 382
operands and, 376
operator overloading and, 371
special dispatching mechanism for, 377
inheritance
across languages, 368
among generic mapping types, 64
from UserDict, 76
interface vs. implementation, 359
lack of in virtual subclasses, 332
multiple (see multiple inheritance)
subclassing pitfalls, 348
vs. composition, 276, 361
initializers, definition of term, 720
insort function, 44
instance argument, 630
instance attributes, 649
instances
as callable objects, 145
attributes vs. properties, 608
construction of, 592
descriptor, 627
managed, 627, 720
MGN notation for, 628
interface inheritance, 359
interfaces
ABC (Abstract Base Class), 319�335
definition of term, 309
explicit, 359
in other languages, 345
protocols as informal, 302
Python approach to, 308
Sequence, 310
is operator, 223, 372
isinstance checks, 317
isis2json.py script, 691
iso8859_1 encoding, 104
item access/slicing ([]), 371
iter function
calling with 2 arguments, 436
steps implemented by, 404
vs. special method, 8
word-by-word iteration with, 402
Iterable ABC, 322
iterable objects, 405, 720
iterable unpacking, 28, 720
iteration
fundamental nature of, 401
implicit nature of, 7
special treatment during, 311
with generator functions, 412
Iterator pattern
arithmetic progression generator, 420
benefits of, 401
classic implementation of, 409
common errors, 411
determining iterability, 405
generator expressions and, 417
generator functions and, 412, 424�433
iter built-in function, 404, 436
iterable reducing functions, 434
iterables vs. iterators, 405
lazy implementation, 416
practical example of, 437
word-by-word iteration, 402
iterators
definition of term, 409, 720
entities supported by, 402
vs. generators, 401
vs. iterables, 405
vs. subgenerators, 479
itertools module, 423, 424
K
key argument, 42, 61
keys
handling missing, 68, 70
limitations in hash tables, 90
mapping with flexible key lookup, 70
unpredictable ordering in hash tables, 91
keyword-only arguments, 148
KISS (Keep It Simple, Stupid) principle, 720
Index | 733
L
lambda keyword, 143
latin1 encoding, 104
lazy evaluation, 396, 416
lazy objects, 720
len function, 8, 14
lexical scope, 214
limited-length representations, 277
line breaks, 22
Lisp language, 401
list comprehensions (listcomps)
as alternative to map/filter functions, 142
building sequences with, 21
definition of term, 720
generating lists from cartesian products, 23
nested lists, 37
readability and, 21
variable scope in, 22
vs. generator expressions, 25
vs. map and filter, 23
list.sort method, 42
lists
alternatives to, 48�57
argument lists, 143
building lists of lists, 37
last seen items, 55
mixed types in, 61
sorting, 42
using tuples as immutable, 32
vs. arrays, 49
vs. deque, 56
little-endian byte ordering, 110
liveness property, 720
locale settings, 124
locale.strxfrm function, 125
lru_cache decorator, 200
Lundh�s lambda Refactoring Recipe, 144
LYBYL principle, 449
M
magic methods, 4, 16
(see also special methods)
managed attributes, 627, 720
managed classes, 626, 720
managed instances, 627, 720
map function, 23, 142, 291
mapping
benefits of literal syntax for, 95
creating new mapping types, 76
immutable, 77
mapping types, 64
methods overview, 66
mutable, 237
variations of dict, 75
with flexible key lookup, 70
Mapping ABC, 322
MappingProxyType wrapper class, 77
MappingView ABC, 322
memoization, 200
memory views, 51, 102, 309
memtest.py script, 690
metaclasses
basics of, 666�673
definition of term, 720
evaluation time exercise, 669
for customizing descriptors, 673
metaobject protocol, 16
metaprogramming, 185, 655, 721
(see also class metaprogramming)
method overloading, 203
methods
accessor, 715
as attribute descriptors, 646
as callable objects, 144
bound, 716
built-in, 144
definition of term, 585
mixin, 721
special (see special methods)
unbound, 353, 723
Meyer�s Uniform Access Principle (UAP), 620
MGN (Mills & Gizmos Notation), 628
missing keys, 68
mixin classes, 359, 362�366, 721
mixin methods, 721
monkey patching, 312, 345, 645, 721
MRO (Method Resolution Order), 334, 351�356
multidimensional slicing, 35
multilevel class hierarchies, 367
multiple inheritance
best practices, 359�362
drawbacks of, 347
in Django framework, 362�366
in practice, 356
method resolution order and, 351�356
origins of, 368
subclassing pitfalls, 348
734 | Index
multiplication
commutative property of, 12
elementwise, 380
scalar, 10, 380
multiprocessing package, 57
multiprocessing, alternatives to, 530
mutability, 244
mutable mappings, 237
mutable sequences, 20, 312
MutableMapping ABC, 64
MutableSet, 82
mutators, 715
N
name mangling, 633, 721
named tuple types, 30
nested lists, 37
nested tuple unpacking, 29
NFC (Normalization Form C), 117
nfc_equal function, 120
NFKC/NFKD normalization, 118
nonlocal declaration, 195
nonlocal keyword, 183
nonoverriding descriptors, 640, 721
normalized text matching, 120
not operator, 372
NotImplemented, 378
NotImplementedError, 378
numeric types
emulating, 9
saving, 49
numerical tower, 323
NumPy
benefits of, 52
handling arrays with, 52
installing, 54
vector math with, 276
O
object composition, 361
object IDs, 223
object model, 15
(see also data model)
object publishing concept, 163
object-oriented languages, 3
objects
behaving like functions, 145
bytes-like, 716
callable, 144, 716
classes as, 677
context managers, 717
creating with __new__, 592
decorators, 717
deep copies of, 228, 718
destruction of, 245
eager, 718
file-like, 719
first-class, 139, 681
hashable, 65, 257, 719
iterable, 405, 720
iterators, 720
lazy, 720
mutable, 230, 244
Pythonic, 247�274
referent, 236, 722
serialization of, 722
shallow copies of, 225, 722
singletons, 223, 722
traceback, 236
unreachable, 234
operator module, 156
operator overloading, 371�397
augmented assignment operators, 388�392
benefits of, 371, 395
drawbacks of, 395
for scalar multiplication, 380
for vector addition, 375�380
infix operators, 382
overview of, 372
rich comparison operators, 384�388
unary operators, 372
or operator, 372
OrderedDict, 66, 75
ORM (Object-Relational Mapper), 721
os module, 130
OS processes, 490
os.walk generator function, 424
overriding descriptors, 640, 721
P
parallel assignment, 28, 721
parallel tasks, launching, 515
parallelism, vs. concurrency, 537
parameterized decorators, 206�211
parameters
avoiding mutable objects as defaults, 230
definition of term, 721
Index | 735
flexible handling of, 148
function parameters as references, 229
passing, 245
retrieving information about, 150
using mutable types, 232
parentheses ( () ), 22, 144
pickle module, 594
plain text, definition of term, 135
prime, definition of term, 721
priming decorators, 543
private attributes, 262, 273, 308
process, definition of term, 490
progress displays, 521, 527
properties
advantages of, 681
attribute validation with, 604
coding property factories, 611, 635
documentation strings and, 610
instance attributes and, 608
linked record retrieval with, 598
overview of, 606
property function, 199
protected attributes, 262
protocols
as informal interfaces, 302
benefits of, 312
definition of term, 308
duck typing and, 279
dynamic nature of, 313
implementation of, 309
implementing at runtime, 312
sequence, 280, 310, 402
vs. inheritance, 309
PSF (Python Software Foundation), 684
PUG (Python Users Group), 684
py.test library, 708
PyPI (The Python Package Index), 722
PyPy language, 722
Python Imaging Library (PIL), 102
Python standard library
ABCs in, 321
advantages of special methods, 6
array.array, 251
decorators in, 199�205
deques package, 55
generator functions in, 424�433
implementing queues, 57
Tkinter GUI toolkit, 356, 361
truth value testing rules, 12
Pythonic objects, 247�274
alternative constructor for, 251
classmethod vs. staticmethod decorators,
252
formatted displays, 253
hashability of, 257
object representations, 248
overriding class attributes, 267
private and protected attributes, 262, 308
Vector2d class example, 248
__slots__ class attribute, 264
Pythonic, definition of term, 722
PyUCA library, 126
Q
queue package, 57
R
RC4 algorithm, 700
re.findall function, 417
re.finditer function, 417
readability, 21
real-time Web, 581
record retrieval, 598
reduce function, 142, 288, 290, 434
reference counting (refcount), 235, 245, 722
reference variables, 220
references
strong, 723
weak, 236, 724
referent objects, 236, 722
register method, 332, 338
registration decorators, 187, 206
regular expressions, 129
REPL (Read-eval-print-loop), 722
repr function, 248
reprlib module, 277
requests library, 509
reverse argument, 42
reversed operators, 13
rich comparison operators, 372, 384�388
running average, computing, 192, 468
runtime, 187, 312, 661
run_in_executor method, 560
S
sanitize.py module, 121
736 | Index
scalar multiplication, 380
schedule1.py module, 595, 708
SciPy
benefits of, 52
installing, 54
vector math with, 276
scope rules, 189
security issues, with private attributes, 273, 308
self argument, 630, 652
Sequence ABC, 322
Sequence interface, 310
sequence protocol, 310, 402
sequences, 19�62
alternative to lists, 48
arrays, 48
augmented assignment with, 38�42
binary, 99, 716
building, 21�26
concatenating multiple copies of, 36
definition of term, 722
flat, 719
flat vs. container, 61
handling arrays with memory views, 51
handling with deques and queues, 55
handling with NumPy/SciPy, 52
immutable flat, 276
iterability of, 404
managing ordered, 44
mutable, 36, 312
overview of built-in, 20
slicing of, 33�36, 722
sorting, 42
tuples, 26�33
UML class diagram for, 20
serialization, 722
Set ABC, 322
set theory
element hashability, 79
history of sets, 79
mathematical set operations, 83
overview of, 79
set comparison operators, 84
set comprehensions (setcomps), 81
set literals, 80
set operations, 82
setattr(object, name, value) function, 617
setlocale function, 124
sets (see dictionaries and sets)
shallow copies, 225, 722
shared-memory sequences, 51, 102
shelve module, 594
SimPy, 490
simulation, discrete vs. continuous, 489
single underscore (_), 264
singledispatch decorator, 202
singletons, 223, 722
Sized ABC, 322
slicing
assigning to slices, 36
definition of term, 722
demonstration of, 281
features of, 33
multidimensional, 35
slice objects, 34
snake_case, 722
sorting, 42, 62
special attributes, 616
special methods
advantages of, 6
alternate terms for, 4
arithmetic operators, 12, 13
boolean value of custom types, 12
built-in types, 8
definition of term, 723
emulating numeric types, 9
example of implementation, 4, 9
for attribute handling, 617
implicit nature of, 8
naming of, 3
overview of available, 13
purpose of, 3
string representation, 11
use of, 8
square brackets ([]), 6, 22, 35, 371
stacked decorators, 205
static type checking, 343
staticmethod decorator, 252
storage attributes, 627, 631, 723
str argument, 129
str function, 8, 248
str.casefold() method, 119
str.format method, 11, 253
Strategy pattern, 168�177, 187
strings
concept of, 98
representation of, 11, 136, 248
StrKeyDict, 76
strong references, 723
Index | 737
strong vs. weak typing, 344
struct module, 102
subclasses
concrete, 320, 329
creating, 329
declaring virtual, 332
pitfalls of subclassing, 348
testing, 335
virtual, 338, 381, 724
subgenerators, 478
super() function, 287
surrogateescape codec error handler, 131
SyntaxError, 108
T
tables, 60
Tasks, obtaining in asyncio, 547
text files, encoding/decoding, 111�117
text matching, 120
thread-safe interpreters, 515
Threading module, 530, 539
time.sleep(�), 543
Timsort sorting algorithm, 62
Tkinter GUI toolkit, 356, 361
top-level code, 662
TQDM package, 521, 527
traceback objects, 236
Trollius project, 538
truthiness, 12, 723
try/else clause, 448
tuple unpacking
definition of term, 723
examples of, 27
grabbing excess items, 29
nested, 29
vs. iterable unpacking, 28
tuples
building with generator expressions, 25
named, 30
relative immutability of, 224, 240
returning reference to, 240
sequence-like behavior of, 60
used as immutable lists, 32
used as records, 26
type hints, 343
type metaclass, 667
type, definition of term, 723
t[:] operator, 240
U
unary operators, overloading of, 372
unbound methods, 353, 723
underscore (_), 28, 264
Unicode
canonical equivalents, 117
case folding, 119
characters vs. byte representations, 98
combining characters in, 117
compatibility characters, 118
database for, 127
diacritics, 121
imprecise qualifiers in, 135
normalization of, 117
normalized text matching, 120
potential encoding/decoding errors, 105
sorting text, 124
SyntaxError, 108
Unicode Collation Algorithm (UCA), 126
Unicode sandwich, 111
UnicodeDecodeError, 106
UnicodeEncodeError, 105
uniform access principle, 585, 620, 723
user-defined
callable types, 145
definition of term, 723
functions, 144
UserDict, 76
utf-16le encoding, 105
utf-8 encoding, 105
V
validation descriptors, 648
variable scope rules, 189
variables
as sticky notes, 220
dummy, 28
free, 194
reference, 220
vars([object]) function, 617
vector addition, 375�380
Vector space model, 276
virtual subclasses, 332, 338, 381, 724
W
warts, definition of term, 724
weak references, 236, 724
738 | Index
weak vs. strong typing, 344
WeakKeyDictionary class, 239
weakref module, 239
WeakValueDictionary class, 237
web downloads
aiohttp web server, 573
asyncio TCP server, 568
concurrent vs. sequential, 505
error handling for, 520, 525
multiple requests for, 564
progress displays, 521, 527
with aiohttp package, 548
while statements, 12
while/else clause, 448
with statements
behavior of, 447
context managers and, 450
usefulness of, 461
X
XOR operator (^), 258, 288
Y
yield from construct
as for loop replacement, 433
in asyncio package, 546, 551
in Trollius project, 538
main feature of, 478
meaning of, 483
significance of, 502
using, 477�483
yield keyword
coroutines and, 467
in generator functions, 413
purpose of, 401, 463
Trollius project and, 538
Z
zip function, 293
Index | 739
About the Author
Luciano Ramalho was already a web developer before the Netscape IPO in 1995, and
switched from Perl to Java to Python in 1998. Since then, he has worked on some of the
largest news portals in Brazil using Python, and taught Python web development in the
Brazilian media, banking, and government sectors. His speaking credentials include
PyCon US (2013), OSCON (2002, 2013, 2014), and 15 talks over the years at Python?
Brasil (the Brazilian PyCon) and FISL (the largest FLOSS conference in the Southern
Hemisphere). Ramalho is a fellow of the Python Software Foundation and cofounder
of Garoa Hacker Clube, the first hackerspace in Brazil. He is co-owner of
Python.pro.br, a training company.
Colophon
The animal on the cover of Fluent Python is a Namaqua sand lizard (Pedioplanis namaquensis)�
a slender creature with a long, pinkish-brown tail. It is black with four
white stripes, brown legs with white spots, and a white belly.
Active during the day, it is one of the fastest lizards. It inhabits sparsely vegetated sand
gravel flats, remaining dormant in burrows during the winter, which it digs at the base
of bushes. The Namaqua sand lizard can be found throughout Namibia, in arid savannah
and semi-desert regions. It feeds on small insects. In November, females lay between
three to five eggs.
Many of the animals on O�Reilly covers are endangered; all of them are important to
the world. To learn more about how you can help, go to animals.oreilly.com.
The cover image is from Wood�s Natural History #3. The cover fonts are URW Typewriter
and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe
Myriad Condensed; and the code font is Dalton Maag�s Ubuntu Mono.
